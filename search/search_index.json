{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DNALLM - DNA Large Language Model Toolkit","text":"<p>DNALLM is an open-source toolkit designed for large language model (LLM) applications in DNA sequence analysis and bioinformatics. It provides a comprehensive suite for model training, fine-tuning, inference, benchmarking, and evaluation, specifically tailored for DNA and genomics tasks.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Model Training &amp; Fine-tuning: Supports a variety of DNA-related tasks, including classification, regression, named entity recognition (NER), masked language modeling (MLM), and more.</li> <li>Inference &amp; Benchmarking: Enables efficient model inference, batch prediction, mutagenesis effect analysis, and multi-model benchmarking with visualization tools.</li> <li>Data Processing: Tools for dataset generation, cleaning, formatting, and adaptation to various DNA sequence formats.</li> <li>Model Management: Flexible loading and switching between different DNA language models, supporting both native mamba and transformer-compatible architectures.</li> <li>Extensibility: Modular design with utility functions and configuration modules for easy integration and secondary development.</li> <li>Protocol Support: Implements Model Context Protocol (MCP) for server/client deployment and integration into larger systems.</li> <li>Rich Examples &amp; Documentation: Includes interactive examples (marimo, notebooks) and detailed documentation to help users get started quickly.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li>Install dependencies (recommended: uv)</li> </ol> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n\ngit clone https://github.com/zhangtaolab/DNALLM.git\n\ncd DNALLM\n\nuv venv\n\nsource .venv/bin/activate\n\nuv pip install -e '.[base]'\n</code></pre> <ol> <li>Launch Jupyter Lab or Marimo for interactive development:</li> </ol> <pre><code>uv run jupyter lab\n   # or\nuv run marimo run xxx.py\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<ul> <li><code>dnallm/</code> : Core library (CLI, configuration, datasets, finetune, inference, models, tasks, utils, MCP)</li> <li><code>example/</code> : Interactive and notebook-based examples</li> <li><code>docs/</code> : Documentation</li> <li><code>scripts/</code> : Utility scripts</li> <li><code>tests/</code> : Test suite</li> </ul> <p>For more details, please refer to the README.md and contribution guidelines.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides auto-generated API documentation for all DNALLM modules.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":"<ul> <li>Configuration - Configuration management classes and loaders</li> <li>Models - Model loading and utilities</li> <li>Automatic Loading - Auto model detection and loading</li> <li>Classification Heads - Task-specific classification heads</li> <li>Tokenizers - DNA tokenization utilities</li> <li>Special Models - Specialized model implementations<ul> <li>Basenji2 - Basenji-2 model support</li> <li>Borzoi - Borzoi model support</li> <li>DNABERT2 - DNABERT-2 model support</li> <li>EVO - EVO-1/EVO-2 model support</li> <li>Enformer - Enformer model support</li> <li>GPN - GPN model support</li> <li>LucaOne - LucaOne model support</li> <li>megaDNA - megaDNA model support</li> <li>MutBERT - MutBERT model support</li> <li>Omni-DNA - Omni-DNA model support</li> <li>SPACE - SPACE model support</li> </ul> </li> <li>Data Handling - Dataset management</li> <li>Automatic Builders - Auto dataset creation</li> <li>Tasks - Task definitions and evaluation</li> <li>Metrics - Evaluation metrics</li> </ul>"},{"location":"api/#training-and-inference","title":"Training and Inference","text":"<ul> <li>Fine-tuning - Training pipeline and utilities</li> <li>Inference - Prediction engine</li> <li>Benchmark - Multi-model comparison</li> <li>Mutagenesis - In-silico mutation analysis</li> <li>Interpretation - Model interpretation</li> <li>Visualization - Plotting utilities</li> </ul>"},{"location":"api/#mcp-server","title":"MCP Server","text":"<ul> <li>Server - Main server implementation</li> <li>Config Manager - Configuration management</li> <li>Config Validators - Input validation</li> <li>Model Manager - Model lifecycle management</li> </ul>"},{"location":"api/#utilities","title":"Utilities","text":"<ul> <li>Sequence Utils - DNA sequence processing</li> <li>Logger - Logging configuration</li> <li>Support - Helper functions</li> </ul>"},{"location":"api/configuration/configs/","title":"Configs","text":""},{"location":"api/configuration/configs/#dnallm.configuration.configs","title":"dnallm.configuration.configs","text":""},{"location":"api/configuration/configs/#dnallm.configuration.configs-classes","title":"Classes","text":""},{"location":"api/configuration/configs/#dnallm.configuration.configs.BenchmarkConfig","title":"BenchmarkConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level configuration for the DNA Language Model benchmark. This class validates and structures the entire YAML configuration file, where each top-level key in the YAML corresponds to an attribute of this class.</p>"},{"location":"api/configuration/configs/#dnallm.configuration.configs.BenchmarkInfoConfig","title":"BenchmarkInfoConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the benchmark's metadata.</p>"},{"location":"api/configuration/configs/#dnallm.configuration.configs.DatasetConfig","title":"DatasetConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a single dataset used in the benchmark.</p>"},{"location":"api/configuration/configs/#dnallm.configuration.configs.EvaluationConfig","title":"EvaluationConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the evaluation phase of the benchmark.</p>"},{"location":"api/configuration/configs/#dnallm.configuration.configs.HeadConfig","title":"HeadConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the classification/regression head</p>"},{"location":"api/configuration/configs/#dnallm.configuration.configs.InferenceConfig","title":"InferenceConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for model inference</p>"},{"location":"api/configuration/configs/#dnallm.configuration.configs.LoraConfig","title":"LoraConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for LoRA (Low-Rank Adaptation)</p>"},{"location":"api/configuration/configs/#dnallm.configuration.configs.ModelConfig","title":"ModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a single model to be benchmarked.</p>"},{"location":"api/configuration/configs/#dnallm.configuration.configs.OutputConfig","title":"OutputConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for generating benchmark reports and artifacts.</p>"},{"location":"api/configuration/configs/#dnallm.configuration.configs.TaskConfig","title":"TaskConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for different fine-tuning tasks</p>"},{"location":"api/configuration/configs/#dnallm.configuration.configs.TrainingConfig","title":"TrainingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for training</p>"},{"location":"api/configuration/configs/#dnallm.configuration.configs-functions","title":"Functions","text":""},{"location":"api/configuration/configs/#dnallm.configuration.configs.load_config","title":"load_config","text":"<pre><code>load_config(config_path)\n</code></pre> <p>Load configuration from a YAML file and return a dictionary of configuration objects. Args:     config_path (str): Path to the YAML configuration file.</p>"},{"location":"api/datahandling/data/","title":"datahandling/data API","text":""},{"location":"api/datahandling/data/#dnallm.datahandling.data","title":"dnallm.datahandling.data","text":"<p>DNA Dataset handling and processing utilities.</p> <p>This module provides comprehensive tools for loading, processing, and managing DNA sequence datasets. It supports various file formats, data augmentation techniques, and statistical analysis.</p>"},{"location":"api/datahandling/data/#dnallm.datahandling.data-classes","title":"Classes","text":""},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset","title":"DNADataset","text":"<pre><code>DNADataset(ds, tokenizer=None, max_length=512)\n</code></pre> <p>A comprehensive wrapper for DNA sequence datasets with advanced processing capabilities.</p> <p>This class provides methods for loading DNA datasets from various sources (local files, Hugging Face Hub, ModelScope), encoding sequences with tokenizers, data augmentation, statistical analysis, and more.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <p>The underlying Hugging Face Dataset or DatasetDict</p> <code>tokenizer</code> <p>Tokenizer for sequence encoding</p> <code>max_length</code> <p>Maximum sequence length for tokenization</p> <code>sep</code> <code>str | None</code> <p>Separator for multi-label data</p> <code>multi_label_sep</code> <code>str | None</code> <p>Separator for multi-label sequences</p> <code>data_type</code> <code>str | None</code> <p>Type of the dataset (classification, regression, etc.)</p> <code>stats</code> <code>dict | None</code> <p>Cached dataset statistics</p> <code>stats_for_plot</code> <code>DataFrame | None</code> <p>Cached statistics for plotting</p> <p>Initialize a DNADataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset | DatasetDict</code> <p>A Hugging Face Dataset containing at least 'sequence' and 'label' fields</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase | None</code> <p>A Hugging Face tokenizer for encoding sequences</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Maximum length for tokenization</p> <code>512</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def __init__(\n    self,\n    ds: Dataset | DatasetDict,\n    tokenizer: PreTrainedTokenizerBase | None = None,\n    max_length: int = 512,\n) -&gt; None:\n    \"\"\"Initialize a DNADataset.\n\n    Args:\n        ds: A Hugging Face Dataset containing at least 'sequence' and\n            'label' fields\n        tokenizer: A Hugging Face tokenizer for encoding sequences\n        max_length: Maximum length for tokenization\n    \"\"\"\n    if ds is None:\n        raise TypeError(\"Dataset cannot be None\")\n\n    if max_length &lt;= 0:\n        raise ValueError(\"max_length must be positive\")\n\n    self.dataset = ds\n    self.tokenizer = tokenizer\n    self.max_length = max_length\n    self.sep: str | None = None\n    self.multi_label_sep: str | None = None\n    self.data_type: str | None = None\n    self.stats: dict | None = None\n    self.stats_for_plot: pd.DataFrame | None = None\n    self.__data_type__()  # Determine the data type of the dataset\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset-functions","title":"Functions","text":""},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.__data_type__","title":"__data_type__","text":"<pre><code>__data_type__()\n</code></pre> <p>Get the data type of the dataset (classification, regression, etc.).</p> <p>This method analyzes the labels to determine if the dataset is for: - classification (integer or string labels) - regression (float labels) - multi-label (multiple labels per sample) - multi-regression (multiple float values per sample)</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def __data_type__(self) -&gt; None:\n    \"\"\"Get the data type of the dataset (classification, regression, etc.).\n\n    This method analyzes the labels to determine if the dataset is for:\n    - classification (integer or string labels)\n    - regression (float labels)\n    - multi-label (multiple labels per sample)\n    - multi-regression (multiple float values per sample)\n    \"\"\"\n    labels = self._extract_labels()\n    if labels is None:\n        self.data_type = \"unknown\"\n        return\n\n    if not self._is_valid_labels(labels):\n        self.data_type = \"unknown\"\n        return\n\n    first_label = self._get_first_label(labels)\n    if first_label is None:\n        self.data_type = \"unknown\"\n        return\n\n    self.data_type = self._determine_data_type(first_label)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Get an item from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the item to retrieve</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The item at the specified index</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset is a DatasetDict</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict[str, Any]:\n    \"\"\"Get an item from the dataset.\n\n    Args:\n        idx: Index of the item to retrieve\n\n    Returns:\n        The item at the specified index\n\n    Raises:\n        ValueError: If dataset is a DatasetDict\n    \"\"\"\n    if isinstance(self.dataset, DatasetDict):\n        raise ValueError(\n            \"Dataset is a DatasetDict Object, please use \"\n            \"`DNADataset.dataset[datatype].__getitem__(idx)` \"\n            \"instead.\"\n        )\n    else:\n        return self.dataset[idx]  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the length of the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Length of the dataset or total length for DatasetDict</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\n\n    Returns:\n        Length of the dataset or total length for DatasetDict\n    \"\"\"\n    if isinstance(self.dataset, DatasetDict):\n        # Return total length across all splits\n        return sum(len(self.dataset[dt]) for dt in self.dataset)\n    else:\n        return len(self.dataset)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.augment_reverse_complement","title":"augment_reverse_complement","text":"<pre><code>augment_reverse_complement(reverse=True, complement=True)\n</code></pre> <p>Augment the dataset by adding reverse complement sequences.</p> <p>This method doubles the dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>reverse</code> <code>bool</code> <p>Whether to do reverse</p> <code>True</code> <code>complement</code> <code>bool</code> <p>Whether to do complement</p> <code>True</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def augment_reverse_complement(\n    self, reverse: bool = True, complement: bool = True\n) -&gt; None:\n    \"\"\"Augment the dataset by adding reverse complement sequences.\n\n    This method doubles the dataset size.\n\n    Args:\n        reverse: Whether to do reverse\n        complement: Whether to do complement\n    \"\"\"\n\n    def process(ds, reverse, complement):\n        # Create a dataset with an extra field for the reverse complement.\n        def add_rc(example):\n            example[\"rc_sequence\"] = reverse_complement(\n                example[\"sequence\"], reverse=reverse, complement=complement\n            )\n            return example\n\n        ds_with_rc = ds.map(add_rc, desc=\"Reverse complementary\")\n        # Build a new dataset where the reverse complement becomes the\n        # 'sequence'\n        rc_ds = ds_with_rc.map(\n            lambda ex: {\n                \"sequence\": ex[\"rc_sequence\"],\n                \"labels\": ex[\"labels\"],\n            },\n            desc=\"Data augment\",\n        )\n        ds = concatenate_datasets([ds, rc_ds])\n        ds.remove_columns([\"rc_sequence\"])\n        return ds\n\n    if isinstance(self.dataset, DatasetDict):\n        for dt in self.dataset:\n            self.dataset[dt] = process(\n                self.dataset[dt], reverse, complement\n            )\n    else:\n        self.dataset = process(self.dataset, reverse, complement)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.concat_reverse_complement","title":"concat_reverse_complement","text":"<pre><code>concat_reverse_complement(\n    reverse=True, complement=True, sep=\"\"\n)\n</code></pre> <p>Augment each sample by concatenating the sequence with its reverse complement.</p> <p>Parameters:</p> Name Type Description Default <code>reverse</code> <code>bool</code> <p>Whether to do reverse</p> <code>True</code> <code>complement</code> <code>bool</code> <p>Whether to do complement</p> <code>True</code> <code>sep</code> <code>str</code> <p>Separator between the original and reverse complement sequences</p> <code>''</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def concat_reverse_complement(\n    self, reverse: bool = True, complement: bool = True, sep: str = \"\"\n) -&gt; None:\n    \"\"\"Augment each sample by concatenating the sequence with its reverse\n    complement.\n\n    Args:\n        reverse: Whether to do reverse\n        complement: Whether to do complement\n        sep: Separator between the original and reverse complement\n            sequences\n    \"\"\"\n\n    def process(ds, reverse, complement, sep):\n        def concat_fn(example):\n            rc = reverse_complement(\n                example[\"sequence\"], reverse=reverse, complement=complement\n            )\n            example[\"sequence\"] = example[\"sequence\"] + sep + rc\n            return example\n\n        ds = ds.map(concat_fn, desc=\"Data augment\")\n        return ds\n\n    if isinstance(self.dataset, DatasetDict):\n        for dt in self.dataset:\n            self.dataset[dt] = process(\n                self.dataset[dt], reverse, complement, sep\n            )\n    else:\n        self.dataset = process(self.dataset, reverse, complement, sep)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.encode_sequences","title":"encode_sequences","text":"<pre><code>encode_sequences(\n    padding=\"max_length\",\n    padding_side=None,\n    return_tensors=\"pt\",\n    remove_unused_columns=False,\n    uppercase=False,\n    lowercase=False,\n    task=\"SequenceClassification\",\n    tokenizer=None,\n    seq_sep=None,\n)\n</code></pre> <p>Encode all sequences using the provided tokenizer.</p> <p>The dataset is mapped to include tokenized fields along with the label, making it directly usable with Hugging Face Trainer.</p> <p>Parameters:</p> Name Type Description Default <code>padding</code> <code>str</code> <p>Padding strategy for sequences. Can be 'max_length' or 'longest'. Use 'longest' to pad to the length of the longest sequence in case of memory outage</p> <code>'max_length'</code> <code>return_tensors</code> <code>str</code> <p>Returned tensor types, can be 'pt', 'tf', 'np', or 'jax'</p> <code>'pt'</code> <code>remove_unused_columns</code> <code>bool</code> <p>Whether to remove the original 'sequence' and 'label' columns</p> <code>False</code> <code>uppercase</code> <code>bool</code> <p>Whether to convert sequences to uppercase</p> <code>False</code> <code>lowercase</code> <code>bool</code> <p>Whether to convert sequences to lowercase</p> <code>False</code> <code>task</code> <code>str | None</code> <p>Task type for the tokenizer. If not provided, defaults to 'SequenceClassification'</p> <code>'SequenceClassification'</code> <code>tokenizer</code> <code>PreTrainedTokenizerBase | None</code> <p>Tokenizer to use for encoding. If not provided, uses the instance's tokenizer</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tokenizer is not provided</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def encode_sequences(\n    self,\n    padding: str = \"max_length\",\n    padding_side: str | None = None,\n    return_tensors: str = \"pt\",\n    remove_unused_columns: bool = False,\n    uppercase: bool = False,\n    lowercase: bool = False,\n    task: str | None = \"SequenceClassification\",\n    tokenizer: PreTrainedTokenizerBase | None = None,\n    seq_sep: str | None = None,\n) -&gt; None:\n    \"\"\"Encode all sequences using the provided tokenizer.\n\n    The dataset is mapped to include tokenized fields along with the\n    label, making it directly usable with Hugging Face Trainer.\n\n    Args:\n        padding: Padding strategy for sequences. Can be 'max_length' or\n            'longest'. Use 'longest' to pad to the length of the longest\n            sequence in case of memory outage\n        return_tensors: Returned tensor types, can be 'pt', 'tf', 'np', or\n            'jax'\n        remove_unused_columns: Whether to remove the original 'sequence'\n            and 'label' columns\n        uppercase: Whether to convert sequences to uppercase\n        lowercase: Whether to convert sequences to lowercase\n        task: Task type for the tokenizer. If not provided, defaults to\n            'SequenceClassification'\n        tokenizer: Tokenizer to use for encoding. If not provided, uses\n            the instance's tokenizer\n\n    Raises:\n        ValueError: If tokenizer is not provided\n    \"\"\"\n    if not self.tokenizer:\n        if tokenizer:\n            self.tokenizer = tokenizer\n        else:\n            raise ValueError(\"Tokenizer is required\")\n\n    # Get tokenizer configuration\n    tokenizer_config = self._get_tokenizer_config()\n\n    # Judge the task type and apply appropriate tokenization\n    if task is None:\n        task = \"sequenceclassification\"\n    task = task.lower()\n\n    if task == \"sequenceclassification\":\n        if padding_side is None:\n            padding_side = \"right\"\n    else:\n        if hasattr(self.tokenizer, \"padding_side\"):\n            padding_side = self.tokenizer.padding_side\n        else:\n            padding_side = \"right\"\n\n    if task in [\"tokenclassification\", \"token\", \"ner\"]:\n        self._apply_token_classification_tokenization(\n            tokenizer_config, padding, uppercase, lowercase\n        )\n    else:\n        self._apply_sequence_classification_tokenization(\n            tokenizer_config,\n            padding,\n            padding_side,\n            uppercase,\n            lowercase,\n            seq_sep,\n        )\n\n    # Post-process dataset\n    self._post_process_encoded_dataset(\n        remove_unused_columns, return_tensors\n    )\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.from_huggingface","title":"from_huggingface  <code>classmethod</code>","text":"<pre><code>from_huggingface(\n    dataset_name,\n    seq_col=\"sequence\",\n    label_col=\"labels\",\n    data_dir=None,\n    tokenizer=None,\n    max_length=512,\n)\n</code></pre> <p>Load a dataset from the Hugging Face Hub.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset</p> required <code>seq_col</code> <code>str</code> <p>Column name for the DNA sequence</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for the label</p> <code>'labels'</code> <code>data_dir</code> <code>str | None</code> <p>Data directory in a dataset</p> <code>None</code> <code>tokenizer</code> <code>PreTrainedTokenizerBase | None</code> <p>Tokenizer for sequence encoding</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length</p> <code>512</code> <p>Returns:</p> Type Description <code>DNADataset</code> <p>An instance wrapping a datasets.Dataset</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>@classmethod\ndef from_huggingface(\n    cls,\n    dataset_name: str,\n    seq_col: str = \"sequence\",\n    label_col: str = \"labels\",\n    data_dir: str | None = None,\n    tokenizer: PreTrainedTokenizerBase | None = None,\n    max_length: int = 512,\n) -&gt; \"DNADataset\":\n    \"\"\"Load a dataset from the Hugging Face Hub.\n\n    Args:\n        dataset_name: Name of the dataset\n        seq_col: Column name for the DNA sequence\n        label_col: Column name for the label\n        data_dir: Data directory in a dataset\n        tokenizer: Tokenizer for sequence encoding\n        max_length: Max token length\n\n    Returns:\n        An instance wrapping a datasets.Dataset\n    \"\"\"\n    if data_dir:\n        ds = load_dataset(dataset_name, data_dir=data_dir)\n    else:\n        ds = load_dataset(dataset_name)\n    # Rename columns if necessary\n    if seq_col != \"sequence\":\n        ds = ds.rename_column(seq_col, \"sequence\")\n    if label_col != \"labels\":\n        ds = ds.rename_column(label_col, \"labels\")\n    return cls(ds, tokenizer=tokenizer, max_length=max_length)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.from_modelscope","title":"from_modelscope  <code>classmethod</code>","text":"<pre><code>from_modelscope(\n    dataset_name,\n    seq_col=\"sequence\",\n    label_col=\"labels\",\n    data_dir=None,\n    tokenizer=None,\n    max_length=512,\n)\n</code></pre> <p>Load a dataset from the ModelScope.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset</p> required <code>seq_col</code> <code>str</code> <p>Column name for the DNA sequence</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for the label</p> <code>'labels'</code> <code>data_dir</code> <code>str | None</code> <p>Data directory in a dataset</p> <code>None</code> <code>tokenizer</code> <code>PreTrainedTokenizerBase | None</code> <p>Tokenizer for sequence encoding</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length</p> <code>512</code> <p>Returns:</p> Type Description <code>DNADataset</code> <p>An instance wrapping a datasets.Dataset</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>@classmethod\ndef from_modelscope(\n    cls,\n    dataset_name: str,\n    seq_col: str = \"sequence\",\n    label_col: str = \"labels\",\n    data_dir: str | None = None,\n    tokenizer: PreTrainedTokenizerBase | None = None,\n    max_length: int = 512,\n) -&gt; \"DNADataset\":\n    \"\"\"Load a dataset from the ModelScope.\n\n    Args:\n        dataset_name: Name of the dataset\n        seq_col: Column name for the DNA sequence\n        label_col: Column name for the label\n        data_dir: Data directory in a dataset\n        tokenizer: Tokenizer for sequence encoding\n        max_length: Max token length\n\n    Returns:\n        An instance wrapping a datasets.Dataset\n    \"\"\"\n    from modelscope import MsDataset\n\n    if data_dir:\n        ds = MsDataset.load(dataset_name, data_dir=data_dir)\n    else:\n        ds = MsDataset.load(dataset_name)\n    # Rename columns if necessary\n    if seq_col != \"sequence\":\n        ds = ds.rename_column(seq_col, \"sequence\")\n    if label_col != \"labels\":\n        ds = ds.rename_column(label_col, \"labels\")\n    return cls(ds, tokenizer=tokenizer, max_length=max_length)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.get_split_lengths","title":"get_split_lengths","text":"<pre><code>get_split_lengths()\n</code></pre> <p>Get lengths of individual splits for DatasetDict.</p> <p>Returns:</p> Type Description <code>dict | None</code> <p>Dictionary of split names and their lengths, or None for single</p> <code>dict | None</code> <p>dataset</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def get_split_lengths(self) -&gt; dict | None:\n    \"\"\"Get lengths of individual splits for DatasetDict.\n\n    Returns:\n        Dictionary of split names and their lengths, or None for single\n        dataset\n    \"\"\"\n    if isinstance(self.dataset, DatasetDict):\n        return {dt: len(self.dataset[dt]) for dt in self.dataset}\n    else:\n        return None\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.head","title":"head","text":"<pre><code>head(head=10, show=False)\n</code></pre> <p>Fetch the head n data from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>head</code> <code>int</code> <p>Number of samples to fetch</p> <code>10</code> <code>show</code> <code>bool</code> <p>Whether to print the data or return it</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[Any, Any] | None</code> <p>A dictionary containing the first n samples if show=False,</p> <code>dict[Any, Any] | None</code> <p>otherwise None</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def head(\n    self, head: int = 10, show: bool = False\n) -&gt; dict[Any, Any] | None:\n    \"\"\"Fetch the head n data from the dataset.\n\n    Args:\n        head: Number of samples to fetch\n        show: Whether to print the data or return it\n\n    Returns:\n        A dictionary containing the first n samples if show=False,\n        otherwise None\n    \"\"\"\n    import pprint\n\n    def format_convert(data):\n        df: dict[Any, Any] = {}\n        length = len(data[\"sequence\"])\n        for i in range(length):\n            df[i] = {}\n            for key in data.keys():\n                df[i][key] = data[key][i]\n        return df\n\n    dataset = self.dataset\n    if isinstance(dataset, DatasetDict):\n        df = {}\n        for dt in dataset.keys():\n            data = dataset[dt][:head]\n            if show:\n                print(f\"Dataset: {dt}\")\n                pprint.pp(format_convert(data))\n            else:\n                df[dt] = data\n        return df if not show else None\n    else:\n        data = dataset[:head]\n        if show:\n            pprint.pp(format_convert(data))\n            return None\n        else:\n            return dict(data)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.iter_batches","title":"iter_batches","text":"<pre><code>iter_batches(batch_size)\n</code></pre> <p>Generator that yields batches of examples from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Size of each batch</p> required <p>Yields:</p> Type Description <code>Any</code> <p>A batch of examples</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset is a DatasetDict</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def iter_batches(self, batch_size: int) -&gt; Any:\n    \"\"\"Generator that yields batches of examples from the dataset.\n\n    Args:\n        batch_size: Size of each batch\n\n    Yields:\n        A batch of examples\n\n    Raises:\n        ValueError: If dataset is a DatasetDict\n    \"\"\"\n    if isinstance(self.dataset, DatasetDict):\n        raise ValueError(\n            \"Dataset is a DatasetDict Object, please use \"\n            \"`DNADataset.dataset[datatype].iter_batches(batch_size)` \"\n            \"instead.\"\n        )\n    else:\n        for i in range(0, len(self.dataset), batch_size):\n            yield self.dataset[i : i + batch_size]\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.load_local_data","title":"load_local_data  <code>classmethod</code>","text":"<pre><code>load_local_data(\n    file_paths,\n    seq_col=\"sequence\",\n    label_col=\"labels\",\n    sep=None,\n    fasta_sep=\"|\",\n    multi_label_sep=None,\n    tokenizer=None,\n    max_length=512,\n)\n</code></pre> <p>Load DNA sequence datasets from one or multiple local files.</p> <p>Supports input formats: csv, tsv, json, parquet, arrow, dict, fasta, txt, pkl, pickle.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>str | list | dict</code> <p>Single dataset: Provide one file path (e.g., \"data.csv\"). Pre-split datasets: Provide a dict like {\"train\": \"train.csv\", \"test\": \"test.csv\"}</p> required <code>seq_col</code> <code>str</code> <p>Column name for DNA sequences</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for labels</p> <code>'labels'</code> <code>sep</code> <code>str | None</code> <p>Delimiter for CSV, TSV, or TXT</p> <code>None</code> <code>fasta_sep</code> <code>str</code> <p>Delimiter for FASTA files</p> <code>'|'</code> <code>multi_label_sep</code> <code>str | None</code> <p>Delimiter for multi-label sequences</p> <code>None</code> <code>tokenizer</code> <code>PreTrainedTokenizerBase | None</code> <p>A tokenizer for sequence encoding</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length</p> <code>512</code> <p>Returns:</p> Type Description <code>DNADataset</code> <p>An instance wrapping a Dataset or DatasetDict</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If file type is not supported</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>@classmethod\ndef load_local_data(\n    cls,\n    file_paths: str | list | dict,\n    seq_col: str = \"sequence\",\n    label_col: str = \"labels\",\n    sep: str | None = None,\n    fasta_sep: str = \"|\",\n    multi_label_sep: str | None = None,\n    tokenizer: PreTrainedTokenizerBase | None = None,\n    max_length: int = 512,\n) -&gt; \"DNADataset\":\n    \"\"\"Load DNA sequence datasets from one or multiple local files.\n\n    Supports input formats: csv, tsv, json, parquet, arrow, dict, fasta,\n    txt, pkl, pickle.\n\n    Args:\n        file_paths: Single dataset: Provide one file path\n            (e.g., \"data.csv\").\n            Pre-split datasets: Provide a dict like\n            {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n        seq_col: Column name for DNA sequences\n        label_col: Column name for labels\n        sep: Delimiter for CSV, TSV, or TXT\n        fasta_sep: Delimiter for FASTA files\n        multi_label_sep: Delimiter for multi-label sequences\n        tokenizer: A tokenizer for sequence encoding\n        max_length: Max token length\n\n    Returns:\n        An instance wrapping a Dataset or DatasetDict\n\n    Raises:\n        ValueError: If file type is not supported\n    \"\"\"\n    # Set separators\n    cls.sep = sep\n    cls.multi_label_sep = multi_label_sep\n    # Check if input is a list or dict\n    if isinstance(\n        file_paths, dict\n    ):  # Handling multiple files (pre-split datasets)\n        ds_dict = {}\n        for split, path in file_paths.items():\n            ds_dict[split] = cls._load_single_data(\n                path, seq_col, label_col, sep, fasta_sep, multi_label_sep\n            )\n        dataset = DatasetDict(ds_dict)\n    else:  # Handling a single file\n        dataset = cls._load_single_data(\n            file_paths, seq_col, label_col, sep, fasta_sep, multi_label_sep\n        )\n    dataset.stats = None  # Initialize stats as None\n\n    return cls(dataset, tokenizer=tokenizer, max_length=max_length)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.plot_statistics","title":"plot_statistics","text":"<pre><code>plot_statistics(save_path=None)\n</code></pre> <p>Plot statistics of the dataset.</p> <p>Includes sequence length distribution (histogram), GC content distribution (box plot) for each sequence. If dataset is a DatasetDict, length plots and GC content plots from different datasets will be concatenated into a single chart, respectively. Sequence length distribution is shown as a histogram, with min and max lengths for its' limit.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str | None</code> <p>Path to save the plots. If None, plots will be shown interactively</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If statistics have not been computed yet</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def plot_statistics(self, save_path: str | None = None) -&gt; None:\n    \"\"\"Plot statistics of the dataset.\n\n    Includes sequence length distribution (histogram),\n    GC content distribution (box plot) for each sequence.\n    If dataset is a DatasetDict, length plots and GC content plots from\n    different datasets will be concatenated into a single chart,\n    respectively. Sequence length distribution is shown as a histogram,\n    with min and max lengths for its' limit.\n\n    Args:\n        save_path: Path to save the plots. If None, plots will be shown\n            interactively\n\n    Raises:\n        ValueError: If statistics have not been computed yet\n    \"\"\"\n    import altair as alt\n\n    alt.data_transformers.enable(\"vegafusion\")\n\n    if self.stats is None or self.stats_for_plot is None:\n        raise ValueError(\n            \"Statistics have not been computed yet. Please call \"\n            \"`statistics()` method first.\"\n        )\n\n    task_type = self.data_type or \"unknown\"\n    if isinstance(self.stats_for_plot, dict):\n        df_list = {}\n        for split_name, df in self.stats_for_plot.items():\n            df_list[split_name] = df.copy()\n        final = self._create_final_chart(df_list, task_type)\n    else:\n        df = self.stats_for_plot.copy()\n        final = self._create_final_chart(df, task_type)\n    self._display_or_save_chart(final, save_path)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.process_missing_data","title":"process_missing_data","text":"<pre><code>process_missing_data()\n</code></pre> <p>Filter out samples with missing or empty sequences or labels.</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def process_missing_data(self) -&gt; None:\n    \"\"\"Filter out samples with missing or empty sequences or labels.\"\"\"\n\n    def non_missing(example):\n        return (\n            example[\"sequence\"]\n            and example[\"labels\"] is not None\n            and example[\"sequence\"].strip() != \"\"\n        )\n\n    self.dataset = self.dataset.filter(non_missing)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.random_generate","title":"random_generate","text":"<pre><code>random_generate(\n    minl,\n    maxl=0,\n    samples=1,\n    gc=(0, 1),\n    n_ratio=0.0,\n    padding_size=0,\n    seed=None,\n    label_func=None,\n    append=False,\n)\n</code></pre> <p>Replace the current dataset with randomly generated DNA sequences.</p> <p>Parameters:</p> Name Type Description Default <code>minl</code> <code>int</code> <p>Minimum length of the sequences</p> required <code>maxl</code> <code>int</code> <p>Maximum length of the sequences, default is the same as minl</p> <code>0</code> <code>samples</code> <code>int</code> <p>Number of sequences to generate, default 1</p> <code>1</code> <code>gc</code> <code>tuple</code> <p>GC content range, default (0,1)</p> <code>(0, 1)</code> <code>n_ratio</code> <code>float</code> <p>Include N base in the generated sequence, default 0.0</p> <code>0.0</code> <code>padding_size</code> <code>int</code> <p>Padding size for sequence length, default 0</p> <code>0</code> <code>seed</code> <code>int | None</code> <p>Random seed, default None</p> <code>None</code> <code>label_func</code> <code>Callable | None</code> <p>A function that generates a label from a sequence</p> <code>None</code> <code>append</code> <code>bool</code> <p>Append the random generated data to the existing dataset or use the data as a dataset</p> <code>False</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def random_generate(\n    self,\n    minl: int,\n    maxl: int = 0,\n    samples: int = 1,\n    gc: tuple = (0, 1),\n    n_ratio: float = 0.0,\n    padding_size: int = 0,\n    seed: int | None = None,\n    label_func: Callable | None = None,\n    append: bool = False,\n) -&gt; None:\n    \"\"\"Replace the current dataset with randomly generated DNA sequences.\n\n    Args:\n        minl: Minimum length of the sequences\n        maxl: Maximum length of the sequences, default is the same as minl\n        samples: Number of sequences to generate, default 1\n        gc: GC content range, default (0,1)\n        n_ratio: Include N base in the generated sequence, default 0.0\n        padding_size: Padding size for sequence length, default 0\n        seed: Random seed, default None\n        label_func: A function that generates a label from a sequence\n        append: Append the random generated data to the existing dataset\n            or use the data as a dataset\n    \"\"\"\n\n    def process(\n        minl, maxl, number, gc, n_ratio, padding_size, seed, label_func\n    ):\n        sequences = random_generate_sequences(\n            minl=minl,\n            maxl=maxl,\n            samples=number,\n            gc=gc,\n            n_ratio=n_ratio,\n            padding_size=padding_size,\n            seed=seed,\n        )\n        labels = []\n        for seq in sequences:\n            labels.append(label_func(seq) if label_func else 0)\n        random_ds = Dataset.from_dict({\n            \"sequence\": sequences,\n            \"labels\": labels,\n        })\n        return random_ds\n\n    if append:\n        if isinstance(self.dataset, DatasetDict):\n            for dt in self.dataset:\n                total_length = sum(\n                    len(self.dataset[split])\n                    for split in self.dataset.keys()\n                )\n                number = round(\n                    samples * len(self.dataset[dt]) / total_length\n                )\n                random_ds = process(\n                    minl,\n                    maxl,\n                    number,\n                    gc,\n                    n_ratio,\n                    padding_size,\n                    seed,\n                    label_func,\n                )\n                self.dataset[dt] = concatenate_datasets([\n                    self.dataset[dt],\n                    random_ds,\n                ])\n        else:\n            random_ds = process(\n                minl,\n                maxl,\n                samples,\n                gc,\n                n_ratio,\n                padding_size,\n                seed,\n                label_func,\n            )\n            self.dataset = concatenate_datasets([self.dataset, random_ds])\n    else:\n        self.dataset = process(\n            minl,\n            maxl,\n            samples,\n            gc,\n            n_ratio,\n            padding_size,\n            seed,\n            label_func,\n        )\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.raw_reverse_complement","title":"raw_reverse_complement","text":"<pre><code>raw_reverse_complement(ratio=0.5, seed=None)\n</code></pre> <p>Do reverse complement of sequences in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>Ratio of sequences to reverse complement</p> <code>0.5</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducibility</p> <code>None</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def raw_reverse_complement(\n    self, ratio: float = 0.5, seed: int | None = None\n) -&gt; None:\n    \"\"\"Do reverse complement of sequences in the dataset.\n\n    Args:\n        ratio: Ratio of sequences to reverse complement\n        seed: Random seed for reproducibility\n    \"\"\"\n\n    def process(ds, ratio, seed):\n        random.seed(seed)\n        number = len(ds[\"sequence\"])\n        idxlist = set(random.sample(range(number), int(number * ratio)))\n\n        def concat_fn(example, idx):\n            rc = reverse_complement(example[\"sequence\"])\n            if idx in idxlist:\n                example[\"sequence\"] = rc\n            return example\n\n        # Create a dataset with random reverse complement.\n        ds.map(concat_fn, with_indices=True, desc=\"Reverse complementary\")\n        return ds\n\n    if isinstance(self.dataset, DatasetDict):\n        for dt in self.dataset:\n            self.dataset[dt] = process(self.dataset[dt], ratio, seed)\n    else:\n        self.dataset = process(self.dataset, ratio, seed)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.sampling","title":"sampling","text":"<pre><code>sampling(ratio=1.0, seed=None, overwrite=False)\n</code></pre> <p>Randomly sample a fraction of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>Fraction of the dataset to sample. Default is 1.0 (no sampling)</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducibility</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the original dataset with the sampled one</p> <code>False</code> <p>Returns:</p> Type Description <code>DNADataset</code> <p>A DNADataset object with sampled data</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def sampling(\n    self,\n    ratio: float = 1.0,\n    seed: int | None = None,\n    overwrite: bool = False,\n) -&gt; \"DNADataset\":\n    \"\"\"Randomly sample a fraction of the dataset.\n\n    Args:\n        ratio: Fraction of the dataset to sample. Default is 1.0\n            (no sampling)\n        seed: Random seed for reproducibility\n        overwrite: Whether to overwrite the original dataset with the\n            sampled one\n\n    Returns:\n        A DNADataset object with sampled data\n    \"\"\"\n    if ratio &lt;= 0 or ratio &gt; 1:\n        raise ValueError(\"ratio must be between 0 and 1\")\n\n    random.seed(seed)\n    dataset = self.dataset\n    if isinstance(dataset, DatasetDict):\n        for dt in dataset.keys():\n            random_idx = random.sample(\n                range(len(dataset[dt])), int(len(dataset[dt]) * ratio)\n            )\n            dataset[dt] = dataset[dt].select(random_idx)\n    else:\n        random_idx = random.sample(\n            range(len(dataset)), int(len(dataset) * ratio)\n        )\n        dataset = dataset.select(random_idx)\n\n    if overwrite:\n        self.dataset = dataset\n        return self\n    else:\n        # Create a new DNADataset object with the sampled data\n        return DNADataset(dataset, self.tokenizer, self.max_length)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.show","title":"show","text":"<pre><code>show(head=10)\n</code></pre> <p>Display the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>head</code> <code>int</code> <p>Number of samples to display</p> <code>10</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def show(self, head: int = 10) -&gt; None:\n    \"\"\"Display the dataset.\n\n    Args:\n        head: Number of samples to display\n    \"\"\"\n    self.head(head=head, show=True)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.shuffle","title":"shuffle","text":"<pre><code>shuffle(seed=None)\n</code></pre> <p>Shuffle the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Random seed for reproducibility</p> <code>None</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def shuffle(self, seed: int | None = None) -&gt; None:\n    \"\"\"Shuffle the dataset.\n\n    Args:\n        seed: Random seed for reproducibility\n    \"\"\"\n    self.dataset.shuffle(seed=seed)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.split_data","title":"split_data","text":"<pre><code>split_data(test_size=0.2, val_size=0.1, seed=None)\n</code></pre> <p>Split the dataset into train, test, and validation sets.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>float</code> <p>Proportion of the dataset to include in the test split</p> <code>0.2</code> <code>val_size</code> <code>float</code> <p>Proportion of the dataset to include in the validation split</p> <code>0.1</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducibility</p> <code>None</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def split_data(\n    self,\n    test_size: float = 0.2,\n    val_size: float = 0.1,\n    seed: int | None = None,\n) -&gt; None:\n    \"\"\"Split the dataset into train, test, and validation sets.\n\n    Args:\n        test_size: Proportion of the dataset to include in the test\n            split\n        val_size: Proportion of the dataset to include in the validation\n            split\n        seed: Random seed for reproducibility\n    \"\"\"\n    # check if the dataset is already a DatasetDict\n    if isinstance(self.dataset, DatasetDict):\n        raise ValueError(\n            \"Dataset is already a DatasetDict, no need to split\"\n        )\n    # First, split off test+validation from training data\n    split_result = self.dataset.train_test_split(\n        test_size=test_size + val_size, seed=seed\n    )\n    train_ds = split_result[\"train\"]\n    temp_ds = split_result[\"test\"]\n    # Further split temp_ds into test and validation sets\n    if val_size &gt; 0:\n        rel_val_size = val_size / (test_size + val_size)\n        temp_split = temp_ds.train_test_split(\n            test_size=rel_val_size, seed=seed\n        )\n        test_ds = temp_split[\"train\"]\n        val_ds = temp_split[\"test\"]\n        self.dataset = DatasetDict({\n            \"train\": train_ds,\n            \"test\": test_ds,\n            \"val\": val_ds,\n        })\n    else:\n        self.dataset = DatasetDict({\"train\": train_ds, \"test\": temp_ds})\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.statistics","title":"statistics","text":"<pre><code>statistics()\n</code></pre> <p>Get statistics of the dataset.</p> <p>Includes number of samples, sequence length (min, max, average, median), label distribution, GC content (by labels), nucleotide composition (by labels).</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing statistics of the dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If statistics have not been computed yet</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def statistics(self) -&gt; dict:\n    \"\"\"Get statistics of the dataset.\n\n    Includes number of samples, sequence length (min, max, average,\n    median), label distribution, GC content (by labels), nucleotide\n    composition (by labels).\n\n    Returns:\n        A dictionary containing statistics of the dataset\n\n    Raises:\n        ValueError: If statistics have not been computed yet\n    \"\"\"\n\n    def prepare_dataframe(dataset) -&gt; pd.DataFrame:\n        \"\"\"Convert a datasets.Dataset to pandas DataFrame if needed.\n\n        If the input is already a pandas DataFrame, return a copy.\n        \"\"\"\n        # avoid importing datasets at top-level to keep dependency optional\n        try:\n            from datasets import Dataset\n\n            is_dataset = isinstance(dataset, Dataset)\n        except Exception:\n            is_dataset = False\n\n        df: pd.DataFrame\n        if is_dataset:\n            df = dataset.to_pandas()\n        elif isinstance(dataset, pd.DataFrame):\n            df = dataset.copy()\n        else:\n            raise ValueError(\n                \"prepare_dataframe expects a datasets.Dataset or \"\n                \"pandas.DataFrame\"\n            )\n        return df\n\n    def compute_basic_stats(\n        df: pd.DataFrame, seq_col: str = \"sequence\"\n    ) -&gt; dict:\n        \"\"\"Compute number of samples and sequence length statistics.\"\"\"\n        seqs = df[seq_col].fillna(\"\").astype(str)\n        lens = seqs.str.len()\n        return {\n            \"n_samples\": len(lens),\n            \"min_len\": int(lens.min()) if len(lens) &gt; 0 else 0,\n            \"max_len\": int(lens.max()) if len(lens) &gt; 0 else 0,\n            \"mean_len\": float(lens.mean())\n            if len(lens) &gt; 0\n            else float(\"nan\"),\n            \"median_len\": float(lens.median())\n            if len(lens) &gt; 0\n            else float(\"nan\"),\n        }\n\n    stats = {}\n    seq_col = \"sequence\"\n    # label_col = \"labels\"  # Not used in current implementation\n    if isinstance(self.dataset, DatasetDict):\n        self.stats_for_plot = {}\n        for split_name, split_ds in self.dataset.items():\n            df = prepare_dataframe(split_ds)\n            data_type = self.data_type\n            basic = compute_basic_stats(df, seq_col)\n            stats[split_name] = {\"data_type\": data_type, **basic}\n            self.stats_for_plot[split_name] = df\n    else:\n        df = prepare_dataframe(self.dataset)\n        data_type = self.data_type\n        basic = compute_basic_stats(df, seq_col)\n        stats[\"full\"] = {\"data_type\": data_type, **basic}\n        self.stats_for_plot = df\n\n    self.stats = stats  # Store stats in the instance for later use\n\n    return stats\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.validate_sequences","title":"validate_sequences","text":"<pre><code>validate_sequences(\n    minl=20, maxl=6000, gc=(0, 1), valid_chars=\"ACGTN\"\n)\n</code></pre> <p>Filter the dataset to keep sequences containing valid DNA bases or allowed length.</p> <p>Parameters:</p> Name Type Description Default <code>minl</code> <code>int</code> <p>Minimum length of the sequences</p> <code>20</code> <code>maxl</code> <code>int</code> <p>Maximum length of the sequences</p> <code>6000</code> <code>gc</code> <code>tuple</code> <p>GC content range between 0 and 1</p> <code>(0, 1)</code> <code>valid_chars</code> <code>str</code> <p>Allowed characters in the sequences</p> <code>'ACGTN'</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def validate_sequences(\n    self,\n    minl: int = 20,\n    maxl: int = 6000,\n    gc: tuple = (0, 1),\n    valid_chars: str = \"ACGTN\",\n) -&gt; None:\n    \"\"\"Filter the dataset to keep sequences containing valid DNA bases or\n    allowed length.\n\n    Args:\n        minl: Minimum length of the sequences\n        maxl: Maximum length of the sequences\n        gc: GC content range between 0 and 1\n        valid_chars: Allowed characters in the sequences\n    \"\"\"\n    self.dataset = self.dataset.filter(\n        lambda example: check_sequence(\n            example[\"sequence\"], minl, maxl, gc, valid_chars\n        )\n    )\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data-functions","title":"Functions","text":""},{"location":"api/datahandling/data/#dnallm.datahandling.data.load_preset_dataset","title":"load_preset_dataset","text":"<pre><code>load_preset_dataset(dataset_name, task=None)\n</code></pre> <p>Load a preset dataset from Hugging Face or ModelScope.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset</p> required <code>task</code> <code>str | None</code> <p>Task directory in a dataset</p> <code>None</code> <p>Returns:</p> Type Description <code>DNADataset</code> <p>An instance wrapping a datasets.Dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset is not found in preset datasets</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def load_preset_dataset(\n    dataset_name: str, task: str | None = None\n) -&gt; \"DNADataset\":\n    \"\"\"Load a preset dataset from Hugging Face or ModelScope.\n\n    Args:\n        dataset_name: Name of the dataset\n        task: Task directory in a dataset\n\n    Returns:\n        An instance wrapping a datasets.Dataset\n\n    Raises:\n        ValueError: If dataset is not found in preset datasets\n    \"\"\"\n    from .dataset_auto import PRESET_DATASETS\n\n    ds_info = _get_dataset_info(dataset_name, PRESET_DATASETS)\n    ds = _load_dataset_from_modelscope(ds_info, task)\n    ds = _standardize_column_names(ds)\n    return _create_dna_dataset(ds, ds_info)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.show_preset_dataset","title":"show_preset_dataset","text":"<pre><code>show_preset_dataset()\n</code></pre> <p>Show all preset datasets available in Hugging Face or ModelScope.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing dataset names and their descriptions</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def show_preset_dataset() -&gt; dict:\n    \"\"\"Show all preset datasets available in Hugging Face or ModelScope.\n\n    Returns:\n        A dictionary containing dataset names and their descriptions\n    \"\"\"\n    from .dataset_auto import PRESET_DATASETS\n\n    return PRESET_DATASETS\n</code></pre>"},{"location":"api/datahandling/dataset_auto/","title":"Automatic Dataset Builders","text":""},{"location":"api/datahandling/dataset_auto/#dnallm.datahandling.dataset_auto","title":"dnallm.datahandling.dataset_auto","text":""},{"location":"api/finetune/trainer/","title":"finetune/trainer API","text":""},{"location":"api/finetune/trainer/#dnallm.finetune.trainer","title":"dnallm.finetune.trainer","text":"<p>DNA Language Model Trainer Module.</p> <p>This module implements the training process management for DNA language models,     with the following main features:</p> <ol> <li>DNATrainer Class</li> <li>Unified management of model training, evaluation, and prediction processes<ul> <li>Support for multiple task types (    classification,    regression,    masked language modeling)</li> </ul> </li> <li>Integration of task-specific prediction heads</li> <li>Training parameter configuration</li> <li> <p>Training process monitoring and model saving</p> </li> <li> <p>Core Features:</p> </li> <li>Model initialization and device management</li> <li>Training parameter configuration</li> <li>Training loop control</li> <li>Evaluation metrics calculation</li> <li>Model saving and loading</li> <li> <p>Prediction result generation</p> </li> <li> <p>Supported Training Features:</p> </li> <li>Automatic evaluation and best model saving</li> <li>Training log recording</li> <li>Flexible batch size settings</li> <li>Learning rate and weight decay configuration</li> <li>Distributed training support</li> <li>LoRA (Low-Rank Adaptation) for efficient fine-tuning</li> </ol> Usage Example <pre><code>trainer = DNATrainer(\n    model=model,\n    config=config,\n    datasets=datasets\n)\nmetrics = trainer.train()\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer-classes","title":"Classes","text":""},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer","title":"DNATrainer","text":"<pre><code>DNATrainer(\n    model,\n    config,\n    datasets=None,\n    extra_args=None,\n    use_lora=False,\n)\n</code></pre> <p>DNA Language Model Trainer that supports multiple model types.</p> <p>This trainer class provides a unified interface for training, evaluating, and predicting with DNA language models. It supports various task types including classification, regression, and masked language modeling.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The DNA language model to be trained</p> <code>task_config</code> <p>Configuration for the specific task</p> <code>train_config</code> <p>Configuration for training parameters</p> <code>datasets</code> <p>Dataset for training and evaluation</p> <code>extra_args</code> <p>Additional training arguments</p> <code>trainer</code> <p>HuggingFace Trainer instance</p> <code>training_args</code> <p>Training arguments configuration</p> <code>data_split</code> <p>Available dataset splits</p> <p>Examples:</p> <pre><code>trainer = DNATrainer(\n    model=model,\n    config=config,\n    datasets=datasets\n)\nmetrics = trainer.train()\n</code></pre> <p>Initialize the DNA trainer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The DNA language model to be trained         config: Configuration dictionary containing task and training settings</p> required <code>datasets</code> <code>DNADataset | None</code> <p>Dataset for training and evaluation</p> <code>None</code> <code>extra_args</code> <code>dict | None</code> <p>Additional training arguments to override defaults</p> <code>None</code> <code>use_lora</code> <code>bool</code> <p>Whether to use LoRA for efficient fine-tuning</p> <code>False</code> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def __init__(\n    self,\n    model: Any,\n    config: dict,\n    datasets: DNADataset | None = None,\n    extra_args: dict | None = None,\n    use_lora: bool = False,\n):\n    \"\"\"Initialize the DNA trainer.\n\n    Args:\n        model: The DNA language model to be trained\n                    config: Configuration dictionary containing task and\n            training settings\n        datasets: Dataset for training and evaluation\n        extra_args: Additional training arguments to override defaults\n        use_lora: Whether to use LoRA for efficient fine-tuning\n    \"\"\"\n    self.model = model\n    self.task_config = config[\"task\"]\n    self.train_config = config[\"finetune\"]\n    self.datasets = datasets\n    self.extra_args = extra_args\n    self.use_lora = use_lora\n\n    # LoRA\n    if use_lora:\n        from ..models.model import peft_forward_compatiable\n\n        print(\"[Info] Applying LoRA to the model...\")\n        lora_config = LoraConfig(**config[\"lora\"].dict())\n        model = peft_forward_compatiable(model)\n        self.model = get_peft_model(model, lora_config)\n        self.model.print_trainable_parameters()\n\n    # Multi-GPU support\n    if torch.cuda.device_count() &gt; 1:\n        print(f\"[Info] Using {torch.cuda.device_count()} GPUs.\")\n        self.model = torch.nn.DataParallel(self.model)\n\n    self.set_up_trainer()\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer-functions","title":"Functions","text":""},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.compute_task_metrics","title":"compute_task_metrics","text":"<pre><code>compute_task_metrics()\n</code></pre> <p>Compute task-specific evaluation metrics.</p> <p>This method returns a callable function that computes appropriate metrics         for the specific task type (classification, regression, etc.).</p> <pre><code>    Returns:\n</code></pre> <p>Callable: A function that computes metrics for the specific task type</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def compute_task_metrics(self) -&gt; Callable[..., dict[str, float]]:\n    \"\"\"Compute task-specific evaluation metrics.\n\n    This method returns a callable function that computes appropriate\n    metrics\n            for the specific task type (classification, regression, etc.).\n\n            Returns:\n    Callable: A function that computes metrics for the specific task type\n    \"\"\"\n    return compute_metrics(self.task_config)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.customize_trainer","title":"customize_trainer","text":"<pre><code>customize_trainer(trainer_cls)\n</code></pre> <p>Customize the HuggingFace Trainer instance.</p> <p>This method allows users to replace the default Trainer instance with a custom one, enabling advanced customization of the training process.</p> <p>Parameters:</p> Name Type Description Default <code>trainer_cls</code> <code>Trainer</code> <p>A custom HuggingFace Trainer instance to replace the default one</p> required Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def customize_trainer(self, trainer_cls: Trainer):\n    \"\"\"Customize the HuggingFace Trainer instance.\n\n    This method allows users to replace the default Trainer instance\n    with a custom one, enabling advanced customization of the training\n    process.\n\n    Args:\n        trainer_cls: A custom HuggingFace Trainer instance to replace the\n            default one\n    \"\"\"\n    # Use custom loss function if provided\n    if isinstance(trainer_cls, type):\n        # Directly replace the class of the existing trainer\n        self.trainer.__class__ = trainer_cls\n    else:\n        # Replace the entire trainer instance\n        self.trainer = trainer_cls\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.evaluate","title":"evaluate","text":"<pre><code>evaluate()\n</code></pre> <p>Evaluate the model on the evaluation dataset.</p> <p>This method runs evaluation on the configured evaluation dataset and returns task-specific metrics.</p> <pre><code>    Returns:\n</code></pre> <p>Dictionary containing evaluation metrics for the current model state</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def evaluate(self) -&gt; dict[str, float]:\n    \"\"\"Evaluate the model on the evaluation dataset.\n\n    This method runs evaluation on the configured evaluation dataset and\n    returns task-specific metrics.\n\n            Returns:\n    Dictionary containing evaluation metrics for the current model state\n    \"\"\"\n    self.model.eval()\n    result: dict[str, float] = self.trainer.evaluate()\n    return result\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.infer","title":"infer","text":"<pre><code>infer()\n</code></pre> <p>Generate inference results on the test dataset.</p> <p>This method generates inference results on the test dataset if available and returns both predictions and evaluation metrics.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary containing inference results and</p> <pre><code>    metrics if test dataset exists,\notherwise empty dictionary\n</code></pre> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def infer(self) -&gt; dict[str, float]:\n    \"\"\"Generate inference results on the test dataset.\n\n    This method generates inference results on the test dataset if\n    available and returns both predictions and evaluation metrics.\n\n    Returns:\n                    Dictionary containing inference results and\n            metrics if test dataset exists,\n        otherwise empty dictionary\n    \"\"\"\n    self.model.eval()\n    result = {}\n    if \"test\" in self.data_split:\n        test_dataset = self.datasets.dataset[\"test\"]\n        result = self.trainer.predict(test_dataset)\n    return result\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.set_up_trainer","title":"set_up_trainer","text":"<pre><code>set_up_trainer()\n</code></pre> <p>Set up the HuggingFace Trainer with appropriate configurations.</p> <p>This method configures the training environment by: 1. Setting up training arguments from configuration 2. Configuring dataset splits (train/eval/test) 3. Setting up task-specific metrics computation 4. Configuring appropriate data collator for different task types 5. Initializing the HuggingFace Trainer instance</p> <p>The method automatically handles: - Dataset split detection and validation - Task-specific data collator selection - Evaluation strategy configuration - Metrics computation setup</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def set_up_trainer(self):\n    \"\"\"Set up the HuggingFace Trainer with appropriate configurations.\n\n    This method configures the training environment by:\n    1. Setting up training arguments from configuration\n    2. Configuring dataset splits (train/eval/test)\n    3. Setting up task-specific metrics computation\n    4. Configuring appropriate data collator for different task types\n    5. Initializing the HuggingFace Trainer instance\n\n    The method automatically handles:\n    - Dataset split detection and validation\n    - Task-specific data collator selection\n    - Evaluation strategy configuration\n    - Metrics computation setup\n    \"\"\"\n    # Setup training arguments\n    training_args = self.train_config.model_dump()\n    if self.extra_args:\n        training_args.update(self.extra_args)\n    self.training_args = TrainingArguments(\n        **training_args,\n    )\n    self.training_args.remove_unused_columns = (\n        False if self.use_lora else True\n    )\n    # Check if the dataset has been split\n    if isinstance(self.datasets.dataset, DatasetDict):\n        self.data_split = self.datasets.dataset.keys()\n    else:\n        self.data_split = []\n    # Get datasets\n    if \"train\" in self.data_split:\n        train_dataset = self.datasets.dataset[\"train\"]\n    else:\n        if len(self.data_split) == 0:\n            train_dataset = self.datasets.dataset\n        else:\n            raise KeyError(\"Cannot find train data.\")\n    eval_key = [x for x in self.data_split if x not in [\"train\", \"test\"]]\n    if eval_key:\n        eval_dataset = self.datasets.dataset[eval_key[0]]\n    elif \"test\" in self.data_split:\n        eval_dataset = self.datasets.dataset[\"test\"]\n    else:\n        eval_dataset = None\n        self.training_args.eval_strategy = \"no\"\n\n    # Set problem type specific settings\n    if self.task_config.task_type == \"regression\":\n        self.model.config.problem_type = \"regression\"\n    # Get compute metrics\n    if self.task_config.task_type in [\"mask\", \"generation\", \"embedding\"]:\n        compute_metrics = None\n    else:\n        compute_metrics = self.compute_task_metrics()\n    # Set data collator\n    if self.task_config.task_type == \"mask\":\n        from transformers import DataCollatorForLanguageModeling\n\n        mlm_probability = self.task_config.mlm_probability\n        mlm_probability = mlm_probability if mlm_probability else 0.15\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.datasets.tokenizer,\n            mlm=True,\n            mlm_probability=mlm_probability,\n        )\n    elif self.task_config.task_type == \"generation\":\n        from transformers import DataCollatorForLanguageModeling\n\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.datasets.tokenizer, mlm=False\n        )\n    else:\n        data_collator = None\n    # Initialize trainer\n    self.trainer = Trainer(\n        model=self.model,\n        args=self.training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n        preprocess_logits_for_metrics=preprocess_logits,\n    )\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.train","title":"train","text":"<pre><code>train(save_tokenizer=True)\n</code></pre> <p>Train the model and return training metrics.</p> <p>This method executes the training process using the configured HuggingFace Trainer, automatically saving the best model and optionally the tokenizer.</p> <pre><code>    Args:\nsave_tokenizer: Whether to save the tokenizer along with the model,\n    default True\n\n    Returns:\nDictionary containing training metrics including loss, learning\nrate, etc.\n</code></pre> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def train(self, save_tokenizer: bool = True) -&gt; dict[str, float]:\n    \"\"\"Train the model and return training metrics.\n\n    This method executes the training process using the configured\n    HuggingFace Trainer, automatically saving the best model and optionally\n    the tokenizer.\n\n            Args:\n        save_tokenizer: Whether to save the tokenizer along with the model,\n            default True\n\n            Returns:\n        Dictionary containing training metrics including loss, learning\n        rate, etc.\n    \"\"\"\n    self.model.train()\n    train_result = self.trainer.train()\n    metrics: dict[str, float] = train_result.metrics\n    # Save the model\n    self.trainer.save_model()\n    self.model.save_pretrained(\n        self.train_config.output_dir,\n        safe_serialization=self.trainer.args.save_safetensors,\n    )\n    if save_tokenizer:\n        self.datasets.tokenizer.save_pretrained(\n            self.train_config.output_dir\n        )\n    return metrics\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer-functions","title":"Functions","text":""},{"location":"api/inference/benchmark/","title":"inference/benchmark API","text":""},{"location":"api/inference/benchmark/#dnallm.inference.benchmark","title":"dnallm.inference.benchmark","text":"<p>DNA Language Model Benchmarking Module.</p> <p>This module provides comprehensive benchmarking capabilities for DNA language models, including performance evaluation, metrics calculation, and     result visualization.</p>"},{"location":"api/inference/benchmark/#dnallm.inference.benchmark-classes","title":"Classes","text":""},{"location":"api/inference/benchmark/#dnallm.inference.benchmark.Benchmark","title":"Benchmark","text":"<pre><code>Benchmark(config)\n</code></pre> <p>Class for benchmarking DNA Language Models.</p> <p>This class provides methods to evaluate the performance of different DNA     language         models on various tasks, including classification, regression, and         token classification.</p> <pre><code>Attributes:\n    config: Configuration dictionary containing task settings and\n        inference parameters\n    all_models: Dictionary mapping source names to sets of available\n        model names\n    dataset: The dataset used for benchmarking\n</code></pre> <p>Initialize the Benchmark class.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration object containing task settings and inference parameters</p> required Source code in <code>dnallm/inference/benchmark.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"Initialize the Benchmark class.\n\n    Args:\n        config: Configuration object containing task settings and\n            inference parameters\n    \"\"\"\n    self.config = config\n    self.all_models = {\n        \"huggingface\": set(\n            np.concatenate([\n                MODEL_INFO[m][\"huggingface\"] for m in MODEL_INFO\n            ]).tolist()\n        ),\n        \"modelscope\": set(\n            np.concatenate([\n                MODEL_INFO[m][\"modelscope\"] for m in MODEL_INFO\n            ]).tolist()\n        ),\n    }\n    self.datasets: list[str] = []\n    # Load preset benchmark configuration if available\n    if \"benchmark\" in config:\n        self.prepared = self.__load_from_config()\n\n    else:\n        self.prepared = None\n</code></pre>"},{"location":"api/inference/benchmark/#dnallm.inference.benchmark.Benchmark-functions","title":"Functions","text":""},{"location":"api/inference/benchmark/#dnallm.inference.benchmark.Benchmark.__load_from_config","title":"__load_from_config","text":"<pre><code>__load_from_config()\n</code></pre> <p>Load the benchmark-specific parameters from the configuration.</p> Source code in <code>dnallm/inference/benchmark.py</code> <pre><code>def __load_from_config(self):\n    \"\"\"Load the benchmark-specific parameters from the configuration.\"\"\"\n    benchmark_config = self.config[\"benchmark\"]\n    config_path = os.path.dirname(benchmark_config.config_path)\n    models = benchmark_config.models\n    model_names = {m.name: m.path for m in models}\n    sources = [m.source if hasattr(m, \"source\") else None for m in models]\n    self.config[\"inference\"] = InferenceConfig\n    for k, v in dict(benchmark_config.evaluation).items():\n        setattr(self.config[\"inference\"], k, v)\n    self.config[\"inference\"].output_dir = benchmark_config.output.path\n    if hasattr(benchmark_config, \"datasets\"):\n        datasets = benchmark_config.datasets\n        task_configs = []\n        for d in datasets:\n            self.config[\"task\"] = TaskConfig\n            self.config[\"task\"].task_type = d.task\n            self.config[\"task\"].num_labels = d.num_labels\n            self.config[\"task\"].label_names = d.label_names\n            self.config[\"task\"].threshold = d.threshold\n            data_path = (\n                d.path\n                if os.path.isfile(d.path)\n                else os.path.abspath(config_path + \"/\" + d.path)\n            )\n            self.get_dataset(data_path, d.text_column, d.label_column)\n            task_configs.append(self.config[\"task\"])\n    else:\n        datasets = []\n        task_configs = [self.config[\"task\"]]\n    metrics = benchmark_config.metrics\n    plot_format = benchmark_config.output.format\n    return {\n        \"models\": model_names,\n        \"sources\": sources,\n        \"tasks\": task_configs,\n        \"dataset\": datasets,\n        \"metrics\": metrics,\n        \"plot_format\": plot_format,\n    }\n</code></pre>"},{"location":"api/inference/benchmark/#dnallm.inference.benchmark.Benchmark.available_models","title":"available_models","text":"<pre><code>available_models(show_all=True)\n</code></pre> <p>List all available models.</p> <p>Parameters:</p> Name Type Description Default <code>show_all</code> <code>bool</code> <p>If True, show all models. If False,</p> <code>True</code> <pre><code>    show only the models that are available\n</code></pre> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>List of model names if show_all=True,</p> <pre><code>    otherwise dictionary mapping model names to tags\n</code></pre> Source code in <code>dnallm/inference/benchmark.py</code> <pre><code>def available_models(self, show_all: bool = True) -&gt; dict[str, Any]:\n    \"\"\"List all available models.\n\n    Args:\n                    show_all: If True, show all models. If False,\n            show only the models that are available\n\n    Returns:\n                    List of model names if show_all=True,\n            otherwise dictionary mapping model names to tags\n    \"\"\"\n    # Load the model information\n    if show_all:\n        return MODEL_INFO\n    else:\n        models_list = {m: MODEL_INFO[m][\"model_tags\"] for m in MODEL_INFO}\n        return models_list\n</code></pre>"},{"location":"api/inference/benchmark/#dnallm.inference.benchmark.Benchmark.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(\n    seq_or_path, seq_col=\"sequence\", label_col=\"labels\"\n)\n</code></pre> <p>Load the dataset from the specified path or list of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>seq_or_path</code> <code>str | list[str]</code> <p>Path to the sequence file or list of sequences</p> required <code>seq_col</code> <code>str</code> <p>Column name for DNA sequences, default \"sequence\"</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for labels, default \"labels\"</p> <code>'labels'</code> <p>Returns:</p> Name Type Description <code>DNADataset</code> <code>DNADataset</code> <p>Dataset object containing the sequences and labels</p> Source code in <code>dnallm/inference/benchmark.py</code> <pre><code>def get_dataset(\n    self,\n    seq_or_path: str | list[str],\n    seq_col: str = \"sequence\",\n    label_col: str = \"labels\",\n) -&gt; DNADataset:\n    \"\"\"Load the dataset from the specified path or list of sequences.\n\n    Args:\n        seq_or_path: Path to the sequence file or list of sequences\n        seq_col: Column name for DNA sequences, default \"sequence\"\n        label_col: Column name for labels, default \"labels\"\n\n    Returns:\n        DNADataset: Dataset object containing the sequences and labels\n    \"\"\"\n    inference_engine = DNAInference(\n        model=None, tokenizer=None, config=self.config\n    )\n    ds, _ = inference_engine.generate_dataset(\n        seq_or_path=seq_or_path,\n        seq_col=seq_col,\n        label_col=label_col,\n        keep_seqs=False,\n        do_encode=False,\n    )\n    self.datasets.append(ds.dataset)\n    return ds\n</code></pre>"},{"location":"api/inference/benchmark/#dnallm.inference.benchmark.Benchmark.get_inference_engine","title":"get_inference_engine","text":"<pre><code>get_inference_engine(model, tokenizer)\n</code></pre> <p>Create an inference engine object for the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The model to be used for inference</p> required <code>tokenizer</code> <code>Any</code> <p>The tokenizer to be used for encoding sequences</p> required <p>Returns:</p> Name Type Description <code>DNAInference</code> <code>DNAInference</code> <p>The inference engine object configured with the given model and tokenizer</p> Source code in <code>dnallm/inference/benchmark.py</code> <pre><code>def get_inference_engine(self, model: Any, tokenizer: Any) -&gt; DNAInference:\n    \"\"\"Create an inference engine object for the model.\n\n    Args:\n        model: The model to be used for inference\n        tokenizer: The tokenizer to be used for encoding sequences\n\n    Returns:\n        DNAInference: The inference engine object configured with the given\n            model and tokenizer\n    \"\"\"\n\n    inference_engine = DNAInference(\n        model=model, tokenizer=tokenizer, config=self.config\n    )\n    return inference_engine\n</code></pre>"},{"location":"api/inference/benchmark/#dnallm.inference.benchmark.Benchmark.plot","title":"plot","text":"<pre><code>plot(\n    metrics,\n    show_score=True,\n    save_path=None,\n    separate=False,\n    dataset=0,\n)\n</code></pre> <p>Plot the benchmark results.</p> <p>This method generates various types of plots based on the task type: - For classification tasks: bar charts for metrics and ROC curves - For regression tasks: bar charts for metrics and scatter plots - For token classification: bar charts for metrics only</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict</code> <p>Dictionary containing model metrics</p> required <code>show_score</code> <code>bool</code> <p>Whether to show the score on the plot         save_path: Path to save the plot. If None, plots will be shown interactively</p> <code>True</code> <code>separate</code> <code>bool</code> <p>Whether to save the plots separately</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>dnallm/inference/benchmark.py</code> <pre><code>def plot(\n    self,\n    metrics: dict,\n    show_score: bool = True,\n    save_path: str | None = None,\n    separate: bool = False,\n    dataset: int | str = 0,\n) -&gt; None:\n    \"\"\"Plot the benchmark results.\n\n    This method generates various types of plots based on the task type:\n    - For classification tasks: bar charts for metrics and ROC curves\n    - For regression tasks: bar charts for metrics and scatter plots\n    - For token classification: bar charts for metrics only\n\n    Args:\n        metrics: Dictionary containing model metrics\n        show_score: Whether to show the score on the plot\n                    save_path: Path to save the plot. If None,\n            plots will be shown interactively\n        separate: Whether to save the plots separately\n\n    Returns:\n        None\n    \"\"\"\n    task_config = self.config[\"task\"]\n    task_type = task_config.task_type\n    # Select dataset if multiple datasets are provided\n    if isinstance(dataset, int):\n        metrics = metrics[list(metrics.keys())[dataset]]\n    elif isinstance(dataset, str):\n        if dataset in metrics:\n            metrics = metrics[dataset]\n        else:\n            raise ValueError(\n                f\"Dataset name '{dataset}' not found in metrics.\"\n            )\n    else:\n        metrics = metrics[next(iter(metrics.keys()))]\n    if task_type in [\"binary\", \"multiclass\", \"multilabel\", \"token\"]:\n        # Prepare data for plotting\n        bars_data, curves_data = prepare_data(metrics, task_type=task_type)\n        if save_path:\n            suffix = os.path.splitext(save_path)[-1]\n            if suffix:\n                bar_chart = save_path.replace(suffix, \"_metrics\" + suffix)\n                line_chart = save_path.replace(suffix, \"_roc\" + suffix)\n            else:\n                bar_chart = os.path.join(save_path, \"metrics.pdf\")\n                line_chart = os.path.join(save_path, \"roc.pdf\")\n        else:\n            bar_chart = None\n            line_chart = None\n        # Plot bar charts\n        pbar = plot_bars(\n            bars_data,\n            show_score=show_score,\n            save_path=bar_chart,\n            separate=separate,\n        )\n        # Plot curve charts\n        if task_type == \"token\":\n            pline = None\n        else:\n            pline = plot_curve(\n                curves_data,\n                show_score=show_score,\n                save_path=line_chart,\n                separate=separate,\n            )\n        return pbar, pline\n    elif task_type == \"regression\":\n        # Prepare data for plotting\n        bars_data, scatter_data = prepare_data(\n            metrics, task_type=task_type\n        )\n        if save_path:\n            suffix = os.path.splitext(save_path)[-1]\n            if suffix:\n                bar_chart = save_path.replace(suffix, \"_metrics\" + suffix)\n                scatter_plot = save_path.replace(\n                    suffix, \"_scatter\" + suffix\n                )\n            else:\n                bar_chart = os.path.join(save_path, \"metrics.pdf\")\n                scatter_plot = os.path.join(save_path, \"scatter.pdf\")\n        else:\n            bar_chart = None\n            scatter_plot = None\n        # Plot bar charts\n        pbar = plot_bars(\n            bars_data,\n            show_score=show_score,\n            save_path=bar_chart,\n            separate=separate,\n        )\n        # Plot scatter plots\n        pdot = plot_scatter(\n            scatter_data,\n            show_score=show_score,\n            save_path=scatter_plot,\n            separate=separate,\n        )\n        return pbar, pdot\n</code></pre>"},{"location":"api/inference/benchmark/#dnallm.inference.benchmark.Benchmark.run","title":"run","text":"<pre><code>run(\n    model_names=None,\n    source=\"huggingface\",\n    use_mirror=False,\n    save_preds=False,\n    save_scores=True,\n)\n</code></pre> <p>Perform the benchmark evaluation on multiple models.</p> <p>This method loads each model, runs predictions on the dataset, calculates metrics, and optionally saves the results.</p> <p>Parameters:</p> Name Type Description Default <code>model_names</code> <code>list[str] | dict | None</code> <p>List of model names or a dictionary mapping model names to paths</p> <code>None</code> <code>source</code> <code>str</code> <p>Source of the models ('local', 'huggingface', 'modelscope')</p> <code>'huggingface'</code> <code>use_mirror</code> <code>bool</code> <p>Whether to use a mirror for downloading models</p> <code>False</code> <code>save_preds</code> <code>bool</code> <p>Whether to save the predictions</p> <code>False</code> <code>save_scores</code> <code>bool</code> <p>Whether to save the metrics</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary containing benchmark results for each dataset and model</p> <p>Raises:</p> Type Description <code>NameError</code> <p>If model cannot be found in either the given source or local storage</p> Source code in <code>dnallm/inference/benchmark.py</code> <pre><code>def run(\n    self,\n    model_names: list[str] | dict | None = None,\n    source: str = \"huggingface\",\n    use_mirror: bool = False,\n    save_preds: bool = False,\n    save_scores: bool = True,\n) -&gt; dict[str, Any]:\n    \"\"\"Perform the benchmark evaluation on multiple models.\n\n    This method loads each model, runs predictions on the dataset,\n    calculates metrics, and optionally saves the results.\n\n    Args:\n        model_names: List of model names or\n            a dictionary mapping model names to paths\n        source: Source of the models ('local', 'huggingface', 'modelscope')\n        use_mirror: Whether to use a mirror for downloading models\n        save_preds: Whether to save the predictions\n        save_scores: Whether to save the metrics\n\n    Returns:\n        dict[str, Any]: Dictionary containing benchmark results for each\n            dataset and model\n\n    Raises:\n        NameError: If model cannot be found in either the given source or\n            local storage\n    \"\"\"\n    all_results: dict[str, Any] = {}\n    selected_results: dict[str, Any] = {}\n    metrics_save: dict[str, Any] = {}\n    if self.prepared:\n        task_configs = self.prepared[\"tasks\"]\n        pred_config = self.config[\"inference\"]\n        # Get datasets and model names from preset config\n        dataset_names = [d.name for d in self.prepared[\"dataset\"]]\n        model_names = self.prepared[\"models\"]\n        sources = [s if s else source for s in self.prepared[\"sources\"]]\n        selected_metrics = self.prepared[\"metrics\"]\n    else:\n        # Get configurations from the provided config\n        task_configs = [self.config[\"task\"]]\n        pred_config = self.config[\"inference\"]\n        # Get dataset and model names manually\n        dataset_names = [\"custom\"]\n        sources = [source] * len(model_names)\n        selected_metrics = []\n        # Load the dataset\n    for di, dname in enumerate(dataset_names):\n        print(\"Dataset name:\", dname)\n        all_results[dname] = {}\n        selected_results[dname] = {}\n        metrics_save[dname] = {}\n        labels = self.datasets[di][\"labels\"]\n        task_config = (\n            task_configs[di] if di &lt; len(task_configs) else task_configs[0]\n        )\n        for mi, model_name in enumerate(model_names):\n            print(\"Model name:\", model_name)\n            # Check if the model name is provided as a string or a\n            # dictionary\n            if isinstance(model_names, dict):\n                model_path = model_names[model_name]\n            else:\n                model_path = model_name\n                model_name = os.path.basename(model_name)\n            # Check if the model is local or remote\n            source = sources[mi]\n            # Load the model and tokenizer\n            if source == \"local\":\n                model, tokenizer = load_model_and_tokenizer(\n                    model_path, task_config=task_config\n                )\n            else:\n                # Check if the model is available in the model library\n                # if model_path not in self.all_models[source]:\n                # print(f\"Model \\'{model_path}\\' not found in our available\n                # models list.\")\n                #     continue\n                # try:\n                model, tokenizer = load_model_and_tokenizer(\n                    model_path,\n                    task_config=task_config,\n                    source=source,\n                    use_mirror=use_mirror,\n                )\n                # except Exception:\n                #     if os.path.exists(model_path):\n                #         model, tokenizer = load_model_and_tokenizer(\n                #             model_path, task_config=task_config\n                #         )\n                #     else:\n                #         raise NameError(\n                # \"Cannot find model in either the given source or local.\"\n                #         ) from None\n            if hasattr(tokenizer, \"model_max_length\"):\n                max_length = min(\n                    tokenizer.model_max_length, pred_config.max_length\n                )\n            else:\n                max_length = pred_config.max_length\n            dataset = DNADataset(\n                self.datasets[di],\n                tokenizer=tokenizer,\n                max_length=max_length,\n            )\n            dataset.encode_sequences(remove_unused_columns=True)\n            dataloader: DataLoader = DataLoader(\n                dataset,\n                batch_size=pred_config.batch_size,\n                num_workers=pred_config.num_workers,\n            )\n            inference_engine = self.get_inference_engine(model, tokenizer)\n            # Perform the prediction\n            logits, _, _ = inference_engine.batch_infer(\n                dataloader, do_pred=False\n            )\n            if len(labels) == len(logits):\n                metrics = inference_engine.calculate_metrics(\n                    logits, labels, plot=True\n                )\n                all_results[dname][model_name] = metrics\n                # keep selected metrics\n                if selected_metrics:\n                    selected_results[dname][model_name] = {}\n                    if \"all\" in selected_metrics:\n                        selected_results[dname][model_name] = metrics\n                    else:\n                        for metric in selected_metrics:\n                            selected_results[dname][model_name][metric] = (\n                                all_results[dname][model_name][metric]\n                            )\n                else:\n                    selected_results[dname][model_name] = metrics\n                # keep all metrics if save_scores is True\n                if save_scores:\n                    metrics2 = dict(metrics)\n                    metrics2.pop(\"curve\", None)\n                    metrics2.pop(\"scatter\", None)\n                    metrics_save[dname][model_name] = metrics2\n    # Save the metrics\n    if save_scores and pred_config.output_dir:\n        save_metrics(metrics_save, Path(pred_config.output_dir))\n    if self.prepared:\n        return selected_results\n    else:\n        return all_results\n</code></pre>"},{"location":"api/inference/benchmark/#dnallm.inference.benchmark-functions","title":"Functions","text":""},{"location":"api/inference/inference/","title":"inference/inference API","text":""},{"location":"api/inference/inference/#dnallm.inference.inference","title":"dnallm.inference.inference","text":"<p>DNA Language Model Inference Module.</p> <p>This module implements core model inference functionality, including:</p> <ol> <li>DNAInference class</li> <li>Model loading and initialization</li> <li>Batch sequence inference</li> <li>Result post-processing</li> <li>Device management</li> <li> <p>Half-precision inference support</p> </li> <li> <p>Core features:</p> </li> <li>Model state management</li> <li>Batch inference</li> <li>Result merging</li> <li>Inference result saving</li> <li> <p>Memory optimization</p> </li> <li> <p>Inference optimization:</p> </li> <li>Batch parallelization</li> <li>GPU acceleration</li> <li>Half-precision computation</li> <li>Memory efficiency optimization</li> </ol> Example <pre><code>inference_engine = DNAInference(\n    model=model,\n    tokenizer=tokenizer,\n    config=config\n)\nresults = inference_engine.infer(sequences)\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference-classes","title":"Classes","text":""},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference","title":"DNAInference","text":"<pre><code>DNAInference(model, tokenizer, config, lora_adapter=None)\n</code></pre> <p>DNA sequence inference engine using fine-tuned models.</p> <p>This class provides comprehensive functionality for performing inference using DNA language models. It handles model loading, inference, result processing, and various output formats including hidden states and attention weights for model interpretability.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>Fine-tuned model instance for inference</p> <code>tokenizer</code> <p>Tokenizer for encoding DNA sequences</p> <code>task_config</code> <p>Configuration object containing task settings</p> <code>pred_config</code> <p>Configuration object containing inference parameters</p> <code>device</code> <p>Device (CPU/GPU/MPS) for model inference</p> <code>sequences</code> <code>list[str]</code> <p>List of input sequences</p> <code>labels</code> <code>list[Any]</code> <p>List of true labels (if available)</p> <code>embeddings</code> <code>list[Any]</code> <p>Dictionary containing hidden states and attention weights</p> <p>Initialize the inference engine.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Fine-tuned model instance for inference</p> required <code>tokenizer</code> <code>Any</code> <p>Tokenizer for encoding DNA sequences</p> required <code>config</code> <code>dict</code> <p>Configuration dictionary containing task settings                    and inference parameters</p> required <code>lora_adapter</code> <code>str | None</code> <p>Optional path to LoRA adapter for model</p> <code>None</code> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def __init__(\n    self,\n    model: Any,\n    tokenizer: Any,\n    config: dict,\n    lora_adapter: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the inference engine.\n\n    Args:\n        model: Fine-tuned model instance for inference\n        tokenizer: Tokenizer for encoding DNA sequences\n        config: Configuration dictionary containing task settings\\\n                and inference parameters\n        lora_adapter: Optional path to LoRA adapter for model\n    \"\"\"\n\n    default_forward_args = {\n        \"input_ids\",\n        \"attention_mask\",\n        \"token_type_ids\",\n        \"position_ids\",\n        \"inputs_embeds\",\n        \"labels\",\n        \"output_attentions\",\n        \"output_hidden_states\",\n        \"return_dict\",\n        \"past_key_values\",\n        \"use_cache\",\n    }\n\n    if lora_adapter:\n        from peft import PeftModel\n        from ..models.model import peft_forward_compatiable\n\n        if os.path.isdir(lora_adapter):\n            source = \"local\"\n        else:\n            source = (\n                model.source if hasattr(model, \"source\") else \"huggingface\"\n            )\n        try:\n            lora_adapter_path, _ = _get_model_path_and_imports(\n                lora_adapter, source\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Failed to load LoRA adapter from {lora_adapter}: {e}\"\n            ) from e\n\n        if model is not None:\n            self.accepted_args = self._get_accepted_forward_args(model)\n        else:\n            self.accepted_args = set(default_forward_args)\n\n        model = peft_forward_compatiable(model)\n        self.model = PeftModel.from_pretrained(model, lora_adapter_path)\n        logger.info(f\"Loaded LoRA adapter from {lora_adapter}\")\n    else:\n        self.model = model\n        if model is not None:\n            if \"CustomEvo\" in str(type(self.model)):\n                self.accepted_args = self._get_accepted_forward_args(\n                    model.model\n                )\n            else:\n                self.accepted_args = self._get_accepted_forward_args(model)\n        else:\n            self.accepted_args = set(default_forward_args)\n    self.tokenizer = tokenizer\n    self.pad_id = self._get_pad_id()\n    self.task_config = config[\"task\"]\n    self.pred_config = config[\"inference\"]\n    self.device = self._get_device()\n    if model:\n        if \"CustomEvo\" in str(type(self.model)):\n            self.model.model.to(self.device)\n        else:\n            self.model.to(self.device)\n        # mamba only support cuda and cpu, and only allow fp32\n        if \"mamba\" in str(type(self.model)).lower():\n            if self.device.type != \"cuda\":\n                self.device = torch.device(\"cpu\")\n            if self.pred_config.use_fp16:\n                self.pred_config.use_fp16 = False\n        logger.info(f\"Using device: {self.device}\")\n    self.sequences: list[str] = []\n    self.labels: list[Any] = []\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference-functions","title":"Functions","text":""},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.batch_infer","title":"batch_infer","text":"<pre><code>batch_infer(\n    dataloader,\n    do_pred=True,\n    output_hidden_states=False,\n    output_attentions=False,\n    reduce_hidden_states=False,\n    reduce_strategy=\"mean\",\n    return_dict=True,\n)\n</code></pre> <p>Perform batch inference on sequences.</p> <p>This method runs inference on batches of sequences and optionally extracts hidden states and attention weights for model interpretability.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>DataLoader object containing sequences for inference</p> required <code>do_pred</code> <code>bool</code> <p>Whether to convert logits to predictions</p> <code>True</code> <code>output_hidden_states</code> <code>bool</code> <p>Whether to output hidden states from all layers</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>Whether to output attention weights from all layers</p> <code>False</code> <code>reduce_hidden_states</code> <code>bool</code> <p>Whether to average hidden states across layers</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Tensor, dict | None, dict]</code> <p>Tuple containing: - torch.Tensor: All logits from the model - Optional[Dict]: Predictions dictionary if do_pred=True,   otherwise None - Dict: Embeddings dictionary containing hidden states   and/or attention weights</p> Note <p>Setting output_hidden_states or output_attentions to True will consume significant memory, especially for long sequences or large models.</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>@torch.inference_mode()\ndef batch_infer(\n    self,\n    dataloader: DataLoader,\n    do_pred: bool = True,\n    output_hidden_states: bool = False,\n    output_attentions: bool = False,\n    reduce_hidden_states: bool = False,\n    reduce_strategy: str | int = \"mean\",\n    return_dict: bool = True,\n) -&gt; tuple[torch.Tensor, dict | None, dict]:\n    \"\"\"Perform batch inference on sequences.\n\n    This method runs inference on batches of sequences and optionally\n    extracts hidden states and attention weights for model\n    interpretability.\n\n    Args:\n        dataloader: DataLoader object containing sequences for inference\n        do_pred: Whether to convert logits to predictions\n        output_hidden_states: Whether to output hidden states from\n            all layers\n        output_attentions: Whether to output attention weights from\n            all layers\n        reduce_hidden_states: Whether to average hidden states across\n            layers\n\n    Returns:\n        Tuple containing:\n            - torch.Tensor: All logits from the model\n            - Optional[Dict]: Predictions dictionary if do_pred=True,\n              otherwise None\n            - Dict: Embeddings dictionary containing hidden states\n              and/or attention weights\n\n    Note:\n        Setting output_hidden_states or output_attentions to True will\n        consume significant memory, especially for long sequences or\n        large models.\n    \"\"\"\n    # Set model to evaluation mode\n    self.model.eval()\n    all_logits = []\n\n    # Setup configurations for outputs\n    output_hidden_states, hidden_embeddings, params = (\n        self._setup_hidden_states_config(output_hidden_states)\n    )\n    output_attentions, attention_embeddings = (\n        self._setup_attentions_config(output_attentions, params)\n    )\n\n    # Combine embeddings dictionaries\n    embeddings = {**hidden_embeddings, **attention_embeddings}\n\n    # Check model precision settings\n    if self.pred_config.use_fp16:\n        dtype = torch.float16\n    elif self.pred_config.use_bf16:\n        dtype = torch.bfloat16\n    else:\n        dtype = torch.float32\n\n    # Iterate over batches\n    for batch in tqdm(dataloader, desc=\"Inferring\"):\n        inputs = {\n            k: v.to(self.device) if hasattr(v, \"to\") else v\n            for k, v in batch.items()\n        }\n        # Add output flags if supported\n        # In case model config does not recognize these args\n        if output_attentions:\n            if \"output_attentions\" in self.accepted_args:\n                inputs[\"output_attentions\"] = True\n            elif \"**kwargs\" in self.accepted_args:\n                inputs[\"output_attentions\"] = True\n        if output_hidden_states:\n            if \"output_hidden_states\" in self.accepted_args:\n                inputs[\"output_hidden_states\"] = True\n            elif \"**kwargs\" in self.accepted_args:\n                inputs[\"output_hidden_states\"] = True\n\n        # Run model inference\n        # check accepted forward method\n        args = inputs.keys()\n        accepted_inputs = {}\n        for arg in args:\n            if (\n                arg in self.accepted_args\n                or \"**kwargs\" in self.accepted_args\n            ):\n                accepted_inputs[arg] = inputs[arg]\n\n        # Use autocast for mixed precision if enabled\n        if self.pred_config.use_fp16 or self.pred_config.use_bf16:\n            with torch.amp.autocast(\"cuda\", dtype=dtype):\n                outputs = self.model(**accepted_inputs)\n        else:\n            outputs = self.model(**accepted_inputs)\n\n        # Process batch outputs\n        logits = self._process_batch_outputs(\n            outputs,\n            inputs,\n            output_hidden_states,\n            output_attentions,\n            embeddings,\n            reduce_hidden_states,\n            reduce_strategy,\n        )\n        all_logits.append(logits)\n\n    # Concatenate all logits\n    if all_logits and all_logits[0] is not None:\n        all_logits = torch.cat(all_logits, dim=0)\n\n    # Finalize embeddings\n    self._finalize_embeddings(\n        embeddings, output_hidden_states, output_attentions\n    )\n\n    # Get predictions if requested\n    predictions = None\n    if do_pred and len(all_logits) &gt; 0:\n        predictions = self.logits_to_preds(all_logits)\n        if return_dict:\n            predictions = self.format_output(predictions)\n\n    return all_logits, predictions, embeddings\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.calculate_metrics","title":"calculate_metrics","text":"<pre><code>calculate_metrics(logits, labels, plot=False)\n</code></pre> <p>Calculate evaluation metrics for model predictions.</p> <p>This method computes task-specific evaluation metrics using the configured metrics computation module.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>list | Tensor</code> <p>Model predictions (logits or probabilities)</p> required <code>labels</code> <code>list | Tensor</code> <p>True labels for evaluation</p> required <code>plot</code> <code>bool</code> <p>Whether to generate metric plots</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[Any, Any]</code> <p>Dictionary containing evaluation metrics for the task</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def calculate_metrics(\n    self,\n    logits: list | torch.Tensor,\n    labels: list | torch.Tensor,\n    plot: bool = False,\n) -&gt; dict[Any, Any]:\n    \"\"\"Calculate evaluation metrics for model predictions.\n\n    This method computes task-specific evaluation metrics using the\n    configured metrics computation module.\n\n    Args:\n        logits: Model predictions (logits or probabilities)\n        labels: True labels for evaluation\n        plot: Whether to generate metric plots\n\n    Returns:\n        Dictionary containing evaluation metrics for the task\n    \"\"\"\n    # Calculate metrics based on task type\n    compute_metrics_func = compute_metrics(self.task_config, plot=plot)\n    metrics: dict[Any, Any] = compute_metrics_func((logits, labels))\n\n    return metrics\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.estimate_memory_usage","title":"estimate_memory_usage","text":"<pre><code>estimate_memory_usage(batch_size=1, sequence_length=1000)\n</code></pre> <p>Estimate memory usage for inference.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for inference</p> <code>1</code> <code>sequence_length</code> <code>int</code> <p>Maximum sequence length</p> <code>1000</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict containing memory usage estimates</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def estimate_memory_usage(\n    self, batch_size: int = 1, sequence_length: int = 1000\n) -&gt; dict[str, Any]:\n    \"\"\"Estimate memory usage for inference.\n\n    Args:\n        batch_size: Batch size for inference\n        sequence_length: Maximum sequence length\n\n    Returns:\n        Dict containing memory usage estimates\n    \"\"\"\n    try:\n        # Get model parameters\n        total_params = sum(p.numel() for p in self.model.parameters())\n        param_memory_mb = (total_params * 4) / (\n            1024 * 1024\n        )  # Assuming float32\n\n        # Estimate activation memory (rough approximation)\n        if hasattr(self.model, \"config\"):\n            config = self.model.config\n            hidden_size = getattr(config, \"hidden_size\", 768)\n            num_layers = getattr(config, \"num_hidden_layers\", 12)\n        else:\n            hidden_size, num_layers = 768, 12\n\n        # Rough estimate for activations\n        activation_memory_mb = (\n            batch_size * sequence_length * hidden_size * num_layers * 2\n        ) / (1024 * 1024)\n\n        total_memory_mb = param_memory_mb + activation_memory_mb\n\n        return {\n            \"total_estimated_mb\": f\"{total_memory_mb:.1f}\",\n            \"parameter_memory_mb\": f\"{param_memory_mb:.1f}\",\n            \"activation_memory_mb\": f\"{activation_memory_mb:.1f}\",\n            \"note\": \"Estimates are approximate and may vary based on \"\n            \"actual usage\",\n        }\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.force_eager_attention","title":"force_eager_attention","text":"<pre><code>force_eager_attention()\n</code></pre> <p>Force the model to use eager attention implementation.</p> <p>This method attempts to switch the model from SDPA to eager attention implementation to ensure compatibility with output_attentions=True.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def force_eager_attention(self) -&gt; bool:\n    \"\"\"Force the model to use eager attention implementation.\n\n    This method attempts to switch the model from SDPA to eager attention\n    implementation to ensure compatibility with output_attentions=True.\n\n    Returns:\n        bool: True if successful, False otherwise\n    \"\"\"\n    try:\n        if hasattr(self.model, \"config\") and hasattr(\n            self.model.config, \"attn_implementation\"\n        ):\n            self.model.config.attn_implementation = \"eager\"\n            logger.success(\"Switched to eager attention implementation\")\n            return True\n    except Exception as e:\n        logger.failure(f\"Failed to switch to eager attention: {e}\")\n    return False\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.format_output","title":"format_output","text":"<pre><code>format_output(predictions)\n</code></pre> <p>Format output predictions into a structured dictionary.</p> <p>This method converts raw predictions into a user-friendly format with sequences, labels, and confidence scores.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>tuple[Tensor, list]</code> <p>Tuple containing (probabilities, labels)</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing formatted predictions with structure:</p> <code>dict</code> <p>{index: {'sequence': str, 'label': str/list, 'scores': dict/list}}</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def format_output(self, predictions: tuple[torch.Tensor, list]) -&gt; dict:\n    \"\"\"Format output predictions into a structured dictionary.\n\n    This method converts raw predictions into a user-friendly format with\n    sequences, labels, and confidence scores.\n\n    Args:\n        predictions: Tuple containing (probabilities, labels)\n\n    Returns:\n        Dictionary containing formatted predictions with structure:\n        {index: {'sequence': str, 'label': str/list, 'scores': dict/list}}\n    \"\"\"\n    # Get task type from config\n    task_type = self.task_config.task_type\n    formatted_predictions = {}\n    probs, labels = predictions\n    probs = probs.numpy().tolist()\n    keep_seqs = True if len(self.sequences) else False\n    label_names = self.task_config.label_names\n    for i, label in enumerate(labels):\n        prob = probs[i]\n        if task_type == \"regression\":\n            scores = {label_names[0]: prob}\n        elif task_type == \"token\":\n            scores = [max(x) for x in prob]\n        else:\n            scores = {label_names[j]: p for j, p in enumerate(prob)}\n        formatted_predictions[i] = {\n            \"sequence\": self.sequences[i] if keep_seqs else \"\",\n            \"label\": label,\n            \"scores\": scores,\n        }\n    return formatted_predictions\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.generate","title":"generate","text":"<pre><code>generate(\n    inputs,\n    n_tokens=400,\n    n_samples=1,\n    temperature=1.0,\n    top_k=4,\n    top_p=1.0,\n    batched=True,\n)\n</code></pre> <p>Generate DNA sequences using the model.</p> <p>This function performs sequence generation tasks using the loaded model, currently supporting CausalLM and EVO2 models for DNA sequence generation.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataLoader | list[str]</code> <p>DataLoader or List containing prompt sequences</p> required <code>n_tokens</code> <code>int</code> <p>Number of tokens to generate, default 400</p> <code>400</code> <code>n_samples</code> <code>int</code> <p>Do samples n times</p> <code>1</code> <code>temperature</code> <code>float</code> <p>Sampling temperature for generation, default 1.0</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k sampling parameter, default 4</p> <code>4</code> <code>top_p</code> <code>float</code> <p>Top-p sampling paramether, default 1</p> <code>1.0</code> <code>batched</code> <code>bool</code> <p>Do batched generation</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[Any, Any]</code> <p>Dictionary containing generated sequences</p> Note <p>Currently only supports Causal language models for sequence generation</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def generate(\n    self,\n    inputs: DataLoader | list[str],\n    n_tokens: int = 400,\n    n_samples: int = 1,\n    temperature: float = 1.0,\n    top_k: int = 4,\n    top_p: float = 1.0,\n    batched: bool = True,\n) -&gt; dict[Any, Any]:\n    \"\"\"Generate DNA sequences using the model.\n\n    This function performs sequence generation tasks using the loaded\n    model, currently supporting CausalLM and EVO2 models for\n    DNA sequence generation.\n\n    Args:\n        inputs: DataLoader or List containing prompt sequences\n        n_tokens: Number of tokens to generate, default 400\n        n_samples: Do samples n times\n        temperature: Sampling temperature for generation, default 1.0\n        top_k: Top-k sampling parameter, default 4\n        top_p: Top-p sampling paramether, default 1\n        batched: Do batched generation\n\n    Returns:\n        Dictionary containing generated sequences\n\n    Note:\n        Currently only supports Causal language models\n        for sequence generation\n    \"\"\"\n    # Prepare prompt sequences\n    prompt_seqs = []\n    if isinstance(inputs, DataLoader):\n        for data in tqdm(inputs, desc=\"Generating\"):\n            seqs = data[\"sequence\"]\n            if isinstance(prompt_seqs, list):\n                seqs.extend([seq for seq in seqs if seq])\n            if not seqs:\n                continue\n    else:\n        prompt_seqs = inputs\n    # Check if model supports generation\n    if \"evo2\" in str(self.model).lower():\n        # Generate sequences\n        outputs = self.model.generate(\n            prompt_seqs=prompt_seqs,\n            n_tokens=n_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            batched=batched,\n            cached_generation=True,\n        )\n        formatted_outputs = []\n        for i, seq in enumerate(prompt_seqs):\n            generated_seqs = outputs.sequences[i]\n            scores = outputs.logprobs_mean[i]\n            formatted_outputs.append({\n                \"Prompt\": seq,\n                \"Output\": generated_seqs,\n                \"Score\": scores,\n            })\n        return formatted_outputs\n    elif \"evo1\" in str(self.model).lower():\n        from evo import generate\n\n        model = self.model.model\n        tokenizer = self.tokenizer\n        # Generate sequences\n        outputs = generate(\n            prompt_seqs * n_samples,\n            model=model,\n            tokenizer=tokenizer,\n            n_tokens=n_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            cached_generation=True,\n            batched=batched,\n            device=self.device,\n            verbose=1,\n        )\n        formatted_outputs = []\n        for i, seq in enumerate(prompt_seqs):\n            generated_seqs = outputs[0][i]\n            scores = outputs[1][i]\n            formatted_outputs.append({\n                \"Prompt\": seq,\n                \"Output\": generated_seqs,\n                \"Score\": scores,\n            })\n        return formatted_outputs\n    elif \"megadna\" in str(self.model).lower():\n        model = self.model\n        tokenizer = self.tokenizer\n        formatted_outputs = []\n        for seq in prompt_seqs:\n            for _ in range(n_samples):\n                input_ids = tokenizer(seq, return_tensors=\"pt\").to(\n                    self.device\n                )[\"input_ids\"]\n                output = model.generate(\n                    input_ids,\n                    seq_len=n_tokens,\n                    temperature=temperature,\n                    filter_thres=top_p,\n                )\n                decoded = tokenizer.decode(output.squeeze().cpu().int())\n                formatted_outputs.append({\n                    \"Prompt\": seq,\n                    \"Output\": decoded.replace(\" \", \"\"),\n                })\n        return formatted_outputs\n    elif (\n        \"causallm\" in str(self.model).lower()\n        or \"lmhead\" in str(self.model).lower()\n    ):\n        outputs = []\n        # Tokenize prompt sequences\n        for seq in prompt_seqs:\n            inputs = self.tokenizer(seq, return_tensors=\"pt\").to(\n                self.device\n            )\n            output = self.model.generate(\n                **inputs,\n                max_new_tokens=n_tokens,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                do_sample=True,\n            )\n            decoded = self.tokenizer.decode(\n                output[0], skip_special_tokens=True\n            )\n            outputs.append({\n                \"Prompt\": seq,\n                \"Output\": decoded.replace(\" \", \"\"),\n            })\n        return outputs\n    else:\n        raise ValueError(\n            \"This model is not supported for sequence generation.\"\n        )\n\n    return {}\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.generate_dataset","title":"generate_dataset","text":"<pre><code>generate_dataset(\n    seq_or_path,\n    batch_size=1,\n    seq_col=\"sequence\",\n    label_col=\"labels\",\n    sep=None,\n    fasta_sep=\"|\",\n    multi_label_sep=None,\n    uppercase=False,\n    lowercase=False,\n    sampling=None,\n    keep_seqs=True,\n    padding=True,\n    do_encode=True,\n)\n</code></pre> <p>Generate dataset from sequences or file path.</p> <p>This method creates a DNADataset and DataLoader from either a list of sequences or a file path, supporting various file formats and preprocessing options.</p> <p>Parameters:</p> Name Type Description Default <code>seq_or_path</code> <code>str | list[str]</code> <p>Single sequence, list of sequences, or path to a file containing sequences</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoader</p> <code>1</code> <code>seq_col</code> <code>str</code> <p>Column name for sequences in the file</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for labels in the file</p> <code>'labels'</code> <code>sep</code> <code>str | None</code> <p>Delimiter for CSV, TSV, or TXT files</p> <code>None</code> <code>fasta_sep</code> <code>str</code> <p>Delimiter for FASTA files</p> <code>'|'</code> <code>multi_label_sep</code> <code>str | None</code> <p>Delimiter for multi-label sequences</p> <code>None</code> <code>uppercase</code> <code>bool</code> <p>Whether to convert sequences to uppercase</p> <code>False</code> <code>lowercase</code> <code>bool</code> <p>Whether to convert sequences to lowercase</p> <code>False</code> <code>sampling</code> <code>float | None</code> <p>Fraction of data to randomly sample for inference</p> <code>None</code> <code>keep_seqs</code> <code>bool</code> <p>Whether to keep sequences in the dataset for later use</p> <code>True</code> <code>padding</code> <code>str | bool</code> <p>Padding strategy for encoding sequences</p> <code>True</code> <code>do_encode</code> <code>bool</code> <p>Whether to encode sequences for the model</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[DNADataset, DataLoader]</code> <p>Tuple containing: - DNADataset: Dataset object with sequences and labels - DataLoader: DataLoader object for batch processing</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input is neither a file path nor a list of sequences</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def generate_dataset(\n    self,\n    seq_or_path: str | list[str],\n    batch_size: int = 1,\n    seq_col: str = \"sequence\",\n    label_col: str = \"labels\",\n    sep: str | None = None,\n    fasta_sep: str = \"|\",\n    multi_label_sep: str | None = None,\n    uppercase: bool = False,\n    lowercase: bool = False,\n    sampling: float | None = None,\n    keep_seqs: bool = True,\n    padding: str | bool = True,\n    do_encode: bool = True,\n) -&gt; tuple[DNADataset, DataLoader]:\n    \"\"\"Generate dataset from sequences or file path.\n\n    This method creates a DNADataset and DataLoader from either a list\n    of sequences or a file path, supporting various file formats and\n    preprocessing options.\n\n    Args:\n        seq_or_path: Single sequence, list of sequences, or path to a\n            file containing sequences\n        batch_size: Batch size for DataLoader\n        seq_col: Column name for sequences in the file\n        label_col: Column name for labels in the file\n        sep: Delimiter for CSV, TSV, or TXT files\n        fasta_sep: Delimiter for FASTA files\n        multi_label_sep: Delimiter for multi-label sequences\n        uppercase: Whether to convert sequences to uppercase\n        lowercase: Whether to convert sequences to lowercase\n        sampling: Fraction of data to randomly sample for inference\n        keep_seqs: Whether to keep sequences in the dataset for later use\n        padding: Padding strategy for encoding sequences\n        do_encode: Whether to encode sequences for the model\n\n    Returns:\n        Tuple containing:\n            - DNADataset: Dataset object with sequences and labels\n            - DataLoader: DataLoader object for batch processing\n\n    Raises:\n        ValueError: If input is neither a file path nor a list of sequences\n    \"\"\"\n    # Initialize dataset to None to avoid unbound variable issues\n    dataset = None\n\n    if isinstance(seq_or_path, str):\n        suffix = seq_or_path.split(\".\")[-1]\n        if suffix and os.path.isfile(seq_or_path):\n            sequences = []\n            dataset = DNADataset.load_local_data(\n                seq_or_path,\n                seq_col=seq_col,\n                label_col=label_col,\n                sep=sep,\n                fasta_sep=fasta_sep,\n                multi_label_sep=multi_label_sep,\n                tokenizer=self.tokenizer,\n                max_length=self.pred_config.max_length,\n            )\n        else:\n            sequences = [seq_or_path]\n    elif isinstance(seq_or_path, list):\n        sequences = seq_or_path\n    else:\n        raise ValueError(\n            \"Input should be a file path or a list of sequences.\"\n        )\n\n    # If sampling is specified, randomly sample the sequences\n    if sampling:\n        dataset = dataset.sampling(sampling) if dataset else None\n\n    # Create dataset from sequences if we have any and no dataset was\n    # loaded from file\n    if len(sequences) &gt; 0 and dataset is None:\n        ds = Dataset.from_dict({\"sequence\": sequences})\n        dataset = DNADataset(\n            ds, self.tokenizer, max_length=self.pred_config.max_length\n        )\n\n    # Ensure dataset is not None before proceeding\n    if not dataset:\n        raise ValueError(\n            \"No valid dataset could be created from the input.\"\n        )\n    # If labels are provided, keep labels\n    if keep_seqs:\n        self.sequences = dataset.dataset[\"sequence\"]\n    # Encode sequences\n    if do_encode:\n        task_type = self.task_config.task_type\n        dataset.encode_sequences(\n            padding=padding,\n            remove_unused_columns=True,\n            task=task_type,\n            uppercase=uppercase,\n            lowercase=lowercase,\n        )\n        all_cols = dataset.dataset.features\n        cols_drop = [c for c in all_cols if c not in self.accepted_args]\n        dataset.dataset = dataset.dataset.remove_columns(cols_drop)\n    else:\n        all_cols = dataset.dataset.features\n        # check if dataset is already encoded\n        if \"sequence\" in all_cols:\n            check_seq = dataset.dataset[\"sequence\"][0]\n            if isinstance(check_seq, torch.Tensor):\n                # already encoded\n                if \"input_ids\" not in all_cols:\n                    dataset.dataset = dataset.dataset.rename_column(\n                        \"sequence\", \"input_ids\"\n                    )\n                dataset.dataset.set_format(type=\"torch\")\n                cols_drop = [\n                    c\n                    for c in dataset.dataset.features\n                    if c not in self.accepted_args\n                ]\n                dataset.dataset = dataset.dataset.remove_columns(cols_drop)\n            else:\n                cols_drop = [\n                    c\n                    for c in dataset.dataset.features\n                    if c not in self.accepted_args\n                ]\n    # Check for labels in dataset - handle both Dataset and\n    # DatasetDict cases\n    if isinstance(dataset.dataset, DatasetDict):\n        # For DatasetDict, check the first available split\n        keys = list(dataset.dataset.keys())\n        if keys and \"labels\" in dataset.dataset[keys[0]].features:\n            self.labels = dataset.dataset[keys[0]][\"labels\"]\n    else:\n        # For single Dataset\n        if \"labels\" in dataset.dataset.features:\n            self.labels = dataset.dataset[\"labels\"]\n    # Create DataLoader\n    dataloader: DataLoader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        num_workers=self.pred_config.num_workers,\n    )\n\n    return dataset, dataloader\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.get_available_outputs","title":"get_available_outputs","text":"<pre><code>get_available_outputs()\n</code></pre> <p>Get information about available model outputs.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict containing information about what outputs are available and</p> <code>dict[str, Any]</code> <p>collected</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def get_available_outputs(self) -&gt; dict[str, Any]:\n    \"\"\"Get information about available model outputs.\n\n    Returns:\n        Dict containing information about what outputs are available and\n        collected\n    \"\"\"\n    capabilities = {\n        \"hidden_states_available\": self._check_hidden_states_support(),\n        \"attentions_available\": self._check_attention_support(),\n        \"hidden_states_collected\": hasattr(self, \"embeddings\")\n        and \"hidden_states\" in self.embeddings\n        and self.embeddings[\"hidden_states\"] is not None,\n        \"attentions_collected\": hasattr(self, \"embeddings\")\n        and \"attentions\" in self.embeddings\n        and self.embeddings[\"attentions\"] is not None,\n    }\n    return capabilities\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    inputs,\n    do_reduce=False,\n    reduce_strategy=\"mean\",\n    force=False,\n)\n</code></pre> <p>Get embeddings from the last inference. This method performs inference on the provided inputs and extracts embeddings from the model's hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataLoader | list[str] | str</code> <p>DataLoader or list of sequences for inference</p> required <code>do_reduce</code> <code>bool</code> <p>Whether to reduce hidden states to 2D using PCA</p> <code>False</code> <code>reduce_strategy</code> <code>str | int</code> <p>Strategy to reduce hidden states ('mean', 'max', 'min', or int for center window size)</p> <code>'mean'</code> <code>force</code> <code>bool</code> <p>Whether to force re-computation of embeddings</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>Dict containing embeddings from the last inference</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def get_embeddings(\n    self,\n    inputs: DataLoader | list[str] | str,\n    do_reduce: bool = False,\n    reduce_strategy: str | int = \"mean\",\n    force: bool = False,\n) -&gt; Any:\n    \"\"\"Get embeddings from the last inference.\n    This method performs inference on the provided inputs and extracts\n    embeddings from the model's hidden states.\n\n    Args:\n        inputs: DataLoader or list of sequences for inference\n        do_reduce: Whether to reduce hidden states to 2D using PCA\n        reduce_strategy: Strategy to reduce hidden states\n            ('mean', 'max', 'min', or int for center window size)\n        force: Whether to force re-computation of embeddings\n\n    Returns:\n        Dict containing embeddings from the last inference\n    \"\"\"\n    # Initialize embeddings\n    if force or not hasattr(self, \"embeddings\"):\n        self.embeddings = {\"hidden_states\": None, \"attention_mask\": None}\n    # Check specific models\n    is_special = \"\"\n    special_list = [\"CustomEvo\", \"MEGADNA\"]\n    for name in special_list:\n        if name in str(self.model):\n            sequences = inputs\n            is_special = name\n            break\n    if not is_special:\n        if isinstance(inputs, list):\n            _, dataloader = self.generate_dataset(\n                inputs, batch_size=self.pred_config.batch_size\n            )\n        elif isinstance(inputs, str):\n            # Assume it's a file path\n            if os.path.isfile(inputs):\n                file_path = os.path.abspath(inputs)\n            else:\n                raise ValueError(\n                    f\"Input {inputs} is not a valid file path. \"\n                    \"Please provide a valid file path \"\n                    \"or a list contains valid sequences.\"\n                )\n            _, dataloader = self.generate_dataset(\n                file_path,\n                do_encode=True,\n                batch_size=self.pred_config.batch_size,\n            )\n        else:\n            dataloader = inputs\n\n    # Check if model supports generation\n    if is_special.startswith(\"CustomEvo\"):\n        # Get model and tokenizer\n        model = self.model.model\n        tokenizer = self.tokenizer\n        # Get layer names\n        layers = []\n        layer_prefix = \"blocks\"\n        for name, _ in model.named_parameters():\n            if name.startswith(layer_prefix):\n                layer = layer_prefix + \".\" + name.split(\".\")[1]\n                if layer not in layers:\n                    layers.append(layer)\n        # Get embeddings\n        all_embeddings = [[] for _ in layers]\n        for sequence in tqdm(sequences):\n            input_ids = (\n                torch\n                .tensor(\n                    tokenizer.tokenize(sequence),\n                    dtype=torch.int,\n                )\n                .unsqueeze(0)\n                .to(self.device)\n            )\n            _, embeddings = self.model(\n                input_ids, return_embeddings=True, layer_names=layers\n            )\n            for i, n in enumerate(layers):\n                tmp = embeddings[n].detach().cpu().to(torch.float32)\n                if do_reduce:\n                    mean_emb = _compute_mean_embeddings(tmp, None).squeeze(\n                        0\n                    )\n                    all_embeddings[i].append(mean_emb)\n                else:\n                    all_embeddings[i].append(tmp)\n        for i, _ in enumerate(layers):\n            all_embeddings[i] = np.stack(all_embeddings[i], axis=0)\n        if self.embeddings[\"hidden_states\"] is None:\n            self.embeddings[\"hidden_states\"] = all_embeddings\n        return all_embeddings\n\n    elif is_special == \"MEGADNA\":\n        model = self.model\n        tokenizer = self.tokenizer\n        all_embeddings = [None] * 3\n        out_embeddings = []\n        for sequence in tqdm(sequences):\n            input_ids = tokenizer(sequence, return_tensors=\"pt\").to(\n                self.device\n            )[\"input_ids\"]\n            if not isinstance(input_ids, torch.LongTensor):\n                input_ids = input_ids.long()\n            with torch.no_grad():\n                embeddings = model(input_ids, return_value=\"embedding\")\n            for i in range(len(embeddings)):\n                if all_embeddings[i] is None:\n                    all_embeddings[i] = []\n                all_embeddings[i].append(embeddings[i].detach().cpu())\n            out_embeddings = all_embeddings\n        for i in range(len(all_embeddings)):\n            emb = (\n                np.stack(all_embeddings[i], axis=0)\n                if i &gt; 0\n                else np.concatenate(all_embeddings[i], axis=0)\n            )\n            reshaped_emb = emb.reshape(emb.shape[0], -1, emb.shape[-1])\n            if do_reduce:\n                mean_emb = _compute_mean_embeddings(reshaped_emb, None)\n            else:\n                mean_emb = reshaped_emb\n            # proj_emb = torch.nn.Linear(mean_emb.shape[-1], 128)\n            all_embeddings[i] = mean_emb\n        # Save embeddings\n        if self.embeddings[\"hidden_states\"] is None:\n            self.embeddings[\"hidden_states\"] = all_embeddings\n        return out_embeddings\n\n    if (\n        \"hidden_states\" not in self.embeddings\n        or self.embeddings[\"hidden_states\"] is None\n    ):\n        _, _, embeddings = self.batch_infer(\n            dataloader,\n            do_pred=False,\n            output_hidden_states=True,\n            reduce_hidden_states=do_reduce,\n            reduce_strategy=reduce_strategy,\n        )\n        self.embeddings = embeddings\n    return self.embeddings[\"hidden_states\"]\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.get_model_info","title":"get_model_info","text":"<pre><code>get_model_info()\n</code></pre> <p>Get information about the loaded model.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict containing model information including type, device, and</p> <code>dict[str, Any]</code> <p>attention support</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def get_model_info(self) -&gt; dict[str, Any]:\n    \"\"\"Get information about the loaded model.\n\n    Returns:\n        Dict containing model information including type, device, and\n        attention support\n    \"\"\"\n    # Get basic model information\n    info = self._get_basic_model_info()\n\n    # Add model-specific configuration information\n    info.update(self._get_model_config_info())\n\n    # Add parameter information\n    info[\"num_parameters\"] = self._get_model_parameters_info()\n\n    # Add configuration as a dictionary\n    info[\"config\"] = self._get_model_config_dict()\n\n    return info\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.get_model_parameters","title":"get_model_parameters","text":"<pre><code>get_model_parameters()\n</code></pre> <p>Get information about model parameters.</p> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dict containing parameter counts</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def get_model_parameters(self) -&gt; dict[str, int]:\n    \"\"\"Get information about model parameters.\n\n    Returns:\n        Dict containing parameter counts\n    \"\"\"\n    try:\n        total_params = sum(p.numel() for p in self.model.parameters())\n        trainable_params = sum(\n            p.numel() for p in self.model.parameters() if p.requires_grad\n        )\n        frozen_params = total_params - trainable_params\n\n        return {\n            \"total\": total_params,\n            \"trainable\": trainable_params,\n            \"frozen\": frozen_params,\n        }\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.infer","title":"infer","text":"<pre><code>infer(\n    sequences=None,\n    file_path=None,\n    evaluate=False,\n    output_hidden_states=False,\n    output_attentions=False,\n    save_to_file=False,\n    **kwargs,\n)\n</code></pre> <p>Main inference method for sequences or files.</p> <p>This is the primary entry point for performing inference. It automatically determines whether to process sequences directly or load from a file.</p> <p>Parameters:</p> Name Type Description Default <code>sequences</code> <code>str | list[str] | None</code> <p>Single sequence or list of sequences for inference</p> <code>None</code> <code>file_path</code> <code>str | None</code> <p>Path to file containing sequences for inference</p> <code>None</code> <code>evaluate</code> <code>bool</code> <p>Whether to evaluate predictions against true labels</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>Whether to output hidden states for visualization</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>Whether to output attention weights for visualization</p> <code>False</code> <code>save_to_file</code> <code>bool</code> <p>Whether to save predictions to output directory</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to specific inference methods</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Either</code> <code>dict | tuple[dict, dict]</code> <ul> <li>Dict: Dictionary containing predictions</li> <li>Tuple[Dict, Dict]: (predictions, metrics) if evaluate=True</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither sequences nor file_path is provided</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def infer(\n    self,\n    sequences: str | list[str] | None = None,\n    file_path: str | None = None,\n    evaluate: bool = False,\n    output_hidden_states: bool = False,\n    output_attentions: bool = False,\n    save_to_file: bool = False,\n    **kwargs: Any,\n) -&gt; dict | tuple[dict, dict]:\n    \"\"\"Main inference method for sequences or files.\n\n    This is the primary entry point for performing inference. It\n    automatically determines whether to process sequences directly or\n    load from a file.\n\n    Args:\n        sequences: Single sequence or list of sequences for inference\n        file_path: Path to file containing sequences for inference\n        evaluate: Whether to evaluate predictions against true labels\n        output_hidden_states: Whether to output hidden states for\n            visualization\n        output_attentions: Whether to output attention weights for\n            visualization\n        save_to_file: Whether to save predictions to output directory\n        **kwargs: Additional arguments passed to specific inference methods\n\n    Returns:\n        Either:\n            - Dict: Dictionary containing predictions\n            - Tuple[Dict, Dict]: (predictions, metrics) if evaluate=True\n\n    Raises:\n        ValueError: If neither sequences nor file_path is provided\n    \"\"\"\n    if sequences is not None:\n        return self.infer_seqs(\n            sequences=sequences,\n            evaluate=evaluate,\n            output_hidden_states=output_hidden_states,\n            output_attentions=output_attentions,\n            save_to_file=save_to_file,\n        )\n    elif file_path is not None:\n        return self.infer_file(\n            file_path=file_path,\n            evaluate=evaluate,\n            output_hidden_states=output_hidden_states,\n            output_attentions=output_attentions,\n            save_to_file=save_to_file,\n            **kwargs,\n        )\n    else:\n        raise ValueError(\"Either sequences or file_path must be provided\")\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.infer_file","title":"infer_file","text":"<pre><code>infer_file(\n    file_path,\n    evaluate=False,\n    output_hidden_states=False,\n    output_attentions=False,\n    seq_col=\"sequence\",\n    label_col=\"labels\",\n    sep=None,\n    fasta_sep=\"|\",\n    multi_label_sep=None,\n    uppercase=False,\n    lowercase=False,\n    sampling=None,\n    do_encode=True,\n    save_to_file=False,\n    plot_metrics=False,\n)\n</code></pre> <p>Infer from a file containing sequences.</p> <p>This method loads sequences from a file and performs inference, with optional evaluation, visualization, and saving capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Dataset</code> <p>Path to the file containing sequences</p> required <code>evaluate</code> <code>bool</code> <p>Whether to evaluate predictions against true labels</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>Whether to output hidden states for visualization</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>Whether to output attention weights for visualization</p> <code>False</code> <code>seq_col</code> <code>str</code> <p>Column name for sequences in the file</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for labels in the file</p> <code>'labels'</code> <code>sep</code> <code>str | None</code> <p>Delimiter for CSV, TSV, or TXT files</p> <code>None</code> <code>fasta_sep</code> <code>str</code> <p>Delimiter for FASTA files</p> <code>'|'</code> <code>multi_label_sep</code> <code>str | None</code> <p>Delimiter for multi-label sequences</p> <code>None</code> <code>uppercase</code> <code>bool</code> <p>Whether to convert sequences to uppercase</p> <code>False</code> <code>lowercase</code> <code>bool</code> <p>Whether to convert sequences to lowercase</p> <code>False</code> <code>sampling</code> <code>float | None</code> <p>Fraction of data to randomly sample for inference</p> <code>None</code> <code>save_to_file</code> <code>bool</code> <p>Whether to save predictions and metrics to output directory</p> <code>False</code> <code>plot_metrics</code> <code>bool</code> <p>Whether to generate metric plots</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Either</code> <code>dict | tuple[dict, dict]</code> <ul> <li>Dict: Dictionary containing predictions</li> <li>Tuple[Dict, Dict]: (predictions, metrics) if evaluate=True</li> </ul> Note <p>Setting output_attentions=True may consume significant memory</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def infer_file(\n    self,\n    file_path: str | Dataset,\n    evaluate: bool = False,\n    output_hidden_states: bool = False,\n    output_attentions: bool = False,\n    seq_col: str = \"sequence\",\n    label_col: str = \"labels\",\n    sep: str | None = None,\n    fasta_sep: str = \"|\",\n    multi_label_sep: str | None = None,\n    uppercase: bool = False,\n    lowercase: bool = False,\n    sampling: float | None = None,\n    do_encode: bool = True,\n    save_to_file: bool = False,\n    plot_metrics: bool = False,\n) -&gt; dict | tuple[dict, dict]:\n    \"\"\"Infer from a file containing sequences.\n\n    This method loads sequences from a file and performs inference,\n    with optional evaluation, visualization, and saving capabilities.\n\n    Args:\n        file_path: Path to the file containing sequences\n        evaluate: Whether to evaluate predictions against true labels\n        output_hidden_states: Whether to output hidden states for\n            visualization\n        output_attentions: Whether to output attention weights for\n            visualization\n        seq_col: Column name for sequences in the file\n        label_col: Column name for labels in the file\n        sep: Delimiter for CSV, TSV, or TXT files\n        fasta_sep: Delimiter for FASTA files\n        multi_label_sep: Delimiter for multi-label sequences\n        uppercase: Whether to convert sequences to uppercase\n        lowercase: Whether to convert sequences to lowercase\n        sampling: Fraction of data to randomly sample for inference\n        save_to_file: Whether to save predictions and metrics to\n            output directory\n        plot_metrics: Whether to generate metric plots\n\n    Returns:\n        Either:\n            - Dict: Dictionary containing predictions\n            - Tuple[Dict, Dict]: (predictions, metrics) if evaluate=True\n\n    Note:\n        Setting output_attentions=True may consume significant memory\n    \"\"\"\n    # Get dataset and dataloader from file\n    if isinstance(file_path, Dataset):\n        dataloader = DataLoader(\n            file_path,\n            batch_size=self.pred_config.batch_size,\n            num_workers=self.pred_config.num_workers,\n        )\n        self.labels = (\n            file_path[\"labels\"] if \"labels\" in file_path.features else []\n        )\n    else:\n        _, dataloader = self.generate_dataset(\n            file_path,\n            seq_col=seq_col,\n            label_col=label_col,\n            sep=sep,\n            fasta_sep=fasta_sep,\n            multi_label_sep=multi_label_sep,\n            uppercase=uppercase,\n            lowercase=lowercase,\n            sampling=sampling,\n            do_encode=do_encode,\n            batch_size=self.pred_config.batch_size,\n        )\n    # Do batch inference\n    if output_attentions:\n        warnings.warn(\n            \"Cautions: output_attentions may consume a lot of memory.\\n\",\n            stacklevel=2,\n        )\n    logits, predictions, embeddings = self.batch_infer(\n        dataloader,\n        output_hidden_states=output_hidden_states,\n        output_attentions=output_attentions,\n    )\n    # Keep hidden states\n    if output_hidden_states or output_attentions:\n        self.embeddings = embeddings\n    # Save predictions\n    if save_to_file and self.pred_config.output_dir:\n        save_predictions(predictions, Path(self.pred_config.output_dir))\n    # Do evaluation\n    if len(self.labels) == len(logits) and evaluate:\n        metrics = self.calculate_metrics(\n            logits, self.labels, plot=plot_metrics\n        )\n        metrics_save = dict(metrics)\n        metrics_save.pop(\"curve\", None)\n        metrics_save.pop(\"scatter\", None)\n        if save_to_file and self.pred_config.output_dir:\n            save_metrics(metrics, Path(self.pred_config.output_dir))\n        # Whether to plot metrics\n        if plot_metrics:\n            return predictions, metrics\n        else:\n            return predictions, metrics_save\n\n    return predictions\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.infer_seqs","title":"infer_seqs","text":"<pre><code>infer_seqs(\n    sequences,\n    do_pred=True,\n    evaluate=False,\n    output_hidden_states=False,\n    output_attentions=False,\n    save_to_file=False,\n)\n</code></pre> <p>Infer for a list of sequences.</p> <p>This method provides a convenient interface for performing inference on sequences, with optional evaluation and saving capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>sequences</code> <code>str | list[str]</code> <p>Single sequence or list of sequences for inference</p> required <code>evaluate</code> <code>bool</code> <p>Whether to evaluate predictions against true labels</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>Whether to output hidden states for visualization</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>Whether to output attention weights for visualization</p> <code>False</code> <code>save_to_file</code> <code>bool</code> <p>Whether to save predictions to output directory</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Either</code> <code>dict | tuple[dict, dict]</code> <ul> <li>Dict: Dictionary containing predictions</li> <li>Tuple[Dict, Dict]: (predictions, metrics) if evaluate=True</li> </ul> Note <p>Evaluation requires that labels are available in the dataset</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def infer_seqs(\n    self,\n    sequences: str | list[str],\n    do_pred: bool = True,\n    evaluate: bool = False,\n    output_hidden_states: bool = False,\n    output_attentions: bool = False,\n    save_to_file: bool = False,\n) -&gt; dict | tuple[dict, dict]:\n    \"\"\"Infer for a list of sequences.\n\n    This method provides a convenient interface for performing\n    inference on sequences, with optional evaluation and saving\n    capabilities.\n\n    Args:\n        sequences: Single sequence or list of sequences for inference\n        evaluate: Whether to evaluate predictions against true labels\n        output_hidden_states: Whether to output hidden states for\n            visualization\n        output_attentions: Whether to output attention weights for\n            visualization\n        save_to_file: Whether to save predictions to output directory\n\n    Returns:\n        Either:\n            - Dict: Dictionary containing predictions\n            - Tuple[Dict, Dict]: (predictions, metrics) if evaluate=True\n\n    Note:\n        Evaluation requires that labels are available in the dataset\n    \"\"\"\n    # Get dataset and dataloader from sequences\n    _, dataloader = self.generate_dataset(\n        sequences, batch_size=self.pred_config.batch_size\n    )\n    # Do batch inference\n    logits, predictions, embeddings = self.batch_infer(\n        dataloader,\n        output_hidden_states=output_hidden_states,\n        output_attentions=output_attentions,\n        do_pred=do_pred,\n    )\n    # Keep hidden states\n    if output_hidden_states or output_attentions:\n        self.embeddings = embeddings\n    # Save predictions\n    if save_to_file and self.pred_config.output_dir:\n        save_predictions(predictions, Path(self.pred_config.output_dir))\n    # Do evaluation\n    if len(self.labels) == len(logits) and evaluate:\n        metrics = self.calculate_metrics(logits, self.labels)\n        metrics_save = dict(metrics)\n        metrics_save.pop(\"curve\", None)\n        metrics_save.pop(\"scatter\", None)\n        if save_to_file and self.pred_config.output_dir:\n            save_metrics(metrics_save, Path(self.pred_config.output_dir))\n        return predictions, metrics\n\n    return predictions\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.logits_to_preds","title":"logits_to_preds","text":"<pre><code>logits_to_preds(logits)\n</code></pre> <p>Convert model logits to predictions and human-readable labels.</p> <p>This method processes raw model outputs based on the task type to generate appropriate predictions and convert them to human-readable labels.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Model output logits tensor</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, list]</code> <p>Tuple containing: - torch.Tensor: Model predictions (probabilities or raw values) - List: Human-readable labels corresponding to predictions</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If task type is not supported</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def logits_to_preds(\n    self, logits: torch.Tensor\n) -&gt; tuple[torch.Tensor, list]:\n    \"\"\"Convert model logits to predictions and human-readable labels.\n\n    This method processes raw model outputs based on the task type to\n    generate appropriate predictions and convert them to human-readable\n    labels.\n\n    Args:\n        logits: Model output logits tensor\n\n    Returns:\n        Tuple containing:\n            - torch.Tensor: Model predictions (probabilities or raw values)\n            - List: Human-readable labels corresponding to predictions\n\n    Raises:\n        ValueError: If task type is not supported\n    \"\"\"\n    # Get task type and threshold from config\n    task_type = self.task_config.task_type\n    threshold = self.task_config.threshold\n    label_names = self.task_config.label_names\n    # Convert logits to predictions based on task type\n    if task_type == \"binary\":\n        probs = torch.softmax(logits, dim=-1)\n        preds = (probs[:, 1] &gt; threshold).long()\n        labels = [label_names[pred] for pred in preds]\n    elif task_type == \"multiclass\":\n        probs = torch.softmax(logits, dim=-1)\n        preds = torch.argmax(probs, dim=-1)\n        labels = [label_names[pred] for pred in preds]\n    elif task_type == \"multilabel\":\n        probs = torch.sigmoid(logits)\n        preds = (probs &gt; threshold).long()\n        labels = []\n        for pred in preds:\n            label = [\n                label_names[i] for i in range(len(pred)) if pred[i] == 1\n            ]\n            labels.append(label)\n    elif task_type == \"regression\":\n        preds = logits.squeeze(-1)\n        probs = preds\n        labels = label_names\n    elif task_type == \"token\":\n        probs = torch.softmax(logits, dim=-1)\n        preds = torch.argmax(logits, dim=-1)\n        labels = []\n        for pred in preds:\n            label = [label_names[pred[i]] for i in range(len(pred))]\n            labels.append(label)\n    else:\n        raise ValueError(f\"Unsupported task type: {task_type}\")\n    return probs, labels\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.plot_attentions","title":"plot_attentions","text":"<pre><code>plot_attentions(\n    seq_idx=0,\n    layer=-1,\n    head=-1,\n    norm_method=None,\n    skip_cls=True,\n    width=800,\n    height=800,\n    save_path=None,\n)\n</code></pre> <p>Plot attention map visualization.</p> <p>This method creates a heatmap visualization of attention weights between tokens in a sequence, showing how the model attends to different parts of the input.</p> <p>Parameters:</p> Name Type Description Default <code>seq_idx</code> <code>int</code> <p>Index of the sequence to plot, default 0</p> <code>0</code> <code>layer</code> <code>int</code> <p>Layer index to visualize, default -1 (last layer)</p> <code>-1</code> <code>head</code> <code>int</code> <p>Attention head index to visualize, default -1 (last head)</p> <code>-1</code> <code>width</code> <code>int</code> <p>Width of the plot</p> <code>800</code> <code>height</code> <code>int</code> <p>Height of the plot</p> <code>800</code> <code>save_path</code> <code>str | None</code> <p>Path to save the plot. If None, plot will be shown interactively</p> <code>None</code> <p>Returns:</p> Type Description <code>Any | None</code> <p>Attention map visualization if available, otherwise None</p> Note <p>This method requires that attention weights were collected during inference by setting output_attentions=True in prediction methods</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def plot_attentions(\n    self,\n    seq_idx: int = 0,\n    layer: int = -1,\n    head: int = -1,\n    norm_method: str | None = None,\n    skip_cls=True,\n    width: int = 800,\n    height: int = 800,\n    save_path: str | None = None,\n) -&gt; Any | None:\n    \"\"\"Plot attention map visualization.\n\n    This method creates a heatmap visualization of attention weights\n    between tokens in a sequence, showing how the model attends to\n    different parts of the input.\n\n    Args:\n        seq_idx: Index of the sequence to plot, default 0\n        layer: Layer index to visualize, default -1 (last layer)\n        head: Attention head index to visualize, default -1 (last head)\n        width: Width of the plot\n        height: Height of the plot\n        save_path: Path to save the plot. If None, plot will be shown\n            interactively\n\n    Returns:\n        Attention map visualization if available, otherwise None\n\n    Note:\n        This method requires that attention weights were collected\n        during inference by setting output_attentions=True in prediction\n        methods\n    \"\"\"\n    if hasattr(self, \"embeddings\"):\n        attentions = self.embeddings[\"attentions\"]\n        if save_path:\n            suffix = os.path.splitext(save_path)[-1]\n            if suffix:\n                heatmap = save_path.replace(suffix, \"_heatmap\" + suffix)\n            else:\n                heatmap = os.path.join(save_path, \"heatmap.pdf\")\n        else:\n            heatmap = None\n        # Plot attention map\n        attn_map = plot_attention_map(\n            attentions,\n            self.sequences,\n            self.tokenizer,\n            seq_idx=seq_idx,\n            layer=layer,\n            norm_method=norm_method,\n            skip_cls=skip_cls,\n            head=head,\n            width=width,\n            height=height,\n            save_path=heatmap,\n        )\n        return attn_map\n    else:\n        logger.warning(\"No attention weights available to plot.\")\n        return None\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.plot_hidden_states","title":"plot_hidden_states","text":"<pre><code>plot_hidden_states(\n    reducer=\"t-SNE\",\n    reduced=False,\n    quality=\"fast\",\n    ncols=4,\n    width=300,\n    height=300,\n    point_size=10,\n    save_path=None,\n)\n</code></pre> <p>Visualize embeddings using dimensionality reduction.</p> <p>This method creates 2D visualizations of high-dimensional embeddings from different model layers using PCA, t-SNE, or UMAP dimensionality reduction.</p> <p>Parameters:</p> Name Type Description Default <code>reducer</code> <code>str</code> <p>Dimensionality reduction method to use ('PCA', 't-SNE', 'UMAP')</p> <code>'t-SNE'</code> <code>reduced</code> <code>bool</code> <p>Whether to use already reduced embeddings if available</p> <code>False</code> <code>quality</code> <code>str</code> <p>Quality/speed trade-off for reduction</p> <code>'fast'</code> <code>ncols</code> <code>int</code> <p>Number of columns in the plot grid</p> <code>4</code> <code>width</code> <code>int</code> <p>Width of each plot</p> <code>300</code> <code>height</code> <code>int</code> <p>Height of each plot</p> <code>300</code> <code>point_size</code> <code>int</code> <p>Size of points in the plot</p> <code>10</code> <code>save_path</code> <code>str | None</code> <p>Path to save the plot. If None, plot will be shown interactively</p> <code>None</code> <p>Returns:</p> Type Description <code>Any | None</code> <p>Embedding visualization if available, otherwise None</p> Note <p>This method requires that hidden states were collected during inference by setting output_hidden_states=True in prediction methods</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def plot_hidden_states(\n    self,\n    reducer: str = \"t-SNE\",\n    reduced: bool = False,\n    quality: str = \"fast\",\n    ncols: int = 4,\n    width: int = 300,\n    height: int = 300,\n    point_size: int = 10,\n    save_path: str | None = None,\n) -&gt; Any | None:\n    \"\"\"Visualize embeddings using dimensionality reduction.\n\n    This method creates 2D visualizations of high-dimensional\n    embeddings from different model layers using PCA, t-SNE, or UMAP\n    dimensionality reduction.\n\n    Args:\n        reducer: Dimensionality reduction method to use\n            ('PCA', 't-SNE', 'UMAP')\n        reduced: Whether to use already reduced embeddings if available\n        quality: Quality/speed trade-off for reduction\n        ncols: Number of columns in the plot grid\n        width: Width of each plot\n        height: Height of each plot\n        point_size: Size of points in the plot\n        save_path: Path to save the plot. If None, plot will be shown\n            interactively\n\n    Returns:\n        Embedding visualization if available, otherwise None\n\n    Note:\n        This method requires that hidden states were collected during\n        inference by setting output_hidden_states=True in prediction\n        methods\n    \"\"\"\n    if hasattr(self, \"embeddings\"):\n        hidden_states = self.embeddings[\"hidden_states\"]\n        # attention_mask = torch.unsqueeze(\n        #     self.embeddings[\"attention_mask\"], dim=-1\n        # )\n        attention_mask = self.embeddings[\"attention_mask\"]\n        labels = self.embeddings[\"labels\"]\n        if save_path:\n            suffix = os.path.splitext(save_path)[-1]\n            if suffix:\n                embedding = save_path.replace(\n                    suffix, \"_embedding\" + suffix\n                )\n            else:\n                embedding = os.path.join(save_path, \"embedding.pdf\")\n        else:\n            embedding = None\n        # Plot hidden states\n        label_names = self.task_config.label_names\n        embeddings_vis = plot_embeddings(\n            hidden_states,\n            attention_mask,\n            reducer=reducer,\n            quality=quality,\n            labels=labels,\n            label_names=label_names,\n            ncols=ncols,\n            width=width,\n            height=height,\n            point_size=point_size,\n            save_path=embedding,\n            reduced=reduced,\n        )\n        return embeddings_vis\n    else:\n        logger.warning(\"No hidden states available to plot.\")\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.DNAInference.scoring","title":"scoring","text":"<pre><code>scoring(\n    inputs,\n    reduce_method=\"mean\",\n    score_type=\"embedding\",\n    reduce_hidden_states=False,\n)\n</code></pre> <p>Score sequences using the model.</p> <p>This function computes scores for input sequences using the loaded model. It supports specific scoring methods for EVO2, EVO1, and MegaDNA models, as well as a general scoring approach for other base models.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataLoader | list[str]</code> <p>DataLoader or List containing sequences to score</p> required <code>reduce_method</code> <code>str</code> <p>Method to reduce scores ('mean', 'max', 'min', 'last'), default 'mean'</p> <code>'mean'</code> <code>score_type</code> <code>str</code> <p>Type of score to compute ('embedding', 'logits', 'probability', 'loss'), default 'embedding'</p> <code>'embedding'</code> <code>reduce_hidden_states</code> <code>bool</code> <p>Whether to reduce hidden states across layers, default False</p> <code>False</code> <p>Returns:     Dictionary containing sequences and their corresponding scores</p> Source code in <code>dnallm/inference/inference.py</code> <pre><code>def scoring(\n    self,\n    inputs: DataLoader | list[str],\n    reduce_method: str = \"mean\",\n    score_type: str = \"embedding\",\n    reduce_hidden_states: bool = False,\n) -&gt; dict[Any, Any]:\n    \"\"\"Score sequences using the model.\n\n    This function computes scores for input sequences using the loaded\n    model. It supports specific scoring methods for EVO2, EVO1, and\n    MegaDNA models, as well as a general scoring approach for other\n    base models.\n\n    Args:\n        inputs: DataLoader or List containing sequences to score\n        reduce_method: Method to reduce scores ('mean', 'max', 'min',\n            'last'), default 'mean'\n        score_type: Type of score to compute ('embedding', 'logits',\n            'probability', 'loss'), default 'embedding'\n        reduce_hidden_states: Whether to reduce hidden states across\n            layers, default False\n    Returns:\n        Dictionary containing sequences and their corresponding scores\n    \"\"\"\n    # Prepare score sequences\n    score_seqs = []\n    if isinstance(inputs, DataLoader):\n        for data in tqdm(inputs, desc=\"Scoring\"):\n            seqs = (\n                data.get(\"sequence\", None)\n                if isinstance(data, dict)\n                else getattr(data, \"sequence\", None)\n            )\n            if not seqs:\n                continue\n            score_seqs.extend([s for s in seqs if s])\n    else:\n        score_seqs = inputs\n    # Check if model supports scoring\n    model_name = str(self.model).lower()\n    if \"evo2\" in model_name:\n        outputs = self.model.score_sequences(\n            score_seqs, reduce_method=reduce_method\n        )\n        outputs = [\n            {\"Input\": score_seqs[i], \"Score\": s}\n            for i, s in enumerate(outputs)\n        ]\n        return outputs\n    elif \"evo1\" in model_name:\n        from evo import score_sequences\n\n        model = self.model.model\n        tokenizer = self.tokenizer\n        outputs = score_sequences(\n            score_seqs,\n            model=model,\n            tokenizer=tokenizer.raw_tokenizer,\n            reduce_method=reduce_method,\n            device=self.device,\n        )\n        outputs = [\n            {\"Input\": score_seqs[i], \"Score\": s}\n            for i, s in enumerate(outputs)\n        ]\n        return outputs\n    elif \"megadna\" in model_name:\n        model = self.model\n        tokenizer = self.tokenizer\n        outputs = []\n        for seq in score_seqs:\n            input_ids = tokenizer(seq, return_tensors=\"pt\").to(\n                self.device\n            )[\"input_ids\"]\n            with torch.no_grad():\n                loss = model(input_ids, return_value=\"loss\")\n            outputs.append({\"Input\": seq, \"Score\": loss})\n        return outputs\n\n    # General scoring for other base models (No classification head)\n    # Use batch_infer to get embeddings and compute scores\n    if isinstance(inputs, list):\n        _, dataloader = self.generate_dataset(\n            inputs,\n            batch_size=self.pred_config.batch_size,\n            padding=\"longest\",\n        )\n    elif isinstance(inputs, dict):\n        _, dataloader = self.generate_dataset(\n            inputs[\"sequence\"],\n            batch_size=self.pred_config.batch_size,\n            padding=\"longest\",\n        )\n    elif isinstance(inputs, DataLoader):\n        if \"sequence\" in inputs.dataset.dataset.column_names:\n            seqs = inputs.dataset[\"sequence\"]\n            if len(seqs) != len(inputs.dataset):\n                raise ValueError(\n                    \"Some sequences are missing in the dataset.\"\n                )\n            score_seqs = [s for s in seqs if s]\n        if score_seqs:\n            _, dataloader = self.generate_dataset(\n                score_seqs,\n                batch_size=self.pred_config.batch_size,\n                padding=\"longest\",\n            )\n        else:\n            dataloader = inputs\n    else:\n        dataloader = inputs\n    all_logits, _, embeddings = self.batch_infer(\n        dataloader,\n        output_hidden_states=True if score_type == \"embedding\" else False,\n        reduce_hidden_states=reduce_hidden_states,\n        reduce_strategy=reduce_method,\n        do_pred=False,\n    )\n    # Prepare logits list for scoring\n    logits_list = []\n    if isinstance(all_logits, torch.Tensor):\n        # assume shape (N, L, V)\n        for i in range(all_logits.size(0)):\n            logits_list.append(all_logits[i].detach().cpu())\n    elif isinstance(all_logits, (list, tuple)):\n        for item in all_logits:\n            if item is not None:\n                logits_list.append(\n                    item.detach().cpu()\n                    if isinstance(item, torch.Tensor)\n                    else torch.tensor(item)\n                )\n    else:\n        logits_list = []\n    # Compute scores\n    scores = []\n    if \"hidden_states\" in embeddings and score_type == \"embedding\":\n        hidden_states = embeddings[\"hidden_states\"]\n        for i in range(len(score_seqs)):\n            # (layers, seq_len, dim)\n            if reduce_method == \"last\":\n                seq_hidden = hidden_states[-1][i]\n            elif reduce_method == \"first\":\n                seq_hidden = hidden_states[0][i]\n            else:\n                seq_hidden = torch.stack(\n                    [h[i] for h in hidden_states], dim=0\n                )\n            if reduce_method == \"mean\":\n                score = seq_hidden.mean().item()\n            elif reduce_method == \"max\":\n                score = seq_hidden.max().item()\n            elif reduce_method == \"min\":\n                score = seq_hidden.min().item()\n            else:\n                score = seq_hidden.mean().item()\n            scores.append({\"Input\": score_seqs[i], \"Score\": score})\n        return scores\n    elif logits_list and score_type == \"logits\":\n        # use logits as scores\n        for i in range(len(score_seqs)):\n            logits = logits_list[i]\n            if logits.dim() == 3:\n                logits = logits.squeeze(0)\n            if reduce_method == \"mean\":\n                score = logits.mean().item()\n            elif reduce_method == \"max\":\n                score = logits.max().item()\n            elif reduce_method == \"min\":\n                score = logits.min().item()\n            else:\n                score = logits.mean().item()\n            scores.append({\"Input\": score_seqs[i], \"Score\": score})\n        return scores\n    elif logits_list and score_type == \"probability\":\n        tokenizer = self.tokenizer\n        for i, seq in enumerate(score_seqs):\n            logits = logits_list[i].to(self.device)  # (L, V) or (1,L,V)\n            if logits.dim() == 3 and logits.size(0) == 1:\n                logits = logits.squeeze(0)\n            # Re-tokenize the sequence to obtain input_ids &amp; attention_mask\n            enc = tokenizer(seq, return_tensors=\"pt\", padding=False)\n            input_ids = enc[\"input_ids\"].to(self.device)  # (1, L)\n            attention_mask = enc.get(\"attention_mask\", None)\n            if attention_mask is not None:\n                attention_mask = attention_mask.to(self.device)\n            # Decide model type by presence of mask token\n            mask_token_id = getattr(tokenizer, \"mask_token_id\", None)\n            is_mlm = False\n            if mask_token_id is not None:\n                if (input_ids == mask_token_id).any():\n                    is_mlm = True\n            with torch.no_grad():\n                # (L, V)\n                logprobs = torch.log_softmax(\n                    logits.to(self.device), dim=-1\n                )\n            tgt_ids = input_ids.squeeze(0)\n            if is_mlm:\n                # MLM: logits[t] predicts token at t\n                # Gather token logprobs at each position\n                gathered = logprobs.gather(\n                    1, tgt_ids.unsqueeze(-1)\n                ).squeeze(-1)  # (L,)\n                # only keep positions where mask appears\n                mask_positions = tgt_ids == mask_token_id\n                if mask_positions.any():\n                    token_logprobs = gathered[mask_positions]\n                else:\n                    # No explicit mask found; fall back to using all tokens\n                    token_logprobs = gathered\n            else:\n                # Causal LM: logits[t] predicts token at t+1\n                # Align: drop last logit, drop first input id\n                if logprobs.size(0) &gt;= 2 and tgt_ids.size(0) &gt;= 2:\n                    lp = logprobs[:-1, :]  # (L-1, V)\n                    tgt = tgt_ids[1:]  # (L-1,)\n                    gathered = lp.gather(1, tgt.unsqueeze(-1)).squeeze(-1)\n                    # apply attention_mask if available (exclude padding)\n                    if attention_mask is not None:\n                        attn = attention_mask.squeeze(0)[1:].to(torch.bool)\n                        if attn.any():\n                            token_logprobs = gathered[attn]\n                        else:\n                            token_logprobs = gathered\n                    else:\n                        token_logprobs = gathered\n                else:\n                    # fallback: gather directly (if short)\n                    gathered = logprobs.gather(\n                        1, tgt_ids.unsqueeze(-1)\n                    ).squeeze(-1)\n                    token_logprobs = gathered\n            if token_logprobs.numel() == 0:\n                score = float(\"nan\")  # no tokens to score\n            else:\n                if reduce_method == \"mean\":\n                    score = float(token_logprobs.mean().item())\n                elif reduce_method == \"sum\":\n                    score = float(token_logprobs.sum().item())\n                elif reduce_method == \"max\":\n                    score = float(token_logprobs.max().item())\n                elif reduce_method == \"min\":\n                    score = float(token_logprobs.min().item())\n                else:\n                    score = float(token_logprobs.mean().item())\n            scores.append({\"Input\": seq, \"Score\": score})\n        return scores\n\n    return {}\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference-functions","title":"Functions","text":""},{"location":"api/inference/inference/#dnallm.inference.inference.save_metrics","title":"save_metrics","text":"<pre><code>save_metrics(metrics, output_dir)\n</code></pre> <p>Save evaluation metrics to JSON file.</p> <p>This function saves computed evaluation metrics in JSON format to the specified output directory.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict</code> <p>Dictionary containing metrics to save</p> required <code>output_dir</code> <code>Path</code> <p>Directory path where metrics will be saved</p> required Source code in <code>dnallm/inference/inference.py</code> <pre><code>def save_metrics(metrics: dict, output_dir: Path) -&gt; None:\n    \"\"\"Save evaluation metrics to JSON file.\n\n    This function saves computed evaluation metrics in JSON format to the\n    specified output directory.\n\n    Args:\n        metrics: Dictionary containing metrics to save\n        output_dir: Directory path where metrics will be saved\n    \"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save metrics\n    with open(output_dir / \"metrics.json\", \"w\") as f:\n        json.dump(metrics, f, indent=4)\n</code></pre>"},{"location":"api/inference/inference/#dnallm.inference.inference.save_predictions","title":"save_predictions","text":"<pre><code>save_predictions(predictions, output_dir)\n</code></pre> <p>Save predictions to JSON file.</p> <p>This function saves model predictions in JSON format to the specified output directory.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>dict</code> <p>Dictionary containing predictions to save</p> required <code>output_dir</code> <code>Path</code> <p>Directory path where predictions will be saved</p> required Source code in <code>dnallm/inference/inference.py</code> <pre><code>def save_predictions(predictions: dict, output_dir: Path) -&gt; None:\n    \"\"\"Save predictions to JSON file.\n\n    This function saves model predictions in JSON format to the specified\n    output directory.\n\n    Args:\n        predictions: Dictionary containing predictions to save\n        output_dir: Directory path where predictions will be saved\n    \"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save predictions\n    with open(output_dir / \"predictions.json\", \"w\") as f:\n        json.dump(predictions, f, indent=4)\n</code></pre>"},{"location":"api/inference/interpret/","title":"Model Interpretation","text":""},{"location":"api/inference/interpret/#dnallm.inference.interpret","title":"dnallm.inference.interpret","text":""},{"location":"api/inference/interpret/#dnallm.inference.interpret-classes","title":"Classes","text":""},{"location":"api/inference/interpret/#dnallm.inference.interpret.DNAInterpret","title":"DNAInterpret","text":"<pre><code>DNAInterpret(model, tokenizer, config=None)\n</code></pre> <p>A class for interpreting DNA language models using Captum.</p> <p>Usage:</p> <p>model, tokenizer = load_model_and_tokenizer(...) interpreter = DNAInterpret(model, tokenizer) tokens, scores = interpreter.run_lig(     input_seq=\"ACGT...\",     target=1,     task_type=\"seq_clf\" )</p> <p>Initialize the interpreter.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>Model with a task head.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>Tokenizer for the model.</p> required Source code in <code>dnallm/inference/interpret.py</code> <pre><code>def __init__(\n    self,\n    model: PreTrainedModel,\n    tokenizer: PreTrainedTokenizer,\n    config: dict | None = None,\n):\n    \"\"\"\n    Initialize the interpreter.\n\n    Args:\n        model (PreTrainedModel): Model with a task head.\n        tokenizer (PreTrainedTokenizer): Tokenizer for the model.\n    \"\"\"\n    self.model = model.eval()\n    self.tokenizer = tokenizer\n    self.task_config = config[\"task\"]\n    self.pred_config = config[\"inference\"]\n    self.device = self.pred_config.device\n    self.embedding_layer = None\n    self.model.to(self.device)\n\n    # Find suitable PAD token ID for baselines\n    if hasattr(tokenizer, \"pad_token_id\"):\n        if tokenizer.pad_token_id is not None:\n            self.pad_token_id = tokenizer.pad_token_id\n        else:\n            if hasattr(tokenizer, \"eos_token_id\"):\n                if tokenizer.eos_token_id is not None:\n                    print(\n                        \"Warning: tokenizer.pad_token_id is None. \"\n                        \"Using tokenizer.eos_token_id as pad token \"\n                        \"for baselines.\"\n                    )\n                    self.pad_token_id = tokenizer.eos_token_id\n                else:\n                    print(\n                        \"Warning: No pad_token_id or eos_token_id \"\n                        \"found. Using 0. This may be incorrect.\"\n                    )\n                    self.pad_token_id = 0\n    else:\n        if hasattr(tokenizer, \"pad_token\"):\n            pad_token = tokenizer.pad_token\n            if hasattr(tokenizer, \"convert_tokens_to_ids\"):\n                self.pad_token_id = tokenizer.convert_tokens_to_ids(\n                    pad_token\n                )\n            elif hasattr(tokenizer, \"tokenize\"):\n                self.pad_token_id = tokenizer.tokenize(pad_token)[0]\n            elif hasattr(tokenizer, \"encode\"):\n                self.pad_token_id = tokenizer.encode(\n                    pad_token, add_special_tokens=False\n                )[0]\n            else:\n                print(\n                    \"Warning: Cannot determine pad_token_id. \"\n                    \"Using 0. This may be incorrect.\"\n                )\n                self.pad_token_id = 0\n</code></pre>"},{"location":"api/inference/interpret/#dnallm.inference.interpret.DNAInterpret-functions","title":"Functions","text":""},{"location":"api/inference/interpret/#dnallm.inference.interpret.DNAInterpret.batch_interpret","title":"batch_interpret","text":"<pre><code>batch_interpret(\n    input_seqs,\n    method,\n    targets,\n    token_indices=None,\n    target_layers=None,\n    max_length=None,\n    plot=True,\n    **kwargs,\n)\n</code></pre> <p>Batch interpret multiple sequences.</p> <p>Parameters:</p> Name Type Description Default <code>input_seqs</code> <code>list[str]</code> <p>List of input DNA sequences.</p> required <code>method</code> <code>str</code> <p>Attribution method name.</p> required <code>targets</code> <code>list[int]</code> <p>List of target class indices for each sequence.</p> required <code>token_indices</code> <code>list[int]</code> <p>Each sequence's token_index list.</p> <code>None</code> <code>target_layers</code> <code>list[Module]</code> <p>Each sequence's target_layer list.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length for tokenizer.</p> <code>None</code> <code>plot</code> <code>bool</code> <p>Whether to store attributions for plotting.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Extra args for specific attribution methods.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[tuple[list[str], ndarray]]</code> <p>list[tuple[list[str], np.ndarray]]: List of (tokens list, attribution scores array) tuples.</p> Source code in <code>dnallm/inference/interpret.py</code> <pre><code>def batch_interpret(\n    self,\n    input_seqs: list[str],\n    method: str,\n    targets: list[int],\n    token_indices: list[int] | None = None,\n    target_layers: list[nn.Module] | None = None,\n    max_length: int | None = None,\n    plot: bool = True,\n    **kwargs: Any,\n) -&gt; list[tuple[list[str], np.ndarray]]:\n    \"\"\"\n    Batch interpret multiple sequences.\n\n    Args:\n        input_seqs (list[str]): List of input DNA sequences.\n        method (str): Attribution method name.\n        targets (list[int]): List of target class indices for each\n            sequence.\n        token_indices (list[int], optional): Each sequence's token_index\n            list.\n        target_layers (list[nn.Module], optional): Each sequence's\n            target_layer list.\n        max_length (int, optional): Max token length for tokenizer.\n        plot (bool): Whether to store attributions for plotting.\n        **kwargs (Any): Extra args for specific attribution methods.\n\n    Returns:\n        list[tuple[list[str], np.ndarray]]: List of (tokens list,\n            attribution scores array) tuples.\n    \"\"\"\n    results = []\n    for i, seq in enumerate(input_seqs):\n        token_index = None\n        if token_indices is not None:\n            token_index = token_indices[i]\n        target_layer = None\n        if target_layers is not None:\n            target_layer = target_layers[i]\n        tokens, scores = self.interpret(\n            input_seq=seq,\n            method=method,\n            target=targets[i],\n            token_index=token_index,\n            target_layer=target_layer,\n            max_length=max_length,\n            **kwargs,\n        )\n        results.append((tokens, scores))\n    if plot:\n        self.attributions = results\n    else:\n        self.attributions = None\n\n    return results\n</code></pre>"},{"location":"api/inference/interpret/#dnallm.inference.interpret.DNAInterpret.interpret","title":"interpret","text":"<pre><code>interpret(\n    input_seq,\n    method,\n    target,\n    token_index=None,\n    target_layer=None,\n    max_length=None,\n    plot=True,\n    **kwargs,\n)\n</code></pre> <p>A unified interpretation interface that runs the specified attribution method.</p> <p>Parameters:</p> Name Type Description Default <code>input_seq</code> <code>str</code> <p>Input DNA sequence.</p> required <code>method</code> <code>str</code> <p>Attribution method name. Supported: 'lig', 'deeplift', 'gradshap', 'occlusion', 'feature_ablation', 'layer_conductance', 'noise_tunnel'.</p> required <code>target</code> <code>int | str</code> <p>Target for attribution.</p> required <code>token_index</code> <code>int</code> <p>For 'token_cls'/'causal_lm' token position to explain.</p> <code>None</code> <code>target_layer</code> <code>Module</code> <p>For 'layer_conductance', internal layer to analyze.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length for tokenizer.</p> <code>None</code> <code>plot</code> <code>bool</code> <p>Whether to store attributions for plotting.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Extra args for specific attribution methods.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[list[str], ndarray]</code> <p>Tuple[List[str], np.ndarray]: tokens list, attribution scores array</p> Source code in <code>dnallm/inference/interpret.py</code> <pre><code>def interpret(\n    self,\n    input_seq: str,\n    method: str,\n    target: int | str,\n    token_index: int | None = None,\n    target_layer: nn.Module | None = None,\n    max_length: int | None = None,\n    plot: bool = True,\n    **kwargs: Any,\n) -&gt; tuple[list[str], np.ndarray]:\n    \"\"\"\n    A unified interpretation interface that runs\n    the specified attribution method.\n\n    Args:\n        input_seq (str): Input DNA sequence.\n        method (str): Attribution method name.\n            Supported: 'lig', 'deeplift', 'gradshap',\n            'occlusion', 'feature_ablation',\n            'layer_conductance', 'noise_tunnel'.\n        target (int | str): Target for attribution.\n        token_index (int, optional): For 'token_cls'/'causal_lm'\n            token position to explain.\n        target_layer (nn.Module, optional): For 'layer_conductance',\n            internal layer to analyze.\n        max_length (int, optional): Max token length for tokenizer.\n        plot (bool): Whether to store attributions for plotting.\n        **kwargs (Any): Extra args for specific attribution methods.\n\n    Returns:\n        Tuple[List[str], np.ndarray]: tokens list, attribution scores array\n    \"\"\"\n    method = method.lower()\n    if method == \"lig\":\n        tokens, attr_scores = self.run_lig(\n            input_seq, target, token_index, max_length=max_length, **kwargs\n        )\n    elif method == \"deeplift\":\n        tokens, attr_scores = self.run_deeplift(\n            input_seq, target, token_index, max_length=max_length, **kwargs\n        )\n    elif method == \"gradshap\":\n        tokens, attr_scores = self.run_gradshap(\n            input_seq, target, token_index, max_length=max_length, **kwargs\n        )\n    elif method == \"occlusion\":\n        tokens, attr_scores = self.run_occlusion(\n            input_seq, target, token_index, max_length=max_length, **kwargs\n        )\n    elif method == \"feature_ablation\":\n        tokens, attr_scores = self.run_feature_ablation(\n            input_seq, target, token_index, max_length=max_length, **kwargs\n        )\n    elif method == \"layer_conductance\":\n        if target_layer is None:\n            raise ValueError(\n                \"`target_layer` must be provided for \"\n                \"`layer_conductance` method.\"\n            )\n        tokens, attr_scores = self.run_layer_conductance(\n            input_seq,\n            target,\n            target_layer,\n            token_index,\n            max_length=max_length,\n            **kwargs,\n        )\n    elif method == \"noise_tunnel\":\n        tokens, attr_scores = self.run_noise_tunnel(\n            input_seq, target, token_index, max_length=max_length, **kwargs\n        )\n    else:\n        raise ValueError(\n            f\"Unknown method: {method}. \"\n            \"Supported methods: 'lig', 'deeplift', \"\n            \"'gradshap', 'occlusion', 'feature_ablation', \"\n            \"'layer_conductance', 'noise_tunnel'.\"\n        )\n    if plot:\n        self.attributions = (tokens, attr_scores)\n    else:\n        self.attributions = None\n\n    return tokens, attr_scores\n</code></pre>"},{"location":"api/inference/interpret/#dnallm.inference.interpret.DNAInterpret.plot_attributions","title":"plot_attributions","text":"<pre><code>plot_attributions(plot_type='token', **kwargs)\n</code></pre> <p>Plot the attributions using specified plot type.</p> <p>Parameters:</p> Name Type Description Default <code>plot_type</code> <code>str</code> <p>Plot type, 'token' (default), 'line', or 'multi'.</p> <code>'token'</code> <code>**kwargs</code> <p>Extra parameters for specific plotting functions.</p> <code>{}</code> Source code in <code>dnallm/inference/interpret.py</code> <pre><code>def plot_attributions(self, plot_type: str = \"token\", **kwargs):\n    \"\"\"\n    Plot the attributions using specified plot type.\n\n    Args:\n        plot_type (str): Plot type, 'token' (default), 'line', or 'multi'.\n        **kwargs: Extra parameters for specific plotting functions.\n    \"\"\"\n    if self.attributions is None:\n        raise RuntimeError(\n            \"No attributions found. \"\n            \"Please run `interpret` or `batch_interpret` \"\n            \"with `plot=True` first.\"\n        )\n\n    plot_type = plot_type.lower()\n    if isinstance(self.attributions, list):\n        if plot_type != \"multi\":\n            print(\n                \"Warning: Multiple attributions found, \"\n                \"falling back to 'multi' plot.\"\n            )\n        # Multiple sequences' attributions\n        plot = plot_attributions_multi(self.attributions, **kwargs)\n    elif plot_type == \"token\":\n        # Single sequence's token attribution plot\n        tokens, scores = self.attributions\n        plot = plot_attributions_token(tokens, scores, **kwargs)\n    elif plot_type == \"line\":\n        # Single sequence's line attribution plot\n        tokens, scores = self.attributions\n        plot = plot_attributions_line(tokens, scores, **kwargs)\n    elif plot_type == \"multi\":\n        # Multiple sequences' attributions\n        plot = plot_attributions_multi(self.attributions, **kwargs)\n    else:\n        raise ValueError(\n            f\"Unknown plot_type: {plot_type}. \"\n            \"Supported: 'token', 'line', 'multi'.\"\n        )\n    return plot\n</code></pre>"},{"location":"api/inference/interpret/#dnallm.inference.interpret.DNAInterpret.run_deeplift","title":"run_deeplift","text":"<pre><code>run_deeplift(\n    input_seq,\n    target,\n    token_index=None,\n    embedding_layer=None,\n    max_length=None,\n    **kwargs,\n)\n</code></pre> <p>Run DeepLIFT for attribution to Embedding layer.</p> Source code in <code>dnallm/inference/interpret.py</code> <pre><code>def run_deeplift(\n    self,\n    input_seq: str,\n    target: int | str,\n    token_index: int | None = None,\n    embedding_layer: nn.Module | None = None,\n    max_length: int | None = None,\n    **kwargs: Any,\n) -&gt; tuple[list[str], np.ndarray]:\n    \"\"\"\n    Run DeepLIFT for attribution to Embedding layer.\n    \"\"\"\n    wrapper = _CaptumWrapperInputIDs(self.model)\n    if embedding_layer is None:\n        embedding_layer = self._find_embedding_layer()\n\n    # Use LayerDeepLift\n    ldl = LayerDeepLift(wrapper, embedding_layer)\n\n    # Get max token length from config if not provided\n    if max_length is None:\n        max_length = self.pred_config.max_length\n\n    input_ids, attention_mask = self._get_input_tensors(\n        input_seq, max_length\n    )\n    baseline_input_ids = self._get_pad_baseline(input_ids)\n    captum_target = self._format_captum_target(target, token_index)\n\n    attributions = ldl.attribute(\n        inputs=input_ids,\n        baselines=baseline_input_ids,\n        target=captum_target,\n        additional_forward_args=(attention_mask,),\n        **kwargs,\n    )\n\n    attr_scores = (\n        attributions.sum(dim=-1).squeeze(0).cpu().detach().numpy()\n    )\n\n    tokens = self._ids_to_tokens(\n        token_ids=input_ids.squeeze(0), input_seq=input_seq\n    )\n    return tokens, attr_scores\n</code></pre>"},{"location":"api/inference/interpret/#dnallm.inference.interpret.DNAInterpret.run_feature_ablation","title":"run_feature_ablation","text":"<pre><code>run_feature_ablation(\n    input_seq,\n    target,\n    token_index=None,\n    max_length=None,\n    **kwargs,\n)\n</code></pre> <p>Run Feature Ablation (perturbation-based method).</p> <p>Parameters:</p> Name Type Description Default <code>input_seq</code> <code>str</code> <p>Input DNA sequence.</p> required <code>target</code> <code>int | str</code> <p>Target for attribution.</p> required <code>token_index</code> <code>int</code> <p>Token position to explain.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length for tokenizer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for FeatureAblation.attribute.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[list[str], ndarray]</code> <p>Tuple[List[str], np.ndarray]: tokens list, attribution scores array</p> Source code in <code>dnallm/inference/interpret.py</code> <pre><code>def run_feature_ablation(\n    self,\n    input_seq: str,\n    target: int | str,\n    token_index: int | None = None,\n    max_length: int | None = None,\n    **kwargs: Any,\n) -&gt; tuple[list[str], np.ndarray]:\n    \"\"\"\n    Run Feature Ablation (perturbation-based method).\n\n    Args:\n        input_seq (str): Input DNA sequence.\n        target (int | str): Target for attribution.\n        token_index (int, optional): Token position to explain.\n        max_length (int, optional): Max token length for tokenizer.\n        **kwargs (Any): Additional arguments for FeatureAblation.attribute.\n\n    Returns:\n        Tuple[List[str], np.ndarray]: tokens list, attribution scores array\n    \"\"\"\n    # 1. Use wrapper that accepts input_ids\n    wrapper = _CaptumWrapperInputIDs(self.model)\n    ablation = FeatureAblation(wrapper)\n\n    # Get max token length from config if not provided\n    if max_length is None:\n        max_length = self.pred_config.max_length\n\n    # 2. Prepare inputs and baselines\n    # (FeatureAblation requires a tensor baseline)\n    input_ids, attention_mask = self._get_input_tensors(\n        input_seq, max_length\n    )\n    baselines = self._get_pad_baseline(input_ids)\n\n    # 3. Format Captum target\n    captum_target = self._format_captum_target(target, token_index)\n\n    # 4. FeatureAblation requires feature_mask to define features\n    # (default one feature per token)\n    # Shape: (batch_size, num_features) -&gt; (1, seq_len)\n    feature_mask = (\n        torch.arange(input_ids.shape[1]).unsqueeze(0).to(self.device)\n    )\n\n    # 5. Compute attributions\n    attributions = ablation.attribute(\n        inputs=input_ids,\n        target=captum_target,\n        additional_forward_args=(attention_mask,),\n        baselines=baselines,\n        feature_mask=feature_mask,\n        **kwargs,\n    )\n\n    # 6. Process results\n    # Shape: (batch, seq_len) -&gt; (seq_len)\n    attr_scores = attributions.squeeze(0).cpu().detach().numpy()\n\n    tokens = self._ids_to_tokens(\n        token_ids=input_ids.squeeze(0), input_seq=input_seq\n    )\n    return tokens, attr_scores\n</code></pre>"},{"location":"api/inference/interpret/#dnallm.inference.interpret.DNAInterpret.run_gradshap","title":"run_gradshap","text":"<pre><code>run_gradshap(\n    input_seq,\n    target,\n    token_index=None,\n    max_length=None,\n    n_samples=5,\n    **kwargs,\n)\n</code></pre> <p>Run GradientSHAP at the Embedding layer. Attention: GradientSHAP can be slow.</p> <p>Parameters:</p> Name Type Description Default <code>input_seq</code> <code>str</code> <p>Input DNA sequence.</p> required <code>target</code> <code>int | str</code> <p>Target for attribution.</p> required <code>token_index</code> <code>int</code> <p>Token position to explain.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length for tokenizer.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of samples to draw from the baseline.</p> <code>5</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for GradientShap.attribute.</p> <code>{}</code> Source code in <code>dnallm/inference/interpret.py</code> <pre><code>def run_gradshap(\n    self,\n    input_seq: str,\n    target: int | str,\n    token_index: int | None = None,\n    max_length: int | None = None,\n    n_samples: int = 5,\n    **kwargs: Any,\n) -&gt; tuple[list[str], np.ndarray]:\n    \"\"\"\n    Run GradientSHAP at the Embedding layer.\n    Attention: GradientSHAP can be slow.\n\n    Args:\n        input_seq (str): Input DNA sequence.\n        target (int | str): Target for attribution.\n        token_index (int, optional): Token position to explain.\n        max_length (int, optional): Max token length for tokenizer.\n        n_samples (int): Number of samples to draw from the baseline.\n        **kwargs (Any): Additional arguments for GradientShap.attribute.\n    \"\"\"\n    # 1. Use wrapper that accepts inputs_embeds\n    wrapper = _CaptumWrapperInputEmbeds(self.model)\n\n    # 2. Use basic GradientShap (non-layer version)\n    gs = GradientShap(wrapper)\n\n    # Get max token length from config if not provided\n    if max_length is None:\n        max_length = self.pred_config.max_length\n\n    # 3. Prepare Embeddings (the same as LayerConductance)\n    input_ids, attention_mask = self._get_input_tensors(\n        input_seq, max_length\n    )\n    baseline_input_ids = self._get_pad_baseline(input_ids)\n    embedding_layer = self._find_embedding_layer()\n    with torch.no_grad():\n        inputs_embeds = embedding_layer(input_ids)\n        baseline_embeds = embedding_layer(baseline_input_ids)\n\n    # 4. Format Captum target\n    captum_target = self._format_captum_target(target, token_index)\n\n    # 5. Call attribute at the embeds level\n    attributions = gs.attribute(\n        inputs=inputs_embeds,  # Pass in embeds\n        baselines=baseline_embeds,  # Pass in baseline embeds\n        n_samples=n_samples,\n        target=captum_target,\n        additional_forward_args=(attention_mask,),\n        **kwargs,\n    )\n\n    # 6. Process results (shape: (batch, seq_len, embed_dim) -&gt; (seq_len))\n    attr_scores = (\n        attributions.sum(dim=-1).squeeze(0).cpu().detach().numpy()\n    )\n\n    tokens = self._ids_to_tokens(\n        token_ids=input_ids.squeeze(0), input_seq=input_seq\n    )\n    return tokens, attr_scores\n</code></pre>"},{"location":"api/inference/interpret/#dnallm.inference.interpret.DNAInterpret.run_layer_conductance","title":"run_layer_conductance","text":"<pre><code>run_layer_conductance(\n    input_seq,\n    target,\n    target_layer,\n    token_index=None,\n    max_length=None,\n    **kwargs,\n)\n</code></pre> <p>Run Layer Conductance for attribution to an internal layer.</p> <p>Important: 1. This method assumes self.model.forward supports 'inputs_embeds'. 2. You must manually pass in 'target_layer'.</p> <p>Parameters:</p> Name Type Description Default <code>input_seq</code> <code>str</code> <p>Input DNA sequence.</p> required <code>target</code> <code>int | str</code> <p>Target for attribution.</p> required <code>target_layer</code> <code>Module</code> <p>Internal layer to analyze. For example: <code>model.bert.encoder.layer[-1]</code>.</p> required <code>token_index</code> <code>int</code> <p>Token position to explain.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length for tokenizer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for LayerConductance.attribute.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[list[str], ndarray]</code> <p>Tuple[List[str], np.ndarray]: tokens list, attribution scores array</p> Source code in <code>dnallm/inference/interpret.py</code> <pre><code>def run_layer_conductance(\n    self,\n    input_seq: str,\n    target: int | str,\n    target_layer: nn.Module,\n    token_index: int | None = None,\n    max_length: int | None = None,\n    **kwargs: Any,\n) -&gt; tuple[list[str], np.ndarray]:\n    \"\"\"\n    Run Layer Conductance for attribution to an internal layer.\n\n    Important:\n    1. This method assumes self.model.forward supports 'inputs_embeds'.\n    2. You must manually pass in 'target_layer'.\n\n    Args:\n        input_seq (str): Input DNA sequence.\n        target (int | str): Target for attribution.\n        target_layer (nn.Module): Internal layer to analyze.\n            For example: `model.bert.encoder.layer[-1]`.\n        token_index (int, optional): Token position to explain.\n        max_length (int, optional): Max token length for tokenizer.\n        **kwargs (Any): Additional arguments for\n            LayerConductance.attribute.\n\n    Returns:\n        Tuple[List[str], np.ndarray]: tokens list, attribution scores array\n    \"\"\"\n    # 1. Use wrapper that accepts inputs_embeds\n    wrapper = _CaptumWrapperInputEmbeds(self.model)\n    lc = LayerConductance(wrapper, target_layer)\n\n    # Get max token length from config if not provided\n    if max_length is None:\n        max_length = self.pred_config.max_length\n\n    # 2. Prepare inputs (input_ids)\n    input_ids, attention_mask = self._get_input_tensors(\n        input_seq, max_length\n    )\n    baseline_input_ids = self._get_pad_baseline(input_ids)\n\n    # 3. Manually convert IDs to Embeddings\n    embedding_layer = self._find_embedding_layer()\n    with torch.no_grad():\n        inputs_embeds = embedding_layer(input_ids)\n        baseline_embeds = embedding_layer(baseline_input_ids)\n\n    # 4. Format Captum target\n    captum_target = self._format_captum_target(target, token_index)\n\n    # 5. Compute attributions (inputs are embeds)\n    attributions_tuple = lc.attribute(\n        inputs=inputs_embeds,  # &lt;--- Pass in embeds\n        baselines=baseline_embeds,  # &lt;--- Pass in embeds\n        target=captum_target,\n        additional_forward_args=(attention_mask,),\n        **kwargs,\n    )\n\n    # 6. Process results\n    if isinstance(attributions_tuple, tuple):\n        attributions = attributions_tuple[0]\n    else:\n        attributions = attributions_tuple\n    # Shape: (batch, seq_len, hidden_dim) -&gt; (seq_len)\n    attr_scores = attributions.sum(dim=-1).squeeze(0)\n    attr_scores = attr_scores.cpu().detach().numpy()\n\n    tokens = self._ids_to_tokens(\n        token_ids=input_ids.squeeze(0), input_seq=input_seq\n    )\n    return tokens, attr_scores\n</code></pre>"},{"location":"api/inference/interpret/#dnallm.inference.interpret.DNAInterpret.run_lig","title":"run_lig","text":"<pre><code>run_lig(\n    input_seq,\n    target,\n    token_index=None,\n    embedding_layer=None,\n    max_length=None,\n    **kwargs,\n)\n</code></pre> <p>Run Layer Integrated Gradients (LIG) for attribution to the Embedding layer. This is the most recommended method for k-mer importance analysis.</p> <p>Parameters:</p> Name Type Description Default <code>input_seq</code> <code>str</code> <p>input DNA sequence.</p> required <code>target</code> <code>int</code> <p>Target for attribution.           - For 'seq_cls': target index (e.g., 1).           - for 'token_cls': target type index                 (e.g., 2 for 'Promoter').           - For 'causal_lm': Target Token ID                  (e.g., 8 for 'G').</p> required <code>task_type</code> <code>str</code> <p>Task type</p> required <code>token_index</code> <code>int</code> <p>for 'token_cls'/'causal_lm' Token position to explain.</p> <code>None</code> <code>embedding_layer</code> <code>Module</code> <p>Specific Embedding layer. Auto-detected if None.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length for tokenizer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments for captum.attr.LayerIntegratedGradients.attribute (for example: internal_batch_size=4).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[list[str], ndarray]</code> <p>Tuple[List[str], np.ndarray]: tokens list, attribution scores array</p> Source code in <code>dnallm/inference/interpret.py</code> <pre><code>def run_lig(\n    self,\n    input_seq: str,\n    target: int | str,\n    token_index: int | None = None,\n    embedding_layer: nn.Module | None = None,\n    max_length: int | None = None,\n    **kwargs: Any,\n) -&gt; tuple[list[str], np.ndarray]:\n    \"\"\"\n    Run Layer Integrated Gradients (LIG) for\n    attribution to the Embedding layer.\n    This is the most recommended method for k-mer importance analysis.\n\n    Args:\n        input_seq (str): input DNA sequence.\n        target (int): Target for attribution.\n                      - For 'seq_cls': target index (e.g., 1).\n                      - for 'token_cls': target type index\n                            (e.g., 2 for 'Promoter').\n                      - For 'causal_lm': Target *Token ID*\n                             (e.g., 8 for 'G').\n        task_type (str): Task type\n        token_index (int, optional): for 'token_cls'/'causal_lm'\n            Token position to explain.\n        embedding_layer (nn.Module, optional): Specific Embedding layer.\n            Auto-detected if None.\n        max_length (int): Max token length for tokenizer.\n        **kwargs (Any): Extra arguments for\n            captum.attr.LayerIntegratedGradients.attribute\n            (for example: internal_batch_size=4).\n\n    Returns:\n        Tuple[List[str], np.ndarray]: tokens list, attribution scores array\n    \"\"\"\n    # 1. Use wrapper that accepts input_ids\n    wrapper = _CaptumWrapperInputIDs(self.model)\n\n    # 2. Find or use specified Embedding layer\n    if embedding_layer is None:\n        embedding_layer = self._find_embedding_layer()\n\n    lig = LayerIntegratedGradients(wrapper, embedding_layer)\n\n    # Get max token length from config if not provided\n    if max_length is None:\n        max_length = self.pred_config.max_length\n\n    # 3. Prepare inputs and baselines\n    input_ids, attention_mask = self._get_input_tensors(\n        input_seq, max_length\n    )\n    baseline_input_ids = self._get_pad_baseline(input_ids)\n\n    # 4. Format Captum target\n    captum_target = self._format_captum_target(target, token_index)\n\n    # 5. Compute attributions\n    attributions = lig.attribute(\n        inputs=input_ids,\n        baselines=baseline_input_ids,\n        target=captum_target,\n        additional_forward_args=(attention_mask,),\n        **kwargs,\n    )\n\n    # 6. Process results\n    # Shape: (batch, seq_len, embed_dim) -&gt; (seq_len)\n    attr_scores = attributions.sum(dim=-1).squeeze(0)\n    attr_scores = attr_scores.cpu().detach().numpy()\n\n    tokens = self._ids_to_tokens(\n        token_ids=input_ids.squeeze(0), input_seq=input_seq\n    )\n    return tokens, attr_scores\n</code></pre>"},{"location":"api/inference/interpret/#dnallm.inference.interpret.DNAInterpret.run_noise_tunnel","title":"run_noise_tunnel","text":"<pre><code>run_noise_tunnel(\n    input_seq,\n    target,\n    base_method,\n    token_index=None,\n    max_length=None,\n    nt_type=\"smoothgrad\",\n    nt_samples=5,\n    nt_stdevs=0.1,\n    **kwargs,\n)\n</code></pre> <p>Run NoiseTunnel (e.g., SmoothGrad) for smoother attributions. This will run at the 'inputs_embeds' level to avoid type conflicts with 'nn.Embedding'.</p> <p>Parameters:</p> Name Type Description Default <code>input_seq</code> <code>str</code> <p>Input DNA sequence.</p> required <code>target</code> <code>int | str</code> <p>Target for attribution.</p> required <code>base_method</code> <code>str</code> <p>The base attribution method to use. Supported: 'lig' (IntegratedGradients), 'deeplift' (DeepLift), 'gradshap' (GradientShap).</p> required <code>token_index</code> <code>int</code> <p>Token position to explain.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length for tokenizer.</p> <code>None</code> <code>nt_type</code> <code>str</code> <p>'smoothgrad' (default), 'smoothgrad_sq' or 'vargrad'.</p> <code>'smoothgrad'</code> <code>nt_samples</code> <code>int</code> <p>The number of noise samples.</p> <code>5</code> <code>nt_stdevs</code> <code>float</code> <p>The standard deviation of the noise.</p> <code>0.1</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for NoiseTunnel.</p> <code>{}</code> Source code in <code>dnallm/inference/interpret.py</code> <pre><code>def run_noise_tunnel(\n    self,\n    input_seq: str,\n    target: int | str,\n    base_method: str,\n    token_index: int | None = None,\n    max_length: int | None = None,\n    nt_type: str = \"smoothgrad\",\n    nt_samples: int = 5,\n    nt_stdevs: float = 0.1,\n    **kwargs: Any,\n) -&gt; tuple[list[str], np.ndarray]:\n    \"\"\"\n    Run NoiseTunnel (e.g., SmoothGrad) for smoother attributions.\n    This will run at the 'inputs_embeds' level to avoid\n    type conflicts with 'nn.Embedding'.\n\n    Args:\n        input_seq (str): Input DNA sequence.\n        target (int | str): Target for attribution.\n        base_method (str): The base attribution method to use.\n            Supported: 'lig' (IntegratedGradients), 'deeplift' (DeepLift),\n            'gradshap' (GradientShap).\n        token_index (int, optional): Token position to explain.\n        max_length (int, optional): Max token length for tokenizer.\n        nt_type (str): 'smoothgrad' (default), 'smoothgrad_sq'\n            or 'vargrad'.\n        nt_samples (int): The number of noise samples.\n        nt_stdevs (float): The standard deviation of the noise.\n        **kwargs (Any): Additional arguments for NoiseTunnel.\n    \"\"\"\n    print(\n        f\"Running NoiseTunnel ({nt_type}) \"\n        f\"with base method: {base_method}...\"\n    )\n\n    # 1. Use wrapper that accepts inputs_embeds\n    wrapper = _CaptumWrapperInputEmbeds(self.model)\n\n    # 2. Select base attribution method (non-Layer version)\n    if base_method.lower() == \"lig\":\n        attr_method = IntegratedGradients(wrapper)\n    elif base_method.lower() == \"deeplift\":\n        attr_method = DeepLift(wrapper)\n    elif base_method.lower() == \"gradshap\":\n        attr_method = GradientShap(wrapper)\n    else:\n        raise ValueError(\n            f\"Unknown base_method: {base_method}. \"\n            \"Supported: 'lig', 'deeplift', 'gradshap'\"\n        )\n\n    # 3. Wrap with NoiseTunnel\n    nt = NoiseTunnel(attr_method)\n\n    # Get max token length from config if not provided\n    if max_length is None:\n        max_length = self.pred_config.max_length\n\n    # 4. Prepare Embeddings (same as LayerConductance)\n    input_ids, attention_mask = self._get_input_tensors(\n        input_seq, max_length\n    )\n    baseline_input_ids = self._get_pad_baseline(input_ids)\n    embedding_layer = self._find_embedding_layer()\n    with torch.no_grad():\n        inputs_embeds = embedding_layer(input_ids)\n        baseline_embeds = embedding_layer(baseline_input_ids)\n\n    # 5. Format Captum target\n    captum_target = self._format_captum_target(target, token_index)\n\n    # 6. Prepare attribution parameters\n    attr_kwargs = {\n        \"target\": captum_target,\n        \"additional_forward_args\": (attention_mask,),\n        \"nt_type\": nt_type,\n        \"nt_samples\": nt_samples,\n        \"stdevs\": nt_stdevs,\n        **kwargs,\n    }\n\n    # 7. Call attribute method\n    # GradientShap need n_samples and baselines (distribution)\n    if base_method.lower() == \"gradshap\":\n        attributions = nt.attribute(\n            inputs=inputs_embeds,\n            baselines=baseline_embeds,\n            n_samples=nt_samples,  # gradshap own n_samples\n            **attr_kwargs,\n        )\n    else:  # LIG or DeepLIFT\n        attributions = nt.attribute(\n            inputs=inputs_embeds, baselines=baseline_embeds, **attr_kwargs\n        )\n\n    # 8. Process results\n    # Shape: (batch, seq_len, hidden_dim) -&gt; (seq_len)\n    attr_scores = (\n        attributions.sum(dim=-1).squeeze(0).cpu().detach().numpy()\n    )\n\n    tokens = self._ids_to_tokens(\n        token_ids=input_ids.squeeze(0), input_seq=input_seq\n    )\n    return tokens, attr_scores\n</code></pre>"},{"location":"api/inference/interpret/#dnallm.inference.interpret.DNAInterpret.run_occlusion","title":"run_occlusion","text":"<pre><code>run_occlusion(\n    input_seq,\n    target,\n    token_index=None,\n    max_length=None,\n    sliding_window_shapes=(1,),\n    **kwargs,\n)\n</code></pre> <p>Run Occlusion (perturbation-based method). Attention: This method can be very slow.</p> <p>Parameters:</p> Name Type Description Default <code>input_seq</code> <code>str</code> <p>Input DNA sequence.</p> required <code>target</code> <code>int | str</code> <p>Target for attribution.</p> required <code>token_index</code> <code>int</code> <p>Token position to explain.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length for tokenizer.</p> <code>None</code> <code>sliding_window_shapes</code> <code>tuple[int]</code> <p>Size of the occlusion window. (1,) means occluding 1 token at a time.</p> <code>(1,)</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for Occlusion.attribute.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[list[str], ndarray]</code> <p>Tuple[List[str], np.ndarray]: tokens list, attribution scores array</p> Source code in <code>dnallm/inference/interpret.py</code> <pre><code>def run_occlusion(\n    self,\n    input_seq: str,\n    target: int | str,\n    token_index: int | None = None,\n    max_length: int | None = None,\n    sliding_window_shapes: tuple[int] = (1,),\n    **kwargs: Any,\n) -&gt; tuple[list[str], np.ndarray]:\n    \"\"\"\n    Run Occlusion (perturbation-based method).\n    Attention: This method can be very slow.\n\n    Args:\n        input_seq (str): Input DNA sequence.\n        target (int | str): Target for attribution.\n        token_index (int, optional): Token position to explain.\n        max_length (int, optional): Max token length for tokenizer.\n        sliding_window_shapes (tuple[int]): Size of the occlusion window.\n            (1,) means occluding 1 token at a time.\n        **kwargs (Any): Additional arguments for Occlusion.attribute.\n\n    Returns:\n        Tuple[List[str], np.ndarray]: tokens list, attribution scores array\n    \"\"\"\n    # 1. Use wrapper that accepts input_ids\n    wrapper = _CaptumWrapperInputIDs(self.model)\n    occlusion = Occlusion(wrapper)\n\n    # Get max token length from config if not provided\n    if max_length is None:\n        max_length = self.pred_config.max_length\n\n    # 2. Prepare inputs\n    input_ids, attention_mask = self._get_input_tensors(\n        input_seq, max_length\n    )\n\n    # 3. Occlusion baseline is a single PAD token ID (scalar)\n    baselines = self.pad_token_id\n\n    # 4. Format Captum target\n    captum_target = self._format_captum_target(target, token_index)\n\n    # 5. Compute attributions\n    attributions = occlusion.attribute(\n        inputs=input_ids,\n        sliding_window_shapes=sliding_window_shapes,\n        target=captum_target,\n        additional_forward_args=(attention_mask,),\n        baselines=baselines,\n        **kwargs,\n    )\n\n    # 6. Process results\n    # Shape: (batch, seq_len) -&gt; (seq_len)\n    attr_scores = attributions.squeeze(0).cpu().detach().numpy()\n\n    tokens = self._ids_to_tokens(\n        token_ids=input_ids.squeeze(0), input_seq=input_seq\n    )\n    return tokens, attr_scores\n</code></pre>"},{"location":"api/inference/interpret/#dnallm.inference.interpret-functions","title":"Functions","text":""},{"location":"api/inference/mutagenesis/","title":"inference/mutagenesis API","text":""},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis","title":"dnallm.inference.mutagenesis","text":"<p>In Silico Mutagenesis Analysis Module.</p> <p>This module provides tools for evaluating the impact of sequence mutations on model predictions, including single nucleotide polymorphisms (     SNPs),     deletions,     insertions,     and other sequence variations.</p>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis-classes","title":"Classes","text":""},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis","title":"Mutagenesis","text":"<pre><code>Mutagenesis(model, tokenizer, config)\n</code></pre> <p>Class for evaluating in silico mutagenesis.</p> <p>This class provides methods to analyze how sequence mutations affect model     predictions,         including single base substitutions, deletions, and         insertions. It can be used to         identify important positions in DNA sequences and         understand model interpretability.</p> <pre><code>Attributes:\n    model: Fine-tuned model for prediction\n    tokenizer: Tokenizer for the model\n            config: Configuration object containing task settings and\n        inference parameters\n    sequences: Dictionary containing original and mutated sequences\n    dataloader: DataLoader for batch processing of sequences\n</code></pre> <p>Initialize Mutagenesis class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Fine-tuned model for making predictions</p> required <code>tokenizer</code> <code>Any</code> <p>Tokenizer for encoding DNA sequences</p> required <code>config</code> <code>dict</code> <p>Configuration object containing task settings and inference parameters</p> required Source code in <code>dnallm/inference/mutagenesis.py</code> <pre><code>def __init__(self, model: Any, tokenizer: Any, config: dict):\n    \"\"\"Initialize Mutagenesis class.\n\n    Args:\n        model: Fine-tuned model for making predictions\n        tokenizer: Tokenizer for encoding DNA sequences\n        config: Configuration object containing task settings and\n            inference parameters\n    \"\"\"\n\n    self.model = model\n    self.tokenizer = tokenizer\n    self.config = config\n    self.sequences = None\n</code></pre>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis-functions","title":"Functions","text":""},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis.clm_evaluate","title":"clm_evaluate","text":"<pre><code>clm_evaluate(return_sum=True)\n</code></pre> <p>Calculate sequence log-probability using causal language modeling.</p> <p>This method computes the log-probability of each sequence under a causal language model by summing the log probabilities of each token given its preceding context.</p> <p>Returns:</p> Type Description <code>list[float]</code> <p>List of log-probabilities for each sequence</p> Source code in <code>dnallm/inference/mutagenesis.py</code> <pre><code>@torch.no_grad()\ndef clm_evaluate(self, return_sum: bool = True) -&gt; list[float]:\n    \"\"\"Calculate sequence log-probability using causal language modeling.\n\n    This method computes the log-probability of each sequence under a\n    causal language model by summing the log probabilities of each token\n    given its preceding context.\n\n    Returns:\n        List of log-probabilities for each sequence\n    \"\"\"\n    all_logprobs = []\n    model = self.model\n    tokenizer = self.tokenizer\n    device = self.get_model_device(model)\n    if len(self.sequences[\"sequence\"]) &gt; 1:\n        input_data = tqdm(self.sequences[\"sequence\"], desc=\"Inferring\")\n    else:\n        input_data = self.sequences[\"sequence\"]\n    for seq in input_data:\n        toks = tokenizer(\n            seq, return_tensors=\"pt\", add_special_tokens=True\n        ).to(device)\n        input_ids = toks[\"input_ids\"]\n        outputs = model(**toks)\n        logits = outputs.logits  # (1, L, V)\n\n        # shift for causal LM: predict token t given tokens &lt; t\n        shift_logits = logits[:, :-1, :].contiguous()\n        shift_labels = input_ids[:, 1:].contiguous()\n        log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n        token_logps = log_probs.gather(\n            -1, shift_labels.unsqueeze(-1)\n        ).squeeze(-1)  # (1, L-1)\n        if return_sum:\n            seq_logp = float(token_logps.sum().item())\n        else:\n            # Get all token logp\n            seq_logp = [\n                float(token_logps[0, i].item())\n                for i in range(len(token_logps[0]))\n            ]\n        all_logprobs.append(seq_logp)\n    return all_logprobs\n</code></pre>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    score_type=\"embedding\",\n    strategy=\"last\",\n    do_pred=False,\n    reduce_hidden_states=True,\n)\n</code></pre> <p>Evaluate the impact of mutations on model predictions.</p> <p>This method runs predictions on all mutated sequences and compares them with the original sequence to calculate mutation effects.</p> <p>Parameters:</p> Name Type Description Default <code>score_type</code> <code>str</code> <p>Type of score to compute: \"embedding\": Use embedding-based scoring \"logits\": Use logits-based scoring \"probability\": Use probability-based scoring</p> <code>'embedding'</code> <code>strategy</code> <code>str | int</code> <p>Strategy for selecting the score from the log fold                change: \"first\": Use the first log fold change \"last\": Use the last log fold change \"sum\": Use the sum of log fold changes \"mean\": Use the mean of log fold changes \"max\": Use the index of the maximum raw score to select                    the log fold change int: Use the log fold change at the specified index</p> <code>'last'</code> <code>do_pred</code> <code>bool</code> <p>Whether to perform prediction (if False, only data is                prepared)</p> <code>False</code> <code>reduce_hidden_states</code> <code>bool</code> <p>Whether to reduce hidden states when using                embedding task</p> <code>True</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>Dictionary containing predictions and metadata for all sequences:</p> <code>list[dict]</code> <ul> <li>'raw': Original sequence predictions and metadata</li> </ul> <code>list[dict]</code> <ul> <li>mutation names: Individual mutation results with scores and log fold changes</li> </ul> Source code in <code>dnallm/inference/mutagenesis.py</code> <pre><code>def evaluate(\n    self,\n    score_type: str = \"embedding\",\n    strategy: str | int = \"last\",\n    do_pred: bool = False,\n    reduce_hidden_states: bool = True,\n) -&gt; list[dict]:\n    \"\"\"Evaluate the impact of mutations on model predictions.\n\n    This method runs predictions on all mutated sequences and compares them\n    with the original sequence to calculate mutation effects.\n\n    Args:\n        score_type: Type of score to compute:\n            \"embedding\": Use embedding-based scoring\n            \"logits\": Use logits-based scoring\n            \"probability\": Use probability-based scoring\n        strategy: Strategy for selecting the score from the log fold\\\n            change:\n            \"first\": Use the first log fold change\n            \"last\": Use the last log fold change\n            \"sum\": Use the sum of log fold changes\n            \"mean\": Use the mean of log fold changes\n            \"max\": Use the index of the maximum raw score to select\\\n                the log fold change\n            int: Use the log fold change at the specified index\n        do_pred: Whether to perform prediction (if False, only data is\\\n            prepared)\n        reduce_hidden_states: Whether to reduce hidden states when using\\\n            embedding task\n\n    Returns:\n        Dictionary containing predictions and metadata for all sequences:\n        - 'raw': Original sequence predictions and metadata\n        - mutation names: Individual mutation results with scores and log\n            fold changes\n    \"\"\"\n    # Load predictor\n    inference_engine = self.get_inference_engine(\n        self.model, self.tokenizer\n    )\n    task_type = self.config[\"task\"].task_type\n    # Do prediction\n    all_predictions = {}\n    if task_type == \"embedding\":\n        scores = inference_engine.scoring(\n            self.dataloader,\n            score_type=score_type,\n            reduce_hidden_states=reduce_hidden_states,\n            reduce_method=strategy,\n        )\n        raw_pred = scores[0][\"Score\"]\n        mut_preds = [score[\"Score\"] for score in scores[1:]]\n    else:\n        if self.config[\"task\"].task_type == \"mask\":\n            scores = self.mlm_evaluate()\n        elif self.config[\"task\"].task_type == \"generation\":\n            scores = self.clm_evaluate()\n        else:\n            outputs = inference_engine.batch_infer(\n                self.dataloader,\n                do_pred=do_pred,\n                return_dict=False,\n            )\n            scores = outputs[1] if do_pred else outputs[0]\n        scores = scores[0] if isinstance(scores, tuple) else scores\n        # Get the raw predictions\n        raw_pred = (\n            scores[0].numpy()\n            if isinstance(scores, torch.Tensor)\n            else scores[0]\n        )\n        # Get the mutated predictions\n        mut_preds = (\n            scores[1:].numpy()\n            if isinstance(scores, torch.Tensor)\n            else scores[1:]\n        )\n\n    # Calculate scores\n    def get_score(values: np.ndarray, raw_score: np.ndarray = None):\n        # Get final score\n        if strategy == \"first\":\n            score = values[0]\n        elif strategy == \"last\":\n            score = values[-1]\n        elif strategy == \"sum\":\n            score = np.sum(values)\n        elif strategy == \"mean\":\n            score = np.mean(values)\n        elif strategy == \"max\":\n            idx = raw_score.index(max(raw_score))\n            score = values[idx]\n        elif isinstance(strategy, int):\n            score = values[strategy]\n        else:\n            score = np.mean(values)\n        if np.isnan(score):\n            score = 0.0\n        return score\n\n    for i, mut_pred in tqdm(\n        enumerate(mut_preds), desc=\"Evaluating mutations\"\n    ):\n        # Get the mutated name\n        mut_name = self.sequences[\"name\"][i + 1]\n        # Get the mutated sequence\n        mut_seq = self.sequences[\"sequence\"][i + 1]\n        # Compare the predictions\n        raw_score, mut_score, logfc, diff = self.pred_comparison(\n            raw_pred, mut_pred\n        )\n        # Store the results\n        if \"raw\" not in all_predictions:\n            all_predictions[\"raw\"] = {\n                \"sequence\": self.sequences[\"sequence\"][0],\n                \"pred\": raw_score,\n                \"logfc\": np.zeros(len(raw_score)),\n                \"diff\": np.zeros(len(raw_score)),\n                \"score\": 0.0,\n                \"logits\": get_score(raw_score),\n            }\n        all_predictions[mut_name] = {\n            \"sequence\": mut_seq,\n            \"pred\": mut_score,\n            \"logfc\": logfc,\n            \"diff\": diff,\n        }\n        all_predictions[mut_name][\"score\"] = get_score(logfc, raw_score)\n        all_predictions[mut_name][\"score2\"] = get_score(diff, raw_score)\n        all_predictions[mut_name][\"logits\"] = get_score(\n            mut_score, raw_score\n        )\n\n    return all_predictions\n</code></pre>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis.find_hotspots","title":"find_hotspots","text":"<pre><code>find_hotspots(\n    preds,\n    strategy=\"maxabs\",\n    window_size=10,\n    percentile_threshold=90.0,\n)\n</code></pre> <p>Identify hotspot regions from base-level importance scores.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Dict[str, Dict]</code> <p>The raw output from the ISM experiment.</p> required <code>strategy</code> <code>str</code> <p>Strategy to aggregate scores at each position.             'maxabs': Use the score of the mutation with                       the max absolute effect.             'mean': Use the mean of all mutation scores.</p> <code>'maxabs'</code> <code>window_size</code> <code>int</code> <p>The size of the sliding window to find hotspots.</p> <code>10</code> <code>percentile_threshold</code> <code>float</code> <p>The percentile of window scores to be                         considered a hotspot.</p> <code>90.0</code> <p>Returns:</p> Type Description <code>list[tuple[int, int]]</code> <p>List[Tuple[int, int]]: A list of (start, end) tuples                    for each hotspot.</p> Source code in <code>dnallm/inference/mutagenesis.py</code> <pre><code>def find_hotspots(\n    self,\n    preds: dict[str, dict],\n    strategy=\"maxabs\",\n    window_size: int = 10,\n    percentile_threshold: float = 90.0,\n) -&gt; list[tuple[int, int]]:\n    \"\"\"\n    Identify hotspot regions from base-level importance scores.\n\n    Args:\n        preds (Dict[str, Dict]): The raw output from the ISM experiment.\n        strategy (str): Strategy to aggregate scores at each position.\n                        'maxabs': Use the score of the mutation with\n                                  the max absolute effect.\n                        'mean': Use the mean of all mutation scores.\n        window_size (int): The size of the sliding window to find hotspots.\n        percentile_threshold (float): The percentile of window scores to be\n                                    considered a hotspot.\n\n    Returns:\n        List[Tuple[int, int]]: A list of (start, end) tuples\n                               for each hotspot.\n    \"\"\"\n    # We care about the magnitude of change, so use absolute scores\n    base_scores = self.process_ism_data(preds, strategy=strategy)\n    abs_scores = pd.Series(np.abs(base_scores))\n\n    # Calculate rolling average of scores\n    rolling_mean = abs_scores.rolling(\n        window=window_size, center=True, min_periods=1\n    ).mean()\n\n    # Determine the score threshold for a hotspot\n    threshold = np.percentile(rolling_mean, percentile_threshold)\n\n    # Find regions above the threshold\n    hotspot_mask = rolling_mean &gt;= threshold\n\n    # Find contiguous blocks of 'True'\n    hotspots = []\n    start = -1\n    for i, is_hot in enumerate(hotspot_mask):\n        if is_hot and start == -1:\n            start = i\n        elif not is_hot and start != -1:\n            hotspots.append((start, i))\n            start = -1\n    if start != -1:\n        hotspots.append((start, len(hotspot_mask)))\n\n    # Return list of hotspot regions with window size\n    hotspots_regioned = []\n    for i, (start, end) in enumerate(hotspots):\n        mid = (start + end) // 2\n        window_start = min(max(0, mid - window_size // 2), start)\n        window_end = max(\n            min(mid + window_size // 2, len(base_scores)), end\n        )\n        # if the window is within last detected hotspot,\n        # skip the current one\n        if i &gt; 0:\n            prev_start, prev_end = hotspots_regioned[-1]\n            if window_start &gt;= prev_start and window_end &lt;= prev_end:\n                continue\n        hotspots_regioned.append((window_start, window_end))\n    self.hotspots = hotspots_regioned\n\n    return hotspots_regioned\n</code></pre>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis.get_inference_engine","title":"get_inference_engine","text":"<pre><code>get_inference_engine(model, tokenizer)\n</code></pre> <p>Create an inference engine object for the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The model to be used for inference</p> required <code>tokenizer</code> <code>Any</code> <p>The tokenizer to be used for encoding sequences</p> required <p>Returns:</p> Name Type Description <code>DNAInference</code> <code>DNAInference</code> <p>The inference engine object configured with the given model and tokenizer</p> Source code in <code>dnallm/inference/mutagenesis.py</code> <pre><code>def get_inference_engine(self, model: Any, tokenizer: Any) -&gt; DNAInference:\n    \"\"\"Create an inference engine object for the model.\n\n    Args:\n        model: The model to be used for inference\n        tokenizer: The tokenizer to be used for encoding sequences\n\n    Returns:\n        DNAInference: The inference engine object configured with the given\n            model and tokenizer\n    \"\"\"\n\n    inference_engine = DNAInference(\n        model=model, tokenizer=tokenizer, config=self.config\n    )\n\n    return inference_engine\n</code></pre>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis.get_model_device","title":"get_model_device","text":"<pre><code>get_model_device(model)\n</code></pre> <p>Get the device of the model.</p> <p>Returns:</p> Type Description <code>device</code> <p>torch.device: The device on which the model is located</p> Source code in <code>dnallm/inference/mutagenesis.py</code> <pre><code>def get_model_device(self, model) -&gt; torch.device:\n    \"\"\"Get the device of the model.\n\n    Returns:\n        torch.device: The device on which the model is located\n    \"\"\"\n    device: torch.device\n    if hasattr(model, \"device\"):\n        device = model.device\n    elif hasattr(model, \"parameters\"):\n        device = next(model.parameters()).device\n    else:\n        device = torch.device(\"cpu\")\n\n    return device\n</code></pre>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis.mlm_evaluate","title":"mlm_evaluate","text":"<pre><code>mlm_evaluate(return_sum=True)\n</code></pre> <p>Calculate pseudo-log-likelihood score using masked token prediction.</p> <p>This method computes the pseudo-log-likelihood (PLL) score for each sequence by iteratively masking each token and predicting it using the model. The PLL score is the sum of the log probabilities of the true tokens given the masked context.</p> <p>Returns:</p> Type Description <code>list[float]</code> <p>List of pseudo-log-likelihood scores for each sequence</p> Source code in <code>dnallm/inference/mutagenesis.py</code> <pre><code>@torch.no_grad()\ndef mlm_evaluate(self, return_sum: bool = True) -&gt; list[float]:\n    \"\"\"Calculate pseudo-log-likelihood score using masked token prediction.\n\n    This method computes the pseudo-log-likelihood (PLL) score for each\n    sequence by iteratively masking each token and predicting it using the\n    model. The PLL score is the sum of the log probabilities of the true\n    tokens given the masked context.\n\n    Returns:\n        List of pseudo-log-likelihood scores for each sequence\n    \"\"\"\n    all_logprobs = []\n    model = self.model\n    tokenizer = self.tokenizer\n    device = self.get_model_device(model)\n    if len(self.sequences[\"sequence\"]) &gt; 1:\n        input_data = tqdm(self.sequences[\"sequence\"], desc=\"Inferring\")\n    else:\n        input_data = self.sequences[\"sequence\"]\n    for seq in input_data:\n        toks = tokenizer(\n            seq, return_tensors=\"pt\", add_special_tokens=True\n        ).to(device)\n        input_ids = toks[\"input_ids\"].clone()\n        seq_len = input_ids.size(1)\n        total = 0.0\n        p_values = []\n\n        for i in range(seq_len):\n            tok_id = input_ids[0, i].item()\n            if tok_id in tokenizer.all_special_ids:\n                continue\n            masked = input_ids.clone()\n            masked[0, i] = tokenizer.mask_token_id\n            masked_inputs = {\n                \"input_ids\": masked,\n                # \"attention_mask\": toks[\"attention_mask\"],\n            }\n            outputs = model(**masked_inputs)\n            logits = outputs.logits\n            logp = torch.nn.functional.log_softmax(logits[0, i], dim=-1)\n            if return_sum:\n                total += float(logp[tok_id].item())\n            else:\n                try:\n                    token = tokenizer.decode([tok_id])[0]\n                except KeyError:\n                    token = tokenizer.convert_ids_to_tokens(tok_id)\n                p_values.append((token, float(logp[tok_id].item())))\n        if return_sum:\n            all_logprobs.append(total)\n        else:\n            all_logprobs.append(p_values)\n    return all_logprobs\n</code></pre>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis.mutate_sequence","title":"mutate_sequence","text":"<pre><code>mutate_sequence(\n    sequence,\n    batch_size=1,\n    replace_mut=True,\n    include_n=False,\n    delete_size=0,\n    cut_size=0,\n    fill_gap=False,\n    insert_seq=None,\n    lowercase=False,\n    do_encode=True,\n)\n</code></pre> <p>Generate dataset from sequences with various mutation types.</p> <p>This method creates mutated versions of the input sequence including: - Single base substitutions (A, C, G, T, optionally N) - Deletions of specified size - Insertions of specified sequences - Case transformations</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>str</code> <p>Single sequence for mutagenesis</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoader</p> <code>1</code> <code>replace_mut</code> <code>bool</code> <p>Whether to perform single base substitutions</p> <code>True</code> <code>include_n</code> <code>bool</code> <p>Whether to include N base in substitutions</p> <code>False</code> <code>delete_size</code> <code>int</code> <p>Size of deletions to create (0 for no deletions)</p> <code>0</code> <code>fill_gap</code> <code>bool</code> <p>Whether to fill deletion gaps with N bases</p> <code>False</code> <code>insert_seq</code> <code>str | None</code> <p>Sequence to insert at various positions</p> <code>None</code> <code>lowercase</code> <code>bool</code> <p>Whether to convert sequences to lowercase</p> <code>False</code> <code>do_encode</code> <code>bool</code> <p>Whether to encode sequences for the model</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None (modifies internal state)</p> Source code in <code>dnallm/inference/mutagenesis.py</code> <pre><code>def mutate_sequence(\n    self,\n    sequence: str,\n    batch_size: int = 1,\n    replace_mut: bool = True,\n    include_n: bool = False,\n    delete_size: int = 0,\n    cut_size: int = 0,\n    fill_gap: bool = False,\n    insert_seq: str | None = None,\n    lowercase: bool = False,\n    do_encode: bool = True,\n) -&gt; None:\n    \"\"\"Generate dataset from sequences with various mutation types.\n\n    This method creates mutated versions of the input sequence including:\n    - Single base substitutions (A, C, G, T, optionally N)\n    - Deletions of specified size\n    - Insertions of specified sequences\n    - Case transformations\n\n    Args:\n        sequence: Single sequence for mutagenesis\n        batch_size: Batch size for DataLoader\n        replace_mut: Whether to perform single base substitutions\n        include_n: Whether to include N base in substitutions\n        delete_size: Size of deletions to create (0 for no deletions)\n        fill_gap: Whether to fill deletion gaps with N bases\n        insert_seq: Sequence to insert at various positions\n        lowercase: Whether to convert sequences to lowercase\n        do_encode: Whether to encode sequences for the model\n\n    Returns:\n        None (modifies internal state)\n    \"\"\"\n    # Get the inference config\n    pred_config = self.config[\"inference\"]\n    # Define the dataset\n    sequences = {\"name\": [\"raw\"], \"sequence\": [sequence]}\n    # Create mutated sequences\n    if replace_mut:\n        if include_n:\n            base_map = [\"A\", \"C\", \"G\", \"T\", \"N\"]\n        else:\n            base_map = [\"A\", \"C\", \"G\", \"T\"]\n        # Mutate sequence\n        for i, base in enumerate(sequence):\n            for mut_base in base_map:\n                if base != mut_base:\n                    name = f\"mut_{i}_{base}_{mut_base}\"\n                    mutated_sequence = (\n                        sequence[:i] + mut_base + sequence[i + 1 :]\n                    )\n                    sequences[\"name\"].append(name)\n                    sequences[\"sequence\"].append(mutated_sequence)\n    # Delete mutations\n    if delete_size &gt; 0:\n        for i in range(len(sequence) - delete_size + 1):\n            name = f\"del_{i}_{delete_size}\"\n            if fill_gap:\n                mutated_sequence = (\n                    sequence[:i]\n                    + \"N\" * delete_size\n                    + sequence[i + delete_size :]\n                )\n            else:\n                mutated_sequence = (\n                    sequence[:i] + sequence[i + delete_size :]\n                )\n            sequences[\"name\"].append(name)\n            sequences[\"sequence\"].append(mutated_sequence)\n    # Insert mutations\n    if insert_seq is not None:\n        for i in range(len(sequence) + 1):\n            name = f\"ins_{i}_{insert_seq}\"\n            mutated_sequence = sequence[:i] + insert_seq + sequence[i:]\n            sequences[\"name\"].append(name)\n            sequences[\"sequence\"].append(mutated_sequence)\n    # Cut mutations\n    if cut_size != 0:\n        step = abs(cut_size)\n        for i in range(0, len(sequence) - step + 1, step):\n            name = f\"cut_{i}_{cut_size}\"\n            if cut_size &gt; 0:\n                mutated_sequence = sequence[i:]\n            else:\n                mutated_sequence = sequence[: len(sequence) - i]\n            sequences[\"name\"].append(name)\n            sequences[\"sequence\"].append(mutated_sequence)\n    # Lowercase sequences\n    if lowercase:\n        sequences[\"sequence\"] = [\n            seq.lower() for seq in sequences[\"sequence\"]\n        ]\n    # Create dataset\n    if len(sequences[\"sequence\"]) &gt; 0:\n        ds = Dataset.from_dict(sequences)\n        dataset = DNADataset(\n            ds, self.tokenizer, max_length=pred_config.max_length\n        )\n        self.sequences = sequences\n    # Encode sequences\n    if do_encode:\n        dataset.encode_sequences(remove_unused_columns=True)\n    # Create DataLoader\n    if batch_size &lt;= 1:\n        batch_size = pred_config.batch_size\n    self.dataloader: DataLoader = DataLoader(\n        dataset, batch_size=batch_size, num_workers=pred_config.num_workers\n    )\n</code></pre>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis.plot","title":"plot","text":"<pre><code>plot(\n    preds,\n    width=None,\n    height=400,\n    show_score=True,\n    save_path=None,\n)\n</code></pre> <p>Plot the mutagenesis analysis results.</p> <pre><code>    This method generates visualizations of mutation effects,\ntypically as heatmaps,\n    bar charts and\nline plots showing how different mutations affect model predictions\n</code></pre> <p>at various positions.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>dict</code> <p>Dictionary containing model predicted scores and metadata</p> required <code>show_score</code> <code>bool</code> <p>Whether to show the score values on the plot         save_path: Path to save the plot. If None, plot will be shown interactively</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>Plot object</p> Source code in <code>dnallm/inference/mutagenesis.py</code> <pre><code>def plot(\n    self,\n    preds: dict,\n    width: int | None = None,\n    height: int = 400,\n    show_score: bool = True,\n    save_path: str | None = None,\n) -&gt; None:\n    \"\"\"Plot the mutagenesis analysis results.\n\n            This method generates visualizations of mutation effects,\n        typically as heatmaps,\n            bar charts and\n        line plots showing how different mutations affect model predictions\n    at various positions.\n\n    Args:\n        preds: Dictionary containing model predicted scores and metadata\n        show_score: Whether to show the score values on the plot\n                    save_path: Path to save the plot. If None,\n            plot will be shown interactively\n\n    Returns:\n        Plot object\n    \"\"\"\n    if save_path:\n        suffix = os.path.splitext(save_path)[-1]\n        if suffix:\n            outfile = save_path\n        else:\n            outfile = os.path.join(save_path, \".pdf\")\n    else:\n        outfile = None\n    # Plot heatmap\n    pmut = plot_muts(\n        preds,\n        width=width,\n        height=height,\n        show_score=show_score,\n        save_path=outfile,\n    )\n    return pmut\n</code></pre>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis.pred_comparison","title":"pred_comparison","text":"<pre><code>pred_comparison(raw_pred, mut_pred)\n</code></pre> <p>Compare raw and mutated predictions.</p> <p>This method calculates the difference between predictions on the original sequence and mutated sequences, providing insights into mutation effects.</p> <pre><code>    Args:\n        raw_pred: Raw predictions from the original sequence\n        mut_pred: Predictions from the mutated sequence\n\n    Returns:\n        Tuple containing (raw_score, mut_score, logfc):\n- raw_score: Processed scores from original sequence\n- mut_score: Processed scores from mutated sequence\n- logfc: Log fold change between mutated and original scores\n\n    Raises:\n        ValueError: If task type is not supported\n</code></pre> Source code in <code>dnallm/inference/mutagenesis.py</code> <pre><code>def pred_comparison(self, raw_pred, mut_pred):\n    \"\"\"Compare raw and mutated predictions.\n\n    This method calculates the difference between predictions on the\n    original sequence and mutated sequences, providing insights into\n    mutation effects.\n\n            Args:\n                raw_pred: Raw predictions from the original sequence\n                mut_pred: Predictions from the mutated sequence\n\n            Returns:\n                Tuple containing (raw_score, mut_score, logfc):\n        - raw_score: Processed scores from original sequence\n        - mut_score: Processed scores from mutated sequence\n        - logfc: Log fold change between mutated and original scores\n\n            Raises:\n                ValueError: If task type is not supported\n    \"\"\"\n    # Get the task config\n    task_config = self.config[\"task\"]\n    # Get the predictions\n    if task_config.task_type == \"binary\":\n        raw_score = expit(raw_pred)\n        mut_score = expit(mut_pred)\n    elif task_config.task_type == \"multiclass\":\n        raw_score = softmax(raw_pred)\n        mut_score = softmax(mut_pred)\n    elif task_config.task_type == \"multilabel\":\n        raw_score = expit(raw_pred)\n        mut_score = expit(mut_pred)\n    elif task_config.task_type == \"regression\":\n        raw_score = raw_pred\n        mut_score = mut_pred\n    elif task_config.task_type == \"token\":\n        raw_score = np.argmax(raw_pred, axis=-1)\n        mut_score = np.argmax(mut_pred, axis=-1)\n    elif task_config.task_type == \"generation\":\n        raw_score = np.array([raw_pred])\n        mut_score = np.array([mut_pred])\n    elif task_config.task_type == \"mask\":\n        raw_score = np.array([raw_pred])\n        mut_score = np.array([mut_pred])\n    elif task_config.task_type == \"embedding\":\n        raw_score = np.array([raw_pred])\n        mut_score = np.array([mut_pred])\n    else:\n        raise ValueError(f\"Unknown task type: {task_config.task_type}\")\n\n    # eps = 1e-8\n    # logfc = np.log2((mut_score + eps) / (raw_score + eps))\n    logfc = np.log2(mut_score / raw_score)\n    diff = mut_score - raw_score\n\n    return raw_score, mut_score, logfc, diff\n</code></pre>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis.prepare_tfmodisco_inputs","title":"prepare_tfmodisco_inputs","text":"<pre><code>prepare_tfmodisco_inputs(ism_results_list)\n</code></pre> <p>Prepares inputs required for a TF-MoDISco run from a list of ISM results.</p> Source code in <code>dnallm/inference/mutagenesis.py</code> <pre><code>def prepare_tfmodisco_inputs(\n    self, ism_results_list: list[dict[str, dict]]\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Prepares inputs required for a TF-MoDISco run from\n    a list of ISM results.\n    \"\"\"\n    print(\"Preparing inputs for TF-MoDISco...\")\n    acgt = [\"A\", \"C\", \"G\", \"T\"]\n    acgt_to_idx = {base: i for i, base in enumerate(acgt)}\n\n    all_one_hot, all_hyp_scores = [], []\n\n    for ism_results in ism_results_list:\n        raw_seq = ism_results[\"raw\"][\"sequence\"].upper()\n        seq_len = len(raw_seq)\n\n        one_hot = np.zeros((seq_len, 4))\n        for i, base in enumerate(raw_seq):\n            idx = acgt_to_idx.get(base)\n            if idx is not None:\n                one_hot[i, idx] = 1\n        all_one_hot.append(one_hot)\n\n        hyp_scores = np.zeros((seq_len, 4))\n        for key, value in ism_results.items():\n            if key.startswith(\"mut_\"):\n                parts = key.split(\"_\")\n                pos, _, mut_base = int(parts[1]), parts[2], parts[-1]\n                if mut_base in acgt_to_idx:\n                    hyp_scores[pos, acgt_to_idx[mut_base]] = value[\"score\"]\n        all_hyp_scores.append(hyp_scores)\n\n    one_hot_seqs = np.array(all_one_hot)\n    hyp_scores = np.array(all_hyp_scores)\n    contrib_scores = hyp_scores * one_hot_seqs\n    return one_hot_seqs, hyp_scores, contrib_scores\n</code></pre>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis.Mutagenesis.process_ism_data","title":"process_ism_data","text":"<pre><code>process_ism_data(ism_results, strategy='maxabs')\n</code></pre> <p>Process raw ISM result dictionary to get a single importance score per base.</p> <p>Parameters:</p> Name Type Description Default <code>ism_results</code> <code>Dict[str, Dict]</code> <p>The raw output from the ISM experiment.</p> required <code>strategy</code> <code>str</code> <p>Strategy to aggregate scores at each position.             'maxabs': Use the score of the mutation with                       the max absolute effect.             'mean': Use the mean of all mutation scores.</p> <code>'maxabs'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D array of importance scores, one per base pair.</p> Source code in <code>dnallm/inference/mutagenesis.py</code> <pre><code>def process_ism_data(\n    self,\n    ism_results: dict[str, dict],\n    strategy: str = \"maxabs\",\n) -&gt; np.ndarray:\n    \"\"\"\n    Process raw ISM result dictionary to get a single importance score\n    per base.\n\n    Args:\n        ism_results (Dict[str, Dict]): The raw output from\n            the ISM experiment.\n        strategy (str): Strategy to aggregate scores at each position.\n                        'maxabs': Use the score of the mutation with\n                                  the max absolute effect.\n                        'mean': Use the mean of all mutation scores.\n\n    Returns:\n        np.ndarray: A 1D array of importance scores, one per base pair.\n    \"\"\"\n    raw_seq = ism_results[\"raw\"][\"sequence\"]\n    seq_len = len(raw_seq)\n    base_scores = np.zeros(seq_len)\n\n    # Group mutations by position\n    pos_muts = {}\n    for key, value in ism_results.items():\n        if key.startswith(\"mut_\"):\n            parts = key.split(\"_\")\n            pos = int(parts[1])\n            if pos not in pos_muts:\n                pos_muts[pos] = []\n            pos_muts[pos].append(value[\"score\"])\n\n    # Apply aggregation strategy\n    for pos, scores in pos_muts.items():\n        if not scores:\n            continue\n        if strategy in [\"maxabs\", \"min\", \"max\"]:\n            max_abs_idx = np.argmax(np.abs(scores))\n            base_scores[pos] = scores[max_abs_idx]\n        elif strategy == \"mean\":\n            base_scores[pos] = np.mean(scores)\n        else:\n            raise ValueError(f\"Unknown strategy: {strategy}\")\n\n    return base_scores\n</code></pre>"},{"location":"api/inference/mutagenesis/#dnallm.inference.mutagenesis-functions","title":"Functions","text":""},{"location":"api/inference/plot/","title":"inference/plot API","text":""},{"location":"api/inference/plot/#dnallm.inference.plot","title":"dnallm.inference.plot","text":"<p>DNA Language Model Visualization and Plotting Module.</p> <p>This module provides comprehensive plotting capabilities for DNA language model results, including metrics visualization, attention maps, embeddings, and     mutation effects analysis.</p>"},{"location":"api/inference/plot/#dnallm.inference.plot-functions","title":"Functions","text":""},{"location":"api/inference/plot/#dnallm.inference.plot.plot_annotations","title":"plot_annotations","text":"<pre><code>plot_annotations(\n    data,\n    start=None,\n    end=None,\n    custom_colors=None,\n    width=800,\n    height=None,\n    save_path=None,\n)\n</code></pre> <p>Plot sequence annotations such as domains, motifs, or other features.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary where keys are annotation types and values.</p> required <code>start</code> <code>int | None</code> <p>Start position for the plot.</p> <code>None</code> <code>end</code> <code>int | None</code> <p>End position for the plot.</p> <code>None</code> <code>custom_colors</code> <code>dict | None</code> <p>A dictionary mapping annotation types to colors.</p> <code>None</code> <code>width</code> <code>int</code> <p>Width of the plot.</p> <code>800</code> <code>height</code> <code>int | None</code> <p>Height of the plot.</p> <code>None</code> <code>save_path</code> <code>str | None</code> <p>If provided, saves the plot to this path.</p> <code>None</code> <p>Returns:</p> Type Description <code>Chart</code> <p>An Altair chart object representing the annotations.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_annotations(\n    data: dict,\n    start: int | None = None,\n    end: int | None = None,\n    custom_colors: dict | None = None,\n    width: int = 800,\n    height: int | None = None,\n    save_path: str | None = None,\n) -&gt; alt.Chart:\n    \"\"\"\n    Plot sequence annotations such as domains, motifs, or other features.\n\n    Args:\n        data: A dictionary where keys are annotation types and values.\n        start: Start position for the plot.\n        end: End position for the plot.\n        custom_colors: A dictionary mapping annotation types to colors.\n        width: Width of the plot.\n        height: Height of the plot.\n        save_path: If provided, saves the plot to this path.\n\n    Returns:\n        An Altair chart object representing the annotations.\n    \"\"\"\n    from itertools import groupby\n    from operator import itemgetter\n\n    def get_intervals_from_set(pos_set, min_val, max_val):\n        \"\"\"\n        Given a set of positions, return a list of continuous intervals\n        within the specified min and max values.\n        \"\"\"\n        valid_pos = sorted([p for p in pos_set if min_val &lt;= p &lt; max_val])\n\n        if not valid_pos:\n            return []\n\n        intervals = []\n        for _, g in groupby(enumerate(valid_pos), lambda ix: ix[0] - ix[1]):\n            group = list(map(itemgetter(1), g))\n            start = group[0]\n            end = group[-1] + 1\n            intervals.append((start, end))\n\n        return intervals\n\n    records = []\n    all_types = set()\n    for model, anno in data.items():\n        for ann_type, pos_set in anno.items():\n            all_types.add(ann_type)\n\n            intervals = get_intervals_from_set(pos_set, start, end)\n            for s, e in intervals:\n                records.append({\n                    \"Model\": model,\n                    \"Type\": ann_type,\n                    \"Start\": s,\n                    \"End\": e,\n                    \"Length\": e - s,\n                })\n\n    df = pd.DataFrame(records)\n    if df.empty:\n        print(\"Warning: No annotations to plot.\")\n        return None\n\n    # Assign Set1 color to each types\n    type_list = sorted(all_types)\n    if custom_colors:\n        color_map = custom_colors\n    else:\n        default_colors = [\n            \"#e41a1c\",\n            \"#377eb8\",\n            \"#4daf4a\",\n            \"#984ea3\",\n            \"#ff7f00\",\n            \"#ffff33\",\n            \"#a65628\",\n            \"#f781bf\",\n            \"#999999\",\n        ]\n        color_map = {}\n        for i, t in enumerate(type_list):\n            if custom_colors and t in custom_colors:\n                color_map[t] = custom_colors[t]\n            else:\n                color_map[t] = default_colors[i % len(default_colors)]\n    range_colors = [color_map.get(t, \"#7f7f7f\") for t in type_list]\n\n    if height is None:\n        height = len(data) * 30\n\n    chart: alt.Chart = (\n        alt\n        .Chart(df)\n        .mark_bar()\n        .encode(\n            x=alt.X(\n                \"Start\",\n                scale=alt.Scale(domain=[start, end]),\n                axis=alt.Axis(format=\"d\", title=\"Genomic Position (bp)\"),\n            ),\n            x2=\"End\",\n            y=alt.Y(\"Model\", title=None, sort=list(data.keys())),\n            color=alt.Color(\n                \"Type\",\n                scale=alt.Scale(domain=type_list, range=range_colors),\n                legend=alt.Legend(title=\"Type\"),\n            ),\n            tooltip=[\n                alt.Tooltip(\"Model\", title=\"Model\"),\n                alt.Tooltip(\"Type\", title=\"Type\"),\n                alt.Tooltip(\"Start\", title=\"Start\"),\n                alt.Tooltip(\"End\", title=\"End\"),\n                alt.Tooltip(\"Length\", title=\"Length (bp)\"),\n            ],\n        )\n        .properties(width=width, height=height)\n        .configure_axis(grid=False)\n        .configure_view(strokeWidth=0)\n        .interactive()\n    )\n\n    if save_path:\n        chart.save(save_path)\n        print(f\"Annotation heatmap saved to {save_path}\")\n\n    return chart\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_attention_map","title":"plot_attention_map","text":"<pre><code>plot_attention_map(\n    attentions,\n    sequences,\n    tokenizer,\n    seq_idx=0,\n    layer=-1,\n    head=-1,\n    norm_method=None,\n    skip_cls=True,\n    width=800,\n    height=800,\n    save_path=None,\n)\n</code></pre> <p>Plot attention map visualization for transformer models.</p> <p>This function creates a heatmap visualization of attention weights between     tokens         in a sequence,         showing how the model attends to different parts of the input.</p> <pre><code>Args:\n            attentions: Tuple or\n        list containing attention weights from model layers\n    sequences: List of input sequences\n    tokenizer: Tokenizer object for converting tokens to readable text\n    seq_idx: Index of the sequence to plot, default 0\n    layer: Layer index to visualize, default -1 (last layer)\n            attention_head: Attention head index to visualize,\n        default -1 (last head)\n    width: Width of the plot\n    height: Height of the plot\n            save_path: Path to save the plot. If None,\n        plot will be shown interactively\n\nReturns:\n    Altair chart object showing the attention heatmap\n</code></pre> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_attention_map(\n    attentions: tuple | list,\n    sequences: list[str],\n    tokenizer,\n    seq_idx: int = 0,\n    layer: int = -1,\n    head: int | str = -1,\n    norm_method: str | None = None,\n    skip_cls: bool = True,\n    width: int = 800,\n    height: int = 800,\n    save_path: str | None = None,\n) -&gt; alt.Chart:\n    \"\"\"Plot attention map visualization for transformer models.\n\n    This function creates a heatmap visualization of attention weights between\n        tokens\n            in a sequence,\n            showing how the model attends to different parts of the input.\n\n        Args:\n                    attentions: Tuple or\n                list containing attention weights from model layers\n            sequences: List of input sequences\n            tokenizer: Tokenizer object for converting tokens to readable text\n            seq_idx: Index of the sequence to plot, default 0\n            layer: Layer index to visualize, default -1 (last layer)\n                    attention_head: Attention head index to visualize,\n                default -1 (last head)\n            width: Width of the plot\n            height: Height of the plot\n                    save_path: Path to save the plot. If None,\n                plot will be shown interactively\n\n        Returns:\n            Altair chart object showing the attention heatmap\n    \"\"\"\n    # More efficient attention data extraction with numpy\n    # Original: attn_layer = attentions[layer].numpy()\n    attn_layer = np.array(attentions[layer])\n    if head == \"all\":\n        # Average over all heads\n        attn_head = np.mean(attn_layer[seq_idx], axis=0)\n    else:\n        attn_head = attn_layer[seq_idx][head]\n\n    # do normalization\n    if norm_method == \"softmax\":\n        exp_attn = np.exp(attn_head - np.max(attn_head))\n        attn_head = exp_attn / exp_attn.sum(axis=-1, keepdims=True)\n    elif norm_method == \"minmax\":\n        attn_head = (attn_head - np.min(attn_head)) / (\n            np.max(attn_head) - np.min(attn_head)\n        )\n    elif norm_method == \"l1\":\n        attn_head = attn_head / np.sum(\n            np.abs(attn_head), axis=-1, keepdims=True\n        )\n    elif norm_method == \"l2\":\n        attn_head = attn_head / np.sqrt(\n            np.sum(attn_head**2, axis=-1, keepdims=True)\n        )\n    elif norm_method == \"log1p\":\n        attn_head = np.log1p(attn_head) / attn_head.max()\n    elif norm_method == \"zscore\":\n        attn_head = (attn_head - np.mean(attn_head)) / np.std(attn_head)\n    elif norm_method == \"entropy\":\n        from scipy.stats import entropy\n\n        ent = entropy(attn_head + 1e-12, base=2, axis=-1, keepdims=True)\n        attn_head = 1 - (ent / np.log2(attn_head.shape[-1] + 1e-12))\n    else:\n        pass  # No normalization\n\n    # More efficient token processing with error handling\n    # Original: seq = sequences[seq_idx]; tokens_id = tokenizer.encode(seq)\n    seq = sequences[seq_idx]\n    try:\n        tokens_id = tokenizer.encode(seq)\n        tokens = tokenizer.convert_ids_to_tokens(tokens_id)\n    except (AttributeError, TypeError):\n        # Fallback tokenization for different tokenizer types\n        tokens = tokenizer.decode(seq).split()\n\n    # Pre-allocate DataFrame data structure for better performance\n    # Original: num_tokens = len(tokens); flen = len(str(num_tokens))\n    num_tokens = len(tokens)\n    flen = len(str(num_tokens))\n\n    # Use list comprehension for more efficient data creation\n    # Original: Multiple loops with append operations\n    if skip_cls and len(tokens) &gt; 0:\n        if tokens[0].lower() in [\"[cls]\", \"&lt;cls&gt;\", \"&lt;s&gt;\", \"cls\"]:\n            tokens = tokens[1:]\n            attn_head = attn_head[1:, 1:]\n            num_tokens -= 1\n        if tokens[-1].lower() in [\"[sep]\", \"&lt;sep&gt;\", \"&lt;/s&gt;\", \"sep\"]:\n            tokens = tokens[:-1]\n            attn_head = attn_head[:-1, :-1]\n            num_tokens -= 1\n    df_data = {\n        \"token1\": [\n            f\"{str(i).zfill(flen)}{t1}\"\n            for i, t1 in enumerate(tokens)\n            for _ in range(num_tokens)\n        ],\n        \"token2\": [\n            f\"{str(num_tokens - j).zfill(flen)}{t2}\"\n            for _ in range(num_tokens)\n            for j, t2 in enumerate(tokens)\n        ],\n        \"attn\": [\n            attn_head[i][j]\n            for i in range(num_tokens)\n            for j in range(num_tokens)\n        ],\n    }\n\n    # More efficient DataFrame creation\n    # Original: source = pd.DataFrame(df)\n    source = pd.DataFrame(df_data)\n\n    # Enable VegaFusion for Altair performance\n    alt.data_transformers.enable(\"vegafusion\")\n\n    # Create attention map with optimized encoding and axis configuration\n    # Original: Multiple axis configurations\n    attn_map: alt.Chart = (\n        alt\n        .Chart(source)\n        .mark_rect()\n        .encode(\n            x=alt.X(\n                \"token1:O\",\n                axis=alt.Axis(\n                    labelExpr=f\"substring(datum.value, {flen}, 100)\",\n                    labelAngle=-45,\n                ),\n            ).title(None),\n            y=alt.Y(\n                \"token2:O\",\n                axis=alt.Axis(\n                    labelExpr=f\"substring(datum.value, {flen}, 100)\",\n                    labelAngle=0,\n                ),\n            ).title(None),\n            color=alt.Color(\"attn:Q\").scale(scheme=\"bluepurple\"),\n        )\n        .properties(width=width, height=height)\n        .configure_axis(grid=False)\n    ).interactive()\n\n    # Save the plot\n    if save_path:\n        attn_map.save(save_path)\n        print(f\"Attention map saved to {save_path}\")\n\n    return attn_map\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_attributions_line","title":"plot_attributions_line","text":"<pre><code>plot_attributions_line(\n    tokens,\n    scores,\n    title=\"Positional Attribution Scores\",\n    window_size=5,\n    special_tokens=None,\n)\n</code></pre> <p>Plot attribution scores as a line chart to show regional importance.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[str]</code> <p>List of tokens from the tokenizer.</p> required <code>scores</code> <code>ndarray</code> <p>Array of attribution scores.</p> required <code>title</code> <code>str</code> <p>The title for the chart.</p> <code>'Positional Attribution Scores'</code> <code>special_tokens</code> <code>List[str]</code> <p>Special tokens to filter out.</p> <code>None</code> <p>Returns:</p> Type Description <code>Chart</code> <p>alt.Chart: An Altair chart object.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_attributions_line(\n    tokens: list[str],\n    scores: np.ndarray,\n    title: str = \"Positional Attribution Scores\",\n    window_size: int | None = 5,\n    special_tokens: list[str] | None = None,\n) -&gt; alt.Chart:\n    \"\"\"\n    Plot attribution scores as a line chart to show regional importance.\n\n    Args:\n        tokens (List[str]): List of tokens from the tokenizer.\n        scores (np.ndarray): Array of attribution scores.\n        title (str): The title for the chart.\n        special_tokens (List[str]): Special tokens to filter out.\n\n    Returns:\n        alt.Chart: An Altair chart object.\n    \"\"\"\n    if special_tokens is None:\n        special_tokens = [\n            \"[CLS]\",\n            \"[SEP]\",\n            \"[PAD]\",\n            \"&lt;s&gt;\",\n            \"&lt;/s&gt;\",\n            \"&lt;pad&gt;\",\n            \"&lt;unk&gt;\",\n            \"[UNK]\",\n            \"&lt;bos&gt;\",\n            \"&lt;eos&gt;\",\n            \"&lt;cls&gt;\",\n            \"&lt;sep&gt;\",\n        ]\n    valid_indices = [\n        i for i, token in enumerate(tokens) if token not in special_tokens\n    ]\n    vis_scores = scores[valid_indices]\n\n    if len(vis_scores) == 0:\n        print(\"Warning: No valid scores to plot after filtering.\")\n        chart: alt.Chart = (\n            alt.Chart().mark_text().properties(title=\"No data to display\")\n        )\n        return chart\n\n    source = pd.DataFrame({\n        \"position\": range(len(vis_scores)),\n        \"Raw\": vis_scores,\n    })\n\n    # Create base chart (raw scores)\n    area = (\n        alt\n        .Chart(source)\n        .mark_area(opacity=0.2, color=\"lightblue\")\n        .encode(\n            x=alt.X(\"position:Q\", title=\"Token Position\"),\n            y=alt.Y(\"Raw:Q\", title=\"Attribution Score\"),\n        )\n    )\n    # Add base line at y=0 (dashed line)\n    area += (\n        alt\n        .Chart(pd.DataFrame({\"y\": [0]}))\n        .mark_rule(color=\"black\", strokeDash=[5, 5])\n        .encode(y=\"y:Q\")\n    )\n\n    # if need to smooth the scores\n    if window_size and window_size &gt; 1:\n        source[\"Smoothed\"] = (\n            source[\"Raw\"]\n            .rolling(window=window_size, center=True, min_periods=1)\n            .mean()\n        )\n        # Convert to long format for Altair\n        long_source = source.melt(\n            id_vars=[\"position\"], var_name=\"type\", value_name=\"value\"\n        )\n    else:  # if no smoothing, just use raw scores\n        long_source = source.melt(\n            id_vars=[\"position\"], var_name=\"type\", value_name=\"value\"\n        )\n\n    # Create line chart layer\n    lines = (\n        alt\n        .Chart(long_source)\n        .mark_line()\n        .encode(\n            x=alt.X(\"position:Q\"),\n            y=alt.Y(\"value:Q\"),\n            color=alt.Color(\"type:N\", legend=alt.Legend(title=\"Score Type\")),\n            tooltip=[\n                alt.Tooltip(\"position:Q\"),\n                alt.Tooltip(\"value:Q\", format=\".4f\", title=\"Score\"),\n                alt.Tooltip(\"type:N\", title=\"Type\"),\n            ],\n        )\n    )\n\n    chart = area + lines\n\n    chart: alt.Chart = (\n        chart\n        .properties(width=800, height=200, title=title)\n        .interactive()\n        .configure_axis(grid=False)\n    )\n    return chart\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_attributions_multi","title":"plot_attributions_multi","text":"<pre><code>plot_attributions_multi(\n    all_attributions, title=\"Aggregated Attribution Heatmap\"\n)\n</code></pre> <p>Plot an aggregated heatmap of attribution scores from multiple sequences.</p> <p>Parameters:</p> Name Type Description Default <code>all_attributions</code> <code>List[Tuple[List[str], ndarray]]</code> <p>List of tuples containing tokens and their corresponding attribution scores for multiple sequences. Crucially, all arrays must be of the same length.</p> required <code>title</code> <code>str</code> <p>The title for the chart.</p> <code>'Aggregated Attribution Heatmap'</code> <p>Returns:</p> Type Description <code>Chart</code> <p>alt.Chart: An Altair chart object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input score arrays have inconsistent lengths.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_attributions_multi(\n    all_attributions: list[tuple[list[str], np.ndarray]],\n    title: str = \"Aggregated Attribution Heatmap\",\n) -&gt; alt.Chart:\n    \"\"\"\n    Plot an aggregated heatmap of attribution scores from multiple sequences.\n\n    Args:\n        all_attributions (List[Tuple[List[str], np.ndarray]]): List of\n            tuples containing tokens and their corresponding attribution\n            scores for multiple sequences.\n            **Crucially, all arrays must be of the same length.**\n        title (str): The title for the chart.\n\n    Returns:\n        alt.Chart: An Altair chart object.\n\n    Raises:\n        ValueError: If the input score arrays have inconsistent lengths.\n    \"\"\"\n    if not all_attributions:\n        print(\"Warning: No scores provided to plot.\")\n        chart: alt.Chart = (\n            alt.Chart().mark_text().properties(title=\"No data to display\")\n        )\n        return chart\n\n    # 0. Remove right padding tokens/scores if any\n    trimmed_attributions = []\n    possible_padding_tokens = [\n        \"[PAD]\",\n        \"&lt;pad&gt;\",\n        \"&lt;/s&gt;\",\n        \"&lt;eos&gt;\",\n        \"[SEP]\",\n        \"&lt;sep&gt;\",\n        \" \",\n        \"#\",\n        \"*\",\n    ]\n    pad_token = possible_padding_tokens[0]\n    for tokens, scores in all_attributions:\n        # Identify the last non-padding token index\n        last_valid_index = len(tokens)\n        for i in reversed(range(len(tokens))):\n            if tokens[i] in possible_padding_tokens:\n                pad_token = tokens[i]\n            else:\n                last_valid_index = i + 1\n                break\n        trimmed_attributions.append((\n            tokens[:last_valid_index],\n            scores[:last_valid_index],\n        ))\n\n    # 1. Check for consistent lengths\n    first_len = len(trimmed_attributions[0][1])\n    if not all(len(scores) == first_len for _, scores in trimmed_attributions):\n        # extend to max length with NaN padding\n        max_len = max(len(scores) for _, scores in trimmed_attributions)\n        extended_attributions = []\n        for tokens, scores in trimmed_attributions:\n            if len(scores) &lt; max_len:\n                extended_scores = np.pad(\n                    scores,\n                    (0, max_len - len(scores)),\n                    mode=\"constant\",\n                    constant_values=np.nan,\n                )\n                extended_tokens = tokens + [pad_token] * (\n                    max_len - len(tokens)\n                )\n                extended_attributions.append((\n                    extended_tokens,\n                    extended_scores,\n                ))\n            else:\n                extended_attributions.append((tokens, scores))\n        all_attributions = extended_attributions\n    else:\n        all_attributions = trimmed_attributions\n\n    # 2. Stack into a matrix and convert to long-form DataFrame\n    all_scores = [scores for _, scores in all_attributions]\n    score_matrix = np.stack(all_scores, axis=0)\n\n    source = pd.DataFrame(score_matrix).unstack().reset_index()\n    source.columns = [\"position\", \"sample_index\", \"score\"]\n\n    # 3. Calculate color range\n    max_abs_score = np.max(np.abs(source[\"score\"]))\n    domain = [-max_abs_score, 0, max_abs_score]\n    color_range = [\"#2166ac\", \"#f7f7f7\", \"#b2182b\"]  # Blue -&gt; White -&gt; Red\n\n    # 4. Create heatmap\n    heatmap: alt.Chart = (\n        alt\n        .Chart(source)\n        .mark_rect()\n        .encode(\n            x=alt.X(\n                \"position:O\",\n                title=\"Aligned Position\",\n                axis=alt.Axis(labels=True, ticks=True),\n            ),\n            y=alt.Y(\n                \"sample_index:O\",\n                title=\"Sample Index\",\n                axis=alt.Axis(labels=False, ticks=False),\n            ),\n            color=alt.Color(\n                \"score:Q\",\n                scale=alt.Scale(domain=domain, range=color_range),\n                legend=alt.Legend(title=\"Attribution\"),\n            ),\n        )\n        .properties(\n            width=len(all_attributions[0][1]) * 15,\n            height=len(all_attributions) * 15,\n            title=title,\n        )\n    )\n\n    return heatmap\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_attributions_token","title":"plot_attributions_token","text":"<pre><code>plot_attributions_token(\n    tokens,\n    scores,\n    title=\"Token-level Attributions\",\n    special_tokens=None,\n)\n</code></pre> <p>Visualize token-level attribution scores as colored text using Altair.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[str]</code> <p>List of tokens from the tokenizer.</p> required <code>scores</code> <code>ndarray</code> <p>Array of attribution scores                  corresponding to each token.</p> required <code>title</code> <code>str</code> <p>The title for the chart.</p> <code>'Token-level Attributions'</code> <code>special_tokens</code> <code>List[str]</code> <p>Special tokens to filter out                         from the visualization.</p> <code>None</code> <p>Returns:</p> Type Description <code>Chart</code> <p>alt.Chart: An Altair chart object.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_attributions_token(\n    tokens: list[str],\n    scores: np.ndarray,\n    title: str = \"Token-level Attributions\",\n    special_tokens: list[str] | None = None,\n) -&gt; alt.Chart:\n    \"\"\"\n    Visualize token-level attribution scores as colored text using Altair.\n\n    Args:\n        tokens (List[str]): List of tokens from the tokenizer.\n        scores (np.ndarray): Array of attribution scores\n                             corresponding to each token.\n        title (str): The title for the chart.\n        special_tokens (List[str]): Special tokens to filter out\n                                    from the visualization.\n\n    Returns:\n        alt.Chart: An Altair chart object.\n    \"\"\"\n    # 1. Filter out special tokens\n    if special_tokens is None:\n        special_tokens = [\n            \"[CLS]\",\n            \"[SEP]\",\n            \"[PAD]\",\n            \"&lt;s&gt;\",\n            \"&lt;/s&gt;\",\n            \"&lt;pad&gt;\",\n            \"&lt;unk&gt;\",\n            \"[UNK]\",\n            \"&lt;bos&gt;\",\n            \"&lt;eos&gt;\",\n            \"&lt;cls&gt;\",\n            \"&lt;sep&gt;\",\n        ]\n    valid_indices = [\n        i for i, token in enumerate(tokens) if token not in special_tokens\n    ]\n    vis_tokens = [tokens[i] for i in valid_indices]\n    vis_scores = scores[valid_indices]\n\n    if len(vis_tokens) == 0:\n        print(\"Warning: No valid tokens to plot after filtering.\")\n        chart: alt.Chart = (\n            alt.Chart().mark_text().properties(title=\"No data to display\")\n        )\n        return chart\n\n    # 2. Calculate each token\"s length and precise position in the sequence\n    token_lengths = [len(t) for t in vis_tokens]\n    end_pos = np.cumsum(token_lengths)\n    start_pos = end_pos - token_lengths\n    center_pos = start_pos + (np.array(token_lengths) / 2.0)\n\n    source = pd.DataFrame({\n        \"token\": vis_tokens,\n        \"score\": vis_scores,\n        \"token_length\": token_lengths,\n        \"start_pos\": start_pos,\n        \"end_pos\": end_pos,\n        \"center_pos\": center_pos,\n    })\n\n    # 3. Calculate color scale domain and range\n    max_abs_score = np.max(np.abs(source[\"score\"])) if not source.empty else 0\n    domain = [-max_abs_score, 0, max_abs_score]\n    color_range = [\"#2166ac\", \"#f7f7f7\", \"#b2182b\"]  # Blue -&gt; White -&gt; Red\n\n    # 4. Create Altair chart\n    # X axis now is quantitative (Quantitative), representing base position\n    base = alt.Chart(source).properties(\n        width=max(600, int(source[\"end_pos\"].max() * 12)),  # dynamic width\n        height=50,\n        title=title,\n    )\n\n    # Background rectangles, using x and x2 to define variable width\n    rects = base.mark_rect().encode(\n        x=alt.X(\"start_pos:Q\", axis=None, title=\"Base Position\"),\n        x2=alt.X2(\"end_pos:Q\"),\n        color=alt.Color(\n            \"score:Q\",\n            scale=alt.Scale(domain=domain, range=color_range),\n            legend=alt.Legend(title=\"Attribution Score\"),\n        ),\n        tooltip=[\n            alt.Tooltip(\"token:N\", title=\"Token\"),\n            alt.Tooltip(\"score:Q\", title=\"Score\", format=\".4f\"),\n            alt.Tooltip(\"token_length:Q\", title=\"Length (bp)\"),\n        ],\n    )\n\n    # Text layer, centered\n    text = base.mark_text(baseline=\"middle\", fontSize=12, clip=True).encode(\n        x=alt.X(\"center_pos:Q\", axis=None),\n        text=\"token:N\",\n        color=alt.condition(\n            alt.datum.score &gt; max_abs_score * 0.5,\n            alt.value(\"white\"),\n            alt.value(\"black\"),\n        ),\n    )\n    chart = (rects + text).configure_view(strokeWidth=0)\n\n    return chart\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_bars","title":"plot_bars","text":"<pre><code>plot_bars(\n    data,\n    show_score=True,\n    ncols=3,\n    width=200,\n    height=None,\n    bar_width=20,\n    domain=(0.0, 1.0),\n    save_path=None,\n    separate=False,\n)\n</code></pre> <p>Plot bar charts for model metrics comparison.</p> <p>This function creates bar charts to compare different metrics across     multiple models. It supports automatic layout with multiple columns and         optional score labels on bars.</p> <pre><code>Args:\n    data: Dictionary containing metrics data with \"models\" as the first\n        key\n    show_score: Whether to show the score values on the bars\n    ncols: Number of columns to arrange the plots\n    width: Width of each individual plot\n    height: Height of each individual plot\n    bar_width: Width of the bars in the plot\n    domain: Y-axis domain range for the plots, default (0.0, 1.0)\n            save_path: Path to save the plot. If None,\n        plot will be shown interactively\n    separate: Whether to return separate plots for each metric\n\nReturns:\n            Altair chart object (combined or\n        separate plots based on separate parameter)\n</code></pre> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_bars(\n    data: dict,\n    show_score: bool = True,\n    ncols: int = 3,\n    width: int = 200,\n    height: int | None = None,\n    bar_width: int = 20,\n    domain: tuple[float, float] | list[float] = (0.0, 1.0),\n    save_path: str | None = None,\n    separate: bool = False,\n) -&gt; alt.Chart | dict[str, alt.Chart]:\n    \"\"\"Plot bar charts for model metrics comparison.\n\n    This function creates bar charts to compare different metrics across\n        multiple models. It supports automatic layout with multiple columns and\n            optional score labels on bars.\n\n        Args:\n            data: Dictionary containing metrics data with \"models\" as the first\n                key\n            show_score: Whether to show the score values on the bars\n            ncols: Number of columns to arrange the plots\n            width: Width of each individual plot\n            height: Height of each individual plot\n            bar_width: Width of the bars in the plot\n            domain: Y-axis domain range for the plots, default (0.0, 1.0)\n                    save_path: Path to save the plot. If None,\n                plot will be shown interactively\n            separate: Whether to return separate plots for each metric\n\n        Returns:\n                    Altair chart object (combined or\n                separate plots based on separate parameter)\n    \"\"\"\n    # Convert to DataFrame once and cache for better performance\n    # Original: dbar = pd.DataFrame(data)\n    dbar = pd.DataFrame(data)\n\n    # Pre-allocate plot dictionaries for better memory management\n    # Original: pbar = {}; p_separate = {}\n    pbar = {}\n    p_separate = {}\n\n    # Filter metrics once and use list comprehension for better performance\n    # Original: for n, metric in enumerate([x for x in data if x != \"models\"]):\n    metrics_list = [x for x in data if x != \"models\"]\n\n    for n, metric in enumerate(metrics_list):\n        # convert to float for proper plotting\n        dbar[metric] = dbar[metric].astype(float)\n        if metric in [\"mae\", \"mse\"]:\n            domain_use = [0, dbar[metric].max() * 1.1]\n        else:\n            domain_use = domain\n\n        # Create bar chart with optimized encoding\n        if height is None:\n            height = 25 * len(dbar[\"models\"])\n        bar = (\n            alt\n            .Chart(dbar)\n            .mark_bar(size=bar_width)\n            .encode(\n                x=alt.X(f\"{metric}:Q\").scale(domain=domain_use),\n                y=alt.Y(\"models\").title(None),\n                color=alt.Color(\"models\").legend(None),\n                tooltip=[\"models\", metric],\n            )\n            .properties(width=width, height=height)\n        )\n\n        if show_score:\n            # Optimized text positioning and formatting\n            text = (\n                alt\n                .Chart(dbar)\n                .mark_text(\n                    dx=-10 if dbar[metric].min() &gt;= 0.2 else 5,\n                    color=\"white\" if dbar[metric].min() &gt;= 0.2 else \"black\",\n                    baseline=\"middle\",\n                    align=\"right\" if dbar[metric].min() &gt;= 0.2 else \"left\",\n                )\n                .encode(\n                    x=alt.X(f\"{metric}:Q\"),\n                    y=alt.Y(\"models\").title(None),\n                    text=alt.Text(metric, format=\".3f\"),\n                )\n            )\n            p = bar + text\n        else:\n            p = bar\n\n        if separate:\n            p_separate[metric] = p.configure_axis(grid=False)\n\n        # More efficient plot arrangement logic\n        idx = n // ncols\n        if n % ncols == 0:\n            pbar[idx] = p\n        else:\n            pbar[idx] |= p\n\n    # More efficient plot combination with reduce-like approach\n    # Original: Multiple conditional checks and assignments\n    pbars: alt.Chart = pbar[0] if pbar else alt.Chart()\n    for i in range(1, len(pbar)):\n        pbars &amp;= pbar[i]\n\n    # Configure chart once at the end\n    pbars = pbars.configure_axis(grid=False)\n\n    # Save the plot\n    if save_path:\n        pbars.save(save_path)\n        print(f\"Metrics bar charts saved to {save_path}\")\n\n    if separate:\n        return p_separate\n    else:\n        return pbars\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_curve","title":"plot_curve","text":"<pre><code>plot_curve(\n    data,\n    show_score=True,\n    width=400,\n    height=400,\n    save_path=None,\n    separate=False,\n)\n</code></pre> <p>Plot ROC and PR curves for classification tasks.</p> <pre><code>This function creates ROC (Receiver Operating Characteristic) and\nPR (Precision-Recall)\n</code></pre> <p>curves to evaluate model performance on classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing ROC and PR curve data with \"ROC\" and   \"PR\" keys, and optionally \"AUROC\" and \"AUPRC\" score dicts.</p> required <code>show_score</code> <code>bool</code> <p>Whether to show the score values on the plot (now   implemented in the legend).</p> <code>True</code> <code>width</code> <code>int</code> <p>Width of each plot</p> <code>400</code> <code>height</code> <code>int</code> <p>Height of each plot     save_path: Path to save the plot. If None,   plot will be shown interactively</p> <code>400</code> <code>separate</code> <code>bool</code> <p>Whether to return separate plots for ROC and PR curves</p> <code>False</code> <p>Returns:</p> Type Description <code>Chart | dict[str, Chart]</code> <p>Altair chart object (combined or</p> <code>Chart | dict[str, Chart]</code> <p>separate plots based on separate parameter)</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_curve(\n    data: dict,\n    show_score: bool = True,\n    width: int = 400,\n    height: int = 400,\n    save_path: str | None = None,\n    separate: bool = False,\n) -&gt; alt.Chart | dict[str, alt.Chart]:\n    \"\"\"Plot ROC and PR curves for classification tasks.\n\n        This function creates ROC (Receiver Operating Characteristic) and\n        PR (Precision-Recall)\n    curves to evaluate model performance on classification tasks.\n\n    Args:\n        data: Dictionary containing ROC and PR curve data with \"ROC\" and\n              \"PR\" keys, and optionally \"AUROC\" and \"AUPRC\" score dicts.\n        show_score: Whether to show the score values on the plot (now\n              implemented in the legend).\n        width: Width of each plot\n        height: Height of each plot\n                save_path: Path to save the plot. If None,\n              plot will be shown interactively\n        separate: Whether to return separate plots for ROC and PR curves\n\n    Returns:\n        Altair chart object (combined or\n        separate plots based on separate parameter)\n    \"\"\"\n    pline = {}\n    p_separate = {}\n\n    roc_data = pd.DataFrame(data[\"ROC\"])\n\n    # ROC chart\n    roc_chart = (\n        alt\n        .Chart(roc_data)\n        .mark_line()\n        .encode(\n            x=alt.X(\"fpr\", title=\"FPR\").scale(domain=(0.0, 1.0)),\n            y=alt.Y(\"tpr\", title=\"TPR\").scale(domain=(0.0, 1.0)),\n            color=alt.Color(\"models:N\", title=\"Models\"),\n            tooltip=[\"fpr\", \"tpr\", \"models\"],\n        )\n        .properties(width=width, height=height, title=\"ROC Curve\")\n    )\n\n    # Diagonal line\n    diag_line = (\n        alt\n        .Chart(pd.DataFrame({\"fpr\": [0, 1], \"tpr\": [0, 1]}))\n        .mark_line(strokeDash=[5, 5], color=\"gray\")\n        .encode(\n            x=alt.X(\"fpr\").scale(domain=(0.0, 1.0)),\n            y=alt.Y(\"tpr\").scale(domain=(0.0, 1.0)),\n        )\n    )\n\n    pline[0] = roc_chart + diag_line\n\n    # Add text annotations for AUROC\n    if show_score and \"AUROC\" in data:\n        auroc_scores = data.get(\"AUROC\", {})\n\n        # Create a DataFrame for the text annotations\n        text_data = []\n        y_offset = 0.05  # Vertical spacing between text lines\n        y_start = 0.05  # Start from the bottom\n\n        # Dynamically calculate y positions based on the number of models\n        for i, (model, score) in enumerate(auroc_scores.items()):\n            text_data.append({\n                \"models\": model,\n                \"label\": f\"{model} (AUC={score:.3f})\",\n                \"fpr\": 0.95,\n                \"tpr\": y_start + i * y_offset,\n            })\n\n        auroc_text_df = pd.DataFrame(\n            text_data, columns=[\"models\", \"label\", \"fpr\", \"tpr\"]\n        )\n        auroc_text_layer = (\n            alt\n            .Chart(auroc_text_df)\n            .mark_text(\n                align=\"right\",\n                baseline=\"bottom\",\n            )\n            .encode(\n                x=alt.X(\"fpr:Q\", scale=alt.Scale(domain=(0.0, 1.0))),\n                y=alt.Y(\"tpr:Q\", scale=alt.Scale(domain=(0.0, 1.0))),\n                text=\"label:N\",\n                # Use the same color encoding as the line chart\n                color=alt.Color(\"models:N\", legend=None),\n            )\n        )\n        # Add the text layer to the ROC plot\n        pline[0] = alt.layer(roc_chart, diag_line, auroc_text_layer)\n\n    if separate:\n        p_separate[\"ROC\"] = pline[0]\n\n    # --- Create PR curve ---\n    pr_data = pd.DataFrame(data[\"PR\"])\n\n    # PR chart\n    pr_chart = (\n        alt\n        .Chart(pr_data)\n        .mark_line()\n        .encode(\n            x=alt.X(\"recall\", title=\"Recall\").scale(domain=(0.0, 1.0)),\n            y=alt.Y(\"precision\", title=\"Precision\").scale(domain=(0.0, 1.0)),\n            color=alt.Color(\"models:N\", title=\"Models\"),\n            tooltip=[\"recall\", \"precision\", \"models\"],\n        )\n        .properties(width=width, height=height, title=\"PR Curve\")\n    )\n    pr_baseline = (\n        alt\n        .Chart(\n            pd.DataFrame({\n                \"recall\": [0, 1],\n                \"precision\": [\n                    pr_data[\"precision\"].min(),\n                    pr_data[\"precision\"].min(),\n                ],\n            })\n        )\n        .mark_line(strokeDash=[5, 5], color=\"gray\")\n        .encode(\n            x=alt.X(\"recall\").scale(domain=(0.0, 1.0)),\n            y=alt.Y(\"precision\").scale(domain=(0.0, 1.0)),\n        )\n    )\n\n    pline[1] = pr_chart + pr_baseline\n\n    # Add text annotations for AUPRC\n    if show_score and \"AUPRC\" in data:\n        auprc_scores = data.get(\"AUPRC\", {})\n\n        # Create a DataFrame for the text annotations\n        text_data = []\n        y_offset = 0.05  # Vertical spacing between text lines\n        y_start = 0.05  # Start from the bottom\n\n        for i, (model, score) in enumerate(auprc_scores.items()):\n            text_data.append({\n                \"models\": model,\n                \"label\": f\"{model} (AUC={score:.3f})\",\n                \"recall\": 0.05,\n                \"precision\": y_start + i * y_offset,\n            })\n\n        auprc_text_df = pd.DataFrame(\n            text_data, columns=[\"models\", \"label\", \"recall\", \"precision\"]\n        )\n        auprc_text_layer = (\n            alt\n            .Chart(auprc_text_df)\n            .mark_text(\n                align=\"left\",\n                baseline=\"bottom\",\n            )\n            .encode(\n                x=alt.X(\"recall:Q\", scale=alt.Scale(domain=(0.0, 1.0))),\n                y=alt.Y(\"precision:Q\", scale=alt.Scale(domain=(0.0, 1.0))),\n                text=\"label:N\",\n                # Use the same color encoding as the line chart\n                color=alt.Color(\"models:N\", legend=None),\n            )\n        )\n\n        pline[1] = alt.layer(pr_chart, pr_baseline, auprc_text_layer)\n\n    if separate:\n        p_separate[\"PR\"] = pline[1]\n\n    # Combine plots if not separate\n    plines: alt.Chart = pline[0] if pline else alt.Chart()\n    for i in range(1, len(pline)):\n        plines |= pline[i]\n\n    plines = plines.configure_axis(grid=False)\n\n    if save_path:\n        plines.save(save_path)\n        print(f\"ROC/PR curves saved to {save_path}\")\n\n    if separate:\n        return p_separate\n    else:\n        return plines\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_embeddings","title":"plot_embeddings","text":"<pre><code>plot_embeddings(\n    hidden_states,\n    attention_mask,\n    reducer=\"t-SNE\",\n    quality=\"fast\",\n    labels=None,\n    label_names=None,\n    ncols=4,\n    width=300,\n    height=300,\n    point_size=10,\n    save_path=None,\n    separate=False,\n    norm=True,\n    reduced=False,\n)\n</code></pre> <p>Visualize embeddings using dimensionality reduction techniques.</p> <p>This function creates 2D visualizations of high-dimensional embeddings     from different model layers using PCA, t-SNE, or UMAP dimensionality     reduction methods.</p> <pre><code>Args:\n    hidden_states: Tuple or list containing hidden states from model\n        layers\n            attention_mask: Tuple or\n        list containing attention masks for sequence padding\n            reducer: Dimensionality reduction method. Options: \"PCA\",\n        \"t-SNE\", \"UMAP\"\n    labels: List of labels for the data points\n    labels_names: List of label names for legend display\n    ncols: Number of columns to arrange the plots\n    width: Width of each plot\n    height: Height of each plot\n            save_path: Path to save the plot. If None,\n        plot will be shown interactively\n    point_size: Size of the points in the scatter plot\n    separate: Whether to return separate plots for each layer\n    norm: Whether to normalize embeddings before reduction\n    reduced: Whether the input hidden states are already 2D\n\nReturns:\n            Altair chart object (combined or\n        separate plots based on separate parameter)\n\nRaises:\n    ValueError: If unsupported dimensionality reduction method is\n        specified\n</code></pre> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_embeddings(\n    hidden_states: tuple | list,\n    attention_mask: tuple | list | None,\n    reducer: str = \"t-SNE\",\n    quality: str = \"fast\",\n    labels: tuple | list | None = None,\n    label_names: str | list | None = None,\n    ncols: int = 4,\n    width: int = 300,\n    height: int = 300,\n    point_size: int = 10,\n    save_path: str | None = None,\n    separate: bool = False,\n    norm: bool = True,\n    reduced: bool = False,\n) -&gt; alt.Chart | dict[str, alt.Chart]:\n    \"\"\"Visualize embeddings using dimensionality reduction techniques.\n\n    This function creates 2D visualizations of high-dimensional embeddings\n        from different model layers using PCA, t-SNE, or UMAP dimensionality\n        reduction methods.\n\n        Args:\n            hidden_states: Tuple or list containing hidden states from model\n                layers\n                    attention_mask: Tuple or\n                list containing attention masks for sequence padding\n                    reducer: Dimensionality reduction method. Options: \"PCA\",\n                \"t-SNE\", \"UMAP\"\n            labels: List of labels for the data points\n            labels_names: List of label names for legend display\n            ncols: Number of columns to arrange the plots\n            width: Width of each plot\n            height: Height of each plot\n                    save_path: Path to save the plot. If None,\n                plot will be shown interactively\n            point_size: Size of the points in the scatter plot\n            separate: Whether to return separate plots for each layer\n            norm: Whether to normalize embeddings before reduction\n            reduced: Whether the input hidden states are already 2D\n\n        Returns:\n                    Altair chart object (combined or\n                separate plots based on separate parameter)\n\n        Raises:\n            ValueError: If unsupported dimensionality reduction method is\n                specified\n    \"\"\"\n    # Initialize dimensionality reducer\n    n_samples = hidden_states[0].shape[0] if hidden_states else None\n    dim_reducer = _get_dimensionality_reducer(\n        reducer,\n        n_samples=n_samples,\n        quality=quality,\n    )\n    # Type assertion: dim_reducer is guaranteed to be non-None from helper\n    # function\n    if dim_reducer is None:\n        raise ValueError(\n            \"Dimensionality reducer is None - this should not happen\"\n        )\n\n    # Process each layer and create plots\n    plots = []\n    p_separate = {}\n\n    for i, hidden in enumerate(hidden_states):\n        # Compute mean embeddings\n        if reduced:\n            mean_embeddings = np.array(hidden)\n        else:\n            if attention_mask is None:\n                mean_embeddings = _compute_mean_embeddings(hidden)\n            else:\n                mean_embeddings = _compute_mean_embeddings(\n                    hidden, attention_mask[i]\n                )\n        # Apply dimensionality reduction\n        if norm:\n            # embeddings_normalized = normalize(mean_embeddings)\n            mean_embeddings = StandardScaler().fit_transform(mean_embeddings)\n        layer_dim_reduced_vectors = np.array(\n            dim_reducer.fit_transform(mean_embeddings)\n        )\n\n        # Prepare data for plotting\n        source_df = _prepare_embedding_dataframe(\n            layer_dim_reduced_vectors, labels, label_names\n        )\n\n        # Create individual plot\n        plot = _create_embedding_plot(source_df, i, width, height, point_size)\n        plots.append(plot)\n\n        if separate:\n            p_separate[f\"Layer{i + 1}\"] = plot.configure_axis(grid=False)\n\n    # Arrange plots in grid\n    combined_plot = _arrange_plots(plots, ncols)\n\n    # Save the plot\n    if save_path:\n        combined_plot.save(save_path)\n        print(f\"Embeddings visualization saved to {save_path}\")\n\n    return p_separate if separate else combined_plot\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_line","title":"plot_line","text":"<pre><code>plot_line(scores, width=400, height=400, save_path=None)\n</code></pre> <p>Plot a line chart for average score ratios.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>dict</code> <p>A dictionary including labels and lists of scores.</p> required <code>width</code> <code>int</code> <p>Width of the plot.</p> <code>400</code> <code>height</code> <code>int</code> <p>Height of the plot.</p> <code>400</code> <code>save_path</code> <code>str | None</code> <p>If provided, saves the plot to this path.</p> <code>None</code> <p>Returns:</p> Type Description <code>Chart</code> <p>An Altair chart object representing the line plot.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_line(\n    scores: dict,\n    width: int = 400,\n    height: int = 400,\n    save_path: str | None = None,\n) -&gt; alt.Chart:\n    \"\"\"\n    Plot a line chart for average score ratios.\n\n    Args:\n        scores: A dictionary including labels and lists of scores.\n        width: Width of the plot.\n        height: Height of the plot.\n        save_path: If provided, saves the plot to this path.\n\n    Returns:\n        An Altair chart object representing the line plot.\n    \"\"\"\n    records = []\n    for key, values in scores.items():\n        for pos, val in enumerate(values):\n            records.append({\"type\": key, \"position\": pos, \"score\": val})\n    df = pd.DataFrame(records)\n\n    chart: alt.Chart = (\n        alt\n        .Chart(df)\n        .mark_line(interpolate=\"monotone\")\n        .encode(\n            x=alt.X(\"position:Q\", title=\"Position\"),\n            y=alt.Y(\n                \"score:Q\", title=\"Likelihood\", scale=alt.Scale(zero=False)\n            ),\n            color=alt.Color(\"type:N\", title=\"Types\"),\n            tooltip=[\"type\", \"position\", \"score\"],\n        )\n        .properties(width=width, height=height)\n        .configure_axis(grid=False)\n        .interactive()\n    )\n\n    if save_path:\n        chart.save(save_path)\n\n    return chart\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_muts","title":"plot_muts","text":"<pre><code>plot_muts(\n    data,\n    show_score=False,\n    width=None,\n    height=100,\n    save_path=None,\n)\n</code></pre> <p>Visualize mutation effects on model predictions.</p> <p>This function creates comprehensive visualizations of how different     mutations affect model predictions, including:     - Heatmap showing mutation effects at each position     - Line plot showing gain/loss of function     - Bar chart showing maximum effect mutations</p> <pre><code>Args:\n    data: Dictionary containing mutation data with \"raw\" and mutation\n        keys\n</code></pre> <p>show_score: Whether to show the score values on the plot (currently not         implemented)                 width: Width of the plot. If None,             automatically calculated based on sequence length         height: Height of the plot                 save_path: Path to save the plot. If None,             plot will be shown interactively</p> <pre><code>Returns:\n    Altair chart object showing the combined mutation effects\n        visualization\n</code></pre> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_muts(\n    data: dict,\n    show_score: bool = False,\n    width: int | None = None,\n    height: int = 100,\n    save_path: str | None = None,\n) -&gt; alt.Chart | alt.VConcatChart:\n    \"\"\"Visualize mutation effects on model predictions.\n\n    This function creates comprehensive visualizations of how different\n        mutations affect model predictions, including:\n        - Heatmap showing mutation effects at each position\n        - Line plot showing gain/loss of function\n        - Bar chart showing maximum effect mutations\n\n        Args:\n            data: Dictionary containing mutation data with \"raw\" and mutation\n                keys\n    show_score: Whether to show the score values on the plot (currently not\n            implemented)\n                    width: Width of the plot. If None,\n                automatically calculated based on sequence length\n            height: Height of the plot\n                    save_path: Path to save the plot. If None,\n                plot will be shown interactively\n\n        Returns:\n            Altair chart object showing the combined mutation effects\n                visualization\n    \"\"\"\n    # Extract basic data\n    _sequence, raw_bases, seqlen, flen, mut_list = _extract_mutation_data(data)\n\n    # Build visualization datasets\n    dheat, dline, dbar = _build_mutation_datasets(\n        data, raw_bases, mut_list, seqlen, flen\n    )\n\n    # Calculate width if not provided\n    if width is None and dheat[\"score\"]:\n        width = int(height * len(raw_bases) / len(set(dheat[\"mut\"])))\n    elif width is None:\n        width = 400  # Default width\n\n    # Create charts\n    pmerge = _create_mutation_charts(\n        dheat, dbar, dline, width, height, flen, show_score=show_score\n    ).interactive(bind_y=False)\n\n    # Save the plot if requested\n    if save_path and dheat[\"score\"]:\n        pmerge.save(save_path)\n        print(f\"Mutation effects visualization saved to {save_path}\")\n\n    return pmerge\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_polar_bar","title":"plot_polar_bar","text":"<pre><code>plot_polar_bar(\n    data, title=None, width=400, height=400, save_path=None\n)\n</code></pre> <p>Plot a polar bar chart.</p> <p>Returns:</p> Type Description <code>Chart</code> <p>Altair chart object.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_polar_bar(\n    data: dict,\n    title: str | None = None,\n    width: int = 400,\n    height: int = 400,\n    save_path: str | None = None,\n) -&gt; alt.Chart:\n    \"\"\"Plot a polar bar chart.\n\n    Returns:\n        Altair chart object.\n    \"\"\"\n    # _default_colors = [\n    #     \"#a6cee3\",\n    #     \"#1f78b4\",\n    #     \"#b2df8a\",\n    #     \"#33a02c\",\n    #     \"#fb9a99\",\n    #     \"#e31a1c\",\n    #     \"#fdbf6f\",\n    #     \"#ff7f00\",\n    #     \"#cab2d6\",\n    #     \"#6a3d9a\",\n    #     \"#ffff99\",\n    #     \"#b15928\",\n    # ]\n\n    metrics = {}\n    for name in data:\n        metric = name.replace(\"test_\", \"\")\n        if metric in [\n            \"loss\",\n            \"runtime\",\n            \"samples_per_second\",\n            \"steps_per_second\",\n            \"curve\",\n            \"scatter\",\n        ]:\n            continue\n        metrics[metric.capitalize()] = data[name]\n\n    df = pd.DataFrame({\n        \"Metric\": list(metrics.keys()),\n        \"Value\": [v * 100 for v in metrics.values()],\n        \"index\": list(range(len(metrics))),\n        # \"color\": _default_colors[: len(metrics)],\n    })\n\n    base = (\n        alt\n        .Chart(df)\n        .mark_arc(stroke=\"grey\", padAngle=0.05, cornerRadius=10, tooltip=True)\n        .encode(\n            theta=alt.Theta(\"Metric:O\", sort=None),\n            radius=alt.Radius(\"Value\").scale(type=\"sqrt\", zero=True),\n            radius2=alt.datum(5),\n            color=alt.Color(\"Metric:N\", sort=None).scale(scheme=\"tableau20\"),\n            order=alt.Order(\"index:Q\"),\n        )\n    )\n\n    text = base.mark_text(radiusOffset=10).encode(\n        radius=alt.Radius(\"Value:Q\", scale=alt.Scale(type=\"sqrt\", zero=True)),\n        angle=alt.Angle(\"Metric:N\", sort=None),\n        text=alt.Text(\"Value:Q\", format=\".2f\"),\n        color=alt.value(\"black\"),\n    )\n\n    chart = alt.layer(base, text)\n    if title:\n        chart = chart.properties(title=title)\n\n    if save_path:\n        chart.save(save_path)\n        print(f\"Polar bar chart saved to {save_path}\")\n\n    return chart\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_radar","title":"plot_radar","text":"<pre><code>plot_radar(\n    data,\n    metric=\"AUROC\",\n    models=None,\n    width=400,\n    height=400,\n    save_path=None,\n)\n</code></pre> <p>Plot a radar chart.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing the data to plot.</p> required <code>metric</code> <code>str</code> <p>Metric to plot (e.g., \"AUROC\", \"AUPRC\").</p> <code>'AUROC'</code> <code>models</code> <code>str | list[str] | None</code> <p>List of models to include in the chart.</p> <code>None</code> <code>width</code> <code>int</code> <p>Width of the chart.</p> <code>400</code> <code>height</code> <code>int</code> <p>Height of the chart.</p> <code>400</code> <code>save_path</code> <code>str | None</code> <p>Path to save the chart.</p> <code>None</code> <p>Returns:</p> Type Description <code>Chart</code> <p>Altair chart object.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_radar(\n    data: dict,\n    metric: str = \"AUROC\",\n    models: str | list[str] | None = None,\n    width: int = 400,\n    height: int = 400,\n    save_path: str | None = None,\n) -&gt; alt.Chart:\n    \"\"\"\n    Plot a radar chart.\n\n    Args:\n        data: Dictionary containing the data to plot.\n        metric: Metric to plot (e.g., \"AUROC\", \"AUPRC\").\n        models: List of models to include in the chart.\n        width: Width of the chart.\n        height: Height of the chart.\n        save_path: Path to save the chart.\n\n    Returns:\n        Altair chart object.\n    \"\"\"\n    # Load metrics and models if available\n    metrics = []\n    if models is None:\n        default_model = \"Model\"\n    elif isinstance(models, str):\n        default_model = models\n    metric = metric.upper()\n    selected_models = set()\n    for name in data:\n        met = name.replace(\"test_\", \"\")\n        if met == \"curve\":\n            for label in data[met]:\n                item = data[met][label][metric]\n                if isinstance(item, (int, float)):\n                    score = item\n                    metrics.append({\n                        \"labels\": label,\n                        \"score\": score,\n                        \"model\": default_model,\n                    })\n                    selected_models.add(default_model)\n                else:\n                    for model, score in item.items():\n                        if model in models or models is None:\n                            selected_models.add(model)\n                            metrics.append({\n                                \"labels\": label,\n                                \"score\": score,\n                                \"model\": model,\n                            })\n        else:\n            continue\n    # Convert to DataFrame\n    source = pd.DataFrame(metrics)\n    labels = source[\"labels\"].unique()\n    angles = np.linspace(\n        np.pi / 2, np.pi / 2 - 2 * np.pi, len(labels), endpoint=False\n    )\n    cat_angle_map = dict(zip(labels, angles, strict=True))\n\n    source[\"angle\"] = source[\"labels\"].map(cat_angle_map)\n    source[\"x\"] = source[\"score\"] * np.cos(source[\"angle\"])\n    source[\"y\"] = source[\"score\"] * np.sin(source[\"angle\"])\n\n    closed_source = []\n    for model in selected_models:\n        subset = source[source[\"model\"] == model].copy()\n        first_row = subset.iloc[[0]].copy()\n        closed_source.append(pd.concat([subset, first_row], ignore_index=True))\n    plot_df = pd.concat(closed_source).reset_index()\n\n    # Create base radar chart\n    rings = pd.DataFrame({\"ring\": [0.2, 0.4, 0.6, 0.8, 1.0]})\n    base_rings = (\n        alt\n        .Chart(rings)\n        .mark_arc(stroke=\"lightgrey\", fill=None)\n        .encode(\n            theta=alt.value(np.pi * 2), radius=alt.Radius(\"ring\").stack(False)\n        )\n    )\n    base_rings_labels = base_rings.mark_text(\n        color=\"black\", radiusOffset=5, align=\"center\"\n    ).encode(text=\"ring\", theta=alt.value(np.pi / len(labels)))\n    out_rings = (\n        alt\n        .Chart(rings)\n        .mark_arc(stroke=\"black\", fill=None)\n        .encode(theta=alt.value(np.pi * 2), radius=alt.datum(1))\n    )\n    # Create spokes\n    spokes = (\n        alt\n        .Chart(plot_df)\n        .mark_arc(stroke=\"lightgrey\", fill=None, strokeDash=[5, 5])\n        .encode(\n            theta=alt.Theta(\"labels:O\"),\n            radius=alt.datum(1),\n            radius2=alt.datum(0),\n        )\n    )\n    labels_text = (\n        alt\n        .Chart(plot_df)\n        .mark_text(radiusOffset=10, align=\"center\")\n        .encode(\n            radius=alt.datum(1.1),\n            theta=alt.Theta(\"labels:O\", sort=None),\n            text=alt.Text(\"labels\"),\n            color=alt.value(\"black\"),\n        )\n    )\n    # Create radar fill and line\n    radar_fill = (\n        alt\n        .Chart(plot_df)\n        .mark_line(filled=True, fillOpacity=0.3)\n        .encode(\n            x=alt.X(\"x\", scale=alt.Scale(domain=[-1, 1]), axis=None),\n            y=alt.Y(\"y\", scale=alt.Scale(domain=[-1, 1]), axis=None),\n            color=\"model\",\n            order=alt.Order(\"index\"),\n        )\n    )\n    radar_line = (\n        alt\n        .Chart(plot_df)\n        .mark_line(\n            point=True,\n            fill=None,\n        )\n        .encode(\n            x=alt.X(\"x\", scale=alt.Scale(domain=[-1, 1]), axis=None),\n            y=alt.Y(\"y\", scale=alt.Scale(domain=[-1, 1]), axis=None),\n            color=\"model\",\n            order=alt.Order(\"index\"),\n        )\n    )\n    # Create merge and configure chart\n    chart: alt.Chart = (\n        (\n            base_rings\n            + spokes\n            + out_rings\n            + radar_fill\n            + radar_line\n            + base_rings_labels\n            + labels_text\n        )\n        .properties(width=width, height=height, title=metric)\n        .configure_view(stroke=None)\n        .configure_axis(grid=False, domain=False, ticks=False)\n    )\n    # Check if save path is provided\n    if save_path:\n        chart.save(save_path)\n        print(f\"Radar chart saved to {save_path}\")\n    return chart\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_scatter","title":"plot_scatter","text":"<pre><code>plot_scatter(\n    data,\n    show_score=True,\n    ncols=3,\n    width=400,\n    height=400,\n    save_path=None,\n    separate=False,\n)\n</code></pre> <p>Plot scatter plots for regression task evaluation.</p> <p>This function creates scatter plots to compare predicted vs. experimental     values     for regression tasks, with optional R\u00b2 score display.</p> <pre><code>Args:\n    data: Dictionary containing scatter plot data for each model\n    show_score: Whether to show the R\u00b2 score on the plot\n    ncols: Number of columns to arrange the plots\n    width: Width of each plot\n    height: Height of each plot\n            save_path: Path to save the plot. If None,\n        plot will be shown interactively\n    separate: Whether to return separate plots for each model\n\nReturns:\n            Altair chart object (combined or\n        separate plots based on separate parameter)\n</code></pre> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_scatter(\n    data: dict,\n    show_score: bool = True,\n    ncols: int = 3,\n    width: int = 400,\n    height: int = 400,\n    save_path: str | None = None,\n    separate: bool = False,\n) -&gt; alt.Chart | dict[str, alt.Chart]:\n    \"\"\"Plot scatter plots for regression task evaluation.\n\n    This function creates scatter plots to compare predicted vs. experimental\n        values\n        for regression tasks, with optional R\u00b2 score display.\n\n        Args:\n            data: Dictionary containing scatter plot data for each model\n            show_score: Whether to show the R\u00b2 score on the plot\n            ncols: Number of columns to arrange the plots\n            width: Width of each plot\n            height: Height of each plot\n                    save_path: Path to save the plot. If None,\n                plot will be shown interactively\n            separate: Whether to return separate plots for each model\n\n        Returns:\n                    Altair chart object (combined or\n                separate plots based on separate parameter)\n    \"\"\"\n    from scipy.stats import gaussian_kde\n\n    # Pre-allocate plot dictionaries for better memory management\n    # Original: pdot = {}; p_separate = {}\n    pdot = {}\n    p_separate = {}\n\n    # More efficient data processing with list comprehension\n    # Original: for n, model in enumerate(data):\n    for n, (model, model_data) in enumerate(data.items()):\n        # Create a copy to avoid modifying original data\n        # Original: scatter_data = dict(data[model])\n        scatter_data = dict(model_data)\n        r2 = scatter_data.pop(\"r2\", 0)  # Use pop with default value\n\n        # More efficient DataFrame creation\n        # Original: ddot = pd.DataFrame(scatter_data)\n        ddot = pd.DataFrame(scatter_data)\n\n        try:\n            # Make density calculation more efficient\n            xy = np.vstack([ddot[\"experiment\"], ddot[\"predicted\"]])\n            ddot[\"density\"] = gaussian_kde(xy)(xy)\n            density_calculated = True\n\n            # Order by density\n            ddot = ddot.sort_values(by=\"density\", ascending=True)\n\n        except (np.linalg.LinAlgError, ValueError):\n            # If KDE fails (e.g., all points are identical), fall back\n            ddot[\"density\"] = 1.0\n            density_calculated = False\n\n        # Create scatter plot with optimized encoding\n        # dot = (\n        #     alt.Chart(ddot, title=model)\n        #     .mark_point(filled=True)\n        #     .encode(\n        #         x=alt.X(\"predicted:Q\"),\n        #         y=alt.Y(\"experiment:Q\"),\n        #     )\n        #     .properties(width=width, height=height)\n        # )\n        base = alt.Chart(ddot, title=model).properties(\n            width=width, height=height\n        )\n        dot = base.mark_point(filled=True, size=15, opacity=1).encode(\n            x=alt.X(\"experiment:Q\", title=\"Observed\"),\n            y=alt.Y(\"predicted:Q\", title=\"Predicted\"),\n            color=alt.Color(\n                \"density:Q\",\n                scale=alt.Scale(scheme=\"viridis\"),\n                legend=alt.Legend(title=\"Density\"),\n            )\n            if density_calculated\n            else alt.value(\"blue\"),\n            tooltip=[\"experiment\", \"predicted\", \"density\"],\n        )\n\n        if show_score:\n            # More efficient text positioning calculation\n            # Original: min_x = ddot[\"predicted\"].min(); max_y =\n            # ddot[\"experiment\"].max()\n            min_x = ddot[\"predicted\"].min()\n            max_y = ddot[\"experiment\"].max()\n\n            text = (\n                alt\n                .Chart()\n                .mark_text(size=14, align=\"left\", baseline=\"top\", dx=5, dy=5)\n                .encode(\n                    x=alt.datum(min_x + 0.5),\n                    y=alt.datum(max_y - 0.5),\n                    text=alt.datum(f\"R\u00b2={r2:.3f}\"),  # Format R\u00b2 value\n                )\n            )\n            p = dot + text\n        else:\n            p = dot\n\n        if separate:\n            p_separate[model] = p.configure_axis(grid=False)\n\n        # More efficient plot arrangement\n        idx = n // ncols\n        if n % ncols == 0:\n            pdot[idx] = p\n        else:\n            pdot[idx] |= p\n\n    # More efficient plot combination\n    # Original: Multiple conditional checks and assignments\n    pdots: alt.Chart = pdot[0] if pdot else alt.Chart()\n    for i in range(1, len(pdot)):\n        pdots &amp;= pdot[i]\n\n    # Configure chart once at the end\n    pdots = pdots.configure_axis(grid=False)\n\n    # Save the plot\n    if save_path:\n        pdots.save(save_path)\n        print(f\"Metrics scatter plots saved to {save_path}\")\n\n    if separate:\n        return p_separate\n    else:\n        return pdots\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.plot_token_scatter","title":"plot_token_scatter","text":"<pre><code>plot_token_scatter(\n    scores,\n    threshold_std=2.0,\n    show_labels=False,\n    extra_data=None,\n    width=800,\n    height=300,\n    save_path=None,\n)\n</code></pre> <p>Uses Z-score to plot outlier scatter plot.</p> <p>This function takes a list of (name, value) pairs, computes Z-scores, and uses Altair to plot a scatter plot highlighting outliers with different colors and sizes. This mimics the functionality of the matplotlib code you provided.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>list[tuple[str, float]]</code> <p>a list of [(name, value), ...]\u3002</p> required <code>threshold_std</code> <code>float</code> <p>Z-score threshold to identify outliers.</p> <code>2.0</code> <code>width</code> <code>int</code> <p>figure width.</p> <code>800</code> <code>height</code> <code>int</code> <p>figure height.</p> <code>300</code> <code>save_path</code> <code>str | None</code> <p>If provided, saves the plot to this path.</p> <code>None</code> <p>Returns:</p> Type Description <code>Chart</code> <p>alt.Chart: Altair chart object.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_token_scatter(\n    scores: list[tuple[str, float]],\n    threshold_std: float = 2.0,\n    show_labels: bool = False,\n    extra_data: list[tuple[str, int, int]] | None = None,\n    width: int = 800,\n    height: int = 300,\n    save_path: str | None = None,\n) -&gt; alt.Chart:\n    \"\"\"\n    Uses Z-score to plot outlier scatter plot.\n\n    This function takes a list of (name, value) pairs, computes Z-scores,\n    and uses Altair to plot a scatter plot highlighting outliers with different\n    colors and sizes. This mimics the functionality of the matplotlib code you\n    provided.\n\n    Args:\n        scores: a list of [(name, value), ...]\u3002\n        threshold_std: Z-score threshold to identify outliers.\n        width: figure width.\n        height: figure height.\n        save_path: If provided, saves the plot to this path.\n\n    Returns:\n        alt.Chart: Altair chart object.\n    \"\"\"\n\n    try:\n        df = pd.DataFrame(\n            [(x[0], -x[1]) for x in scores], columns=[\"Token\", \"Value\"]\n        )\n    except Exception as e:\n        print(f\"Format Error: {e}\")\n        return alt.Chart()\n\n    # Use index as a base x-axis\n    df = df.reset_index()\n    mean_val = df[\"Value\"].mean()\n    std_val = df[\"Value\"].std()\n    # Calculate upper limit for outlier detection\n    upper_limit = mean_val + threshold_std * std_val\n\n    # Mark outliers based on Z-score\n    df[\"zscore\"] = (df[\"Value\"] - mean_val) / std_val\n    # Only consider positive Z-scores for outlier detection\n    df[\"is_outlier\"] = df[\"zscore\"] &gt; threshold_std\n\n    # For legend and size encoding\n    num_outliers = df[\"is_outlier\"].sum()\n    num_normal = len(df) - num_outliers\n\n    df[\"Status\"] = np.where(\n        df[\"is_outlier\"],\n        f\"Outlier (n={num_outliers})\",\n        f\"Normal (n={num_normal})\",\n    )\n\n    # Base Scatter Layer\n    # Use status for color encoding\n    base_scatter = (\n        alt\n        .Chart(df)\n        .mark_point(filled=True)\n        .encode(\n            x=alt.X(\"index:Q\", title=\"Token Index\"),\n            y=alt.Y(\n                \"Value:Q\",\n                title=\"-LogP Score\",\n                scale=alt.Scale(domain=(0.0, df[\"Value\"].max() * 1.1)),\n            ),\n            color=alt.Color(\n                \"Status:N\",\n                scale=alt.Scale(\n                    domain=[\n                        f\"Outlier (n={num_outliers})\",\n                        f\"Normal (n={num_normal})\",\n                    ],\n                    range=[\"#fb8072\", \"#80b1d3\"],\n                ),\n                legend=alt.Legend(title=\"Importance\"),\n            ),\n            # Point size encoding\n            size=alt.condition(\n                alt.datum.is_outlier, alt.value(40), alt.value(20)\n            ),\n            # Alpha encoding\n            opacity=alt.condition(\n                alt.datum.is_outlier, alt.value(1.0), alt.value(0.6)\n            ),\n            tooltip=[\"index\", \"Token\", \"Value\", \"zscore\"],\n        )\n    )\n    # Rule Layers\n    line_data = pd.DataFrame([\n        {\n            \"label\": f\"Mean ({mean_val:.2f})\",\n            \"value\": mean_val,\n            \"color\": \"grey\",\n            \"dash\": [3, 3],\n        },\n        {\n            \"label\": f\"+{threshold_std}\u03c3 ({upper_limit:.2f})\",  # noqa: RUF001\n            \"value\": upper_limit,\n            \"color\": \"orange\",\n            \"dash\": [3, 3],\n        },\n    ])\n    rule_lines = (\n        alt\n        .Chart(line_data)\n        .mark_rule(strokeWidth=2)\n        .encode(\n            y=\"value:Q\",\n            color=alt.Color(\n                \"label:N\",\n                scale={\n                    \"domain\": line_data[\"label\"].tolist(),\n                    \"range\": line_data[\"color\"].tolist(),\n                },\n                legend=alt.Legend(title=\"Threshold\"),\n            ),\n            strokeDash=alt.StrokeDash(\n                \"label:N\",\n                scale={\n                    \"domain\": line_data[\"label\"].tolist(),\n                    \"range\": line_data[\"dash\"].tolist(),\n                },\n                legend=None,\n            ),\n        )\n    )\n    # Text Layer for Outliers\n    text_labels = (\n        alt\n        .Chart(df)\n        .mark_text(align=\"left\", dx=5, color=\"darkred\")\n        .encode(x=\"index:Q\", y=\"Value:Q\", text=\"Token:N\")\n        .transform_filter(alt.datum.is_outlier)\n    )\n    if extra_data:\n        # Convert to zero started index if necessary\n        start_pos = extra_data[0][1]\n        if len(extra_data[0]) == 4:\n            color_map = {item[0]: item[3] for item in extra_data}\n        else:\n            color_map = {}\n        for i, item in enumerate(extra_data):\n            region_type, start, end = item[:3]\n            extra_data[i] = (region_type, start - start_pos, end - start_pos)\n        extra_df = pd.DataFrame(extra_data, columns=[\"Type\", \"Start\", \"End\"])\n        # assign colors if provided\n        if color_map:\n            domain = list(color_map.keys())\n            range_colors = [color_map[k] for k in domain]\n        else:\n            # assign default colors based on region type\n            # Use Set3 color palette (12 colors)\n            colors = [\n                \"#8dd3c7\",\n                \"#ffffb3\",\n                \"#bebada\",\n                \"#fb8072\",\n                \"#80b1d3\",\n                \"#fdb462\",\n                \"#b3b3b3\",\n                \"#fccde5\",\n                \"#d9d9d9\",\n                \"#bc80bd\",\n                \"#ccebc5\",\n                \"#ffed6f\",\n            ]\n            unique_types = list(\n                dict.fromkeys([x[0] for x in extra_data])\n            )  # Preserve order\n            domain = unique_types\n            range_colors = colors[: len(unique_types)]\n        band_chart = (\n            alt\n            .Chart(extra_df)\n            .mark_rect(opacity=0.3)\n            .encode(\n                x=alt.X(\"Start:Q\", title=\"Token Index\"),\n                x2=\"End:Q\",\n                color=alt.Color(\n                    \"Type:N\",\n                    scale=alt.Scale(domain=domain, range=range_colors),\n                    legend=alt.Legend(title=\"Region Type\"),\n                ),\n                tooltip=[\"Type\", \"Start\", \"End\"],\n            )\n        )\n\n    # Combine Layers\n    chart: alt.Chart = (base_scatter + rule_lines).resolve_scale(\n        color=\"independent\"\n    )\n    if show_labels:\n        chart += text_labels\n    if extra_data:\n        chart = band_chart + chart\n    chart = (\n        chart\n        .properties(width=width, height=height)\n        .configure_axis(grid=False)\n        .interactive()\n    )\n\n    if save_path:\n        chart.save(save_path)\n        print(f\"Figure saved to {save_path}\")\n\n    return chart\n</code></pre>"},{"location":"api/inference/plot/#dnallm.inference.plot.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(metrics, task_type='binary')\n</code></pre> <p>Prepare data for plotting various types of visualizations.</p> <p>This function organizes model metrics data into formats suitable for     different plot types:     - Bar charts for classification and regression metrics     - ROC and PR curves for classification tasks     - Scatter plots for regression tasks</p> <pre><code>Args:\n    metrics: Dictionary containing model metrics for different models\n            task_type: Type of task (\n        \"binary\",\n        \"multiclass\",\n        \"multilabel\",\n        \"token\",\n        \"regression\")\n\nReturns:\n    Tuple containing:\n    - bars_data: Data formatted for bar chart visualization\n            - curves_data/scatter_data: Data formatted for curve or\n        scatter plot visualization\n\nRaises:\n    ValueError: If task type is not supported for plotting\n</code></pre> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def prepare_data(\n    metrics: dict[str, dict], task_type: str = \"binary\"\n) -&gt; tuple[dict, dict | dict]:\n    \"\"\"Prepare data for plotting various types of visualizations.\n\n    This function organizes model metrics data into formats suitable for\n        different plot types:\n        - Bar charts for classification and regression metrics\n        - ROC and PR curves for classification tasks\n        - Scatter plots for regression tasks\n\n        Args:\n            metrics: Dictionary containing model metrics for different models\n                    task_type: Type of task (\n                \"binary\",\n                \"multiclass\",\n                \"multilabel\",\n                \"token\",\n                \"regression\")\n\n        Returns:\n            Tuple containing:\n            - bars_data: Data formatted for bar chart visualization\n                    - curves_data/scatter_data: Data formatted for curve or\n                scatter plot visualization\n\n        Raises:\n            ValueError: If task type is not supported for plotting\n    \"\"\"\n    if task_type in [\"binary\", \"multiclass\", \"multilabel\", \"token\"]:\n        return _prepare_classification_data(metrics)\n    elif task_type == \"regression\":\n        return _prepare_regression_data(metrics)\n    else:\n        raise ValueError(f\"Unsupported task type {task_type} for plotting\")\n</code></pre>"},{"location":"api/mcp/config_manager/","title":"MCP Config Manager","text":""},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager","title":"dnallm.mcp.config_manager","text":"<p>Configuration Manager for MCP Server.</p> <p>This module provides configuration management functionality for the MCP server, including loading, validating, and managing both main server configurations and individual model configurations.</p>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager-classes","title":"Classes","text":""},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager","title":"MCPConfigManager","text":"<pre><code>MCPConfigManager(\n    config_dir, server_config_file=\"mcp_server_config.yaml\"\n)\n</code></pre> <p>Manages MCP server configurations and model configurations.</p> <p>Initialize the configuration manager.</p> <p>Parameters:</p> Name Type Description Default <code>config_dir</code> <code>str</code> <p>Path to the configuration directory</p> required <code>server_config_file</code> <code>str</code> <p>Name of the server configuration file</p> <code>'mcp_server_config.yaml'</code> Source code in <code>dnallm/mcp/config_manager.py</code> <pre><code>def __init__(\n    self,\n    config_dir: str,\n    server_config_file: str = \"mcp_server_config.yaml\",\n):\n    \"\"\"Initialize the configuration manager.\n\n    Args:\n        config_dir: Path to the configuration directory\n        server_config_file: Name of the server configuration file\n    \"\"\"\n    self.config_dir = Path(config_dir)\n    self.server_config_path = self.config_dir / server_config_file\n    self.server_config: MCPServerConfig | None = None\n    self.model_configs: dict[str, InferenceModelConfig] = {}\n    self._load_configurations()\n</code></pre>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager-functions","title":"Functions","text":""},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager.get_enabled_models","title":"get_enabled_models","text":"<pre><code>get_enabled_models()\n</code></pre> <p>Get list of enabled model names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of enabled model names</p> Source code in <code>dnallm/mcp/config_manager.py</code> <pre><code>def get_enabled_models(self) -&gt; list[str]:\n    \"\"\"Get list of enabled model names.\n\n    Returns:\n        List of enabled model names\n    \"\"\"\n    if not self.server_config:\n        return []\n\n    return [\n        name\n        for name, entry in self.server_config.models.items()\n        if entry.enabled\n    ]\n</code></pre>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager.get_logging_config","title":"get_logging_config","text":"<pre><code>get_logging_config()\n</code></pre> <p>Get logging configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of logging configuration parameters</p> Source code in <code>dnallm/mcp/config_manager.py</code> <pre><code>def get_logging_config(self) -&gt; dict[str, Any]:\n    \"\"\"Get logging configuration.\n\n    Returns:\n        Dictionary of logging configuration parameters\n    \"\"\"\n    if not self.server_config:\n        return {}\n\n    logging_config = self.server_config.logging\n    return {\n        \"level\": logging_config.level,\n        \"format\": logging_config.format,\n        \"file\": logging_config.file,\n        \"max_size\": logging_config.max_size,\n        \"backup_count\": logging_config.backup_count,\n    }\n</code></pre>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager.get_model_config","title":"get_model_config","text":"<pre><code>get_model_config(model_name)\n</code></pre> <p>Get configuration for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model</p> required <p>Returns:</p> Type Description <code>InferenceModelConfig | None</code> <p>InferenceModelConfig object or None if not found</p> Source code in <code>dnallm/mcp/config_manager.py</code> <pre><code>def get_model_config(self, model_name: str) -&gt; InferenceModelConfig | None:\n    \"\"\"Get configuration for a specific model.\n\n    Args:\n        model_name: Name of the model\n\n    Returns:\n        InferenceModelConfig object or None if not found\n    \"\"\"\n    return self.model_configs.get(model_name)\n</code></pre>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager.get_model_configs","title":"get_model_configs","text":"<pre><code>get_model_configs()\n</code></pre> <p>Get all loaded model configurations.</p> <p>Returns:</p> Type Description <code>dict[str, InferenceModelConfig]</code> <p>Dictionary of model configurations</p> Source code in <code>dnallm/mcp/config_manager.py</code> <pre><code>def get_model_configs(self) -&gt; dict[str, InferenceModelConfig]:\n    \"\"\"Get all loaded model configurations.\n\n    Returns:\n        Dictionary of model configurations\n    \"\"\"\n    return self.model_configs.copy()\n</code></pre>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager.get_model_info_summary","title":"get_model_info_summary","text":"<pre><code>get_model_info_summary()\n</code></pre> <p>Get summary information about all loaded models.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing model information summary</p> Source code in <code>dnallm/mcp/config_manager.py</code> <pre><code>def get_model_info_summary(self) -&gt; dict[str, Any]:\n    \"\"\"Get summary information about all loaded models.\n\n    Returns:\n        Dictionary containing model information summary\n    \"\"\"\n    summary = {\n        \"total_models\": len(self.model_configs),\n        \"enabled_models\": len(self.get_enabled_models()),\n        \"models\": {},\n    }\n\n    for model_name, model_config in self.model_configs.items():\n        summary[\"models\"][model_name] = {\n            \"task_type\": model_config.task.task_type,\n            \"num_labels\": model_config.task.num_labels,\n            \"label_names\": model_config.task.label_names,\n            \"model_path\": model_config.model.path,\n            \"model_source\": model_config.model.source,\n            \"architecture\": model_config.model.task_info.architecture,\n            \"tokenizer\": model_config.model.task_info.tokenizer,\n            \"species\": model_config.model.task_info.species,\n            \"task_category\": model_config.model.task_info.task_category,\n        }\n\n    return summary\n</code></pre>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager.get_model_priority","title":"get_model_priority","text":"<pre><code>get_model_priority(model_name)\n</code></pre> <p>Get priority of a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model</p> required <p>Returns:</p> Type Description <code>int</code> <p>Priority value (1-10, higher is more important)</p> Source code in <code>dnallm/mcp/config_manager.py</code> <pre><code>def get_model_priority(self, model_name: str) -&gt; int:\n    \"\"\"Get priority of a specific model.\n\n    Args:\n        model_name: Name of the model\n\n    Returns:\n        Priority value (1-10, higher is more important)\n    \"\"\"\n    if (\n        not self.server_config\n        or model_name not in self.server_config.models\n    ):\n        return 1\n\n    return self.server_config.models[model_name].priority\n</code></pre>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager.get_multi_model_configs","title":"get_multi_model_configs","text":"<pre><code>get_multi_model_configs()\n</code></pre> <p>Get multi-model parallel prediction configurations.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of multi-model configurations</p> Source code in <code>dnallm/mcp/config_manager.py</code> <pre><code>def get_multi_model_configs(self) -&gt; dict[str, Any]:\n    \"\"\"Get multi-model parallel prediction configurations.\n\n    Returns:\n        Dictionary of multi-model configurations\n    \"\"\"\n    if not self.server_config:\n        return {}\n\n    return {\n        name: {\n            \"name\": config.name,\n            \"description\": config.description,\n            \"models\": config.models,\n            \"enabled\": config.enabled,\n        }\n        for name, config in self.server_config.multi_model.items()\n    }\n</code></pre>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager.get_server_config","title":"get_server_config","text":"<pre><code>get_server_config()\n</code></pre> <p>Get the main server configuration.</p> <p>Returns:</p> Type Description <code>MCPServerConfig | None</code> <p>MCPServerConfig object or None if not loaded</p> Source code in <code>dnallm/mcp/config_manager.py</code> <pre><code>def get_server_config(self) -&gt; MCPServerConfig | None:\n    \"\"\"Get the main server configuration.\n\n    Returns:\n        MCPServerConfig object or None if not loaded\n    \"\"\"\n    return self.server_config\n</code></pre>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager.get_sse_config","title":"get_sse_config","text":"<pre><code>get_sse_config()\n</code></pre> <p>Get SSE configuration.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of SSE configuration parameters</p> Source code in <code>dnallm/mcp/config_manager.py</code> <pre><code>def get_sse_config(self) -&gt; dict[str, Any]:\n    \"\"\"Get SSE configuration.\n\n    Returns:\n        Dictionary of SSE configuration parameters\n    \"\"\"\n    if not self.server_config:\n        return {}\n\n    sse_config = self.server_config.sse\n    return {\n        \"heartbeat_interval\": sse_config.heartbeat_interval,\n        \"max_connections\": sse_config.max_connections,\n        \"connection_timeout\": sse_config.connection_timeout,\n        \"enable_compression\": sse_config.enable_compression,\n    }\n</code></pre>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager.reload_configurations","title":"reload_configurations","text":"<pre><code>reload_configurations()\n</code></pre> <p>Reload all configurations from files.</p> Source code in <code>dnallm/mcp/config_manager.py</code> <pre><code>def reload_configurations(self) -&gt; None:\n    \"\"\"Reload all configurations from files.\"\"\"\n    logger.info(\"Reloading configurations...\")\n    self.model_configs.clear()\n    self._load_configurations()\n    logger.info(\"Configurations reloaded successfully\")\n</code></pre>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager.MCPConfigManager.validate_model_references","title":"validate_model_references","text":"<pre><code>validate_model_references()\n</code></pre> <p>Validate that all model references are valid.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of validation error messages</p> Source code in <code>dnallm/mcp/config_manager.py</code> <pre><code>def validate_model_references(self) -&gt; list[str]:\n    \"\"\"Validate that all model references are valid.\n\n    Returns:\n        List of validation error messages\n    \"\"\"\n    errors = []\n\n    if not self.server_config:\n        errors.append(\"Server configuration not loaded\")\n        return errors\n\n    # Check that all multi-model configurations reference existing models\n    available_models = set(self.server_config.models.keys())\n\n    for multi_name, multi_config in self.server_config.multi_model.items():\n        for model_name in multi_config.models:\n            if model_name not in available_models:\n                errors.append(\n                    f\"Multi-model config '{multi_name}' references \"\n                    f\"non-existent model '{model_name}'\"\n                )\n\n    # Check that all model config files exist and are valid\n    for model_name, _model_entry in self.server_config.models.items():\n        if model_name not in self.model_configs:\n            errors.append(\n                f\"Model configuration not loaded for '{model_name}'\"\n            )\n\n    return errors\n</code></pre>"},{"location":"api/mcp/config_manager/#dnallm.mcp.config_manager-functions","title":"Functions","text":""},{"location":"api/mcp/config_validators/","title":"MCP Config Validators","text":""},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators","title":"dnallm.mcp.config_validators","text":"<p>Configuration validators for MCP Server.</p> <p>This module provides Pydantic models for validating MCP server configurations and inference model configurations.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators-classes","title":"Classes","text":""},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.InferenceConfig","title":"InferenceConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Inference configuration for model prediction.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.InferenceConfig-functions","title":"Functions","text":""},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.InferenceConfig.set_use_fp16","title":"set_use_fp16  <code>classmethod</code>","text":"<pre><code>set_use_fp16(v, info)\n</code></pre> <p>Set use_fp16 based on precision setting.</p> Source code in <code>dnallm/mcp/config_validators.py</code> <pre><code>@field_validator(\"use_fp16\", mode=\"before\")\n@classmethod\ndef set_use_fp16(cls, v, info):\n    \"\"\"Set use_fp16 based on precision setting.\"\"\"\n    if info.data and \"precision\" in info.data:\n        return info.data[\"precision\"] == \"float16\"\n    return v\n</code></pre>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.InferenceModelConfig","title":"InferenceModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete inference model configuration.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.InferenceModelConfig-functions","title":"Functions","text":""},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.InferenceModelConfig.validate_task_labels","title":"validate_task_labels  <code>classmethod</code>","text":"<pre><code>validate_task_labels(v)\n</code></pre> <p>Validate that num_labels matches label_names length.</p> Source code in <code>dnallm/mcp/config_validators.py</code> <pre><code>@field_validator(\"task\")\n@classmethod\ndef validate_task_labels(cls, v):\n    \"\"\"Validate that num_labels matches label_names length.\"\"\"\n    if v.num_labels != len(v.label_names):\n        raise ValueError(\"num_labels must match the length of label_names\")\n    return v\n</code></pre>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.LoggingConfig","title":"LoggingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Logging configuration.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.MCPConfig","title":"MCPConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MCP protocol configuration.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.MCPServerConfig","title":"MCPServerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete MCP server configuration.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.MCPServerConfig-functions","title":"Functions","text":""},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.MCPServerConfig.validate_model_names","title":"validate_model_names  <code>classmethod</code>","text":"<pre><code>validate_model_names(v)\n</code></pre> <p>Validate that model names are unique.</p> Source code in <code>dnallm/mcp/config_validators.py</code> <pre><code>@field_validator(\"models\")\n@classmethod\ndef validate_model_names(cls, v):\n    \"\"\"Validate that model names are unique.\"\"\"\n    names = [entry.name for entry in v.values()]\n    if len(names) != len(set(names)):\n        raise ValueError(\"Model names must be unique\")\n    return v\n</code></pre>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.MCPServerConfig.validate_multi_model_references","title":"validate_multi_model_references  <code>classmethod</code>","text":"<pre><code>validate_multi_model_references(v, info)\n</code></pre> <p>Validate that multi-model configurations reference existing models.</p> Source code in <code>dnallm/mcp/config_validators.py</code> <pre><code>@field_validator(\"multi_model\")\n@classmethod\ndef validate_multi_model_references(cls, v, info):\n    \"\"\"Validate that multi-model configurations reference existing\n    models.\"\"\"\n    if info.data and \"models\" in info.data:\n        available_models = set(info.data[\"models\"].keys())\n        for config in v.values():\n            for model_name in config.models:\n                if model_name not in available_models:\n                    raise ValueError(\n                        f\"Model '{model_name}' referenced in multi-model \"\n                        f\"config but not defined in models\"\n                    )\n    return v\n</code></pre>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.ModelConfig","title":"ModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Individual model configuration.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.ModelEntryConfig","title":"ModelEntryConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model entry in the main configuration.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.ModelEntryConfig-functions","title":"Functions","text":""},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.ModelEntryConfig.validate_config_path","title":"validate_config_path  <code>classmethod</code>","text":"<pre><code>validate_config_path(v)\n</code></pre> <p>Validate config path format.</p> Source code in <code>dnallm/mcp/config_validators.py</code> <pre><code>@field_validator(\"config_path\")\n@classmethod\ndef validate_config_path(cls, v):\n    \"\"\"Validate config path format.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"Config path cannot be empty\")\n    return v\n</code></pre>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.ModelInfoConfig","title":"ModelInfoConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model information configuration.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.MultiModelConfig","title":"MultiModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Multi-model parallel prediction configuration.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.SSEConfig","title":"SSEConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>SSE (Server-Sent Events) configuration.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.ServerConfig","title":"ServerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Server configuration.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.TaskConfig","title":"TaskConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Task configuration for DNA prediction models.</p>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators-functions","title":"Functions","text":""},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.validate_inference_model_config","title":"validate_inference_model_config","text":"<pre><code>validate_inference_model_config(config_path)\n</code></pre> <p>Validate inference model configuration file.</p> Source code in <code>dnallm/mcp/config_validators.py</code> <pre><code>def validate_inference_model_config(config_path: str) -&gt; InferenceModelConfig:\n    \"\"\"Validate inference model configuration file.\"\"\"\n    import yaml\n\n    with open(config_path, encoding=\"utf-8\") as f:\n        config_dict = yaml.safe_load(f)\n\n    return InferenceModelConfig(**config_dict)\n</code></pre>"},{"location":"api/mcp/config_validators/#dnallm.mcp.config_validators.validate_mcp_server_config","title":"validate_mcp_server_config","text":"<pre><code>validate_mcp_server_config(config_path)\n</code></pre> <p>Validate MCP server configuration file.</p> Source code in <code>dnallm/mcp/config_validators.py</code> <pre><code>def validate_mcp_server_config(config_path: str) -&gt; MCPServerConfig:\n    \"\"\"Validate MCP server configuration file.\"\"\"\n    import yaml\n\n    with open(config_path, encoding=\"utf-8\") as f:\n        config_dict = yaml.safe_load(f)\n\n    return MCPServerConfig(**config_dict)\n</code></pre>"},{"location":"api/mcp/model_manager/","title":"MCP Model Manager","text":""},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager","title":"dnallm.mcp.model_manager","text":"<p>Model Manager for MCP Server.</p> <p>This module provides model management functionality for the MCP server, including model loading, caching, and prediction orchestration.</p>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager-classes","title":"Classes","text":""},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager","title":"ModelManager","text":"<pre><code>ModelManager(config_manager)\n</code></pre> <p>Manages DNA prediction models and their lifecycle.</p> <p>Initialize the model manager.</p> <p>Parameters:</p> Name Type Description Default <code>config_manager</code> <code>MCPConfigManager</code> <p>MCPConfigManager instance</p> required Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>def __init__(self, config_manager: MCPConfigManager):\n    \"\"\"Initialize the model manager.\n\n    Args:\n        config_manager: MCPConfigManager instance\n    \"\"\"\n    self.config_manager = config_manager\n    self.loaded_models: dict[str, DNAInference] = {}\n    self.model_loading_status: dict[\n        str, str\n    ] = {}  # \"loading\", \"loaded\", \"error\"\n    self._loading_lock = asyncio.Lock()\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager-functions","title":"Functions","text":""},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager.get_all_models_info","title":"get_all_models_info","text":"<pre><code>get_all_models_info()\n</code></pre> <p>Get information about all configured models.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>Dictionary mapping model names to their information</p> Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>def get_all_models_info(self) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Get information about all configured models.\n\n    Returns:\n        Dictionary mapping model names to their information\n    \"\"\"\n    all_models = {}\n    for model_name in self.config_manager.get_enabled_models():\n        info = self.get_model_info(model_name)\n        if info:\n            all_models[model_name] = info\n\n    return all_models\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager.get_inference_engine","title":"get_inference_engine","text":"<pre><code>get_inference_engine(model_name)\n</code></pre> <p>Get inference engine instance for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model</p> required <p>Returns:</p> Type Description <code>DNAInference | None</code> <p>DNAInference instance or None if not loaded</p> Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>def get_inference_engine(self, model_name: str) -&gt; DNAInference | None:\n    \"\"\"Get inference engine instance for a specific model.\n\n    Args:\n        model_name: Name of the model\n\n    Returns:\n        DNAInference instance or None if not loaded\n    \"\"\"\n    return self.loaded_models.get(model_name)\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager.get_loaded_models","title":"get_loaded_models","text":"<pre><code>get_loaded_models()\n</code></pre> <p>Get list of currently loaded model names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of loaded model names</p> Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>def get_loaded_models(self) -&gt; list[str]:\n    \"\"\"Get list of currently loaded model names.\n\n    Returns:\n        List of loaded model names\n    \"\"\"\n    return list(self.loaded_models.keys())\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager.get_model_info","title":"get_model_info","text":"<pre><code>get_model_info(model_name)\n</code></pre> <p>Get information about a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Model information dictionary or None if not found</p> Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>def get_model_info(self, model_name: str) -&gt; dict[str, Any] | None:\n    \"\"\"Get information about a specific model.\n\n    Args:\n        model_name: Name of the model\n\n    Returns:\n        Model information dictionary or None if not found\n    \"\"\"\n    model_config = self.config_manager.get_model_config(model_name)\n    if not model_config:\n        return None\n\n    inference_engine = self.get_inference_engine(model_name)\n\n    info = {\n        \"name\": model_name,\n        \"task_type\": model_config.task.task_type,\n        \"num_labels\": model_config.task.num_labels,\n        \"label_names\": model_config.task.label_names,\n        \"model_path\": model_config.model.path,\n        \"model_source\": model_config.model.source,\n        \"architecture\": model_config.model.task_info.architecture,\n        \"tokenizer\": model_config.model.task_info.tokenizer,\n        \"species\": model_config.model.task_info.species,\n        \"task_category\": model_config.model.task_info.task_category,\n        \"performance_metrics\": (\n            model_config.model.task_info.performance_metrics\n        ),\n        \"status\": self.get_model_status(model_name),\n        \"loaded\": model_name in self.loaded_models,\n    }\n\n    if inference_engine:\n        try:\n            memory_usage = inference_engine.estimate_memory_usage()\n            info[\"memory_usage\"] = memory_usage\n        except Exception as e:\n            logger.warning(\n                f\"Could not estimate memory usage for {model_name}: {e}\"\n            )\n\n    return info\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager.get_model_status","title":"get_model_status","text":"<pre><code>get_model_status(model_name)\n</code></pre> <p>Get loading status of a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model</p> required <p>Returns:</p> Type Description <code>str</code> <p>Status string: \"loading\", \"loaded\", \"error\", or \"not_found\"</p> Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>def get_model_status(self, model_name: str) -&gt; str:\n    \"\"\"Get loading status of a specific model.\n\n    Args:\n        model_name: Name of the model\n\n    Returns:\n        Status string: \"loading\", \"loaded\", \"error\", or \"not_found\"\n    \"\"\"\n    return self.model_loading_status.get(model_name, \"not_found\")\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager.load_all_enabled_models","title":"load_all_enabled_models  <code>async</code>","text":"<pre><code>load_all_enabled_models()\n</code></pre> <p>Load all enabled models asynchronously.</p> <p>Returns:</p> Type Description <code>dict[str, bool]</code> <p>Dictionary mapping model names to loading success status</p> Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>async def load_all_enabled_models(self) -&gt; dict[str, bool]:\n    \"\"\"Load all enabled models asynchronously.\n\n    Returns:\n        Dictionary mapping model names to loading success status\n    \"\"\"\n    enabled_models = self.config_manager.get_enabled_models()\n    logger.info(\n        f\"\\n\ud83d\ude80 Starting to load {len(enabled_models)} enabled models:\"\n    )\n    for i, model_name in enumerate(enabled_models, 1):\n        logger.info(f\"   {i}. {model_name}\")\n    logger.info(\"\")\n\n    logger.info(\n        f\"Loading {len(enabled_models)} enabled models: {enabled_models}\"\n    )\n\n    # Load models concurrently\n    tasks = [self.load_model(model_name) for model_name in enabled_models]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Process results\n    loading_results = {}\n    for i, model_name in enumerate(enabled_models):\n        result = results[i]\n        if isinstance(result, Exception):\n            logger.error(f\"Exception loading model {model_name}: {result}\")\n            loading_results[model_name] = False\n        else:\n            loading_results[model_name] = result\n\n    successful_loads = sum(loading_results.values())\n    logger.info(\"\\n\ud83d\udcca Loading Summary:\")\n    logger.info(\n        f\"   \u2705 Successfully loaded: {successful_loads}/\"\n        f\"{len(enabled_models)} models\"\n    )\n    logger.info(\n        f\"   \u274c Failed to load: {len(enabled_models) - successful_loads}/\"\n        f\"{len(enabled_models)} models\"\n    )\n\n    if successful_loads &gt; 0:\n        logger.info(\"\\n\ud83c\udf89 Successfully loaded models:\")\n        for model_name, success in loading_results.items():\n            if success:\n                logger.success(f\"   {model_name}\")\n\n    if successful_loads &lt; len(enabled_models):\n        logger.warning_icon(\"Failed to load models:\")\n        for model_name, success in loading_results.items():\n            if not success:\n                logger.failure(f\"   {model_name}\")\n\n    logger.info(\n        f\"Successfully loaded {successful_loads}/{len(enabled_models)} \"\n        f\"models\"\n    )\n\n    return loading_results\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager.load_model","title":"load_model  <code>async</code>","text":"<pre><code>load_model(model_name)\n</code></pre> <p>Load a specific model asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to load</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if model loaded successfully, False otherwise</p> Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>async def load_model(self, model_name: str) -&gt; bool:\n    \"\"\"Load a specific model asynchronously.\n\n    Args:\n        model_name: Name of the model to load\n\n    Returns:\n        True if model loaded successfully, False otherwise\n    \"\"\"\n    async with self._loading_lock:\n        if model_name in self.loaded_models:\n            logger.info(f\"Model {model_name} already loaded\")\n            return True\n\n        if model_name in self.model_loading_status:\n            if self.model_loading_status[model_name] == \"loading\":\n                logger.info(f\"Model {model_name} is already being loaded\")\n                return False\n            elif self.model_loading_status[model_name] == \"error\":\n                logger.warning(\n                    f\"Model {model_name} previously failed to load\"\n                )\n                return False\n\n        self.model_loading_status[model_name] = \"loading\"\n        logger.info(f\"Starting to load model: {model_name}\")\n\n        try:\n            # Get model configuration\n            model_config = self.config_manager.get_model_config(model_name)\n            if not model_config:\n                raise ValueError(\n                    f\"Configuration not found for model: {model_name}\"\n                )\n\n            # Display loading progress\n            logger.progress(f\"Loading model: {model_name}\")\n            logger.info(f\"   Model path: {model_config.model.path}\")\n            logger.info(f\"   Source: {model_config.model.source}\")\n            logger.info(f\"   Task type: {model_config.task.task_type}\")\n            logger.info(\n                f\"   Architecture: \"\n                f\"{model_config.model.task_info.architecture}\"\n            )\n            logger.info(\"   \ud83d\udce5 Downloading/loading model and tokenizer...\")\n\n            # Create task config for model loading\n            task_config = TaskConfig(\n                task_type=model_config.task.task_type,\n                num_labels=model_config.task.num_labels,\n                label_names=model_config.task.label_names,\n                threshold=model_config.task.threshold,\n            )\n\n            # Load model and tokenizer in thread pool to avoid blocking\n            loop = asyncio.get_event_loop()\n            start_time = time.time()\n            model, tokenizer = await loop.run_in_executor(\n                None,\n                self._load_model_sync,\n                model_config.model.path,\n                task_config,\n                model_config.model.source,\n            )\n\n            load_time = time.time() - start_time\n            logger.success(\n                f\"Model and tokenizer loaded in {load_time:.2f} seconds\"\n            )\n\n            # Create predictor\n            logger.info(\"   \ud83d\udd27 Creating DNA inference engine...\")\n            predictor_config = {\n                \"task\": model_config.task,\n                \"inference\": model_config.inference,\n            }\n\n            inference_engine = DNAInference(\n                model, tokenizer, predictor_config\n            )\n            self.loaded_models[model_name] = inference_engine\n            self.model_loading_status[model_name] = \"loaded\"\n\n            total_time = time.time() - start_time\n            logger.success(\n                f\"Successfully loaded model: {model_name} \"\n                f\"(total: {total_time:.2f}s)\"\n            )\n            loguru_logger.info(f\"Successfully loaded model: {model_name}\")\n            return True\n\n        except Exception as e:\n            self.model_loading_status[model_name] = \"error\"\n            logger.failure(f\"Failed to load model {model_name}: {e}\")\n            loguru_logger.error(f\"Failed to load model {model_name}: {e}\")\n            return False\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager.predict_batch","title":"predict_batch  <code>async</code>","text":"<pre><code>predict_batch(model_name, sequences, **kwargs)\n</code></pre> <p>Predict using a specific model on a batch of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to use</p> required <code>sequences</code> <code>list[str]</code> <p>List of DNA sequences to predict</p> required <code>**kwargs</code> <code>Any</code> <p>Additional prediction parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Batch prediction results or None if model not available</p> Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>async def predict_batch(\n    self, model_name: str, sequences: list[str], **kwargs: Any\n) -&gt; dict[str, Any] | None:\n    \"\"\"Predict using a specific model on a batch of sequences.\n\n    Args:\n        model_name: Name of the model to use\n        sequences: List of DNA sequences to predict\n        **kwargs: Additional prediction parameters\n\n    Returns:\n        Batch prediction results or None if model not available\n    \"\"\"\n    inference_engine = self.get_inference_engine(model_name)\n    if not inference_engine:\n        logger.error(f\"Model {model_name} not loaded\")\n        return None\n\n    try:\n        # Run prediction in thread pool to avoid blocking\n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(\n            None, inference_engine.infer_seqs, sequences, **kwargs\n        )\n        return result\n    except Exception as e:\n        logger.error(\n            f\"Batch prediction failed for model {model_name}: {e}\"\n        )\n        return None\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager.predict_multi_model","title":"predict_multi_model  <code>async</code>","text":"<pre><code>predict_multi_model(model_names, sequence, **kwargs)\n</code></pre> <p>Predict using multiple models in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>model_names</code> <code>list[str]</code> <p>List of model names to use</p> required <code>sequence</code> <code>str</code> <p>DNA sequence to predict</p> required <code>**kwargs</code> <code>Any</code> <p>Additional prediction parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary mapping model names to prediction results</p> Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>async def predict_multi_model(\n    self, model_names: list[str], sequence: str, **kwargs: Any\n) -&gt; dict[str, Any]:\n    \"\"\"Predict using multiple models in parallel.\n\n    Args:\n        model_names: List of model names to use\n        sequence: DNA sequence to predict\n        **kwargs: Additional prediction parameters\n\n    Returns:\n        Dictionary mapping model names to prediction results\n    \"\"\"\n    logger.info(\n        f\"Running multi-model prediction with {len(model_names)} models\"\n    )\n\n    # Create prediction tasks\n    tasks = [\n        self.predict_sequence(model_name, sequence, **kwargs)\n        for model_name in model_names\n    ]\n\n    # Run predictions concurrently\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Process results\n    multi_results = {}\n    for i, model_name in enumerate(model_names):\n        result = results[i]\n        if isinstance(result, Exception):\n            logger.error(\n                f\"Exception in multi-model prediction for {model_name}: \"\n                f\"{result}\"\n            )\n            multi_results[model_name] = {\"error\": str(result)}\n        else:\n            multi_results[model_name] = result\n\n    return multi_results\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager.predict_sequence","title":"predict_sequence  <code>async</code>","text":"<pre><code>predict_sequence(model_name, sequence, **kwargs)\n</code></pre> <p>Predict using a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to use</p> required <code>sequence</code> <code>str</code> <p>DNA sequence to predict</p> required <code>**kwargs</code> <code>Any</code> <p>Additional prediction parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Prediction results or None if model not available</p> Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>async def predict_sequence(\n    self, model_name: str, sequence: str, **kwargs: Any\n) -&gt; dict[str, Any] | None:\n    \"\"\"Predict using a specific model.\n\n    Args:\n        model_name: Name of the model to use\n        sequence: DNA sequence to predict\n        **kwargs: Additional prediction parameters\n\n    Returns:\n        Prediction results or None if model not available\n    \"\"\"\n    inference_engine = self.get_inference_engine(model_name)\n    if not inference_engine:\n        logger.error(f\"Model {model_name} not loaded\")\n        return None\n\n    try:\n        # Run prediction in thread pool to avoid blocking\n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(\n            None, inference_engine.infer_seqs, sequence, **kwargs\n        )\n        return result\n    except Exception as e:\n        logger.error(f\"Prediction failed for model {model_name}: {e}\")\n        return None\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager.unload_all_models","title":"unload_all_models","text":"<pre><code>unload_all_models()\n</code></pre> <p>Unload all loaded models.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of models unloaded</p> Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>def unload_all_models(self) -&gt; int:\n    \"\"\"Unload all loaded models.\n\n    Returns:\n        Number of models unloaded\n    \"\"\"\n    unloaded_count = 0\n    for model_name in list(self.loaded_models.keys()):\n        if self.unload_model(model_name):\n            unloaded_count += 1\n\n    logger.info(f\"Unloaded {unloaded_count} models\")\n    return unloaded_count\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager.ModelManager.unload_model","title":"unload_model","text":"<pre><code>unload_model(model_name)\n</code></pre> <p>Unload a specific model to free memory.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to unload</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if model was unloaded, False if not found</p> Source code in <code>dnallm/mcp/model_manager.py</code> <pre><code>def unload_model(self, model_name: str) -&gt; bool:\n    \"\"\"Unload a specific model to free memory.\n\n    Args:\n        model_name: Name of the model to unload\n\n    Returns:\n        True if model was unloaded, False if not found\n    \"\"\"\n    if model_name in self.loaded_models:\n        del self.loaded_models[model_name]\n        if model_name in self.model_loading_status:\n            del self.model_loading_status[model_name]\n\n        # Force garbage collection to free memory\n        import gc\n\n        gc.collect()\n\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        logger.info(f\"Unloaded model: {model_name}\")\n        return True\n\n    return False\n</code></pre>"},{"location":"api/mcp/model_manager/#dnallm.mcp.model_manager-functions","title":"Functions","text":""},{"location":"api/mcp/server/","title":"mcp/server API","text":""},{"location":"api/mcp/server/#dnallm.mcp.server","title":"dnallm.mcp.server","text":"<p>DNALLM MCP Server Implementation.</p> <p>This module implements the main MCP (Model Context Protocol) server using the FastMCP framework with Server-Sent Events (SSE) support for real-time DNA sequence prediction.</p> <p>The server provides a comprehensive set of tools for DNA sequence analysis, including: - Single sequence prediction with specific models - Batch processing of multiple sequences - Multi-model prediction and comparison - Real-time streaming predictions with progress updates - Model management and health monitoring</p> Architecture <p>The server is built on top of the FastMCP framework, which provides MCP protocol implementation with multiple transport options (stdio, SSE, HTTP). The server manages DNA language models through a ModelManager and handles configuration through a ConfigManager.</p> Transport Protocols <ul> <li>stdio: Standard input/output for CLI tools</li> <li>sse: Server-Sent Events for real-time web applications</li> <li>streamable-http: HTTP-based streaming protocol</li> </ul> Example <p>Basic server initialization:</p> <pre><code>server = DNALLMMCPServer(\"config/server_config.yaml\")\nawait server.initialize()\nserver.start_server(host=\"127.0.0.1\", port=8000, transport=\"sse\")\n</code></pre> Note <p>This server requires proper configuration files and model setup before initialization. See the configuration documentation for details.</p>"},{"location":"api/mcp/server/#dnallm.mcp.server-classes","title":"Classes","text":""},{"location":"api/mcp/server/#dnallm.mcp.server.DNALLMMCPServer","title":"DNALLMMCPServer","text":"<pre><code>DNALLMMCPServer(config_path)\n</code></pre> <p>DNALLM MCP Server implementation using FastMCP framework with SSE support.</p> <p>This class provides a comprehensive MCP server for DNA language model inference and analysis. It supports multiple transport protocols and provides real-time streaming capabilities for DNA sequence prediction tasks.</p> <p>The server manages multiple DNA language models and provides various prediction modes including single sequence prediction, batch processing, and multi-model comparison. All operations support progress reporting through SSE for real-time user feedback.</p> <p>Attributes:</p> Name Type Description <code>config_path</code> <code>str</code> <p>Path to the main server configuration file</p> <code>config_manager</code> <code>MCPConfigManager</code> <p>Handles configuration management</p> <code>model_manager</code> <code>ModelManager</code> <p>Manages model loading and prediction</p> <code>app</code> <code>FastMCP | None</code> <p>Main FastMCP application instance</p> <code>sse_app</code> <p>SSE application instance (unused, FastMCP handles SSE internally)</p> <code>_initialized</code> <code>bool</code> <p>Server initialization status flag</p> Example <p>Initialize and start the server:</p> <pre><code># Create server instance\nserver = DNALLMMCPServer(\"config/mcp_server_config.yaml\")\n\n# Initialize asynchronously\nawait server.initialize()\n\n# Start with SSE transport\nserver.start_server(host=\"0.0.0.0\", port=8000,\n                   transport=\"sse\")\n</code></pre> Note <p>The server must be initialized before starting. Configuration files must contain valid model and server settings.</p> <p>Initialize the MCP server instance.</p> <p>Sets up the server with configuration and model managers, but does not load models or start the server. Call initialize() and start_server() separately for complete setup.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Absolute or relative path to the main MCP server configuration file. This file should contain server settings, model configurations, and transport options.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the configuration file doesn't exist</p> <code>ConfigurationError</code> <p>If the configuration file is invalid</p> Example <pre><code>server = DNALLMMCPServer(\"/path/to/config.yaml\")\n</code></pre> Note <p>The configuration directory and filename are extracted separately to support the MCPConfigManager's directory-based configuration loading strategy.</p> Source code in <code>dnallm/mcp/server.py</code> <pre><code>def __init__(self, config_path: str) -&gt; None:\n    \"\"\"Initialize the MCP server instance.\n\n    Sets up the server with configuration and model managers, but does not\n    load models or start the server. Call initialize() and start_server()\n    separately for complete setup.\n\n    Args:\n        config_path (str): Absolute or relative path to the main MCP\n            server configuration file. This file should contain server\n            settings, model configurations, and transport options.\n\n    Raises:\n        FileNotFoundError: If the configuration file doesn't exist\n        ConfigurationError: If the configuration file is invalid\n\n    Example:\n        ```python\n        server = DNALLMMCPServer(\"/path/to/config.yaml\")\n        ```\n\n    Note:\n        The configuration directory and filename are extracted\n        separately to support the MCPConfigManager's directory-based\n        configuration loading strategy.\n    \"\"\"\n    self.config_path = config_path\n    # Extract directory and filename from config file path for\n    # ConfigManager. MCPConfigManager requires separate directory and\n    # filename parameters\n    config_path_obj = Path(config_path)\n    config_dir = config_path_obj.parent\n    config_filename = config_path_obj.name\n    # Initialize core components\n    self.config_manager = MCPConfigManager(\n        str(config_dir), config_filename\n    )\n    self.model_manager = ModelManager(self.config_manager)\n\n    # FastMCP application instances\n    self.app: FastMCP | None = None  # Main MCP application\n    self.sse_app = None  # Not used - FastMCP handles SSE internally\n\n    # Server state tracking\n    self._initialized = False  # Prevents double initialization\n</code></pre>"},{"location":"api/mcp/server/#dnallm.mcp.server.DNALLMMCPServer-functions","title":"Functions","text":""},{"location":"api/mcp/server/#dnallm.mcp.server.DNALLMMCPServer.get_server_info","title":"get_server_info","text":"<pre><code>get_server_info()\n</code></pre> <p>Get server information.</p> Source code in <code>dnallm/mcp/server.py</code> <pre><code>def get_server_info(self) -&gt; dict[str, Any]:\n    \"\"\"Get server information.\"\"\"\n    server_config = self.config_manager.get_server_config()\n    if not server_config:\n        return {\"error\": \"Server configuration not loaded\"}\n\n    return {\n        \"name\": server_config.mcp.name,\n        \"version\": server_config.mcp.version,\n        \"description\": server_config.mcp.description,\n        \"host\": server_config.server.host,\n        \"port\": server_config.server.port,\n        \"loaded_models\": self.model_manager.get_loaded_models(),\n        \"enabled_models\": self.config_manager.get_enabled_models(),\n        \"initialized\": self._initialized,\n    }\n</code></pre>"},{"location":"api/mcp/server/#dnallm.mcp.server.DNALLMMCPServer.initialize","title":"initialize  <code>async</code>","text":"<pre><code>initialize()\n</code></pre> <p>Initialize the server and load all enabled models.</p> <p>This method performs the complete server initialization process: 1. Checks if already initialized (idempotent operation) 2. Loads and validates server configuration 3. Creates the FastMCP application instance 4. Registers all MCP tools 5. Loads all enabled DNA language models</p> <p>The initialization is asynchronous because model loading can be time-consuming, especially for large transformer models.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If server configuration cannot be loaded</p> <code>ModelLoadError</code> <p>If critical models fail to load</p> <code>ConfigurationError</code> <p>If configuration is invalid</p> Example <pre><code>server = DNALLMMCPServer(\"config.yaml\")\nawait server.initialize()  # Required before starting\n</code></pre> Note <p>This method is idempotent - calling it multiple times has no additional effect after the first successful initialization.</p> Source code in <code>dnallm/mcp/server.py</code> <pre><code>async def initialize(self) -&gt; None:\n    \"\"\"Initialize the server and load all enabled models.\n\n    This method performs the complete server initialization process:\n    1. Checks if already initialized (idempotent operation)\n    2. Loads and validates server configuration\n    3. Creates the FastMCP application instance\n    4. Registers all MCP tools\n    5. Loads all enabled DNA language models\n\n    The initialization is asynchronous because model loading can be\n    time-consuming, especially for large transformer models.\n\n    Raises:\n        RuntimeError: If server configuration cannot be loaded\n        ModelLoadError: If critical models fail to load\n        ConfigurationError: If configuration is invalid\n\n    Example:\n        ```python\n        server = DNALLMMCPServer(\"config.yaml\")\n        await server.initialize()  # Required before starting\n        ```\n\n    Note:\n        This method is idempotent - calling it multiple times has no\n        additional effect after the first successful initialization.\n    \"\"\"\n    # Check for duplicate initialization\n    if self._initialized:\n        logger.info(\"Server already initialized\")\n        return\n\n    logger.info(\"Initializing DNALLM MCP Server...\")\n\n    # Load and validate server configuration\n    server_config = self.config_manager.get_server_config()\n    if not server_config:\n        raise RuntimeError(\"Failed to load server configuration\")\n\n    # Create FastMCP application with configuration\n    self.app = FastMCP(\n        name=server_config.mcp.name,\n        instructions=server_config.mcp.description,\n    )\n\n    # Register all available MCP tools\n    self._register_tools()\n\n    # Load all enabled models asynchronously\n    await self.model_manager.load_all_enabled_models()\n\n    # SSE transport is built into FastMCP framework\n    # No need for separate SSE application setup\n\n    # Mark server as initialized\n    self._initialized = True\n    logger.info(\"DNALLM MCP Server initialized successfully\")\n</code></pre>"},{"location":"api/mcp/server/#dnallm.mcp.server.DNALLMMCPServer.shutdown","title":"shutdown  <code>async</code>","text":"<pre><code>shutdown()\n</code></pre> <p>Shutdown the server and cleanup resources.</p> Source code in <code>dnallm/mcp/server.py</code> <pre><code>async def shutdown(self) -&gt; None:\n    \"\"\"Shutdown the server and cleanup resources.\"\"\"\n    logger.info(\"Shutting down DNALLM MCP Server...\")\n\n    # Unload all models\n    unloaded_count = self.model_manager.unload_all_models()\n    logger.info(f\"Unloaded {unloaded_count} models during shutdown\")\n\n    self._initialized = False\n    logger.info(\"DNALLM MCP Server shutdown complete\")\n</code></pre>"},{"location":"api/mcp/server/#dnallm.mcp.server.DNALLMMCPServer.start_server","title":"start_server","text":"<pre><code>start_server(\n    host=\"127.0.0.1\", port=8000, transport=\"stdio\"\n)\n</code></pre> <p>Start the MCP server with the specified transport protocol.</p> <p>This method starts the server using one of the supported transport protocols. The server must be initialized before calling this method. The transport protocol determines how the server communicates with clients:</p> <ul> <li>stdio: Standard input/output for CLI tools and automation</li> <li>sse: Server-Sent Events for real-time web applications</li> <li>streamable-http: HTTP-based streaming for REST API integration</li> </ul> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Host address to bind the server to. Defaults to \"127.0.0.1\". Use \"0.0.0.0\" for all interfaces.</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>Port number to bind the server to. Defaults to 8000. Only used for HTTP-based transports.</p> <code>8000</code> <code>transport</code> <code>str</code> <p>Transport protocol to use. Choices: \"stdio\", \"sse\", \"streamable-http\". Defaults to \"stdio\".</p> <code>'stdio'</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If server is not initialized before starting</p> <code>OSError</code> <p>If port is already in use or host is invalid</p> <code>ConfigurationError</code> <p>If transport configuration is invalid</p> Example <pre><code># Start with SSE for real-time web apps\nserver.start_server(\n    host=\"0.0.0.0\",\n    port=8000,\n    transport=\"sse\"\n)\n\n# Start with stdio for CLI tools\nserver.start_server(transport=\"stdio\")\n</code></pre> Note <p>This method is blocking and will run until the server is stopped. For SSE and HTTP transports, uvicorn handles graceful shutdown on SIGINT/SIGTERM signals.</p> Source code in <code>dnallm/mcp/server.py</code> <pre><code>def start_server(\n    self,\n    host: str = \"127.0.0.1\",\n    port: int = 8000,\n    transport: str = \"stdio\",\n) -&gt; None:\n    \"\"\"Start the MCP server with the specified transport protocol.\n\n    This method starts the server using one of the supported transport\n    protocols.\n    The server must be initialized before calling this method. The\n    transport\n    protocol determines how the server communicates with clients:\n\n    - stdio: Standard input/output for CLI tools and automation\n    - sse: Server-Sent Events for real-time web applications\n    - streamable-http: HTTP-based streaming for REST API integration\n\n    Args:\n        host (str, optional): Host address to bind the server to.\n            Defaults to \"127.0.0.1\". Use \"0.0.0.0\" for all interfaces.\n        port (int, optional): Port number to bind the server to.\n            Defaults to 8000. Only used for HTTP-based transports.\n        transport (str, optional): Transport protocol to use.\n            Choices: \"stdio\", \"sse\", \"streamable-http\".\n            Defaults to \"stdio\".\n\n    Raises:\n        RuntimeError: If server is not initialized before starting\n        OSError: If port is already in use or host is invalid\n        ConfigurationError: If transport configuration is invalid\n\n    Example:\n        ```python\n        # Start with SSE for real-time web apps\n        server.start_server(\n            host=\"0.0.0.0\",\n            port=8000,\n            transport=\"sse\"\n        )\n\n        # Start with stdio for CLI tools\n        server.start_server(transport=\"stdio\")\n        ```\n\n    Note:\n        This method is blocking and will run until the server is stopped.\n        For SSE and HTTP transports, uvicorn handles graceful shutdown\n        on SIGINT/SIGTERM signals.\n    \"\"\"\n    # Validate server initialization state\n    if not self._initialized:\n        raise RuntimeError(\n            \"Server not initialized. Call initialize() first.\"\n        )\n\n    # Override host/port from configuration if available\n    server_config = self.config_manager.get_server_config()\n    if server_config:\n        host = server_config.server.host\n        port = server_config.server.port\n\n    logger.info(\n        f\"Starting DNALLM MCP Server on {host}:{port} with \"\n        f\"{transport} transport\"\n    )\n\n    # Dispatch to appropriate transport handler\n    if transport == \"sse\":\n        self._start_sse_server(host, port)\n    elif transport == \"streamable-http\":\n        self._start_http_server(host, port)\n    else:\n        # Default to stdio transport\n        self._start_stdio_server()\n</code></pre>"},{"location":"api/mcp/server/#dnallm.mcp.server-functions","title":"Functions","text":""},{"location":"api/mcp/server/#dnallm.mcp.server.initialize_mcp_server","title":"initialize_mcp_server  <code>async</code>","text":"<pre><code>initialize_mcp_server(config_path)\n</code></pre> <p>Initialize the MCP server asynchronously.</p> <p>This is a convenience function that creates and initializes a DNALLMMCPServer instance. It's designed to be called from asyncio.run() or other async contexts.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the server configuration file</p> required <p>Returns:</p> Name Type Description <code>DNALLMMCPServer</code> <code>DNALLMMCPServer</code> <p>Fully initialized server instance ready to start</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If configuration is invalid</p> <code>ModelLoadError</code> <p>If model loading fails</p> <code>RuntimeError</code> <p>If initialization fails</p> Example <pre><code>server = await initialize_mcp_server(\"config.yaml\")\nserver.start_server(transport=\"sse\")\n</code></pre> Note <p>This function combines server creation and initialization for convenience in async entry points.</p> Source code in <code>dnallm/mcp/server.py</code> <pre><code>async def initialize_mcp_server(config_path: str) -&gt; DNALLMMCPServer:\n    \"\"\"Initialize the MCP server asynchronously.\n\n    This is a convenience function that creates and initializes a\n    DNALLMMCPServer instance. It's designed to be called from\n    asyncio.run() or other async contexts.\n\n    Args:\n        config_path (str): Path to the server configuration file\n\n    Returns:\n        DNALLMMCPServer: Fully initialized server instance ready to start\n\n    Raises:\n        ConfigurationError: If configuration is invalid\n        ModelLoadError: If model loading fails\n        RuntimeError: If initialization fails\n\n    Example:\n        ```python\n        server = await initialize_mcp_server(\"config.yaml\")\n        server.start_server(transport=\"sse\")\n        ```\n\n    Note:\n        This function combines server creation and initialization\n        for convenience in async entry points.\n    \"\"\"\n    server = DNALLMMCPServer(str(config_path))\n    await server.initialize()\n    return server\n</code></pre>"},{"location":"api/mcp/server/#dnallm.mcp.server.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Main entry point for the DNALLM MCP server CLI.</p> <p>This function provides a command-line interface for starting the DNALLM MCP server with various configuration options. It handles argument parsing, configuration validation, server initialization, and graceful error handling.</p> <p>The CLI supports multiple transport protocols and comprehensive configuration options for production deployment. It includes proper error handling and logging for troubleshooting.</p> Command Line Arguments <p>--config: Path to server configuration file --host: Host address to bind to (default: 0.0.0.0) --port: Port number to bind to (default: 8000) --transport: Protocol (stdio/sse/streamable-http, default: stdio) --log-level: Logging verbosity (DEBUG/INFO/WARNING/ERROR/CRITICAL) --version: Display version information</p> Example Usage <pre><code># Start with SSE transport\npython server.py --config config.yaml --transport sse --port 8000\n\n# Start with stdio (default)\npython server.py --config config.yaml\n\n# Start with debug logging\npython server.py --config config.yaml --log-level DEBUG\n</code></pre> Exit Codes <p>0: Successful execution 1: Configuration file not found or server error</p> Note <p>The server runs in blocking mode. Use Ctrl+C to stop gracefully. For SSE/HTTP transports, uvicorn handles signal processing automatically.</p> Source code in <code>dnallm/mcp/server.py</code> <pre><code>def main():\n    \"\"\"Main entry point for the DNALLM MCP server CLI.\n\n    This function provides a command-line interface for starting the DNALLM\n    MCP server with various configuration options. It handles argument parsing,\n    configuration validation, server initialization, and graceful error\n    handling.\n\n    The CLI supports multiple transport protocols and comprehensive\n    configuration\n    options for production deployment. It includes proper error handling and\n    logging for troubleshooting.\n\n    Command Line Arguments:\n        --config: Path to server configuration file\n        --host: Host address to bind to (default: 0.0.0.0)\n        --port: Port number to bind to (default: 8000)\n        --transport: Protocol (stdio/sse/streamable-http, default: stdio)\n        --log-level: Logging verbosity (DEBUG/INFO/WARNING/ERROR/CRITICAL)\n        --version: Display version information\n\n    Example Usage:\n        ```bash\n        # Start with SSE transport\n        python server.py --config config.yaml --transport sse --port 8000\n\n        # Start with stdio (default)\n        python server.py --config config.yaml\n\n        # Start with debug logging\n        python server.py --config config.yaml --log-level DEBUG\n        ```\n\n    Exit Codes:\n        0: Successful execution\n        1: Configuration file not found or server error\n\n    Note:\n        The server runs in blocking mode. Use Ctrl+C to stop gracefully.\n        For SSE/HTTP transports, uvicorn handles signal processing\n        automatically.\n    \"\"\"\n    import asyncio\n    import argparse\n    import sys\n    from pathlib import Path\n\n    parser = argparse.ArgumentParser(\n        description=\"Start the DNALLM MCP (Model Context Protocol) server\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  dnallm-mcp-server --config dnallm/mcp/configs/mcp_server_config.yaml\n  dnallm-mcp-server --config dnallm/mcp/configs/mcp_server_config_2.yaml \\\n      --transport sse --port 8000\n  dnallm-mcp-server --config dnallm/mcp/configs/mcp_server_config.yaml \\\n      --host 127.0.0.1 --port 9000\n        \"\"\",\n    )\n\n    parser.add_argument(\n        \"--config\",\n        \"-c\",\n        type=str,\n        default=\"dnallm/mcp/configs/mcp_server_config.yaml\",\n        help=\"Path to MCP server configuration file (default: %(default)s)\",\n    )\n\n    parser.add_argument(\n        \"--host\",\n        type=str,\n        default=\"0.0.0.0\",  # noqa: S104\n        help=\"Host to bind the server to (default: %(default)s)\",\n    )\n\n    parser.add_argument(\n        \"--port\",\n        type=int,\n        default=8000,\n        help=\"Port to bind the server to (default: %(default)s)\",\n    )\n\n    parser.add_argument(\n        \"--log-level\",\n        type=str,\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        default=\"INFO\",\n        help=\"Logging level (default: %(default)s)\",\n    )\n\n    parser.add_argument(\n        \"--transport\",\n        type=str,\n        choices=[\"stdio\", \"sse\", \"streamable-http\"],\n        default=\"stdio\",\n        help=\"Transport protocol to use (default: %(default)s)\",\n    )\n\n    parser.add_argument(\n        \"--version\", action=\"version\", version=\"DNALLM MCP Server 1.0.0\"\n    )\n\n    args = parser.parse_args()\n\n    # Check if config file exists\n    config_path = Path(args.config)\n    if not config_path.exists():\n        logger.error(f\"Configuration file not found: {config_path}\")\n        logger.error(\n            \"Please create a configuration file or specify the \"\n            \"correct path with --config\"\n        )\n        sys.exit(1)\n\n    try:\n        logger.info(\"Starting DNALLM MCP Server...\")\n        logger.info(f\"Configuration: {config_path}\")\n        logger.info(f\"Host: {args.host}\")\n        logger.info(f\"Port: {args.port}\")\n        logger.info(f\"Transport: {args.transport}\")\n        logger.info(f\"Log Level: {args.log_level}\")\n        logger.info(\"-\" * 50)\n\n        # Initialize server in asyncio context\n        server = asyncio.run(initialize_mcp_server(str(config_path)))\n\n        # Get server info\n        info = server.get_server_info()\n        logger.info(f\"Server initialized: {info['name']} v{info['version']}\")\n        logger.info(f\"Loaded models: {info['loaded_models']}\")\n        logger.info(f\"Enabled models: {info['enabled_models']}\")\n        logger.info(\"-\" * 50)\n\n        # Start server - let uvicorn handle signals for HTTP/SSE transports\n        logger.info(\n            f\"Starting server on {args.host}:{args.port} with \"\n            f\"{args.transport} transport\"\n        )\n        logger.info(\"Press Ctrl+C to stop the server\")\n\n        # Start server (uvicorn will handle signals properly)\n        server.start_server(\n            host=args.host, port=args.port, transport=args.transport\n        )\n\n    except KeyboardInterrupt:\n        logger.info(\"\\nReceived interrupt signal, shutting down...\")\n    except Exception as e:\n        logger.error(f\"Server error: {e}\")\n        import traceback\n\n        traceback.print_exc()\n        sys.exit(1)\n    finally:\n        logger.info(\"Server stopped\")\n</code></pre>"},{"location":"api/models/head/","title":"Classification Heads","text":""},{"location":"api/models/head/#dnallm.models.head","title":"dnallm.models.head","text":""},{"location":"api/models/head/#dnallm.models.head-classes","title":"Classes","text":""},{"location":"api/models/head/#dnallm.models.head.BasicCNNHead","title":"BasicCNNHead","text":"<pre><code>BasicCNNHead(\n    input_dim,\n    num_classes,\n    task_type=\"binary\",\n    num_filters=128,\n    kernel_sizes=None,\n    dropout=0.2,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A CNN-based head for processing Transformer output sequences. This head applies multiple 1D convolutional layers with different kernel sizes to capture local patterns in the sequence data, followed by a fully connected layer for classification or regression tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes (for classification tasks)</p> required <code>task_type</code> <code>str</code> <p>Type of task - 'binary', 'multiclass',        'multilabel', or 'regression'</p> <code>'binary'</code> <code>num_filters</code> <code>int</code> <p>Number of filters for each convolutional layer</p> <code>128</code> <code>kernel_sizes</code> <code>list | None</code> <p>List of kernel sizes for the convolutional layers</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.2</code> Source code in <code>dnallm/models/head.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    num_classes: int,\n    task_type: str = \"binary\",\n    num_filters: int = 128,\n    kernel_sizes: list | None = None,\n    dropout: float = 0.2,\n    **kwargs: Any,\n):\n    super().__init__()\n    self.task_type = task_type\n    self.num_classes = num_classes\n    if kernel_sizes is None:\n        kernel_sizes = [3, 4, 5]\n\n    # Define multiple parallel 1D convolutional layers\n    self.convs = nn.ModuleList([\n        nn.Conv1d(\n            in_channels=input_dim, out_channels=num_filters, kernel_size=k\n        )\n        for k in kernel_sizes\n    ])\n\n    self.dropout = nn.Dropout(dropout)\n\n    # CNN feature dimension is the concatenation of all conv outputs\n    cnn_output_dim = num_filters * len(kernel_sizes)\n\n    # Define the final output layer\n    self.output_layer = nn.Linear(cnn_output_dim, num_classes)\n</code></pre>"},{"location":"api/models/head/#dnallm.models.head.BasicLSTMHead","title":"BasicLSTMHead","text":"<pre><code>BasicLSTMHead(\n    input_dim,\n    num_classes,\n    task_type=\"binary\",\n    hidden_size=256,\n    num_layers=1,\n    dropout=0.1,\n    bidirectional=True,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A LSTM-based head for processing Transformer output sequences. This head applies a multi-layer LSTM to capture sequential dependencies in the sequence data, followed by a fully connected layer for classification or regression tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes (for classification tasks)</p> required <code>task_type</code> <code>str</code> <p>Type of task - 'binary', 'multiclass',        'multilabel', or 'regression'</p> <code>'binary'</code> <code>hidden_size</code> <code>int</code> <p>Number of features in the hidden state of the LSTM</p> <code>256</code> <code>num_layers</code> <code>int</code> <p>Number of recurrent layers in the LSTM</p> <code>1</code> <code>dropout</code> <code>float</code> <p>Dropout probability between LSTM layers</p> <code>0.1</code> <code>bidirectional</code> <code>bool</code> <p>Whether to use a bidirectional LSTM</p> <code>True</code> Source code in <code>dnallm/models/head.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    num_classes: int,\n    task_type: str = \"binary\",\n    hidden_size: int = 256,\n    num_layers: int = 1,\n    dropout: float = 0.1,\n    bidirectional: bool = True,\n    **kwargs: Any,\n):\n    super().__init__()\n    self.task_type = task_type\n    self.num_classes = num_classes\n\n    # Define the LSTM layer\n    self.lstm = nn.LSTM(\n        input_size=input_dim,\n        hidden_size=hidden_size,\n        num_layers=num_layers,\n        bidirectional=bidirectional,\n        dropout=dropout if num_layers &gt; 1 else 0,\n        batch_first=True,  # Accepts (batch, seq, feature) shaped inputs\n    )\n\n    # LSTM output feature dimension\n    lstm_output_dim = hidden_size * 2 if bidirectional else hidden_size\n\n    # Define the final output layer\n    self.output_layer = nn.Linear(lstm_output_dim, num_classes)\n</code></pre>"},{"location":"api/models/head/#dnallm.models.head.BasicMLPHead","title":"BasicMLPHead","text":"<pre><code>BasicMLPHead(\n    input_dim,\n    num_classes=2,\n    task_type=\"binary\",\n    hidden_dims=None,\n    activation_fn=\"relu\",\n    use_normalization=True,\n    norm_type=\"layernorm\",\n    dropout=0.1,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A universal and customizable MLP model designed to be appended after the embedding output of models like Transformers to perform various downstream tasks such as classification and regression.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes (for classification tasks)</p> <code>2</code> <code>task_type</code> <code>str</code> <p>Type of task - 'binary', 'multiclass',        'multilabel', or 'regression'</p> <code>'binary'</code> <code>hidden_dims</code> <code>list | None</code> <p>List of hidden layer dimensions</p> <code>None</code> <code>activation_fn</code> <code>str</code> <p>Activation function to use ('relu', 'gelu', 'silu',            'tanh', 'sigmoid')</p> <code>'relu'</code> <code>use_normalization</code> <code>bool</code> <p>Whether to use normalization layers</p> <code>True</code> <code>norm_type</code> <code>str</code> <p>Type of normalization - 'batchnorm' or 'layernorm'</p> <code>'layernorm'</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.1</code> Source code in <code>dnallm/models/head.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    num_classes: int = 2,\n    task_type: str = \"binary\",\n    hidden_dims: list | None = None,\n    activation_fn: str = \"relu\",\n    use_normalization: bool = True,\n    norm_type: str = \"layernorm\",\n    dropout: float = 0.1,\n    **kwargs: Any,\n):\n    super().__init__()\n    if hidden_dims is None:\n        hidden_dims = [512]\n    if task_type not in [\n        \"binary\",\n        \"multiclass\",\n        \"multilabel\",\n        \"regression\",\n    ]:\n        raise ValueError(f\"Unsupported task_type: {task_type}\")\n    if norm_type not in [\"batchnorm\", \"layernorm\"]:\n        raise ValueError(f\"Unsupported norm_type: {norm_type}\")\n    self.input_dim = input_dim\n    self.num_classes = num_classes\n    self.task_type = task_type\n    activations = {\n        \"relu\": nn.ReLU(),\n        \"gelu\": nn.GELU(),\n        \"silu\": nn.SiLU(),\n        \"tanh\": nn.Tanh(),\n        \"sigmoid\": nn.Sigmoid(),\n    }\n    activation_layer = activations.get(activation_fn.lower())\n    if activation_layer is None:\n        raise ValueError(f\"Unsupported activation_fn: {activation_fn}\")\n    layers = []\n    current_dim = input_dim\n    for i, h_dim in enumerate(hidden_dims):\n        layers.append((f\"linear_{i}\", nn.Linear(current_dim, h_dim)))\n        if use_normalization:\n            layers.append((\n                f\"norm_{i}\",\n                nn.LayerNorm(h_dim)\n                if norm_type == \"layernorm\"\n                else nn.BatchNorm1d(h_dim),\n            ))\n        layers.append((f\"activation_{i}\", activation_layer))\n        layers.append((f\"dropout_{i}\", nn.Dropout(p=dropout)))\n        current_dim = h_dim\n    self.mlp = nn.Sequential(OrderedDict(layers))\n    self.output_layer = nn.Linear(current_dim, num_classes)\n</code></pre>"},{"location":"api/models/head/#dnallm.models.head.BasicUNet1DHead","title":"BasicUNet1DHead","text":"<pre><code>BasicUNet1DHead(\n    input_dim,\n    num_classes,\n    task_type=\"binary\",\n    num_layers=2,\n    initial_filters=64,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>An U-net architecture adapted for 1D sequence data, suitable for classification and regression tasks. This model consists of an encoder-decoder structure with skip connections, allowing it to capture both local and global features in the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The number of input features (channels) in the inputs.</p> required <code>num_classes</code> <code>int</code> <p>The number of output classes for the classification task.</p> required <code>task_type</code> <code>str</code> <p>The type of task (e.g., \"binary\" or \"multi-class\").</p> <code>'binary'</code> <code>num_layers</code> <code>int</code> <p>The number of downsampling/upsampling layers in the U-net.</p> <code>2</code> <code>initial_filters</code> <code>int</code> <p>The number of filters in the first              convolutional layer.</p> <code>64</code> Source code in <code>dnallm/models/head.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    num_classes: int,\n    task_type: str = \"binary\",\n    num_layers: int = 2,\n    initial_filters: int = 64,\n    **kwargs: Any,\n):\n    super().__init__()\n    self.task_type = task_type\n    self.num_classes = num_classes\n    if initial_filters is None or initial_filters &lt;= 0:\n        initial_filters = input_dim\n\n    self.downs = nn.ModuleList()\n    self.ups = nn.ModuleList()\n\n    # --- Encoder (downsampling path) ---\n    in_c = input_dim\n    out_c = initial_filters\n    for _ in range(num_layers):\n        self.downs.append(DoubleConv(in_c, out_c))\n        in_c = out_c\n        out_c *= 2\n\n    # --- Bottleneck ---\n    self.bottleneck = DoubleConv(in_c, out_c)\n\n    # --- Decoder (upsampling path) ---\n    in_c = out_c\n    out_c //= 2\n    for _ in range(num_layers):\n        self.ups.append(\n            nn.ConvTranspose1d(in_c, out_c, kernel_size=2, stride=2)\n        )\n        self.ups.append(DoubleConv(in_c, out_c))\n        in_c = out_c\n        out_c //= 2\n\n    # --- Final output layer ---\n    # After U-Net processing,\n    # the number of channels becomes initial_filters\n    # We perform average pooling on the enhanced sequence\n    # and then pass it to the linear layer\n    self.output_layer = nn.Linear(initial_filters, num_classes)\n</code></pre>"},{"location":"api/models/head/#dnallm.models.head.DoubleConv","title":"DoubleConv","text":"<pre><code>DoubleConv(in_channels, out_channels)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>(Convolution =&gt; [BatchNorm] =&gt; ReLU) * 2</p> Source code in <code>dnallm/models/head.py</code> <pre><code>def __init__(self, in_channels, out_channels):\n    super().__init__()\n    self.double_conv = nn.Sequential(\n        nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n        nn.BatchNorm1d(out_channels),\n        nn.ReLU(inplace=True),\n        nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n        nn.BatchNorm1d(out_channels),\n        nn.ReLU(inplace=True),\n    )\n</code></pre>"},{"location":"api/models/head/#dnallm.models.head.EVOForSeqClsHead","title":"EVOForSeqClsHead","text":"<pre><code>EVOForSeqClsHead(\n    base_model,\n    num_classes=2,\n    task_type=\"binary\",\n    target_layer=None,\n    pooling_method=\"mean\",\n    dropout_prob=0.1,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A classification head tailored for the embedding outputs of the EVO-series model.</p> <p>Parameters:</p> Name Type Description Default <code>base_model</code> <code>any</code> <p>The EVO model instance providing embeddings.</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes for classification.</p> <code>2</code> <code>task_type</code> <code>str</code> <p>Type of task - 'binary', 'multiclass',        'multilabel', or 'regression'.</p> <code>'binary'</code> <code>target_layer</code> <code>str | list[str] | None</code> <p>Specific layer(s) from which to extract embeddings.           Can be 'all' to average all layers,           a list of layer names, or a single layer name.</p> <code>None</code> <code>pooling_method</code> <code>str</code> <p>Method to pool sequence embeddings.</p> <code>'mean'</code> <code>dropout_prob</code> <code>float</code> <p>Dropout probability for regularization</p> <code>0.1</code> Source code in <code>dnallm/models/head.py</code> <pre><code>def __init__(\n    self,\n    base_model: any,\n    num_classes: int = 2,\n    task_type: str = \"binary\",\n    target_layer: str | list[str] | None = None,\n    pooling_method: str = \"mean\",\n    dropout_prob: float = 0.1,\n    **kwargs: Any,\n):\n    super().__init__()\n    self.num_classes = num_classes\n    self.task_type = task_type\n    self.pooling_method = pooling_method\n\n    if target_layer == \"all\" or target_layer is None:\n        self.target_layers = []\n        for name, _ in base_model.model.named_parameters():\n            if name.startswith(\"blocks\"):\n                layer = \"blocks.\" + name.split(\".\")[1]\n                if layer not in self.target_layers:\n                    self.target_layers.append(layer)\n        if target_layer is None:\n            # Find middle layer which performs better than\n            # the last layer\n            mid_layer = round(len(self.target_layers) * 26 / 32)\n            self.target_layers = [self.target_layers[mid_layer]]\n            self.use_layer_averaging = False\n        else:\n            self.use_layer_averaging = True\n\n    elif isinstance(target_layer, list):\n        self.target_layers = target_layer\n        self.use_layer_averaging = True\n\n    else:\n        self.target_layers = [target_layer]\n        self.use_layer_averaging = False\n\n    if target_layer != \"all\":\n        print(f\"Use layers: {self.target_layers} embeddings.\")\n\n    self.dropout = nn.Dropout(dropout_prob)\n    self.classifier = nn.Linear(base_model.config.hidden_size, num_classes)\n</code></pre>"},{"location":"api/models/head/#dnallm.models.head.MegaDNAMultiScaleHead","title":"MegaDNAMultiScaleHead","text":"<pre><code>MegaDNAMultiScaleHead(\n    embedding_dims=None,\n    num_classes=2,\n    task_type=\"binary\",\n    hidden_dims=None,\n    dropout=0.2,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A classification head tailored for the multi-scale embedding outputs of the MegaDNA model. It takes a list of embedding tensors, pools each tensor, and concatenates the results before passing them to an MLP for classification.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_dims</code> <code>list | None</code> <p>A list of integers representing the dimensions of the input embeddings.</p> <code>None</code> <code>num_classes</code> <code>int</code> <p>The number of output classes for classification.</p> <code>2</code> <code>task_type</code> <code>str</code> <p>The type of task (e.g., \"binary\" or \"multi-class\").</p> <code>'binary'</code> <code>hidden_dims</code> <code>list | None</code> <p>A list of integers representing the sizes of hidden layers in the MLP.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout probability for regularization.</p> <code>0.2</code> Source code in <code>dnallm/models/head.py</code> <pre><code>def __init__(\n    self,\n    embedding_dims: list | None = None,\n    num_classes: int = 2,\n    task_type: str = \"binary\",\n    hidden_dims: list | None = None,\n    dropout: float = 0.2,\n    **kwargs: Any,\n):\n    super().__init__()\n    self.embedding_dims = embedding_dims\n    self.num_classes = num_classes\n    self.task_type = task_type\n    if hidden_dims is None:\n        hidden_dims = [256]\n\n    # Check that embedding_dims has exactly 3 elements\n    if len(embedding_dims) != 3:\n        raise ValueError(\n            \"embedding_dims list must contain 3 integers, \"\n            \"corresponding to the outputs of 3 scales.\"\n        )\n\n    concatenated_dim = sum(embedding_dims)\n\n    # --- Create MLP layers ---\n    mlp_layers = []\n    current_dim = concatenated_dim\n    for i, h_dim in enumerate(hidden_dims):\n        mlp_layers.append((f\"linear_{i}\", nn.Linear(current_dim, h_dim)))\n        mlp_layers.append((f\"norm_{i}\", nn.LayerNorm(h_dim)))\n        mlp_layers.append((f\"activation_{i}\", nn.GELU()))\n        mlp_layers.append((f\"dropout_{i}\", nn.Dropout(p=dropout)))\n        current_dim = h_dim\n\n    self.mlp = nn.Sequential(OrderedDict(mlp_layers))\n\n    # --- Final output layer ---\n    self.output_layer = nn.Linear(current_dim, num_classes)\n</code></pre>"},{"location":"api/models/model/","title":"Model","text":""},{"location":"api/models/model/#dnallm.models.model","title":"dnallm.models.model","text":"<p>DNA Model loading and management utilities.</p> <p>This module provides functions for downloading, loading, and     managing DNA language models from various sources including Hugging Face Hub, ModelScope, and local storage.</p>"},{"location":"api/models/model/#dnallm.models.model-classes","title":"Classes","text":""},{"location":"api/models/model/#dnallm.models.model.DNALLMforSequenceClassification","title":"DNALLMforSequenceClassification","text":"<pre><code>DNALLMforSequenceClassification(config, custom_model=None)\n</code></pre> <p>               Bases: <code>PreTrainedModel</code></p> <p>An automated wrapper that selects an appropriate pooling strategy based on the underlying model architecture and appends a customizable MLP head for sequence classification or regression tasks.</p>"},{"location":"api/models/model/#dnallm.models.model.DNALLMforSequenceClassification-functions","title":"Functions","text":""},{"location":"api/models/model/#dnallm.models.model.DNALLMforSequenceClassification.from_base_model","title":"from_base_model  <code>classmethod</code>","text":"<pre><code>from_base_model(model_name_or_path, config, module=None)\n</code></pre> <p>Handles weights diffusion when loading a model from a pre-trained base model.</p>"},{"location":"api/models/model/#dnallm.models.model-functions","title":"Functions","text":""},{"location":"api/models/model/#dnallm.models.model.clear_model_cache","title":"clear_model_cache","text":"<pre><code>clear_model_cache(source='huggingface')\n</code></pre> <p>Remove all the cached models</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Source to clear model cache from (     'huggingface',     'modelscope'), default 'huggingface'</p> <code>'huggingface'</code>"},{"location":"api/models/model/#dnallm.models.model.download_model","title":"download_model","text":"<pre><code>download_model(\n    model_name, downloader, revision=None, max_try=10\n)\n</code></pre> <p>Download a model with retry mechanism for network issues.</p> <p>In case of network issues, this function will attempt to download the model multiple times before giving up.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to download</p> required <code>downloader</code> <code>Any</code> <p>Download function to use (e.g., snapshot_download)</p> required <code>max_try</code> <code>int</code> <p>Maximum number of download attempts, default 10</p> <code>10</code> <p>Returns:</p> Type Description <code>str</code> <p>Path where the model files are stored</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model download fails after all attempts</p>"},{"location":"api/models/model/#dnallm.models.model.load_model_and_tokenizer","title":"load_model_and_tokenizer","text":"<pre><code>load_model_and_tokenizer(\n    model_name,\n    task_config,\n    source=\"local\",\n    use_mirror=False,\n    revision=None,\n    custom_tokenizer=None,\n)\n</code></pre> <p>Load model and tokenizer from either HuggingFace or ModelScope.</p> <p>This function handles loading of various model types based on the task     configuration,         including sequence classification, token classification,         masked language modeling,     and causal language modeling.</p> <pre><code>Args:\n    model_name: Model name or path\n    task_config: Task configuration object containing task type and\n        label information\n            source: Source to load model and tokenizer from (\n        'local',\n        'huggingface',\n        'modelscope'),\n        default 'local'\n            use_mirror: Whether to use HuggingFace mirror (\n        hf-mirror.com),\n        default False\n\nReturns:\n    Tuple containing (model, tokenizer)\n\nRaises:\n    ValueError: If model is not found locally or loading fails\n</code></pre>"},{"location":"api/models/model/#dnallm.models.model.load_preset_model","title":"load_preset_model","text":"<pre><code>load_preset_model(model_name, task_config)\n</code></pre> <p>Load a preset model and tokenizer based on the task configuration.</p> <p>This function loads models from the preset model registry, which contains pre-configured models for various DNA analysis tasks.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name or path of the model     task_config: Task configuration object containing task type and label information</p> required <p>Returns:</p> Type Description <code>tuple[Any, Any] | int</code> <p>Tuple containing (model, tokenizer) if successful, 0 if model not found</p> Note <p>If the model is not found in preset models,</p> <pre><code>    the function will print a warning\n        and\n    return 0. Use `load_model_and_tokenizer` function for custom model\n    loading.\n</code></pre>"},{"location":"api/models/model/#dnallm.models.model.peft_forward_compatiable","title":"peft_forward_compatiable","text":"<pre><code>peft_forward_compatiable(model)\n</code></pre> <p>Convert base model forward to be compatiable with HF</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Base model</p> required <p>Returns:</p> Type Description <code>Any</code> <p>model with changed forward function</p>"},{"location":"api/models/modeling_auto/","title":"Automatic Model Loading","text":""},{"location":"api/models/modeling_auto/#dnallm.models.modeling_auto","title":"dnallm.models.modeling_auto","text":""},{"location":"api/models/tokenizer/","title":"Tokenizers","text":""},{"location":"api/models/tokenizer/#dnallm.models.tokenizer","title":"dnallm.models.tokenizer","text":""},{"location":"api/models/tokenizer/#dnallm.models.tokenizer-classes","title":"Classes","text":""},{"location":"api/models/tokenizer/#dnallm.models.tokenizer.DNAOneHotTokenizer","title":"DNAOneHotTokenizer","text":"<pre><code>DNAOneHotTokenizer(\n    max_length=196608,\n    padding_side=\"right\",\n    return_embeds=False,\n    embeds_transpose=False,\n)\n</code></pre> Source code in <code>dnallm/models/tokenizer.py</code> <pre><code>def __init__(\n    self,\n    max_length: int | None = 196_608,\n    padding_side: str = \"right\",\n    return_embeds: bool = False,\n    embeds_transpose: bool = False,\n):\n    self.max_length = max_length\n    self.padding_side = padding_side\n    self.return_embeds = return_embeds\n    self.embeds_transpose = embeds_transpose\n\n    self.token_to_id = {\n        \"A\": 0,\n        \"a\": 0,\n        \"C\": 1,\n        \"c\": 1,\n        \"G\": 2,\n        \"g\": 2,\n        \"T\": 3,\n        \"t\": 3,\n        \"U\": 3,\n        \"u\": 3,  # RNA\n        \"N\": 4,\n        \"n\": 4,\n        \"X\": 4,\n        \"x\": 4,\n        \"-\": -1,\n        \".\": -1,  # Padding\n    }\n\n    self.id_to_token = {0: \"A\", 1: \"C\", 2: \"G\", 3: \"T\", 4: \"N\", -1: \"-\"}\n\n    self.pad_token_id = -1\n    self.pad_token = \"-\"  # noqa: S105\n    self.unk_token_id = 4  # N\n\n    self.vocab_vectors = torch.tensor(\n        [\n            [1.0, 0.0, 0.0, 0.0],  # 0: A\n            [0.0, 1.0, 0.0, 0.0],  # 1: C\n            [0.0, 0.0, 1.0, 0.0],  # 2: G\n            [0.0, 0.0, 0.0, 1.0],  # 3: T / U\n            [0.25, 0.25, 0.25, 0.25],  # 4: N / X\n            [0.0, 0.0, 0.0, 0.0],  # 5: Padding, index=-1\n        ],\n        dtype=torch.float32,\n    )\n</code></pre>"},{"location":"api/models/tokenizer/#dnallm.models.tokenizer.DNAOneHotTokenizer-functions","title":"Functions","text":""},{"location":"api/models/tokenizer/#dnallm.models.tokenizer.DNAOneHotTokenizer.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(pretrained_model_name_or_path, **kwargs)\n</code></pre> <p>Load Tokenizer from a specified directory.</p> Source code in <code>dnallm/models/tokenizer.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, **kwargs):\n    \"\"\"\n    Load Tokenizer from a specified directory.\n    \"\"\"\n    config_file = os.path.join(\n        pretrained_model_name_or_path, \"tokenizer_config.json\"\n    )\n\n    if os.path.isfile(config_file):\n        with open(config_file, encoding=\"utf-8\") as f:\n            config = json.load(f)\n\n        # Remove some metadata that should not be passed to __init__\n        config.pop(\"tokenizer_class\", None)\n        config.pop(\"pad_token_id\", None)\n        config.pop(\"unk_token_id\", None)\n\n        config.update(kwargs)\n\n        return cls(**config)\n    else:\n        print(\n            \"Warning: tokenizer_config.json not found\",\n            f\"in {pretrained_model_name_or_path}. Using default init.\",\n        )\n        return cls(**kwargs)\n</code></pre>"},{"location":"api/models/tokenizer/#dnallm.models.tokenizer.DNAOneHotTokenizer.save_pretrained","title":"save_pretrained","text":"<pre><code>save_pretrained(save_directory, **kwargs)\n</code></pre> <p>Save tokenizer configuration and vocabulary. Generates: tokenizer_config.json and vocab.json</p> Source code in <code>dnallm/models/tokenizer.py</code> <pre><code>def save_pretrained(self, save_directory: str, **kwargs):\n    \"\"\"\n    Save tokenizer configuration and vocabulary.\n    Generates: tokenizer_config.json and vocab.json\n    \"\"\"\n    os.makedirs(save_directory, exist_ok=True)\n\n    config = {\n        \"max_length\": self.max_length,\n        \"padding_side\": self.padding_side,\n        \"pad_token_id\": self.pad_token_id,\n        \"unk_token_id\": self.unk_token_id,\n        \"tokenizer_class\": self.__class__.__name__,\n    }\n\n    config_file = os.path.join(save_directory, \"tokenizer_config.json\")\n    with open(config_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(config, f, indent=2, ensure_ascii=False)\n\n    vocab_file = os.path.join(save_directory, \"vocab.json\")\n    with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(self.token_to_id, f, indent=2, ensure_ascii=False)\n\n    return [config_file, vocab_file]\n</code></pre>"},{"location":"api/models/special/basenji2/","title":"Basenji2","text":""},{"location":"api/models/special/basenji2/#dnallm.models.special.basenji2","title":"dnallm.models.special.basenji2","text":""},{"location":"api/models/special/borzoi/","title":"Borzoi","text":""},{"location":"api/models/special/borzoi/#dnallm.models.special.borzoi","title":"dnallm.models.special.borzoi","text":""},{"location":"api/models/special/dnabert2/","title":"DNABERT2","text":""},{"location":"api/models/special/dnabert2/#dnallm.models.special.dnabert2","title":"dnallm.models.special.dnabert2","text":""},{"location":"api/models/special/dnabert2/#dnallm.models.special.dnabert2-functions","title":"Functions","text":""},{"location":"api/models/special/enformer/","title":"Enformer","text":""},{"location":"api/models/special/enformer/#dnallm.models.special.enformer","title":"dnallm.models.special.enformer","text":""},{"location":"api/models/special/evo/","title":"EVO","text":""},{"location":"api/models/special/evo/#dnallm.models.special.evo","title":"dnallm.models.special.evo","text":""},{"location":"api/models/special/evo/#dnallm.models.special.evo-classes","title":"Classes","text":""},{"location":"api/models/special/evo/#dnallm.models.special.evo.EvoTokenizerWrapper","title":"EvoTokenizerWrapper","text":"<pre><code>EvoTokenizerWrapper(\n    raw_tokenizer, model_max_length=8192, **kwargs\n)\n</code></pre> <p>raw_tokenizer: Raw CharLevelTokenizer instance from EVO2 package pad_token_id: Token ID used for padding (usually 1 for EVO2) model_max_length: Maximum context length of the model</p> Source code in <code>dnallm/models/special/evo.py</code> <pre><code>def __init__(self, raw_tokenizer, model_max_length=8192, **kwargs):\n    \"\"\"\n    raw_tokenizer: Raw CharLevelTokenizer instance from EVO2 package\n    pad_token_id: Token ID used for padding (usually 1 for EVO2)\n    model_max_length: Maximum context length of the model\n    \"\"\"\n\n    self.raw_tokenizer = raw_tokenizer\n    self.model_max_length = model_max_length\n    for attr in [\n        \"vocab_size\",\n        \"bos_token_id\",\n        \"eos_token_id\",\n        \"unk_token_id\",\n        \"pad_token_id\",\n        \"pad_id\",\n        \"eos_id\",\n        \"eod_id\",\n    ]:\n        if hasattr(raw_tokenizer, attr):\n            setattr(self, attr, getattr(raw_tokenizer, attr))\n    if not hasattr(self, \"pad_token_id\"):\n        self.pad_token_id = self.raw_tokenizer.pad_id\n    self.pad_token = raw_tokenizer.decode_token(self.pad_token_id)\n    self.padding_side = \"right\"\n    self.init_kwargs = kwargs\n</code></pre>"},{"location":"api/models/special/evo/#dnallm.models.special.evo.EvoTokenizerWrapper-functions","title":"Functions","text":""},{"location":"api/models/special/evo/#dnallm.models.special.evo.EvoTokenizerWrapper.__call__","title":"__call__","text":"<pre><code>__call__(\n    text,\n    padding=False,\n    truncation=False,\n    max_length=None,\n    return_tensors=None,\n    **kwargs,\n)\n</code></pre> <p>call method to tokenize inputs with padding and truncation.</p> Source code in <code>dnallm/models/special/evo.py</code> <pre><code>def __call__(\n    self,\n    text: str | list[str],\n    padding: bool | str = False,\n    truncation: bool = False,\n    max_length: int | None = None,\n    return_tensors: str | None = None,\n    **kwargs,\n):\n    \"\"\"\n    __call__ method to tokenize inputs with padding and truncation.\n    \"\"\"\n    if isinstance(text, str):\n        text = [text]\n        is_batched = False\n    else:\n        is_batched = True\n\n    input_ids_list = [self.raw_tokenizer.tokenize(seq) for seq in text]\n\n    if truncation:\n        limit = (\n            max_length if max_length is not None else self.model_max_length\n        )\n        input_ids_list = [ids[:limit] for ids in input_ids_list]\n\n    if padding:\n        if padding == \"max_length\":\n            target_len = (\n                max_length\n                if max_length is not None\n                else self.model_max_length\n            )\n        elif padding is True or padding == \"longest\":\n            target_len = max(len(ids) for ids in input_ids_list)\n        else:\n            target_len = max(len(ids) for ids in input_ids_list)\n\n        padded_input_ids = []\n        attention_masks = []\n\n        for ids in input_ids_list:\n            current_len = len(ids)\n            pad_len = target_len - current_len\n\n            if pad_len &lt; 0:\n                ids = ids[:target_len]\n                pad_len = 0\n                current_len = target_len\n\n            new_ids = ids + [self.pad_token_id] * pad_len\n            padded_input_ids.append(new_ids)\n\n            mask = [1] * current_len + [0] * pad_len\n            attention_masks.append(mask)\n    else:\n        padded_input_ids = input_ids_list\n        attention_masks = [[1] * len(ids) for ids in input_ids_list]\n\n    if return_tensors == \"pt\":\n        return BatchEncoding({\n            \"input_ids\": torch.tensor(padded_input_ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(\n                attention_masks, dtype=torch.long\n            ),\n        })\n\n    result = {\n        \"input_ids\": padded_input_ids,\n        \"attention_mask\": attention_masks,\n    }\n\n    if not is_batched and return_tensors is None:\n        return {k: v[0] for k, v in result.items()}\n\n    return BatchEncoding(result)\n</code></pre>"},{"location":"api/models/special/evo/#dnallm.models.special.evo-functions","title":"Functions","text":""},{"location":"api/models/special/gpn/","title":"GPN","text":""},{"location":"api/models/special/gpn/#dnallm.models.special.gpn","title":"dnallm.models.special.gpn","text":""},{"location":"api/models/special/lucaone/","title":"LucaOne","text":""},{"location":"api/models/special/lucaone/#dnallm.models.special.lucaone","title":"dnallm.models.special.lucaone","text":""},{"location":"api/models/special/megadna/","title":"megaDNA","text":""},{"location":"api/models/special/megadna/#dnallm.models.special.megadna","title":"dnallm.models.special.megadna","text":""},{"location":"api/models/special/mutbert/","title":"MutBERT","text":""},{"location":"api/models/special/mutbert/#dnallm.models.special.mutbert","title":"dnallm.models.special.mutbert","text":""},{"location":"api/models/special/omnidna/","title":"Omni-DNA","text":""},{"location":"api/models/special/omnidna/#dnallm.models.special.omnidna","title":"dnallm.models.special.omnidna","text":""},{"location":"api/models/special/space/","title":"SPACE","text":""},{"location":"api/models/special/space/#dnallm.models.special.space","title":"dnallm.models.special.space","text":""},{"location":"api/tasks/metrics/","title":"Evaluation Metrics","text":""},{"location":"api/tasks/metrics/#dnallm.tasks.metrics","title":"dnallm.tasks.metrics","text":"<p>DNA Language Model Evaluation Metrics Module.</p> <p>This module provides comprehensive evaluation metrics for DNA language models across various task types including classification, regression, and     token classification.</p> <p>Supported task types: - Binary classification: accuracy, precision, recall, F1, MCC, AUROC, AUPRC - Multi-class classification: macro/micro/weighted metrics - Multi-label classification: per-label and overall metrics - Regression: MSE, MAE, R\u00b2, Spearman correlation - Token classification: sequence-level accuracy, precision, recall, F1</p> <p>The module integrates both scikit-learn metrics and     HuggingFace evaluate library for comprehensive model evaluation.</p>"},{"location":"api/tasks/metrics/#dnallm.tasks.metrics-classes","title":"Classes","text":""},{"location":"api/tasks/metrics/#dnallm.tasks.metrics-functions","title":"Functions","text":""},{"location":"api/tasks/metrics/#dnallm.tasks.metrics.calculate_metric_with_sklearn","title":"calculate_metric_with_sklearn","text":"<pre><code>calculate_metric_with_sklearn(eval_pred)\n</code></pre> <p>Calculate basic classification metrics using scikit-learn.</p> <p>This function computes standard classification metrics for token     classification tasks,     handling padding tokens and reshaping logits as needed.</p> <pre><code>Args:\n    eval_pred: Tuple containing (logits, labels)\n\nReturns:\n    Dictionary containing accuracy, F1, Matthews correlation,\n    precision, and recall\n</code></pre> Source code in <code>dnallm/tasks/metrics.py</code> <pre><code>def calculate_metric_with_sklearn(eval_pred):\n    \"\"\"Calculate basic classification metrics using scikit-learn.\n\n    This function computes standard classification metrics for token\n        classification tasks,\n        handling padding tokens and reshaping logits as needed.\n\n        Args:\n            eval_pred: Tuple containing (logits, labels)\n\n        Returns:\n            Dictionary containing accuracy, F1, Matthews correlation,\n            precision, and recall\n    \"\"\"\n    logits, labels = eval_pred\n    if isinstance(logits, tuple):  # Unpack logits if it's a tuple\n        logits = logits[0]\n    if logits.ndim == 3:\n        # Reshape logits to 2D if needed\n        logits = logits.reshape(-1, logits.shape[-1])\n    predictions = np.argmax(logits, axis=-1)\n    valid_mask = (\n        labels != -100\n    )  # Exclude padding tokens (assuming -100 is the padding token ID)\n    valid_predictions = predictions[valid_mask]\n    valid_labels = labels[valid_mask]\n    print(valid_labels.shape, valid_predictions.shape)\n    return {\n        \"accuracy\": accuracy_score(valid_labels, valid_predictions),\n        \"f1\": f1_score(\n            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n        ),\n        \"matthews_correlation\": matthews_corrcoef(\n            valid_labels, valid_predictions\n        ),\n        \"precision\": precision_score(\n            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n        ),\n        \"recall\": recall_score(\n            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n        ),\n    }\n</code></pre>"},{"location":"api/tasks/metrics/#dnallm.tasks.metrics.classification_metrics","title":"classification_metrics","text":"<pre><code>classification_metrics(plot=False)\n</code></pre> <p>Create metrics computation function for binary classification tasks.</p> <p>This function returns a callable that computes comprehensive binary     classification metrics including accuracy, precision, recall, F1, MCC,     AUROC, AUPRC, and     confusion matrix derived metrics.</p> <pre><code>Args:\n    plot: Whether to include curve data for plotting (ROC and PR\n        curves)\n\nReturns:\n    Callable function that computes binary classification metrics\n</code></pre> Source code in <code>dnallm/tasks/metrics.py</code> <pre><code>def classification_metrics(plot: bool = False):\n    \"\"\"Create metrics computation function for binary classification tasks.\n\n    This function returns a callable that computes comprehensive binary\n        classification metrics including accuracy, precision, recall, F1, MCC,\n        AUROC, AUPRC, and\n        confusion matrix derived metrics.\n\n        Args:\n            plot: Whether to include curve data for plotting (ROC and PR\n                curves)\n\n        Returns:\n            Callable function that computes binary classification metrics\n    \"\"\"\n    # clf_metrics = evaluate.combine(\n    #     [\n    #         metrics_path + \"accuracy/accuracy.py\",\n    #         metrics_path + \"f1/f1.py\",\n    #         metrics_path + \"precision/precision.py\",\n    #         metrics_path + \"recall/recall.py\",\n    #         metrics_path + \"matthews_correlation/matthews_correlation.py\",\n    #     ]\n    # )\n    # auc_metric = evaluate.load(metrics_path + \"roc_auc/roc_auc.py\", \"binary\")\n\n    def compute_metrics(eval_pred: tuple) -&gt; dict:\n        logits, labels = eval_pred\n        logits = logits[0] if isinstance(logits, tuple) else logits\n        predictions = np.argmax(logits, axis=-1)\n        pred_probs = softmax(logits, axis=1)\n        # Handle NaN and infinite values in pred_probs\n        pred_probs = np.nan_to_num(pred_probs, nan=0.0, posinf=1.0, neginf=0.0)\n        # metrics = clf_metrics.compute(predictions=predictions,\n        # references=labels)\n        # roc_auc = auc_metric.compute(references=labels,\n        # prediction_scores=pred_probs[:, 1])\n        # pr_auc = average_precision_score(y_true=labels, y_score=pred_probs[:,\n        # 1])\n        # metrics[\"AUROC\"] = roc_auc[\"roc_auc\"]\n        # metrics[\"AUPRC\"] = pr_auc\n        metrics = {}\n        metrics[\"accuracy\"] = accuracy_score(labels, predictions)\n        metrics[\"precision\"] = precision_score(labels, predictions)\n        metrics[\"recall\"] = recall_score(labels, predictions)\n        metrics[\"f1\"] = f1_score(labels, predictions)\n        metrics[\"mcc\"] = matthews_corrcoef(labels, predictions)\n        metrics[\"AUROC\"] = roc_auc_score(labels, pred_probs[:, 1])\n        metrics[\"AUPRC\"] = average_precision_score(labels, pred_probs[:, 1])\n        tn, fp, fn, tp = sklearn.metrics.confusion_matrix(\n            labels, predictions\n        ).ravel()\n        metrics[\"TPR\"] = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n        metrics[\"TNR\"] = tn / (tn + fp) if (tn + fp) &gt; 0 else 0\n        metrics[\"FPR\"] = fp / (fp + tn) if (fp + tn) &gt; 0 else 0\n        metrics[\"FNR\"] = fn / (fn + tp) if (fn + tp) &gt; 0 else 0\n        if plot:\n            fpr, tpr, _ = roc_curve(labels, pred_probs[:, 1])\n            precision, recall, _ = precision_recall_curve(\n                labels, pred_probs[:, 1]\n            )\n            metrics[\"curve\"] = {\n                \"fpr\": fpr,\n                \"tpr\": tpr,\n                \"precision\": precision,\n                \"recall\": recall,\n            }\n        return metrics\n\n    return compute_metrics\n</code></pre>"},{"location":"api/tasks/metrics/#dnallm.tasks.metrics.compute_metrics","title":"compute_metrics","text":"<pre><code>compute_metrics(task_config, plot=False)\n</code></pre> <p>Compute metrics based on task type.</p> <p>This function serves as the main entry point for metrics computation, automatically selecting the appropriate metrics function based on the task configuration.</p> <p>Parameters:</p> Name Type Description Default <code>task_config</code> <code>TaskConfig</code> <p>Task configuration object containing task type and</p> required <pre><code>    parameters\nplot: Whether to include plotting data for visualization\n</code></pre> <p>Returns:</p> Type Description <p>Callable function that computes appropriate metrics for the task type</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If task type is not supported for evaluation</p> Source code in <code>dnallm/tasks/metrics.py</code> <pre><code>def compute_metrics(task_config: TaskConfig, plot: bool = False):\n    \"\"\"Compute metrics based on task type.\n\n    This function serves as the main entry point for metrics computation,\n    automatically selecting the appropriate metrics function based on the\n    task configuration.\n\n    Args:\n                task_config: Task configuration object containing task type and\n            parameters\n        plot: Whether to include plotting data for visualization\n\n    Returns:\n        Callable function that computes appropriate metrics for the task type\n\n    Raises:\n        ValueError: If task type is not supported for evaluation\n    \"\"\"\n    if task_config.task_type == \"binary\":\n        return classification_metrics(plot=plot)\n    elif task_config.task_type == \"multiclass\":\n        return multi_classification_metrics(task_config.label_names, plot=plot)\n    elif task_config.task_type == \"multilabel\":\n        return multi_labels_metrics(task_config.label_names, plot=plot)\n    elif task_config.task_type == \"regression\":\n        return regression_metrics(plot=plot)\n    elif task_config.task_type == \"token\":\n        return token_classification_metrics(task_config.label_names, plot=plot)\n    else:\n        raise ValueError(\n            f\"Unsupported task type for evaluation: {task_config.task_type}\"\n        )\n</code></pre>"},{"location":"api/tasks/metrics/#dnallm.tasks.metrics.metrics_for_dnabert2","title":"metrics_for_dnabert2","text":"<pre><code>metrics_for_dnabert2(task)\n</code></pre> <p>Create metrics computation function for DNABERT2 model evaluation.</p> <p>This function provides specialized metrics computation for DNABERT2 models, supporting both regression and classification tasks with appropriate metric selection for each task type.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task type ('regression' or 'classification')</p> required <p>Returns:</p> Type Description <code>tuple[callable, callable]</code> <p>Tuple containing: - compute_metrics: Function for computing task-specific metrics - preprocess_logits_for_metrics: Function for preprocessing logits</p> Source code in <code>dnallm/tasks/metrics.py</code> <pre><code>def metrics_for_dnabert2(task: str) -&gt; tuple[callable, callable]:\n    \"\"\"Create metrics computation function for DNABERT2 model evaluation.\n\n    This function provides specialized metrics computation for DNABERT2 models,\n    supporting both regression and classification tasks with appropriate\n    metric selection for each task type.\n\n    Args:\n        task: Task type ('regression' or 'classification')\n\n    Returns:\n        Tuple containing:\n            - compute_metrics: Function for computing task-specific metrics\n            - preprocess_logits_for_metrics: Function for preprocessing logits\n    \"\"\"\n    r2_metric = evaluate.load(\"r_squared\")\n    spm_metric = evaluate.load(\"spearmanr\")\n    clf_metrics = evaluate.combine([\n        \"accuracy\",\n        \"f1\",\n        \"precision\",\n        \"recall\",\n        \"matthews_correlation\",\n    ])\n    metric1 = evaluate.load(\"precision\")\n    metric2 = evaluate.load(\"recall\")\n    metric3 = evaluate.load(\"f1\")\n    metric4 = evaluate.load(\"matthews_correlation\")\n    roc_metric = evaluate.load(\"roc_auc\", \"multiclass\")\n\n    def compute_metrics(eval_pred: tuple) -&gt; dict[str, Any]:\n        logits, labels = eval_pred\n        if task.lower() == \"regression\":\n            r2 = r2_metric.compute(references=labels, predictions=logits[0])\n            spearman = spm_metric.compute(\n                references=labels, predictions=logits[0]\n            )\n            return {\"r2\": r2, \"spearmanr\": spearman[\"spearmanr\"]}\n        else:\n            if task.lower() == \"classification\":\n                predictions = torch.argmax(torch.from_numpy(logits[0]), dim=-1)\n                return cast(\n                    dict[str, Any],\n                    clf_metrics.compute(\n                        predictions=predictions, references=labels\n                    ),\n                )\n            else:\n                pred_probs = softmax(logits[0], axis=1)\n                pred_list: list[int] = [\n                    x.tolist().index(max(x)) for x in pred_probs\n                ]\n                precision = metric1.compute(\n                    predictions=pred_list, references=labels, average=\"micro\"\n                )\n                recall = metric2.compute(\n                    predictions=pred_list, references=labels, average=\"micro\"\n                )\n                f1 = metric3.compute(\n                    predictions=pred_list, references=labels, average=\"micro\"\n                )\n                mcc = metric4.compute(predictions=pred_list, references=labels)\n                roc_auc_ovr = roc_metric.compute(\n                    references=labels,\n                    prediction_scores=pred_probs,\n                    multi_class=\"ovr\",\n                )\n                roc_auc_ovo = roc_metric.compute(\n                    references=labels,\n                    prediction_scores=pred_probs,\n                    multi_class=\"ovo\",\n                )\n                return {\n                    **precision,\n                    **recall,\n                    **f1,\n                    **mcc,\n                    \"AUROC_ovr\": roc_auc_ovr[\"roc_auc\"],\n                    \"AUROC_ovo\": roc_auc_ovo[\"roc_auc\"],\n                }\n\n    preprocessing = preprocess_logits_for_metrics\n\n    return compute_metrics, preprocessing\n</code></pre>"},{"location":"api/tasks/metrics/#dnallm.tasks.metrics.multi_classification_metrics","title":"multi_classification_metrics","text":"<pre><code>multi_classification_metrics(label_list, plot=False)\n</code></pre> <p>Create metrics computation function for multi-class classification tasks.</p> <p>This function returns a callable that computes comprehensive multi-class classification metrics including accuracy, precision, recall, F1, MCC,     AUROC, AUPRC, and     confusion matrix derived metrics with multiple averaging strategies.</p> <p>Parameters:</p> Name Type Description Default <code>label_list</code> <code>list</code> <p>List of class labels</p> required <code>plot</code> <code>bool</code> <p>Whether to include curve data for plotting (ROC and PR curves)</p> <code>False</code> <p>Returns:</p> Type Description <p>Callable function that computes multi-class classification metrics</p> Source code in <code>dnallm/tasks/metrics.py</code> <pre><code>def multi_classification_metrics(label_list: list, plot: bool = False):\n    \"\"\"Create metrics computation function for multi-class classification\n    tasks.\n\n    This function returns a callable that computes comprehensive multi-class\n    classification metrics including accuracy, precision, recall, F1, MCC,\n        AUROC, AUPRC, and\n        confusion matrix derived metrics with multiple averaging strategies.\n\n    Args:\n        label_list: List of class labels\n        plot: Whether to include curve data for plotting (ROC and PR curves)\n\n    Returns:\n        Callable function that computes multi-class classification metrics\n    \"\"\"\n    # metric0 = evaluate.load(metrics_path + \"accuracy/accuracy.py\")\n    # metric1 = evaluate.load(metrics_path + \"precision/precision.py\")\n    # metric2 = evaluate.load(metrics_path + \"recall/recall.py\")\n    # metric3 = evaluate.load(metrics_path + \"f1/f1.py\")\n    # metric4 = evaluate.load(metrics_path +\n    # \"matthews_correlation/matthews_correlation.py\")\n    # roc_metric = evaluate.load(metrics_path + \"roc_auc/roc_auc.py\",\n    # \"multiclass\")\n\n    def compute_metrics(eval_pred: tuple) -&gt; dict:\n        logits, labels = eval_pred\n        logits = logits[0] if isinstance(logits, tuple) else logits\n        if logits.ndim == 3:\n            logits = logits[:, 0, :]  # \u8c03\u6574\u6b64\u5904\u4ee5\u9002\u5e94\u6a21\u578b\u7ed3\u6784\n        pred_probs = softmax(logits, axis=1)\n        # Handle NaN and infinite values in pred_probs\n        pred_probs = np.nan_to_num(pred_probs, nan=0.0, posinf=1.0, neginf=0.0)\n        predictions = np.argmax(logits, axis=-1)\n\n        metrics = {}\n        metrics[\"accuracy\"] = accuracy_score(labels, predictions)\n        metrics[\"precision\"] = precision_score(\n            labels, predictions, average=\"macro\"\n        )\n        metrics[\"recall\"] = recall_score(labels, predictions, average=\"macro\")\n        metrics[\"f1\"] = f1_score(labels, predictions, average=\"macro\")\n        metrics[\"precision_micro\"] = precision_score(\n            labels, predictions, average=\"micro\"\n        )\n        metrics[\"recall_micro\"] = recall_score(\n            labels, predictions, average=\"micro\"\n        )\n        metrics[\"f1_micro\"] = f1_score(labels, predictions, average=\"micro\")\n        metrics[\"precision_weighted\"] = precision_score(\n            labels, predictions, average=\"weighted\"\n        )\n        metrics[\"recall_weighted\"] = recall_score(\n            labels, predictions, average=\"weighted\"\n        )\n        metrics[\"f1_weighted\"] = f1_score(\n            labels, predictions, average=\"weighted\"\n        )\n        metrics[\"mcc\"] = matthews_corrcoef(labels, predictions)\n        metrics[\"AUROC\"] = roc_auc_score(\n            labels, pred_probs, average=\"macro\", multi_class=\"ovr\"\n        )\n        metrics[\"AUPRC\"] = average_precision_score(\n            labels, pred_probs, average=\"macro\"\n        )\n        tpr_list, tnr_list, fpr_list, fnr_list = [], [], [], []\n        for label_cnt in multilabel_confusion_matrix(labels, predictions):\n            tn, fp, fn, tp = label_cnt.ravel()\n            # \u907f\u514d\u9664 0\n            tpr_list.append(tp / (tp + fn) if (tp + fn) &gt; 0 else 0)\n            tnr_list.append(tn / (tn + fp) if (tn + fp) &gt; 0 else 0)\n            fpr_list.append(fp / (fp + tn) if (fp + tn) &gt; 0 else 0)\n            fnr_list.append(fn / (fn + tp) if (fn + tp) &gt; 0 else 0)\n        metrics[\"TPR\"] = float(np.mean(tpr_list))\n        metrics[\"TNR\"] = float(np.mean(tnr_list))\n        metrics[\"FPR\"] = float(np.mean(fpr_list))\n        metrics[\"FNR\"] = float(np.mean(fnr_list))\n        # accuracy = metric0.compute(predictions=predictions,\n        # references=labels)\n        # precision = metric1.compute(predictions=predictions,\n        # references=labels, average=\"micro\")\n        # recall = metric2.compute(predictions=predictions, references=labels,\n        # average=\"micro\")\n        # f1 = metric3.compute(predictions=predictions, references=labels,\n        # average=\"micro\")\n        # mcc = metric4.compute(predictions=predictions, references=labels)\n        # roc_auc_ovr = roc_metric.compute(references=labels,\n        #                                  prediction_scores=pred_probs,\n        #                                  multi_class='ovr')\n        # roc_auc_ovo = roc_metric.compute(references=labels,\n        #                                  prediction_scores=pred_probs,\n        #                                  multi_class='ovo')\n        # metrics = {**accuracy, **precision, **recall, **f1, **mcc,\n        # \"AUROC_ovr\": roc_auc_ovr['roc_auc'], \"AUROC_ovo\":\n        # roc_auc_ovo['roc_auc']}\n        # metrics[\"AUROC_ovr\"] = roc_auc_ovr['roc_auc']\n        # metrics[\"AUROC_ovo\"] = roc_auc_ovo['roc_auc']\n        if plot:\n            label_curves = {}\n            for i, label_name in enumerate(label_list):\n                fpr, tpr, _ = roc_curve(\n                    np.array(labels) == i, pred_probs[:, i]\n                )\n                prec, rec, _ = precision_recall_curve(\n                    np.array(labels) == i, pred_probs[:, i]\n                )\n                label_curves[label_name] = {\n                    \"fpr\": fpr,\n                    \"tpr\": tpr,\n                    \"precision\": prec,\n                    \"recall\": rec,\n                }\n            # overall fpr, tpr for macro-average ROC curve\n            # and precision-recall curve can be added here if needed\n            all_fpr = np.unique(\n                np.concatenate([\n                    label_curves[label][\"fpr\"] for label in label_list\n                ])\n            )\n            mean_tpr = np.zeros_like(all_fpr)\n            for label in label_list:\n                mean_tpr += np.interp(\n                    all_fpr,\n                    label_curves[label][\"fpr\"],\n                    label_curves[label][\"tpr\"],\n                )\n            mean_tpr /= len(label_list)\n            all_recall = np.unique(\n                np.concatenate([\n                    label_curves[label][\"recall\"] for label in label_list\n                ])\n            )\n            mean_precision = np.zeros_like(all_recall)\n            for label in label_list:\n                mean_precision += np.interp(\n                    all_recall,\n                    label_curves[label][\"recall\"],\n                    label_curves[label][\"precision\"],\n                )\n            mean_precision /= len(label_list)\n\n            fpr = all_fpr\n            tpr = mean_tpr\n            precision = mean_precision\n            recall = all_recall\n            metrics[\"curve\"] = {\n                \"fpr\": fpr,\n                \"tpr\": tpr,\n                \"precision\": precision,\n                \"recall\": recall,\n            }\n        return metrics\n\n    return compute_metrics\n</code></pre>"},{"location":"api/tasks/metrics/#dnallm.tasks.metrics.multi_labels_metrics","title":"multi_labels_metrics","text":"<pre><code>multi_labels_metrics(label_list, plot=False)\n</code></pre> <p>Create metrics computation function for multi-label classification tasks.</p> <p>This function returns a callable that computes comprehensive multi-label     classification metrics including per-label and     overall metrics, with support for ROC curves and precision-recall curves for each label.</p> <p>Parameters:</p> Name Type Description Default <code>label_list</code> <code>list</code> <p>List of label names for multi-label classification</p> required <code>plot</code> <code>bool</code> <p>Whether to include curve data for plotting (ROC and PR curves)</p> <code>False</code> <p>Returns:</p> Type Description <p>Callable function that computes multi-label classification metrics</p> Source code in <code>dnallm/tasks/metrics.py</code> <pre><code>def multi_labels_metrics(label_list: list, plot: bool = False):\n    \"\"\"Create metrics computation function for multi-label classification\n    tasks.\n\n    This function returns a callable that computes comprehensive multi-label\n        classification metrics including per-label and\n        overall metrics, with support\n    for ROC curves and precision-recall curves for each label.\n\n    Args:\n        label_list: List of label names for multi-label classification\n        plot: Whether to include curve data for plotting (ROC and PR curves)\n\n    Returns:\n        Callable function that computes multi-label classification metrics\n    \"\"\"\n    # metric0 = evaluate.load(metrics_path + \"accuracy/accuracy.py\")\n    # metric1 = evaluate.load(metrics_path + \"precision/precision.py\")\n    # metric2 = evaluate.load(metrics_path + \"recall/recall.py\")\n    # metric3 = evaluate.load(metrics_path + \"f1/f1.py\")\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    def compute_metrics(eval_pred: tuple) -&gt; dict:\n        logits, labels = eval_pred\n        if hasattr(logits, \"numpy\"):\n            logits = logits.numpy()\n        if hasattr(labels, \"numpy\"):\n            labels = labels.numpy()\n        pred_probs = sigmoid(logits)\n        # Handle NaN and infinite values in pred_probs\n        pred_probs = np.nan_to_num(pred_probs, nan=0.0, posinf=1.0, neginf=0.0)\n        raw_pred = (pred_probs &gt; 0.5).astype(int)\n        # predictions = raw_pred.reshape(-1) # Not used in current\n        # implementation\n        # y_true = labels.astype(int).reshape(-1) # Not used in current\n        # implementation\n\n        # accuracy = metric0.compute(predictions=predictions,\n        # references=y_true)\n        # precision = metric1.compute(predictions=predictions,\n        # references=y_true, average=\"macro\")\n        # recall = metric2.compute(predictions=predictions, references=y_true,\n        # average=\"macro\")\n        # f1 = metric3.compute(predictions=predictions, references=y_true,\n        # average=\"macro\")\n        # metrics = {**accuracy, **precision, **recall, **f1}\n        metrics = {}\n        metrics[\"accuracy\"] = accuracy_score(labels, raw_pred)\n        metrics[\"precision\"] = precision_score(\n            labels, raw_pred, average=\"macro\"\n        )\n        metrics[\"recall\"] = recall_score(labels, raw_pred, average=\"macro\")\n        metrics[\"f1\"] = f1_score(labels, raw_pred, average=\"macro\")\n        metrics[\"precision_micro\"] = precision_score(\n            labels, raw_pred, average=\"micro\"\n        )\n        metrics[\"recall_micro\"] = recall_score(\n            labels, raw_pred, average=\"micro\"\n        )\n        metrics[\"f1_micro\"] = f1_score(labels, raw_pred, average=\"micro\")\n        metrics[\"precision_weighted\"] = precision_score(\n            labels, raw_pred, average=\"weighted\"\n        )\n        metrics[\"recall_weighted\"] = recall_score(\n            labels, raw_pred, average=\"weighted\"\n        )\n        metrics[\"f1_weighted\"] = f1_score(labels, raw_pred, average=\"weighted\")\n        metrics[\"precision_samples\"] = precision_score(\n            labels, raw_pred, average=\"samples\"\n        )\n        metrics[\"recall_samples\"] = recall_score(\n            labels, raw_pred, average=\"samples\"\n        )\n        metrics[\"f1_samples\"] = f1_score(labels, raw_pred, average=\"samples\")\n        mcc_per_label = {}\n        roc_data, roc_auc = {}, {}\n        pr_data, pr_auc = {}, {}\n        for i in range(labels.shape[1]):\n            # Compute matthews correlation coefficient for each class\n            mcc_per_label[label_list[i]] = matthews_corrcoef(\n                labels[:, i], raw_pred[:, i]\n            )\n            # Compute ROC curve and ROC area for each class\n            fpr, tpr, _ = roc_curve(labels[:, i], pred_probs[:, i])\n            auc = roc_auc_score(labels[:, i], pred_probs[:, i])\n            roc_data[label_list[i]] = (fpr, tpr)\n            roc_auc[label_list[i]] = auc\n            # Compute PR curve and PR area for each class\n            prec, rec, _ = precision_recall_curve(\n                labels[:, i], pred_probs[:, i]\n            )\n            ap = average_precision_score(labels[:, i], pred_probs[:, i])\n            pr_data[label_list[i]] = (prec, rec)\n            pr_auc[label_list[i]] = ap\n        metrics[\"mcc\"] = np.mean(list(mcc_per_label.values()))\n        metrics[\"AUROC\"] = np.mean(list(roc_auc.values()))\n        metrics[\"AUPRC\"] = np.mean(list(pr_auc.values()))\n        tpr_list, tnr_list, fpr_list, fnr_list = [], [], [], []\n        for label_cnt in multilabel_confusion_matrix(labels, raw_pred):\n            tn, fp, fn, tp = label_cnt.ravel()\n            # \u907f\u514d\u9664 0\n            tpr_list.append(tp / (tp + fn) if (tp + fn) &gt; 0 else 0)\n            tnr_list.append(tn / (tn + fp) if (tn + fp) &gt; 0 else 0)\n            fpr_list.append(fp / (fp + tn) if (fp + tn) &gt; 0 else 0)\n            fnr_list.append(fn / (fn + tp) if (fn + tp) &gt; 0 else 0)\n        metrics[\"TPR\"] = float(np.mean(tpr_list))\n        metrics[\"TNR\"] = float(np.mean(tnr_list))\n        metrics[\"FPR\"] = float(np.mean(fpr_list))\n        metrics[\"FNR\"] = float(np.mean(fnr_list))\n\n        if plot:\n            metrics[\"curve\"] = {}\n            for label in label_list:\n                metrics[\"curve\"][label] = {\n                    \"fpr\": roc_data[label][0],\n                    \"tpr\": roc_data[label][1],\n                    \"AUROC\": roc_auc[label],\n                    \"precision\": pr_data[label][0],\n                    \"recall\": pr_data[label][1],\n                    \"AUPRC\": pr_auc[label],\n                }\n        return metrics\n\n    return compute_metrics\n</code></pre>"},{"location":"api/tasks/metrics/#dnallm.tasks.metrics.preprocess_logits_for_metrics","title":"preprocess_logits_for_metrics","text":"<pre><code>preprocess_logits_for_metrics(logits, labels)\n</code></pre> <p>Preprocess logits for metrics computation.</p> <p>This function handles logits preprocessing to avoid memory leaks in the original Trainer implementation.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Model output logits</p> required <code>labels</code> <code>Tensor</code> <p>Ground truth labels</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple of (processed_logits, labels)</p> Source code in <code>dnallm/tasks/metrics.py</code> <pre><code>def preprocess_logits_for_metrics(\n    logits: torch.Tensor, labels: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Preprocess logits for metrics computation.\n\n    This function handles logits preprocessing to avoid memory leaks\n    in the original Trainer implementation.\n\n    Args:\n        logits: Model output logits\n        labels: Ground truth labels\n\n    Returns:\n        Tuple of (processed_logits, labels)\n    \"\"\"\n    logits = logits[0] if isinstance(logits, tuple) else logits\n\n    return logits\n</code></pre>"},{"location":"api/tasks/metrics/#dnallm.tasks.metrics.regression_metrics","title":"regression_metrics","text":"<pre><code>regression_metrics(plot=False)\n</code></pre> <p>Create metrics computation function for regression tasks.</p> <p>This function returns a callable that computes regression metrics including MSE, MAE, R2, and Spearman correlation. For multi-output regression, it uses scikit-learn metrics directly.</p> <p>Parameters:</p> Name Type Description Default <code>plot</code> <p>Whether to include scatter plot data for visualization</p> <code>False</code> <p>Returns:</p> Type Description <p>Callable function that computes regression metrics</p> Source code in <code>dnallm/tasks/metrics.py</code> <pre><code>def regression_metrics(plot=False):\n    \"\"\"Create metrics computation function for regression tasks.\n\n    This function returns a callable that computes regression metrics including\n    MSE, MAE, R2, and Spearman correlation. For multi-output regression,\n    it uses scikit-learn metrics directly.\n\n    Args:\n        plot: Whether to include scatter plot data for visualization\n\n    Returns:\n        Callable function that computes regression metrics\n    \"\"\"\n    mse_metric = evaluate.load(metrics_path + \"mse/mse.py\")\n    mae_metric = evaluate.load(metrics_path + \"mae/mae.py\")\n    r2_metric = evaluate.load(metrics_path + \"r_squared/r_squared.py\")\n    pearson_metric = evaluate.load(metrics_path + \"pearsonr/pearsonr.py\")\n    spm_metric = evaluate.load(metrics_path + \"spearmanr/spearmanr.py\")\n\n    def pearson_macro(y_true, y_pred):\n        rs = []\n        for k in range(y_true.shape[1]):\n            yt = y_true[:, k]\n            yp = y_pred[:, k]\n            if np.std(yt) == 0:\n                continue\n            r = pearson_metric.compute(\n                predictions=yp.tolist(), references=yt.tolist()\n            )[\"pearsonr\"]\n            rs.append(r)\n        return np.mean(rs), rs\n\n    def spearman_macro(y_true, y_pred):\n        rs = []\n        for k in range(y_true.shape[1]):\n            yt = y_true[:, k]\n            yp = y_pred[:, k]\n            if np.std(yt) == 0:\n                continue\n            r = spm_metric.compute(\n                predictions=yp.tolist(), references=yt.tolist()\n            )[\"spearmanr\"]\n            rs.append(r)\n        return np.mean(rs), rs\n\n    def compute_metrics(eval_pred: tuple) -&gt; dict[str, float]:\n        logits, labels = eval_pred\n        logits = logits[0] if isinstance(logits, tuple) else logits\n        num_outputs = logits.shape[1]\n        if num_outputs &gt; 1:\n            mse = mean_squared_error(labels, logits)\n            mae = mean_absolute_error(labels, logits)\n            r2 = r2_score(labels, logits, multioutput=\"uniform_average\")\n            pearsonr, _ = pearson_macro(labels, logits)\n            spearmanr, _ = spearman_macro(labels, logits)\n            metrics = {\n                \"mse\": mse,\n                \"mae\": mae,\n                \"r2\": r2,\n                \"pearsonr\": pearsonr,\n                \"spearmanr\": spearmanr,\n            }\n        else:\n            mse = mse_metric.compute(references=labels, predictions=logits)\n            mae = mae_metric.compute(references=labels, predictions=logits)\n            r2 = r2_metric.compute(references=labels, predictions=logits)\n            spearmanr = spm_metric.compute(\n                references=labels, predictions=logits\n            )\n            metrics = {**mse, **mae, \"r2\": r2, **spearmanr}\n            if plot:\n                # Fix: logits is already a numpy array,\n                # no need to call .numpy()\n                if hasattr(logits, \"numpy\"):\n                    predicted = logits.numpy().flatten()\n                else:\n                    predicted = logits.flatten()\n                metrics[\"scatter\"] = {\n                    \"predicted\": predicted,\n                    \"experiment\": labels,\n                }\n        return metrics\n\n    return compute_metrics\n</code></pre>"},{"location":"api/tasks/metrics/#dnallm.tasks.metrics.token_classification_metrics","title":"token_classification_metrics","text":"<pre><code>token_classification_metrics(\n    label_list, plot=False, scheme=\"IOB2\"\n)\n</code></pre> <p>Create metrics computation function for token classification tasks.</p> <p>This function returns a callable that computes sequence-level metrics for token classification tasks using the seqeval library, supporting various tagging schemes like IOB2.</p> <p>Parameters:</p> Name Type Description Default <code>label_list</code> <code>list</code> <p>List of label names for token classification</p> required <code>plot</code> <code>bool</code> <p>Whether to include plotting data (currently not implemented)</p> <code>False</code> <code>scheme</code> <code>str</code> <p>Tagging scheme for sequence evaluation (e.g., \"IOB2\", \"BIOES\")</p> <code>'IOB2'</code> <p>Returns:</p> Type Description <p>Callable function that computes token classification metrics</p> Source code in <code>dnallm/tasks/metrics.py</code> <pre><code>def token_classification_metrics(\n    label_list: list, plot: bool = False, scheme: str = \"IOB2\"\n):\n    \"\"\"Create metrics computation function for token classification tasks.\n\n    This function returns a callable that computes sequence-level metrics for\n    token classification tasks using the seqeval library, supporting various\n    tagging schemes like IOB2.\n\n    Args:\n        label_list: List of label names for token classification\n        plot: Whether to include plotting data (currently not implemented)\n        scheme: Tagging scheme for sequence evaluation (e.g., \"IOB2\", \"BIOES\")\n\n    Returns:\n        Callable function that computes token classification metrics\n    \"\"\"\n    seqeval = evaluate.load(metrics_path + \"seqeval/seqeval.py\")\n\n    def compute_metrics(pred: tuple) -&gt; dict:\n        predictions, labels = pred\n        predictions = np.argmax(predictions, axis=-1)\n\n        # \u5c06id\u8f6c\u6362\u4e3a\u539f\u59cb\u7684\u5b57\u7b26\u4e32\u7c7b\u578b\u7684\u6807\u7b7e\n        true_predictions = [\n            [\n                label_list[p]\n                for p, label_id in zip(prediction, label, strict=False)\n                if label_id != -100\n            ]\n            for prediction, label in zip(predictions, labels, strict=False)\n        ]\n\n        true_labels = [\n            [\n                label_list[label_id]\n                for p, label_id in zip(prediction, label, strict=False)\n                if label_id != -100\n            ]\n            for prediction, label in zip(predictions, labels, strict=False)\n        ]\n\n        result = seqeval.compute(\n            predictions=true_predictions,\n            references=true_labels,\n            mode=\"strict\",\n            scheme=scheme,\n        )\n\n        return {\n            \"accuracy\": result[\"overall_accuracy\"],\n            \"precision\": result[\"overall_precision\"],\n            \"recall\": result[\"overall_recall\"],\n            \"f1\": result[\"overall_f1\"],\n        }\n\n    return compute_metrics\n</code></pre>"},{"location":"api/tasks/task/","title":"Task","text":""},{"location":"api/tasks/task/#dnallm.tasks.task","title":"dnallm.tasks.task","text":"<p>DNA Language Model Fine-tuning Task Definition Module.</p> <p>This module defines various task types and related components supported by DNA language models during fine-tuning, including:</p> <ol> <li>TaskType: Task type enumeration</li> <li>Binary classification (BINARY): e.g., promoter prediction,      enhancer identification</li> <li>Multi-class classification (MULTICLASS): e.g., protein family      classification, functional region classification</li> <li>Regression (REGRESSION): e.g., expression level prediction,      binding strength prediction</li> <li>Token classification (NER): Named Entity Recognition tasks</li> <li> <p>Generation and embedding tasks for different model architectures</p> </li> <li> <p>TaskConfig: Task configuration class</p> </li> <li>Configures task type, number of labels, label names, etc.</li> <li> <p>Provides threshold settings for binary classification tasks</p> </li> <li> <p>TaskHead: Task-specific prediction heads</p> </li> <li>Provides specialized neural network layers for different task types</li> <li>Supports feature dimensionality reduction and dropout to prevent      overfitting</li> <li> <p>Automatically selects output dimensions based on task type</p> </li> <li> <p>compute_metrics: Evaluation metric computation</p> </li> <li>Binary: accuracy, F1 score</li> <li>Multi-class: accuracy, macro F1, weighted F1</li> <li>Regression: mean squared error, R-squared value</li> </ol> Usage example <p>task_config = TaskConfig(     task_type=TaskType.BINARY,     num_labels=2,     label_names=[\"negative\", \"positive\"] )</p>"},{"location":"api/tasks/task/#dnallm.tasks.task-classes","title":"Classes","text":""},{"location":"api/tasks/task/#dnallm.tasks.task.TaskConfig","title":"TaskConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration class for different fine-tuning tasks.</p> <p>This class provides a structured way to configure task-specific parameters including task type, number of labels, label names, and classification thresholds.</p> <p>Attributes:</p> Name Type Description <code>task_type</code> <code>str</code> <p>Type of task to perform (must match regex pattern)</p> <code>num_labels</code> <code>int</code> <p>Number of output labels/classes</p> <code>label_names</code> <code>list | None</code> <p>List of label names for classification tasks</p> <code>threshold</code> <code>float</code> <p>Classification threshold for binary and multi-label tasks</p>"},{"location":"api/tasks/task/#dnallm.tasks.task.TaskConfig-functions","title":"Functions","text":""},{"location":"api/tasks/task/#dnallm.tasks.task.TaskConfig.model_post_init","title":"model_post_init","text":"<pre><code>model_post_init(__context)\n</code></pre> <p>Initialize task configuration after model validation.</p> <p>This method is called after Pydantic model validation and automatically sets appropriate default values based on task type.</p>"},{"location":"api/tasks/task/#dnallm.tasks.task.TaskType","title":"TaskType","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for supported task types in DNALLM.</p> <p>This enum defines the various tasks that can be performed with DNA language models, including classification, regression, and generation tasks.</p>"},{"location":"api/utils/logger/","title":"Logging Utilities","text":""},{"location":"api/utils/logger/#dnallm.utils.logger","title":"dnallm.utils.logger","text":"<p>DNALLM Logging Configuration</p> <p>This module provides a centralized logging configuration for the DNALLM project. It replaces print statements with proper logging for better production readiness.</p>"},{"location":"api/utils/logger/#dnallm.utils.logger-classes","title":"Classes","text":""},{"location":"api/utils/logger/#dnallm.utils.logger.ColoredFormatter","title":"ColoredFormatter","text":"<p>               Bases: <code>Formatter</code></p> <p>Custom formatter with colored output for console.</p>"},{"location":"api/utils/logger/#dnallm.utils.logger.ColoredFormatter-functions","title":"Functions","text":""},{"location":"api/utils/logger/#dnallm.utils.logger.ColoredFormatter.format","title":"format","text":"<pre><code>format(record)\n</code></pre> <p>Format log record with colors.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def format(self, record):\n    \"\"\"Format log record with colors.\"\"\"\n    log_color = self.COLORS.get(record.levelname, \"\")\n    record.levelname = f\"{log_color}{record.levelname}{Style.RESET_ALL}\"\n    return super().format(record)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.DNALLMLogger","title":"DNALLMLogger","text":"<pre><code>DNALLMLogger(name='dnallm', level='INFO')\n</code></pre> <p>Centralized logger for DNALLM with colored output and structured logging.</p> <p>Initialize the DNALLM logger.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name</p> <code>'dnallm'</code> <code>level</code> <code>str</code> <p>Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)</p> <code>'INFO'</code> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def __init__(self, name: str = \"dnallm\", level: str = \"INFO\"):\n    \"\"\"\n    Initialize the DNALLM logger.\n\n    Args:\n        name: Logger name\n        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n    \"\"\"\n    self.logger = logging.getLogger(name)\n    self.logger.setLevel(getattr(logging, level.upper()))\n\n    # Prevent duplicate handlers\n    if not self.logger.handlers:\n        self._setup_handlers()\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.DNALLMLogger-functions","title":"Functions","text":""},{"location":"api/utils/logger/#dnallm.utils.logger.DNALLMLogger.critical","title":"critical","text":"<pre><code>critical(message, **kwargs)\n</code></pre> <p>Log critical message.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def critical(self, message: str, **kwargs):\n    \"\"\"Log critical message.\"\"\"\n    self.logger.critical(message, **kwargs)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.DNALLMLogger.debug","title":"debug","text":"<pre><code>debug(message, **kwargs)\n</code></pre> <p>Log debug message.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def debug(self, message: str, **kwargs):\n    \"\"\"Log debug message.\"\"\"\n    self.logger.debug(message, **kwargs)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.DNALLMLogger.error","title":"error","text":"<pre><code>error(message, **kwargs)\n</code></pre> <p>Log error message.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def error(self, message: str, **kwargs):\n    \"\"\"Log error message.\"\"\"\n    self.logger.error(message, **kwargs)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.DNALLMLogger.failure","title":"failure","text":"<pre><code>failure(message, **kwargs)\n</code></pre> <p>Log failure message with red color.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def failure(self, message: str, **kwargs):\n    \"\"\"Log failure message with red color.\"\"\"\n    self.logger.error(f\"{Fore.RED}\u274c {message}{Style.RESET_ALL}\", **kwargs)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.DNALLMLogger.info","title":"info","text":"<pre><code>info(message, **kwargs)\n</code></pre> <p>Log info message.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def info(self, message: str, **kwargs):\n    \"\"\"Log info message.\"\"\"\n    self.logger.info(message, **kwargs)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.DNALLMLogger.info_icon","title":"info_icon","text":"<pre><code>info_icon(message, **kwargs)\n</code></pre> <p>Log info message with cyan color and icon.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def info_icon(self, message: str, **kwargs):\n    \"\"\"Log info message with cyan color and icon.\"\"\"\n    self.logger.info(f\"{Fore.CYAN}i  {message}{Style.RESET_ALL}\", **kwargs)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.DNALLMLogger.progress","title":"progress","text":"<pre><code>progress(message, **kwargs)\n</code></pre> <p>Log progress message with blue color.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def progress(self, message: str, **kwargs):\n    \"\"\"Log progress message with blue color.\"\"\"\n    self.logger.info(f\"{Fore.BLUE}\ud83d\udd04 {message}{Style.RESET_ALL}\", **kwargs)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.DNALLMLogger.success","title":"success","text":"<pre><code>success(message, **kwargs)\n</code></pre> <p>Log success message with green color.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def success(self, message: str, **kwargs):\n    \"\"\"Log success message with green color.\"\"\"\n    self.logger.info(\n        f\"{Fore.GREEN}\u2705 {message}{Style.RESET_ALL}\", **kwargs\n    )\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.DNALLMLogger.warning","title":"warning","text":"<pre><code>warning(message, **kwargs)\n</code></pre> <p>Log warning message.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def warning(self, message: str, **kwargs):\n    \"\"\"Log warning message.\"\"\"\n    self.logger.warning(message, **kwargs)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.DNALLMLogger.warning_icon","title":"warning_icon","text":"<pre><code>warning_icon(message, **kwargs)\n</code></pre> <p>Log warning message with yellow color and icon.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def warning_icon(self, message: str, **kwargs):\n    \"\"\"Log warning message with yellow color and icon.\"\"\"\n    self.logger.warning(\n        f\"{Fore.YELLOW}\u26a0\ufe0f  {message}{Style.RESET_ALL}\", **kwargs\n    )\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.LoggingContext","title":"LoggingContext","text":"<pre><code>LoggingContext(level)\n</code></pre> <p>Context manager for temporary logging level changes.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def __init__(self, level: str):\n    self.level = level\n    self.original_level: int | None = None\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger-functions","title":"Functions","text":""},{"location":"api/utils/logger/#dnallm.utils.logger.get_logger","title":"get_logger","text":"<pre><code>get_logger(name='dnallm', level='INFO')\n</code></pre> <p>Get or create a logger instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name</p> <code>'dnallm'</code> <code>level</code> <code>str</code> <p>Logging level</p> <code>'INFO'</code> <p>Returns:</p> Type Description <code>DNALLMLogger</code> <p>DNALLMLogger instance</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def get_logger(name: str = \"dnallm\", level: str = \"INFO\") -&gt; DNALLMLogger:\n    \"\"\"\n    Get or create a logger instance.\n\n    Args:\n        name: Logger name\n        level: Logging level\n\n    Returns:\n        DNALLMLogger instance\n    \"\"\"\n    global _logger_instance\n    if _logger_instance is None:\n        _logger_instance = DNALLMLogger(name, level)\n    return _logger_instance\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.log_debug","title":"log_debug","text":"<pre><code>log_debug(message)\n</code></pre> <p>Log debug message.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def log_debug(message: str):\n    \"\"\"Log debug message.\"\"\"\n    get_logger().debug(message)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.log_error","title":"log_error","text":"<pre><code>log_error(message)\n</code></pre> <p>Log error message.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def log_error(message: str):\n    \"\"\"Log error message.\"\"\"\n    get_logger().error(message)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.log_failure","title":"log_failure","text":"<pre><code>log_failure(message)\n</code></pre> <p>Log failure message.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def log_failure(message: str):\n    \"\"\"Log failure message.\"\"\"\n    get_logger().failure(message)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.log_function_call","title":"log_function_call","text":"<pre><code>log_function_call(func)\n</code></pre> <p>Decorator to log function calls.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def log_function_call(func):\n    \"\"\"Decorator to log function calls.\"\"\"\n\n    def wrapper(*args, **kwargs):\n        logger = get_logger()\n        logger.debug(\n            f\"Calling {func.__name__} with args={args}, kwargs={kwargs}\"\n        )\n        try:\n            result = func(*args, **kwargs)\n            logger.debug(f\"{func.__name__} completed successfully\")\n            return result\n        except Exception as e:\n            logger.error(f\"{func.__name__} failed with error: {e}\")\n            raise\n\n    return wrapper\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.log_info","title":"log_info","text":"<pre><code>log_info(message)\n</code></pre> <p>Log info message.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def log_info(message: str):\n    \"\"\"Log info message.\"\"\"\n    get_logger().info(message)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.log_progress","title":"log_progress","text":"<pre><code>log_progress(message)\n</code></pre> <p>Log progress message.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def log_progress(message: str):\n    \"\"\"Log progress message.\"\"\"\n    get_logger().progress(message)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.log_success","title":"log_success","text":"<pre><code>log_success(message)\n</code></pre> <p>Log success message.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def log_success(message: str):\n    \"\"\"Log success message.\"\"\"\n    get_logger().success(message)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.log_warning","title":"log_warning","text":"<pre><code>log_warning(message)\n</code></pre> <p>Log warning message.</p> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def log_warning(message: str):\n    \"\"\"Log warning message.\"\"\"\n    get_logger().warning(message)\n</code></pre>"},{"location":"api/utils/logger/#dnallm.utils.logger.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(level='INFO', log_file=None)\n</code></pre> <p>Set up logging configuration for the entire application.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level</p> <code>'INFO'</code> <code>log_file</code> <code>str | None</code> <p>Optional log file path</p> <code>None</code> Source code in <code>dnallm/utils/logger.py</code> <pre><code>def setup_logging(level: str = \"INFO\", log_file: str | None = None):\n    \"\"\"\n    Set up logging configuration for the entire application.\n\n    Args:\n        level: Logging level\n        log_file: Optional log file path\n    \"\"\"\n    logger = get_logger(level=level)\n\n    if log_file:\n        # Add additional file handler\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(logging.DEBUG)\n        file_formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - \"\n            \"%(filename)s:%(lineno)d - %(message)s\",\n            datefmt=\"%Y-%m-%d %H:%M:%S\",\n        )\n        file_handler.setFormatter(file_formatter)\n        logger.logger.addHandler(file_handler)\n\n    return logger\n</code></pre>"},{"location":"api/utils/sequence/","title":"utils/sequence API","text":""},{"location":"api/utils/sequence/#dnallm.utils.sequence","title":"dnallm.utils.sequence","text":"<p>Sequence utility functions for DNA sequence analysis and generation.</p> <p>This module provides functions for: - Calculating GC content - Generating reverse complements - Converting sequences to k-mers - Validating DNA sequences - Randomly generating DNA sequences with constraints</p> <p>All functions are designed for use in DNA language modeling and bioinformatics pipelines.</p>"},{"location":"api/utils/sequence/#dnallm.utils.sequence-functions","title":"Functions","text":""},{"location":"api/utils/sequence/#dnallm.utils.sequence.calc_gc_content","title":"calc_gc_content","text":"<pre><code>calc_gc_content(seq)\n</code></pre> <p>Calculate the GC content of a DNA sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>str</code> <p>DNA sequence (A/C/G/T/U/N, case-insensitive).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>GC content (0.0 ~ 1.0). Returns 0.0 if sequence is empty.</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def calc_gc_content(seq: str) -&gt; float:\n    \"\"\"\n    Calculate the GC content of a DNA sequence.\n\n    Args:\n        seq (str): DNA sequence (A/C/G/T/U/N, case-insensitive).\n\n    Returns:\n        float: GC content (0.0 ~ 1.0). Returns 0.0 if sequence is empty.\n    \"\"\"\n    seq = seq.upper().replace(\"U\", \"T\").replace(\"N\", \"\")\n    if len(seq) == 0:\n        gc = 0.0\n    else:\n        gc = (seq.count(\"G\") + seq.count(\"C\")) / len(seq)\n    return gc\n</code></pre>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.check_sequence","title":"check_sequence","text":"<pre><code>check_sequence(\n    seq,\n    minl=1,\n    maxl=500000000,\n    gc=(0, 1),\n    valid_chars=\"ACGTN\",\n)\n</code></pre> <p>Check if a DNA sequence is valid based on length, GC content, and allowed characters.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>str</code> <p>DNA sequence.</p> required <code>minl</code> <code>int</code> <p>Minimum length. Defaults to 1.</p> <code>1</code> <code>maxl</code> <code>int</code> <p>Maximum length. Defaults to 500000000.</p> <code>500000000</code> <code>gc</code> <code>tuple</code> <p>GC content range (min, max). Defaults to (0, 1).</p> <code>(0, 1)</code> <code>valid_chars</code> <code>str</code> <p>Allowed characters. Defaults to \"ACGTN\".</p> <code>'ACGTN'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if valid, False otherwise.</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def check_sequence(\n    seq: str,\n    minl: int = 1,\n    maxl: int = 500000000,\n    gc: tuple = (0, 1),\n    valid_chars: str = \"ACGTN\",\n) -&gt; bool:\n    \"\"\"Check if a DNA sequence is valid based on length, GC content, and\n    allowed characters.\n\n    Args:\n        seq (str): DNA sequence.\n        minl (int, optional): Minimum length. Defaults to 1.\n        maxl (int, optional): Maximum length. Defaults to 500000000.\n        gc (tuple, optional): GC content range (min, max). Defaults to (0, 1).\n        valid_chars (str, optional): Allowed characters. Defaults to \"ACGTN\".\n\n    Returns:\n        bool: True if valid, False otherwise.\n    \"\"\"\n    if len(seq) &lt; minl or len(seq) &gt; maxl:\n        return False  # \u5e8f\u5217\u957f\u5ea6\u4e0d\u5728\u6709\u6548\u8303\u56f4\u5185\n    elif gc[0] &gt; calc_gc_content(seq) or gc[1] &lt; calc_gc_content(seq):\n        return False  # GC\u542b\u91cf\u4e0d\u5728\u6709\u6548\u8303\u56f4\u5185\n    elif set(seq.upper()) - set(valid_chars) != set():\n        return False  # \u5e8f\u5217\u5305\u542b\u4e0d\u652f\u6301\u7684\u5b57\u7b26\n    else:\n        return True  # \u5e8f\u5217\u6709\u6548\n</code></pre>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.random_generate_sequences","title":"random_generate_sequences","text":"<pre><code>random_generate_sequences(\n    minl,\n    maxl=0,\n    samples=1,\n    gc=(0, 1),\n    n_ratio=0.0,\n    padding_size=0,\n    seed=None,\n)\n</code></pre> <p>Randomly generate DNA sequences with specified length, GC content, and N ratio.</p> <p>Parameters:</p> Name Type Description Default <code>minl</code> <code>int</code> <p>Minimum sequence length.</p> required <code>maxl</code> <code>int</code> <p>Maximum sequence length. If 0, use minl as fixed length. Defaults to 0.</p> <code>0</code> <code>samples</code> <code>int</code> <p>Number of sequences to generate. Defaults to 1.</p> <code>1</code> <code>gc</code> <code>tuple</code> <p>GC content range (min, max). Defaults to (0, 1).</p> <code>(0, 1)</code> <code>n_ratio</code> <code>float</code> <p>Proportion of 'N' bases (0.0 ~ 1.0). Defaults to 0.0.</p> <code>0.0</code> <code>padding_size</code> <code>int</code> <p>Pad length to nearest multiple. Defaults to 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of generated DNA sequences.</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def random_generate_sequences(\n    minl: int,\n    maxl: int = 0,\n    samples: int = 1,\n    gc: tuple = (0, 1),\n    n_ratio: float = 0.0,\n    padding_size: int = 0,\n    seed: int | None = None,\n) -&gt; list[str]:\n    \"\"\"Randomly generate DNA sequences with specified length, GC content,\n    and N ratio.\n\n    Args:\n        minl (int): Minimum sequence length.\n        maxl (int, optional): Maximum sequence length. If 0, use minl as\n            fixed length. Defaults to 0.\n        samples (int, optional): Number of sequences to generate.\n            Defaults to 1.\n        gc (tuple, optional): GC content range (min, max).\n            Defaults to (0, 1).\n        n_ratio (float, optional): Proportion of 'N' bases (0.0 ~ 1.0).\n            Defaults to 0.0.\n        padding_size (int, optional): Pad length to nearest multiple.\n            Defaults to 0.\n        seed (int, optional): Random seed. Defaults to None.\n\n    Returns:\n        list[str]: List of generated DNA sequences.\n    \"\"\"\n    sequences: list[str] = []\n    basemap = [\"A\", \"C\", \"G\", \"T\"]\n    if 0.0 &lt; n_ratio &lt;= 1.0:\n        basemap.append(\"N\")\n        weights = [(1 - n_ratio) / 4] * 4 + [n_ratio]\n    elif n_ratio &gt; 1.0:\n        basemap.append(\"N\")\n        weights = [(100 - n_ratio) / 4] * 4 + [n_ratio]\n    else:\n        weights = None\n    calc_gc = (\n        False if gc == (0, 1) else True\n    )  # Guanqing Please check this line!\n    if seed:\n        random.seed(seed)\n    # progress bar\n    progress_bar = tqdm(total=samples, desc=\"Generating sequences\")\n    # generate sequences\n    if maxl:\n        # generate sequences with random length\n        while True:\n            if len(sequences) &gt;= samples:\n                break\n            length = random.randint(minl, maxl)  # noqa: S311\n            if padding_size:\n                length = (\n                    (length // padding_size + 1) * padding_size\n                    if length % padding_size\n                    else length\n                )\n                if length &gt; maxl:\n                    length -= padding_size\n            seq = \"\".join(\n                random.choices(  # noqa: S311\n                    basemap,\n                    weights=weights,\n                    k=length,\n                )\n            )\n            if calc_gc:\n                gc_content = calc_gc_content(seq)\n                if gc[0] &lt;= gc_content &lt;= gc[1]:\n                    sequences.append(seq)\n                    progress_bar.update(1)\n            else:\n                sequences.append(seq)\n                progress_bar.update(1)\n    # generate sequences with fixed length\n    else:\n        maxl = minl\n        length = minl\n        while True:\n            if len(sequences) &gt;= samples:\n                break\n            seq = \"\".join(\n                random.choices(  # noqa: S311\n                    basemap,\n                    weights=weights,\n                    k=length,\n                )\n            )\n            # calculate GC content\n            if calc_gc:\n                gc_content = calc_gc_content(seq)\n                if gc[0] &lt;= gc_content &lt;= gc[1]:\n                    sequences.append(seq)\n                    progress_bar.update(1)\n            else:\n                sequences.append(seq)\n                progress_bar.update(1)\n    return sequences\n</code></pre>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.reverse_complement","title":"reverse_complement","text":"<pre><code>reverse_complement(seq, reverse=True, complement=True)\n</code></pre> <p>Compute the reverse complement of a DNA sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>str</code> <p>DNA sequence.</p> required <code>reverse</code> <code>bool</code> <p>Whether to reverse the sequence. Defaults to True.</p> <code>True</code> <code>complement</code> <code>bool</code> <p>Whether to complement the sequence. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The reverse complement (or as specified) of the input sequence.</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def reverse_complement(\n    seq: str, reverse: bool = True, complement: bool = True\n) -&gt; str:\n    \"\"\"Compute the reverse complement of a DNA sequence.\n\n    Args:\n        seq (str): DNA sequence.\n        reverse (bool, optional): Whether to reverse the sequence.\n            Defaults to True.\n        complement (bool, optional): Whether to complement the sequence.\n            Defaults to True.\n\n    Returns:\n        str: The reverse complement (or as specified) of the input sequence.\n    \"\"\"\n    mapping = {\n        \"A\": \"T\",\n        \"T\": \"A\",\n        \"C\": \"G\",\n        \"G\": \"C\",\n        \"a\": \"t\",\n        \"t\": \"a\",\n        \"c\": \"g\",\n        \"g\": \"c\",\n        \"N\": \"N\",\n        \"n\": \"n\",\n    }\n    if reverse:\n        seq = seq[::-1]\n    if complement:\n        seq = \"\".join(mapping.get(base, base) for base in seq)\n    return seq\n</code></pre>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.seq2kmer","title":"seq2kmer","text":"<pre><code>seq2kmer(seqs, k)\n</code></pre> <p>Convert a list of DNA sequences to k-mers (overlapping k-mer tokenization).</p> <p>Parameters:</p> Name Type Description Default <code>seqs</code> <code>list[str]</code> <p>List of DNA sequences.</p> required <code>k</code> <code>int</code> <p>k-mer length.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of k-mer tokenized sequences (space-separated k-mers).</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def seq2kmer(seqs: list[str], k: int) -&gt; list[str]:\n    \"\"\"Convert a list of DNA sequences to k-mers (overlapping k-mer\n    tokenization).\n\n    Args:\n        seqs (list[str]): List of DNA sequences.\n        k (int): k-mer length.\n\n    Returns:\n        list[str]: List of k-mer tokenized sequences (space-separated\n            k-mers).\n    \"\"\"\n    all_kmers = []\n    for seq in seqs:\n        kmer = [seq[x : x + k].upper() for x in range(len(seq) + 1 - k)]\n        kmers = \" \".join(kmer)\n        all_kmers.append(kmers)\n    return all_kmers\n</code></pre>"},{"location":"api/utils/support/","title":"Support Utilities","text":""},{"location":"api/utils/support/#dnallm.utils.support","title":"dnallm.utils.support","text":""},{"location":"api/utils/support/#dnallm.utils.support-functions","title":"Functions","text":""},{"location":"api/utils/support/#dnallm.utils.support.is_flash_attention_capable","title":"is_flash_attention_capable","text":"<pre><code>is_flash_attention_capable()\n</code></pre> <p>Check if Flash Attention has been installed. Returns:             True if Flash Attention is installed and the device supports it         False otherwise</p> Source code in <code>dnallm/utils/support.py</code> <pre><code>def is_flash_attention_capable():\n    \"\"\"Check if Flash Attention has been installed.\n    Returns:\n                True if Flash Attention is installed and the device supports it\n            False otherwise\n    \"\"\"\n    try:\n        import flash_attn  # pyright: ignore[reportMissingImports]\n\n        _ = flash_attn\n        return True\n    except Exception as e:\n        logger.warning(f\"Cannot find supported Flash Attention: {e}\")\n        return False\n</code></pre>"},{"location":"api/utils/support/#dnallm.utils.support.is_fp8_capable","title":"is_fp8_capable","text":"<pre><code>is_fp8_capable()\n</code></pre> <p>Check if the current CUDA device supports FP8 precision.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the device supports FP8 (</p> <pre><code>    compute capability &gt;= 9.0),\n    False otherwise\n</code></pre> Source code in <code>dnallm/utils/support.py</code> <pre><code>def is_fp8_capable() -&gt; bool:\n    \"\"\"Check if the current CUDA device supports FP8 precision.\n\n    Returns:\n                True if the device supports FP8 (\n            compute capability &gt;= 9.0),\n            False otherwise\n    \"\"\"\n    major, minor = get_device_capability()\n    # Hopper (H100) has compute capability 9.0\n    if (major, minor) &gt;= (9, 0):\n        return True\n    else:\n        logger.warning(\n            f\"Current device compute capability is {major}.{minor}, \"\n            \"which does not support FP8.\"\n        )\n        return False\n</code></pre>"},{"location":"concepts/inference/","title":"Inference Concepts","text":"<p>Inference is the process of using a trained DNA language model to generate predictions, analyze sequences, or perform downstream tasks on new DNA data. This document covers the fundamental concepts and methods involved in inference with DNA language models.</p>"},{"location":"concepts/inference/#what-is-inference","title":"What is Inference?","text":"<p>Inference refers to the process of applying a trained model to new, unseen data to make predictions or generate outputs. In the context of DNA language models, inference involves:</p> <ul> <li>Sequence Analysis: Analyzing DNA sequences to understand their properties</li> <li>Prediction Generation: Generating predictions about sequence characteristics</li> <li>Feature Extraction: Extracting meaningful representations from DNA sequences</li> <li>Downstream Tasks: Performing specific biological tasks using the model's learned representations</li> </ul>"},{"location":"concepts/inference/#key-components-of-inference","title":"Key Components of Inference","text":""},{"location":"concepts/inference/#1-model-loading-and-initialization","title":"1. Model Loading and Initialization","text":"<p>Before inference can begin, the trained model must be loaded into memory. This involves:</p> <ul> <li>Model Restoration: Loading the trained model weights and architecture from storage</li> <li>Memory Allocation: Allocating sufficient memory for the model and input data</li> <li>Device Placement: Placing the model on appropriate computational devices (CPU, GPU, or distributed systems)</li> <li>State Configuration: Setting the model to evaluation mode to disable training-specific behaviors</li> </ul>"},{"location":"concepts/inference/#2-input-preprocessing","title":"2. Input Preprocessing","text":"<p>DNA sequences must be properly formatted and tokenized before feeding into the model:</p> <ul> <li>Sequence Cleaning: Removing invalid characters and normalizing sequences</li> <li>Tokenization: Converting DNA sequences into model-compatible tokens</li> <li>Padding/Truncation: Ensuring consistent input lengths for batch processing</li> <li>Batch Preparation: Organizing multiple sequences for efficient processing</li> </ul>"},{"location":"concepts/inference/#3-forward-pass","title":"3. Forward Pass","text":"<p>The core inference step where the model processes the input:</p> <ul> <li>Input Processing: Feeding preprocessed sequences through the model</li> <li>Computation: Performing matrix operations and neural network computations</li> <li>Output Generation: Producing raw model outputs (logits, probabilities, or embeddings)</li> <li>Memory Management: Efficiently managing computational resources during processing</li> </ul>"},{"location":"concepts/inference/#4-output-processing","title":"4. Output Processing","text":"<p>Transform raw model outputs into meaningful results:</p> <ul> <li>Logits Processing: Converting raw scores to probabilities using activation functions</li> <li>Post-processing: Applying task-specific transformations and filtering</li> <li>Result Formatting: Structuring outputs for downstream use and interpretation</li> <li>Confidence Scoring: Assessing the reliability of model predictions</li> </ul>"},{"location":"concepts/mcp/","title":"MCP (Model Context Protocol) Concepts","text":"<p>MCP (Model Context Protocol) is an open standard protocol designed to provide standardized interfaces for Large Language Model (LLM) applications, enabling them to connect to external data sources and tools for secure and efficient interaction. This document introduces the basic concepts of MCP, its integration in the DNALLM project, and the advantages it brings.</p>"},{"location":"concepts/mcp/#what-is-mcp","title":"What is MCP?","text":""},{"location":"concepts/mcp/#basic-definition","title":"Basic Definition","text":"<p>Model Context Protocol (MCP) is an open standard promoted by Anthropic, specifically designed for:</p> <ul> <li>Standardized Interface: Providing unified interfaces for AI models to access tools and data sources</li> <li>Secure Interaction: Ensuring AI models can safely call external tools through user authorization mechanisms</li> <li>Real-time Data Access: Breaking through the temporal limitations of AI model knowledge to access real-time or specialized information</li> <li>Tool Integration: Enabling AI models to use various external tools and functionalities</li> </ul>"},{"location":"concepts/mcp/#core-features","title":"Core Features","text":"<ol> <li>Protocol Standardization: Provides unified JSON-RPC 2.0 protocol specification</li> <li>Multi-transport Support: Supports STDIO, SSE, HTTP, and other transmission methods</li> <li>Security Mechanisms: Built-in permission control and user authorization mechanisms</li> <li>Real-time Communication: Supports streaming data transmission and real-time progress updates</li> <li>Cross-platform Compatibility: Supports multiple programming languages and platforms</li> </ol>"},{"location":"concepts/mcp/#why-does-dnallm-integrate-mcp","title":"Why Does DNALLM Integrate MCP?","text":""},{"location":"concepts/mcp/#1-solving-dna-language-model-integration-challenges","title":"1. Solving DNA Language Model Integration Challenges","text":"<p>Traditional DNA language models typically exist as standalone scripts or Jupyter Notebooks, lacking standardized service interfaces:</p> <ul> <li>Integration Difficulties: Hard to integrate with other tools and systems</li> <li>Inconsistent Interfaces: Each model has its own calling method</li> <li>Lack of Real-time Capabilities: Unable to provide real-time predictions and progress feedback</li> <li>Complex Deployment: Requires manual management of model loading and configuration</li> </ul>"},{"location":"concepts/mcp/#2-providing-standardized-dna-prediction-services","title":"2. Providing Standardized DNA Prediction Services","text":"<p>Through MCP integration, DNALLM achieves:</p> <ul> <li>Unified Interface: All DNA prediction functionalities exposed through standard MCP protocol</li> <li>Service-oriented Deployment: Packaging DNA models as deployable microservices</li> <li>Multi-client Support: Supporting command-line, web applications, APIs, and other clients</li> <li>Real-time Interaction: Providing streaming predictions and real-time progress updates</li> </ul>"},{"location":"concepts/mcp/#3-enhancing-ai-assistant-dna-analysis-capabilities","title":"3. Enhancing AI Assistant DNA Analysis Capabilities","text":"<p>MCP enables AI assistants to:</p> <ul> <li>Directly Call DNA Models: No need for users to manually run complex prediction scripts</li> <li>Real-time Analysis: Perform DNA sequence analysis in real-time during conversations</li> <li>Multi-model Comparison: Use multiple models simultaneously for prediction and comparison</li> <li>Result Interpretation: Combine AI's natural language capabilities to explain prediction results</li> </ul>"},{"location":"concepts/mcp/#advantages-of-mcp-integration","title":"Advantages of MCP Integration","text":""},{"location":"concepts/mcp/#1-standardization-and-interoperability","title":"1. Standardization and Interoperability","text":"<p>Unified Protocol: - All DNA prediction functionalities exposed through standard MCP protocol - Clients don't need to understand specific model implementation details - Supports any MCP-compliant client tools</p> <p>Cross-platform Compatibility: - Supports multiple programming languages like Python, JavaScript, Go - Can run on different operating systems and environments - Seamless integration with existing MCP ecosystem</p>"},{"location":"concepts/mcp/#2-real-time-capabilities-and-user-experience","title":"2. Real-time Capabilities and User Experience","text":"<p>Streaming Predictions: - Supports Server-Sent Events (SSE) for real-time push - Provides prediction progress updates and status feedback - Users can see the prediction process in real-time</p> <p>Multi-transport Protocols: - STDIO: Suitable for command-line tools and script integration - SSE: Suitable for real-time web applications (<code>http://localhost:8000/sse</code>) - Streamable HTTP: Suitable for REST API integration (<code>http://localhost:8000/mcp</code>)</p>"},{"location":"concepts/mcp/#3-security-and-permission-control","title":"3. Security and Permission Control","text":"<p>User Authorization: - Controls model access through MCP's permission mechanisms - Users can precisely control which tools can be called - Prevents unauthorized model access</p> <p>Data Security: - Local model deployment, data doesn't leave user environment - Supports private models and sensitive data processing - Complies with bioinformatics data privacy requirements</p>"},{"location":"concepts/mcp/#4-scalability-and-maintainability","title":"4. Scalability and Maintainability","text":"<p>Modular Design: - Each DNA model as an independent MCP tool - Easy to add new prediction models and functionalities - Supports dynamic model loading and management</p> <p>Configuration-driven: - Manages model and server settings through YAML configuration files - Supports hot reloading and dynamic configuration updates - Simplifies deployment and maintenance processes</p>"},{"location":"concepts/mcp/#5-development-efficiency","title":"5. Development Efficiency","text":"<p>Simplified Integration: - Developers only need to understand MCP protocol to integrate DNA prediction functionality - No need to deeply understand specific model implementations - Provides rich client SDKs and examples</p> <p>Tool Ecosystem: - Seamless integration with AI tools like Claude Desktop, Cline - Supports custom client development - Rich community tools and plugins</p>"},{"location":"concepts/mcp/#dnallm-mcp-architecture","title":"DNALLM MCP Architecture","text":""},{"location":"concepts/mcp/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   MCP Client    \u2502    \u2502   MCP Server     \u2502    \u2502  DNA Models     \u2502\n\u2502                 \u2502    \u2502                  \u2502    \u2502                 \u2502\n\u2502 - Claude Desktop\u2502\u25c4\u2500\u2500\u25ba\u2502 - FastMCP Server \u2502\u25c4\u2500\u2500\u25ba\u2502 - Model Pool    \u2502\n\u2502 - Web Client    \u2502    \u2502 - SSE Transport  \u2502    \u2502 - DNAInference  \u2502\n\u2502 - API Client    \u2502    \u2502 - Task Router    \u2502    \u2502 - Config Mgmt   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                       \u2502  Configuration   \u2502\n                       \u2502                  \u2502\n                       \u2502 - mcp_server_    \u2502\n                       \u2502   config.yaml    \u2502\n                       \u2502 - inference_     \u2502\n                       \u2502   model_config.  \u2502\n                       \u2502   yaml           \u2502\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/mcp/#core-components","title":"Core Components","text":"<ol> <li>MCP Server: Server implementation based on FastMCP framework</li> <li>Model Manager: Manages loading and calling of multiple DNA language models</li> <li>Config Manager: Handles server and model configuration</li> <li>Transport Layer: Supports multiple transport protocols (STDIO, SSE, HTTP)</li> </ol>"},{"location":"concepts/mcp/#available-tools","title":"Available Tools","text":"<p>Basic Prediction Tools: - <code>dna_sequence_predict</code>: Single sequence prediction - <code>dna_batch_predict</code>: Batch sequence prediction - <code>dna_multi_model_predict</code>: Multi-model prediction</p> <p>Streaming Prediction Tools: - <code>dna_stream_predict</code>: Single sequence streaming prediction - <code>dna_stream_batch_predict</code>: Batch streaming prediction - <code>dna_stream_multi_model_predict</code>: Multi-model streaming prediction</p> <p>Model Management Tools: - <code>list_loaded_models</code>: List loaded models - <code>get_model_info</code>: Get detailed model information - <code>health_check</code>: Server health check</p>"},{"location":"concepts/mcp/#client-access-points","title":"Client Access Points","text":"<p>The DNALLM MCP Server provides different access points depending on the transport protocol:</p>"},{"location":"concepts/mcp/#default-configuration","title":"Default Configuration","text":"<ul> <li>Host: <code>0.0.0.0</code> (listens on all interfaces)</li> <li>Port: <code>8000</code></li> <li>Base URL: <code>http://localhost:8000</code></li> </ul>"},{"location":"concepts/mcp/#transport-specific-endpoints","title":"Transport-Specific Endpoints","text":"<p>STDIO Transport: - Access: Direct process communication - Usage: MCP clients like Claude Desktop - Configuration: No URL needed, uses process communication</p> <p>SSE Transport: - SSE Connection: <code>http://localhost:8000/sse</code> - MCP Messages: <code>http://localhost:8000/mcp/messages/</code> - Usage: Real-time web applications with streaming updates</p> <p>Streamable HTTP Transport: - Main Endpoint: <code>http://localhost:8000/mcp</code> - Available Endpoints:   - <code>http://localhost:8000/mcp</code> - Main MCP protocol endpoint   - <code>http://localhost:8000/mcp/tools</code> - Tool listing endpoint   - <code>http://localhost:8000/mcp/messages</code> - MCP message handling endpoint - Usage: REST API integrations and HTTP-based clients</p>"},{"location":"concepts/mcp/#use-cases","title":"Use Cases","text":""},{"location":"concepts/mcp/#1-ai-assistant-integration","title":"1. AI Assistant Integration","text":"<p>Direct DNA analysis through AI assistants like Claude Desktop:</p> <pre><code>User: Please analyze this DNA sequence ATCGATCGATCG\nAI: I'll use DNALLM's promoter prediction model to analyze this sequence...\n[Calling MCP tool for prediction]\nAI: Based on the prediction results, this sequence has an 87.6% probability of being a core promoter region...\n</code></pre>"},{"location":"concepts/mcp/#2-web-application-development","title":"2. Web Application Development","text":"<p>Integrating DNA prediction functionality in web applications:</p>"},{"location":"concepts/mcp/#using-sse-transport","title":"Using SSE Transport","text":"<pre><code>// Establish SSE connection\nconst eventSource = new EventSource('http://localhost:8000/sse');\n\n// Send prediction request\nconst response = await fetch('/mcp/messages/', {\n  method: 'POST',\n  headers: { 'Content-Type': 'application/json' },\n  body: JSON.stringify({\n    jsonrpc: \"2.0\",\n    method: \"tools/call\",\n    params: {\n      name: \"dna_sequence_predict\",\n      arguments: { sequence: \"ATCGATCGATCG\" }\n    }\n  })\n});\n</code></pre>"},{"location":"concepts/mcp/#using-streamable-http-transport","title":"Using Streamable HTTP Transport","text":"<pre><code>// Direct HTTP API calls\nconst response = await fetch('http://localhost:8000/mcp/messages', {\n  method: 'POST',\n  headers: { 'Content-Type': 'application/json' },\n  body: JSON.stringify({\n    jsonrpc: \"2.0\",\n    method: \"tools/call\",\n    params: {\n      name: \"dna_sequence_predict\",\n      arguments: { \n        sequence: \"ATCGATCGATCG\",\n        model_name: \"promoter_model\"\n      }\n    }\n  })\n});\n</code></pre>"},{"location":"concepts/mcp/#3-automation-scripts","title":"3. Automation Scripts","text":"<p>Using DNA prediction in Python scripts:</p> <pre><code>import asyncio\nfrom mcp.client.session import ClientSession\n\nasync def predict_dna_sequence(sequence):\n    async with ClientSession(\"http://localhost:8000/sse\") as session:\n        await session.initialize()\n        result = await session.call_tool(\"dna_sequence_predict\", {\n            \"sequence\": sequence,\n            \"model_name\": \"promoter_model\"\n        })\n        return result\n</code></pre>"},{"location":"concepts/mcp/#summary","title":"Summary","text":"<p>MCP integration brings significant improvements to the DNALLM project:</p> <ol> <li>Standardization: Provides unified DNA prediction service interfaces</li> <li>Real-time Capabilities: Supports streaming predictions and real-time progress updates</li> <li>Security: Built-in permission control and data protection mechanisms</li> <li>Scalability: Easy to add new models and functionalities</li> <li>Interoperability: Seamless integration with various clients and tools</li> </ol> <p>Through the MCP protocol, DNALLM transforms from a traditional research tool into a modern AI service, providing bioinformatics researchers and developers with more powerful and user-friendly DNA analysis capabilities.</p>"},{"location":"concepts/training/","title":"Training Concepts in Machine Learning","text":"<p>Training is the fundamental process in machine learning where a model learns patterns from data to make predictions or perform tasks. This document explains the core concepts of training and provides practical examples using DNALLM for DNA language models.</p>"},{"location":"concepts/training/#what-is-training-in-machine-learning","title":"What is Training in Machine Learning?","text":"<p>Training is the process of optimizing a model's parameters (weights and biases) to minimize a loss function, enabling the model to learn meaningful patterns from data. In the context of DNA language models, training involves:</p> <ul> <li>Parameter Optimization: Adjusting model weights to minimize prediction errors</li> <li>Pattern Learning: Discovering biological patterns and relationships in DNA sequences</li> <li>Representation Learning: Learning meaningful embeddings for DNA tokens and sequences</li> <li>Task Adaptation: Fine-tuning for specific biological tasks and applications</li> </ul>"},{"location":"concepts/architecture/attention_mechanisms/","title":"Attention Mechanisms","text":"<p>The attention mechanism is arguably the most important innovation behind the success of modern Transformer-based Large Language Models. It is the component that allows a model to dynamically focus on the most relevant parts of an input sequence when making a prediction.</p>"},{"location":"concepts/architecture/attention_mechanisms/#1-what-is-attention","title":"1. What is Attention?","text":"<p>Imagine you are translating a sentence. To translate a specific word, you don't just look at that word in isolation; you pay \"attention\" to other words in the source sentence that provide context. The attention mechanism in a neural network does the same thing.</p> <p>For a given token in a sequence, the attention mechanism calculates an \"attention score\" for every other token in the sequence. These scores represent the relevance or importance of those other tokens to the current one. A high score means \"pay close attention to this token.\"</p>"},{"location":"concepts/architecture/attention_mechanisms/#2-the-principle-of-self-attention","title":"2. The Principle of Self-Attention","text":"<p>In Transformers, this process is called self-attention because the model is relating different positions of the same input sequence to each other. The calculation happens in three steps, using learned weight matrices to create three vectors for each input token:</p> <ol> <li>Query (Q): A representation of the current token, asking a \"question\" about the sequence.</li> <li>Key (K): A representation of each token in the sequence, acting as a \"label\" or identifier.</li> <li>Value (V): A representation of the content of each token in the sequence.</li> </ol> <p> Image Credit: \"Attention is all you need (Transformer) - Model explanation (including math), Inference and Training\" from YouTube.</p> <p>The process works as follows: 1.  Calculate Scores: The Query vector of the current token is compared with the Key vector of every other token (usually via a dot product). This produces a raw score indicating how well the two tokens \"match.\" 2.  Normalize Scores: These scores are scaled and then passed through a softmax function. This converts the scores into probabilities that sum to 1, representing the distribution of attention. 3.  Weighted Sum: The Value vector of each token is multiplied by its normalized attention score. These weighted Value vectors are then summed up to produce the final output for the current token.</p> <p>This output is a new representation of the token that is enriched with contextual information from the entire sequence.</p>"},{"location":"concepts/architecture/attention_mechanisms/#3-advantages-and-disadvantages","title":"3. Advantages and Disadvantages","text":""},{"location":"concepts/architecture/attention_mechanisms/#advantages","title":"Advantages","text":"<ul> <li>Captures Long-Range Dependencies: Unlike older architectures like RNNs, attention can directly connect any two tokens in a sequence, regardless of their distance. This is vital for genomics, where regulatory elements can be thousands of base pairs away from the gene they control.</li> <li>Parallelizable: The attention calculation for all tokens can be performed simultaneously, making it highly efficient on modern hardware like GPUs.</li> <li>Interpretability: By visualizing the attention scores (as in <code>DNAInference.plot_attentions()</code>), we can gain insights into which parts of a sequence the model considers important, helping to uncover biological motifs.</li> </ul>"},{"location":"concepts/architecture/attention_mechanisms/#disadvantages","title":"Disadvantages","text":"<ul> <li>Quadratic Complexity: The primary drawback is that the computational cost and memory usage scale quadratically with the sequence length (O(n\u00b2)). For a sequence of length <code>n</code>, it must compute <code>n x n</code> attention scores. This makes it very expensive for the extremely long sequences found in genomics.</li> </ul>"},{"location":"concepts/architecture/attention_mechanisms/#4-flash-attention-and-other-optimizations","title":"4. Flash Attention and Other Optimizations","text":"<p>The quadratic complexity of standard attention has led to the development of more efficient implementations.</p> <ul> <li> <p>Flash Attention: This is a highly optimized version of the attention algorithm that uses techniques like kernel fusion and tiling to reduce the amount of memory read/written to and from the GPU's main memory. It doesn't change the mathematical result but makes the process significantly faster and more memory-efficient. DNALLM automatically uses Flash Attention if it's installed and supported by the model and hardware.</p> </li> <li> <p>Sparse Attention: Models like <code>BigBird</code> use sparse attention patterns (e.g., windowed and random attention) to reduce the number of scores that need to be calculated, approximating the full attention matrix.</p> </li> </ul> <p>These optimizations are crucial for applying Transformer-like models to genome-scale problems.</p> <p>Next: Learn how tokens are converted into rich numerical representations in Embedding Layers.</p>"},{"location":"concepts/architecture/embedding_layers/","title":"Embedding Layers","text":"<p>An embedding layer is the first crucial component of a deep learning model that processes sequence data. Its job is to convert discrete, integer-based tokens into continuous, dense, and meaningful numerical vectors called embeddings.</p>"},{"location":"concepts/architecture/embedding_layers/#1-what-is-an-embedding","title":"1. What is an Embedding?","text":"<p>A language model cannot directly work with text or integer IDs. It needs a numerical representation that captures the semantic meaning and relationships between tokens. An embedding is a low-dimensional, learned vector representation of a discrete variable.</p> <p>Think of it like this: - One-Hot Encoding: A simple but inefficient way to represent a token. If your vocabulary has 10,000 tokens, each token is a vector of 10,000 zeros with a single one. This is sparse and doesn't capture any relationships (e.g., the vector for <code>GAT</code> is no more similar to <code>GAC</code> than it is to <code>TCC</code>). - Dense Embedding: A learned vector of a much smaller, fixed size (e.g., 128 or 768 dimensions). The values in this vector are learned during model training.</p> <p>The key idea is that in the learned embedding space, tokens with similar meanings or contexts will have similar vectors. For DNA, this means that k-mers that appear in similar biological contexts (e.g., different transcription factor binding sites for the same protein family) might be mapped to nearby points in the embedding space.</p>"},{"location":"concepts/architecture/embedding_layers/#2-how-the-embedding-layer-works","title":"2. How the Embedding Layer Works","text":"<p>The embedding layer is essentially a lookup table. 1.  It is initialized as a matrix of size <code>(vocabulary_size, embedding_dimension)</code>. 2.  Each row of the matrix corresponds to a token ID in the vocabulary. 3.  When a sequence of token IDs is passed to the embedding layer, it simply \"looks up\" the corresponding row (vector) for each ID.</p> <p>These initial embeddings are then passed to the subsequent layers of the model (e.g., the attention layers). Importantly, the values in the embedding matrix are parameters that are updated and optimized during the model's training process.</p>"},{"location":"concepts/architecture/embedding_layers/#positional-embeddings","title":"Positional Embeddings","text":"<p>Standard self-attention is permutation-invariant\u2014it doesn't know the order of the tokens. To solve this, Transformers add a positional embedding to the token embedding. This is a vector that depends on the position of the token in the sequence, giving the model information about token order.</p>"},{"location":"concepts/architecture/embedding_layers/#3-the-role-of-embeddings-in-dnallm","title":"3. The Role of Embeddings in DNALLM","text":"<p>In DNALLM, embeddings are not just an internal part of the model; they are also a powerful tool for analysis.</p>"},{"location":"concepts/architecture/embedding_layers/#what-can-you-do-with-embeddings","title":"What can you do with embeddings?","text":"<ul> <li> <p>Feature Extraction: The output embeddings from a pre-trained model (like <code>DNABERT</code>) can be used as high-quality features for simpler downstream models (e.g., a logistic regression classifier). These embeddings are rich representations of the input sequence.</p> </li> <li> <p>Sequence Similarity: You can compare the embeddings of two DNA sequences (e.g., using cosine similarity) to get a semantic measure of their similarity. This can be more powerful than simple sequence alignment.</p> </li> <li> <p>Visualization and Clustering: The <code>DNAInference.plot_hidden_states()</code> function allows you to visualize the embeddings of many sequences using dimensionality reduction techniques like t-SNE or UMAP. This is a powerful way to see if the model has learned to separate sequences into biologically meaningful clusters (e.g., promoters vs. non-promoters).</p> </li> </ul> <p>By extracting and analyzing embeddings, you can gain deep insights into both your biological data and the inner workings of the language model itself.</p> <p>Previous: Learn about Attention Mechanisms.</p>"},{"location":"concepts/architecture/model_architectures/","title":"Introduction to Model Architectures","text":"<p>Large Language Models (LLMs) are at the heart of the DNALLM framework. Understanding their underlying architectures is key to selecting the right model for your biological task. This guide provides an overview of common and emerging architectures used in genomics.</p>"},{"location":"concepts/architecture/model_architectures/#1-what-is-a-large-language-model","title":"1. What is a Large Language Model?","text":"<p>A Large Language Model is a type of artificial intelligence model trained on vast amounts of text data to understand and generate human-like language. In genomics, we adapt this concept by treating DNA sequences as a \"language.\" The model learns the \"grammar\" of DNA\u2014the complex patterns, motifs, and long-range dependencies that encode biological function.</p> <p>LLMs are typically built on deep neural networks, with the Transformer architecture being the most foundational.</p>"},{"location":"concepts/architecture/model_architectures/#2-foundational-architectures","title":"2. Foundational Architectures","text":""},{"location":"concepts/architecture/model_architectures/#the-transformer","title":"The Transformer","text":"<p>The Transformer, introduced in the paper \"Attention Is All You Need,\" revolutionized sequence modeling. It relies on the self-attention mechanism to weigh the importance of different parts of the input sequence, allowing it to capture complex relationships between tokens.</p> <p>There are two primary variants of the Transformer architecture:</p> <ul> <li>Encoder-Only (e.g., BERT): These models, like <code>DNABERT</code>, process the entire input sequence at once, allowing them to gather context from both directions (bi-directional). This makes them exceptionally powerful for understanding tasks.</li> <li>Best for: Sequence classification, feature extraction, and token-level predictions.</li> <li> <p>DNALLM Examples: <code>DNABERT</code>, <code>Nucleotide Transformer</code>, <code>Caduceus</code>.</p> </li> <li> <p>Decoder-Only (e.g., GPT): These models process the sequence token-by-token in one direction (auto-regressive), predicting the next token based on the preceding ones. This makes them ideal for generation tasks.</p> </li> <li>Best for: Generating new DNA sequences, scoring sequence likelihood.</li> <li>DNALLM Examples: <code>DNAGPT</code>, <code>HyenaDNA</code>, <code>Evo</code>.</li> </ul>"},{"location":"concepts/architecture/model_architectures/#3-emerging-and-specialized-architectures","title":"3. Emerging and Specialized Architectures","text":"<p>While Transformers are powerful, their computational cost grows quadratically with sequence length. This has spurred the development of more efficient architectures, which are particularly important for genomics where sequences can be millions of base pairs long.</p>"},{"location":"concepts/architecture/model_architectures/#state-space-models-ssms","title":"State-Space Models (SSMs)","text":"<ul> <li>What they are: SSMs, like Mamba, are a newer class of models designed for efficiency. They process sequences linearly by maintaining a compressed \"state\" that summarizes the information seen so far.</li> <li>Advantages:<ul> <li>Linear Scaling: Much faster and less memory-intensive for long sequences.</li> <li>Long-Range Dependencies: Effectively captures long-range information.</li> </ul> </li> <li>DNALLM Examples:<ul> <li><code>Plant DNAMamba</code>: A causal model based on the Mamba architecture.</li> <li><code>Caduceus</code>: A bi-directional model that uses S4 layers (an early SSM), combining the power of bi-directionality with the efficiency of SSMs.</li> </ul> </li> </ul>"},{"location":"concepts/architecture/model_architectures/#hybrid-architectures","title":"Hybrid Architectures","text":"<ul> <li>What they are: These models combine elements from different architectures to leverage their respective strengths. StripedHyena, the architecture behind the Evo models, is a prime example. It mixes efficient convolutions with data-controlled gating and attention mechanisms.</li> <li>Advantages: Achieves a balance of performance, efficiency, and the ability to model extremely long sequences.</li> <li>DNALLM Examples: <code>Evo-1</code>, <code>Evo-2</code>.</li> </ul>"},{"location":"concepts/architecture/model_architectures/#convolutional-neural-networks-cnns","title":"Convolutional Neural Networks (CNNs)","text":"<ul> <li>What they are: CNNs use sliding filters (kernels) to detect local patterns or motifs in the data. While often associated with image processing, they are also highly effective for finding motifs in DNA sequences.</li> <li>Advantages: Excellent at capturing local, position-invariant patterns.</li> <li>DNALLM Examples: <code>GPN</code> (Genome-wide Pathogen-derived Network).</li> </ul>"},{"location":"concepts/architecture/model_architectures/#4-model-selection-in-dnallm","title":"4. Model Selection in DNALLM","text":"<p>The <code>dnallm.models.modeling_auto</code> module contains a comprehensive mapping of the models supported by DNALLM. When you choose a model, you are selecting one of these underlying architectures, which has been pre-trained on a massive corpus of genomic data.</p> <p>Your choice of architecture should be guided by your task: - For classification or feature extraction on short-to-medium sequences, a BERT-based model is a strong start. - For generating new sequences or zero-shot scoring, a GPT-style or Evo model is appropriate. - For tasks involving very long sequences (&gt;10kb), consider Mamba, Caduceus, or Evo models for their efficiency and long-range modeling capabilities.</p> <p>Next: Learn how DNA sequences are converted into a format models can understand in Tokenization.</p>"},{"location":"concepts/architecture/tokenization/","title":"Tokenization in Genomics","text":"<p>Before a DNA language model can process a sequence, the raw string of nucleotides (<code>\"GATTACA...\"</code>) must be converted into a series of numerical inputs. This process is called tokenization. A tokenizer breaks down the sequence into smaller units called tokens and then maps each token to a unique integer ID.</p>"},{"location":"concepts/architecture/tokenization/#1-what-is-a-tokenizer","title":"1. What is a Tokenizer?","text":"<p>A tokenizer is a crucial component that acts as a bridge between the raw DNA sequence and the model. Its vocabulary defines the set of all possible tokens the model can recognize. The choice of tokenization strategy significantly impacts the model's performance, resolution, and computational efficiency.</p> <p>In DNALLM, the tokenizer is always paired with its corresponding model, as the model was trained to understand a specific set of tokens. The <code>load_model_and_tokenizer</code> function handles this pairing automatically.</p>"},{"location":"concepts/architecture/tokenization/#2-common-tokenization-methods-for-dna","title":"2. Common Tokenization Methods for DNA","text":"<p>DNALLM supports models that use various tokenization strategies. Here are the most common ones:</p>"},{"location":"concepts/architecture/tokenization/#single-nucleotide-tokenization","title":"Single Nucleotide Tokenization","text":"<ul> <li>How it works: Each individual nucleotide (<code>A</code>, <code>C</code>, <code>G</code>, <code>T</code>) is a token. Special tokens like <code>[CLS]</code> (start of sequence), <code>[SEP]</code> (separator), and <code>[PAD]</code> (padding) are also included.</li> <li>Example: The sequence <code>GATTACA</code> is tokenized into <code>['G', 'A', 'T', 'T', 'A', 'C', 'A']</code>.</li> <li>Pros:<ul> <li>Highest Resolution: Provides single-base-pair resolution, which is ideal for identifying SNPs or fine-grained motifs.</li> </ul> </li> <li>Cons:<ul> <li>Longer Sequences: Results in very long sequences of tokens, increasing computational cost.</li> </ul> </li> <li>DNALLM Models: Some <code>Plant DNAMamba</code> and <code>Plant DNAModernBert</code> models use this.</li> </ul>"},{"location":"concepts/architecture/tokenization/#k-mer-tokenization","title":"K-mer Tokenization","text":"<ul> <li>How it works: The sequence is broken down into overlapping chunks of a fixed length <code>k</code>.</li> <li>Example (k=3): The sequence <code>GATTACA</code> is tokenized into <code>['GAT', 'ATT', 'TTA', 'TAC', 'ACA']</code>.</li> <li>Pros:<ul> <li>Captures Local Context: Each token inherently contains local sequence information (e.g., codon-like patterns).</li> <li>Shorter Sequences: Reduces the overall length of the token sequence compared to single-nucleotide methods.</li> </ul> </li> <li>Cons:<ul> <li>Large Vocabulary: The vocabulary size grows exponentially with <code>k</code> (4^k), which can become very large.</li> <li>Fixed Resolution: The model's resolution is limited to the k-mer size.</li> </ul> </li> <li>DNALLM Models: <code>DNABERT</code> is famously based on k-mer tokenization (e.g., 3-mer to 6-mer).</li> </ul>"},{"location":"concepts/architecture/tokenization/#byte-pair-encoding-bpe","title":"Byte Pair Encoding (BPE)","text":"<ul> <li>How it works: BPE is a subword tokenization algorithm that starts with a base vocabulary (e.g., single nucleotides) and iteratively merges the most frequent adjacent pairs of tokens to create new, longer tokens.</li> <li>Example:<ol> <li>Start with base tokens: <code>A, C, G, T</code>.</li> <li>In a large corpus, <code>AT</code> might be a very common pair. BPE merges them to create a new token <code>AT</code>.</li> <li>Next, <code>CG</code> might be common, creating the token <code>CG</code>.</li> <li>Then, <code>ATCG</code> might be a frequent combination, so <code>AT</code> and <code>CG</code> are merged into <code>ATCG</code>. The final vocabulary contains a mix of single bases and common longer motifs.</li> </ol> </li> <li>Pros:<ul> <li>Adaptive: The vocabulary is learned from the data, capturing statistically relevant motifs of variable lengths.</li> <li>Manages Vocabulary Size: Balances sequence length and vocabulary size effectively.</li> </ul> </li> <li>Cons:<ul> <li>Less Interpretable: The learned tokens may not always correspond to known biological motifs.</li> </ul> </li> <li>DNALLM Models: Many modern models, including <code>Plant DNAGPT</code>, <code>Plant DNABERT-BPE</code>, and <code>Nucleotide Transformer</code>, use BPE.</li> </ul>"},{"location":"concepts/architecture/tokenization/#3-why-tokenization-matters","title":"3. Why Tokenization Matters","text":"<p>The tokenization method defines how the model \"sees\" the DNA. - A 6-mer tokenizer might be well-suited for tasks where codon-like patterns are important. - A single-nucleotide tokenizer is essential for predicting the effect of a single nucleotide polymorphism (SNP). - BPE offers a flexible and efficient middle ground for a wide range of tasks.</p> <p>When using DNALLM, you don't need to configure the tokenizer manually, but understanding how your chosen model tokenizes sequences is crucial for interpreting its behavior and results.</p> <p>Next: Discover how models weigh the importance of tokens with Attention Mechanisms.</p>"},{"location":"concepts/biology/biological_tasks/","title":"Common Biological Tasks with DNALLM","text":"<p>DNA Language Models can be applied to a wide variety of computational biology problems. These tasks often involve predicting the function or properties of a DNA sequence. DNALLM is designed to handle these tasks through its flexible configuration system.</p> <p>Here are some of the most common tasks, mapped to their corresponding <code>task_type</code> in DNALLM.</p>"},{"location":"concepts/biology/biological_tasks/#1-sequence-classification","title":"1. Sequence Classification","text":"<p>This is the most common category of tasks, where the goal is to assign a label to a given DNA sequence.</p>"},{"location":"concepts/biology/biological_tasks/#binary-classification","title":"Binary Classification","text":"<ul> <li><code>task_type: binary</code></li> <li>Description: Predict whether a sequence belongs to one of two classes.</li> <li>Examples:<ul> <li>Promoter Prediction: Is this sequence a promoter or not?</li> <li>Enhancer Identification: Is this sequence an enhancer or a non-enhancer region?</li> <li>Splice Site Prediction: Is this position a splice site (donor/acceptor) or not?</li> </ul> </li> </ul>"},{"location":"concepts/biology/biological_tasks/#multi-class-classification","title":"Multi-class Classification","text":"<ul> <li><code>task_type: multiclass</code></li> <li>Description: Assign a sequence to one of several mutually exclusive classes.</li> <li>Examples:<ul> <li>Functional Region Classification: Classify a sequence as a promoter, enhancer, or silencer.</li> <li>Organism of Origin: Predict whether a viral sequence comes from human, bat, or avian hosts.</li> </ul> </li> </ul>"},{"location":"concepts/biology/biological_tasks/#multi-label-classification","title":"Multi-label Classification","text":"<ul> <li><code>task_type: multilabel</code></li> <li>Description: Assign a sequence to one or more non-exclusive labels.</li> <li>Examples:<ul> <li>Transcription Factor Binding: Predict which of several transcription factors (e.g., TCF1, GATA3, RUNX1) can bind to a given sequence.</li> </ul> </li> </ul>"},{"location":"concepts/biology/biological_tasks/#2-expression-prediction-regression","title":"2. Expression Prediction (Regression)","text":"<ul> <li><code>task_type: regression</code></li> <li>Description: Predict a continuous numerical value associated with a sequence.</li> <li>Examples:<ul> <li>Promoter Strength Prediction: Predict the level of gene expression driven by a promoter sequence.</li> <li>Protein-DNA Binding Affinity: Predict the binding strength of a transcription factor to a DNA sequence.</li> </ul> </li> </ul>"},{"location":"concepts/biology/biological_tasks/#3-element-mining-token-classification","title":"3. Element Mining (Token Classification)","text":"<ul> <li><code>task_type: token_classification</code> (also known as Named Entity Recognition or NER)</li> <li>Description: Assign a label to each token (or nucleotide) within a sequence.</li> <li>Examples:<ul> <li>Transcription Factor Binding Site (TFBS) Identification: Pinpoint the exact locations of TFBS motifs within a longer regulatory sequence.</li> <li>Gene Finding: Identify the start codons, stop codons, and exon/intron boundaries within a genomic region.</li> </ul> </li> </ul>"},{"location":"concepts/biology/biological_tasks/#4-new-sequence-generation","title":"4. New Sequence Generation","text":"<ul> <li><code>task_type: generation</code></li> <li>Description: Create novel DNA sequences that have desired properties. This is typically done with Causal Language Models (CLMs) like GPT or Evo.</li> <li>Examples:<ul> <li>Designing High-Strength Promoters: Generate new promoter sequences that are predicted to drive very high levels of gene expression.</li> <li>Creating Synthetic Genes: Design novel genes with specific desired functions.</li> </ul> </li> </ul> <p>These tasks form the core of what DNALLM is designed to accomplish. By providing a unified interface for fine-tuning and inference, DNALLM allows researchers to easily apply state-of-the-art language models to these and other biological challenges.</p> <p>Next: Explore the methods used to analyze the results of these tasks in Sequence Analysis Methods.</p>"},{"location":"concepts/biology/dna_sequences/","title":"What are DNA Sequences?","text":"<p>At the most fundamental level, Deoxyribonucleic Acid (DNA) is the molecule that carries the genetic instructions for the development, functioning, growth, and reproduction of all known organisms and many viruses. A DNA sequence is the linear order of its building blocks, known as nucleotides.</p>"},{"location":"concepts/biology/dna_sequences/#1-the-building-blocks-of-dna","title":"1. The Building Blocks of DNA","text":"<p>A DNA sequence is composed of a series of four nucleotide bases: - Adenine (A) - Guanine (G) - Cytosine (C) - Thymine (T)</p> <p>These bases pair up in a specific way: Adenine pairs with Thymine (A-T), and Guanine pairs with Cytosine (G-C). This pairing forms the rungs of the famous \"double helix\" structure.</p> <p> Image Credit: National Human Genome Research Institute (NHGRI)</p> <p>For computational purposes, we typically represent a DNA sequence by one of its two strands, as a simple string of characters.</p> <p>Example: <pre><code>AGCTAGCTAGCT\n</code></pre></p> <p>This string <code>AGCTAGCTAGCT</code> is the fundamental data type that DNA Language Models, like those in the DNALLM framework, are designed to understand and process.</p>"},{"location":"concepts/biology/dna_sequences/#2-the-central-dogma-of-molecular-biology","title":"2. The Central Dogma of Molecular Biology","text":"<p>The primary role of DNA is to store information. The central dogma describes how this information flows from DNA to create functional products, like proteins.</p> <p>DNA \u2192 RNA \u2192 Protein</p> <ol> <li>Transcription: A segment of DNA (a gene) is copied into a messenger RNA (mRNA) sequence. In RNA, Thymine (T) is replaced by Uracil (U).</li> <li>Translation: The mRNA sequence is read by a ribosome, which translates it into a chain of amino acids, forming a protein.</li> </ol> <p>This process is why DNA sequences are so critical. The order of A, C, G, and T in a gene ultimately determines the structure and function of a protein, which in turn dictates cellular activities and organismal traits.</p>"},{"location":"concepts/biology/dna_sequences/#3-dna-sequences-in-dnallm","title":"3. DNA Sequences in DNALLM","text":"<p>In the context of DNALLM, a DNA sequence is treated as a \"language.\" The models learn the \"grammar\" and \"syntax\" of this language from vast amounts of genomic data.</p> <ul> <li>Tokens: Just like human language is broken down into words or sub-words, a DNA sequence is broken into tokens. This can be as simple as single nucleotides (<code>A</code>, <code>C</code>, <code>G</code>, <code>T</code>) or k-mers (e.g., 6-mers like <code>GATTACA</code>).</li> <li>Sentences: A complete DNA sequence, such as a gene or a regulatory element, is treated as a \"sentence\" or a \"document.\"</li> </ul> <p>By learning the patterns within these sequences, DNALLM can perform a wide range of biological tasks, from predicting the function of a sequence to designing entirely new ones.</p> <p>Next: Learn about the different functional parts of a genome in Genomic Features.</p>"},{"location":"concepts/biology/genomic_features/","title":"Genomic Features","text":"<p>A genome is the complete set of DNA of an organism. It's not just a random string of nucleotides; it is highly organized into distinct functional units called genomic features. Understanding these features is key to interpreting the genome and is the basis for most tasks in computational biology.</p>"},{"location":"concepts/biology/genomic_features/#1-genes-and-related-elements","title":"1. Genes and Related Elements","text":"<ul> <li>Genes: These are the most well-known genomic features. A gene is a specific sequence of DNA that contains the instructions to make a functional product, either an RNA molecule or a protein.<ul> <li>Exons: The coding regions of a gene that are translated into protein.</li> <li>Introns: Non-coding regions within a gene that are spliced out of the mRNA before translation.</li> </ul> </li> </ul>"},{"location":"concepts/biology/genomic_features/#2-regulatory-elements","title":"2. Regulatory Elements","text":"<p>These are non-coding DNA sequences that control when, where, and how much genes are expressed (turned on or off). They are critical for development and cellular function.</p> <ul> <li> <p>Promoters: Located just upstream of a gene, promoters are the binding sites for RNA polymerase, the enzyme that initiates transcription. Task in DNALLM: Predicting whether a sequence is a promoter is a classic <code>binary</code> classification task.</p> </li> <li> <p>Enhancers: DNA sequences that can be located far away from the gene they regulate. They increase the likelihood that the gene will be transcribed.</p> </li> <li> <p>Silencers: The opposite of enhancers; they decrease or shut down gene transcription.</p> </li> <li> <p>Insulators: DNA sequences that act as boundary elements, preventing enhancers or silencers from affecting the wrong genes.</p> </li> <li> <p>Transcription Factor Binding Sites (TFBS): Short, specific DNA sequences (motifs) within regulatory elements where transcription factor proteins bind to control gene expression. Task in DNALLM: Identifying these sites is a <code>token_classification</code> (NER) task.</p> </li> </ul> <p><pre><code>      Enhancer              Promoter\n---[GATA-box]----//----[TATA-box]--[Gene Start]-----&gt;\n      ^                     ^\n  TF binds here        RNA Pol binds here\n</code></pre> A simplified diagram showing regulatory elements relative to a gene.</p>"},{"location":"concepts/biology/genomic_features/#3-non-coding-rna-ncrna","title":"3. Non-Coding RNA (ncRNA)","text":"<p>These are RNA molecules that are not translated into a protein but have functional roles themselves.</p> <ul> <li>Transfer RNA (tRNA) and Ribosomal RNA (rRNA): Essential for protein synthesis.</li> <li>MicroRNAs (miRNAs) and Long non-coding RNAs (lncRNAs): Involved in regulating gene expression.</li> </ul>"},{"location":"concepts/biology/genomic_features/#4-epigenetic-marks","title":"4. Epigenetic Marks","text":"<p>Epigenetics refers to modifications to DNA and its associated proteins that change gene expression without altering the DNA sequence itself. These marks are crucial for cell differentiation and response to the environment.</p> <ul> <li> <p>DNA Methylation: The addition of a methyl group to a cytosine base (often at CpG sites). High methylation in a gene's promoter region is typically associated with gene silencing.</p> </li> <li> <p>Histone Modifications: DNA in eukaryotes is wrapped around proteins called histones. Chemical modifications to these histones (e.g., acetylation, methylation) can make the DNA more accessible (active) or less accessible (inactive) for transcription.</p> </li> </ul>"},{"location":"concepts/biology/genomic_features/#5-other-structural-features","title":"5. Other Structural Features","text":"<ul> <li>Transposable Elements (TEs): \"Jumping genes\" or sequences that can change their position within the genome.</li> <li>Telomeres: Repetitive sequences at the ends of chromosomes that protect them from degradation.</li> <li>Centromeres: Regions of a chromosome that are essential for cell division.</li> </ul> <p>Understanding these features is the goal of many DNALLM applications. By training on sequences labeled with these features, the models can learn to identify them in new, unannotated DNA.</p> <p>Next: See how knowledge of these features is used in Common Biological Tasks.</p>"},{"location":"concepts/biology/sequence_analysis/","title":"Sequence Analysis Methods","text":"<p>After training a model and making predictions, the next crucial step is to analyze the results to gain biological insights. DNALLM provides tools to facilitate several key types of sequence analysis.</p>"},{"location":"concepts/biology/sequence_analysis/#1-functional-analysis","title":"1. Functional Analysis","text":"<p>Functional analysis aims to understand what a DNA sequence does. This is often the direct output of classification or regression tasks.</p> <ul> <li>What it is: Assigning a functional label (e.g., \"promoter,\" \"enhancer\") or a quantitative value (e.g., expression level) to a sequence.</li> <li>How it's done in DNALLM: This is the primary goal of the <code>DNAInference</code> and <code>Benchmark</code> classes. You provide a sequence, and the model predicts its function based on what it learned during training.</li> </ul> <p>Example: <pre><code># Using DNAInference to predict promoter strength (a regression task)\ninference_result = inference_engine.infer(sequence=\"GATTACA...\")\nprint(f\"Predicted Promoter Strength: {inference_result[0]['score']}\")\n</code></pre></p>"},{"location":"concepts/biology/sequence_analysis/#2-key-site-identification-in-silico-mutagenesis","title":"2. Key Site Identification (In Silico Mutagenesis)","text":"<p>This is one of the most powerful analysis methods. It helps identify which specific nucleotides within a sequence are most critical for its function.</p> <ul> <li>What it is: Systematically mutating each position in a sequence and measuring the impact of that mutation on the model's prediction. A large change in the prediction score indicates a functionally important site. This is a computational proxy for saturation mutagenesis experiments.</li> <li>How it's done in DNALLM: The <code>dnallm.Mutagenesis</code> class is designed specifically for this purpose. It automates the process of creating mutations, running inference, and calculating the effect of each mutation.</li> </ul> <p>Example: <pre><code>from dnallm import Mutagenesis\n\n# Initialize mutagenesis analyzer\nmutagenesis = Mutagenesis(config=configs, model=model, tokenizer=tokenizer)\n\n# Generate mutations and evaluate their effects\nmutagenesis.mutate_sequence(sequence, replace_mut=True)\npredictions = mutagenesis.evaluate(strategy=\"mean\")\n\n# The 'predictions' dictionary now contains the effect of every single-base\n# substitution on the model's output.\n</code></pre></p> <p>The results can be visualized as a heatmap, clearly showing \"hotspots\" of functional importance.</p> <p>!Mutation Heatmap An example mutation effect plot generated by <code>mutagenesis.plot()</code>.</p>"},{"location":"concepts/biology/sequence_analysis/#3-model-interpretability-analysis","title":"3. Model Interpretability Analysis","text":"<p>This analysis focuses on understanding how the model makes its decisions, rather than just what the decision is.</p>"},{"location":"concepts/biology/sequence_analysis/#attention-visualization","title":"Attention Visualization","text":"<ul> <li>What it is: For Transformer-based models, attention mechanisms weigh the importance of different tokens when making a prediction for a specific token. Visualizing these attention weights can show which parts of a sequence the model \"focuses on.\"</li> <li>How it's done in DNALLM: The <code>DNAInference.plot_attentions()</code> method can be used to generate heatmaps of attention scores between tokens.</li> </ul>"},{"location":"concepts/biology/sequence_analysis/#embedding-visualization","title":"Embedding Visualization","text":"<ul> <li>What it is: A language model converts a sequence into a high-dimensional numerical vector called an embedding. By using dimensionality reduction techniques (like t-SNE or PCA), we can visualize these embeddings in 2D or 3D. This can reveal whether the model has learned to group sequences with similar functions together in the embedding space.</li> <li>How it's done in DNALLM: The <code>DNAInference.plot_hidden_states()</code> method generates these visualizations.</li> </ul> <p>These analysis methods, supported by DNALLM, allow researchers to move beyond simple predictions and gain deeper, more mechanistic insights into the function of DNA sequences and the behavior of the models themselves.</p>"},{"location":"concepts/technical/evaluation_metrics/","title":"Evaluating Model Performance","text":"<p>Once you have fine-tuned a model, how do you know if it's any good? Evaluation metrics are quantitative measures used to assess the performance of a model on a given task. Choosing the right metric is just as important as choosing the right model architecture.</p> <p>DNALLM's <code>Benchmark</code> and <code>DNATrainer</code> classes automatically calculate a suite of relevant metrics based on your <code>task_type</code>.</p>"},{"location":"concepts/technical/evaluation_metrics/#1-metrics-for-classification-tasks","title":"1. Metrics for Classification Tasks","text":"<p>Classification metrics are derived from the confusion matrix, which tabulates the number of correct and incorrect predictions for each class.</p> <p> Image Credit: image by Amit Chauhan from Medium</p> <ul> <li>True Positives (TP): Correctly predicted positive samples.</li> <li>True Negatives (TN): Correctly predicted negative samples.</li> <li>False Positives (FP): Incorrectly predicted as positive (Type I Error).</li> <li>False Negatives (FN): Incorrectly predicted as negative (Type II Error).</li> </ul>"},{"location":"concepts/technical/evaluation_metrics/#common-classification-metrics","title":"Common Classification Metrics","text":"<ul> <li> <p>Accuracy: <code>(TP + TN) / (TP + TN + FP + FN)</code></p> <ul> <li>What it is: The overall percentage of correct predictions.</li> <li>When to use: Good for balanced datasets.</li> <li>Caveat: Can be very misleading on imbalanced datasets. A model that predicts \"not a promoter\" 99% of the time on a dataset with 1% promoters will have 99% accuracy but is useless.</li> </ul> </li> <li> <p>Precision: <code>TP / (TP + FP)</code></p> <ul> <li>What it is: Of all the samples the model predicted as positive, what fraction were actually positive?</li> <li>When to use: When the cost of a False Positive is high. (e.g., you want to be very sure that a predicted binding site is real before running an expensive experiment).</li> </ul> </li> <li> <p>Recall (Sensitivity): <code>TP / (TP + FN)</code></p> <ul> <li>What it is: Of all the actual positive samples, what fraction did the model correctly identify?</li> <li>When to use: When the cost of a False Negative is high. (e.g., in disease screening, you want to find all possible cases, even if it means some false alarms).</li> </ul> </li> <li> <p>F1-Score: <code>2 * (Precision * Recall) / (Precision + Recall)</code></p> <ul> <li>What it is: The harmonic mean of Precision and Recall.</li> <li>When to use: A great general-purpose metric for imbalanced datasets, as it requires a balance between Precision and Recall.</li> </ul> </li> <li> <p>MCC (Matthews Correlation Coefficient): A correlation coefficient between the observed and predicted classifications. Ranges from -1 to +1.</p> <ul> <li>When to use: Considered one of the most robust metrics for imbalanced binary classification.</li> </ul> </li> <li> <p>AUROC (Area Under the Receiver Operating Characteristic Curve):</p> <ul> <li>What it is: The ROC curve plots the True Positive Rate (Recall) against the False Positive Rate at various threshold settings. The area under this curve (AUC) represents the model's ability to distinguish between classes. An AUC of 1.0 is perfect; 0.5 is random.</li> <li>When to use: Excellent for evaluating a model's discriminative power across all possible thresholds.</li> </ul> </li> <li> <p>AUPRC (Area Under the Precision-Recall Curve):</p> <ul> <li>What it is: Similar to AUROC, but plots Precision vs. Recall.</li> <li>When to use: More informative than AUROC for severely imbalanced datasets.</li> </ul> </li> </ul>"},{"location":"concepts/technical/evaluation_metrics/#2-metrics-for-regression-tasks","title":"2. Metrics for Regression Tasks","text":"<p>Regression tasks involve predicting a continuous value.</p> <ul> <li>Mean Squared Error (MSE): The average of the squared differences between the predicted and actual values. Penalizes large errors heavily.</li> <li>Mean Absolute Error (MAE): The average of the absolute differences between the predicted and actual values. More robust to outliers than MSE.</li> <li>R-squared (R\u00b2): The proportion of the variance in the dependent variable that is predictable from the independent variable(s). A value of 1.0 means the model perfectly predicts the values.</li> </ul>"},{"location":"concepts/technical/evaluation_metrics/#3-choosing-the-right-metric-in-dnallm","title":"3. Choosing the Right Metric in DNALLM","text":"<p>In your DNALLM configuration files (for training or benchmarking), you can specify which metrics to compute. The framework will automatically select the appropriate calculation based on the <code>task_type</code>.</p> <p>Recommendation: For a new classification problem, always look at F1-Score, AUROC, and AUPRC, especially if your data might be imbalanced. Don't rely on accuracy alone.</p> <p>Next: Learn how to make your training and inference faster and more efficient with Performance Optimization.</p>"},{"location":"concepts/technical/fine_tuning_strategies/","title":"Fine-tuning Strategies and Best Practices","text":"<p>Fine-tuning is the process of adapting a pre-trained foundation model to a specific downstream task. While powerful, the success of fine-tuning depends heavily on the right strategies for data preparation, hyperparameter selection, and training approach.</p>"},{"location":"concepts/technical/fine_tuning_strategies/#1-data-selection-and-preprocessing","title":"1. Data Selection and Preprocessing","text":"<p>The quality of your fine-tuning data is the single most important factor for success.</p>"},{"location":"concepts/technical/fine_tuning_strategies/#data-quality-over-quantity","title":"Data Quality over Quantity","text":"<ul> <li>Accurate Labels: Ensure your labels are as accurate as possible. A smaller, high-quality dataset will produce a better model than a large, noisy one.</li> <li>Remove Duplicates and Contaminants: Clean your dataset to remove duplicate sequences or any sequences that don't belong (e.g., adapter sequences, poor quality reads).</li> </ul>"},{"location":"concepts/technical/fine_tuning_strategies/#data-balancing","title":"Data Balancing","text":"<ul> <li>The Problem: If your dataset is highly imbalanced (e.g., 99% negative examples and 1% positive examples), the model may learn to simply predict the majority class.</li> <li>Strategies:<ul> <li>Downsampling: Randomly remove samples from the majority class. This is simple but discards data.</li> <li>Upsampling: Duplicate samples from the minority class. This can lead to overfitting on the minority class.</li> <li>Weighted Loss: A better approach, supported by DNALLM. You can configure the trainer to give more weight to errors on the minority class, forcing the model to pay more attention to it.</li> </ul> </li> </ul>"},{"location":"concepts/technical/fine_tuning_strategies/#data-splitting","title":"Data Splitting","text":"<ul> <li>The Rule: Always split your data into three sets: training, validation, and testing.<ul> <li>Training Set: Used to update the model's weights.</li> <li>Validation Set: Used during training to monitor performance on unseen data, tune hyperparameters, and decide when to stop training (early stopping).</li> <li>Test Set: Held out until the very end. It is used only once to get a final, unbiased estimate of the model's performance.</li> </ul> </li> </ul>"},{"location":"concepts/technical/fine_tuning_strategies/#2-hyperparameter-tuning","title":"2. Hyperparameter Tuning","text":"<p>Hyperparameters are the \"dials\" you can turn to control the training process. Finding the right combination is key.</p> <ul> <li> <p>Learning Rate (<code>learning_rate</code>): The most critical hyperparameter. It controls how much the model's weights are updated in each step.</p> <ul> <li>Too high: The model may diverge and fail to learn.</li> <li>Too low: Training will be very slow and may get stuck in a suboptimal solution.</li> <li>Recommendation: Start with a small value typical for fine-tuning, such as <code>1e-5</code> or <code>2e-5</code>. Use a learning rate scheduler (<code>lr_scheduler_type</code>) like <code>cosine</code> or <code>linear</code> to gradually decrease the learning rate during training.</li> </ul> </li> <li> <p>Batch Size (<code>per_device_train_batch_size</code>): The number of samples processed in each training step.</p> <ul> <li>Constraint: Limited by your GPU memory. Find the largest size that doesn't cause an out-of-memory error.</li> <li>Effect: Larger batch sizes can lead to more stable training but may generalize slightly worse. Common values are 8, 16, 32, or 64.</li> </ul> </li> <li> <p>Number of Epochs (<code>num_train_epochs</code>): An epoch is one full pass through the training dataset.</p> <ul> <li>Too few: The model will be underfit.</li> <li>Too many: The model will overfit to the training data.</li> <li>Recommendation: Use early stopping. Monitor performance on the validation set and stop training when the validation metric (e.g., <code>eval_loss</code> or <code>eval_accuracy</code>) stops improving. DNALLM's <code>DNATrainer</code> handles this automatically.</li> </ul> </li> <li> <p>Weight Decay (<code>weight_decay</code>): A regularization technique that helps prevent overfitting by penalizing large weights. A common value is <code>0.01</code>.</p> </li> </ul>"},{"location":"concepts/technical/fine_tuning_strategies/#3-fine-tuning-strategies","title":"3. Fine-tuning Strategies","text":""},{"location":"concepts/technical/fine_tuning_strategies/#full-fine-tuning","title":"Full Fine-tuning","text":"<ul> <li>What it is: All the weights of the pre-trained model are updated during training.</li> <li>Pros: Can achieve the highest possible performance.</li> <li>Cons: Requires the most memory and computational resources. Can be prone to \"catastrophic forgetting,\" where the model loses some of its general pre-trained knowledge.</li> </ul>"},{"location":"concepts/technical/fine_tuning_strategies/#parameter-efficient-fine-tuning-peft","title":"Parameter-Efficient Fine-Tuning (PEFT)","text":"<p>PEFT methods freeze most of the pre-trained model's weights and only train a small number of new parameters. This is much more efficient.</p> <ul> <li>LoRA (Low-Rank Adaptation): The most popular PEFT method, fully supported in DNALLM.<ul> <li>How it works: LoRA injects small, trainable \"adapter\" matrices into the layers of the Transformer. Only these small matrices are trained, representing the \"update\" to the original weights.</li> <li>Pros:<ul> <li>Drastically reduces memory: Allows you to fine-tune very large models on consumer GPUs.</li> <li>Faster training: Fewer parameters to update.</li> <li>Portable: The trained LoRA adapters are tiny (a few megabytes), making it easy to store and share many different \"specialized\" versions of a single base model.</li> </ul> </li> <li>Configuration: In DNALLM, you can enable LoRA in your training configuration file. See the Advanced Fine-tuning Techniques tutorial.</li> </ul> </li> </ul> <p>Recommendation: For most applications, starting with LoRA is highly recommended due to its efficiency and strong performance.</p> <p>Next: Learn how to measure the success of your fine-tuning with Evaluation Metrics.</p>"},{"location":"concepts/technical/performance_optimization/","title":"Training and Inference Optimization","text":"<p>Optimizing the performance of your deep learning pipeline is crucial for iterating faster and making efficient use of computational resources. This guide covers key techniques for optimizing both training and inference in DNALLM.</p>"},{"location":"concepts/technical/performance_optimization/#1-hardware-acceleration","title":"1. Hardware Acceleration","text":""},{"location":"concepts/technical/performance_optimization/#use-a-gpu","title":"Use a GPU","text":"<p>This is the single most important factor. Training and inference for large models are orders of magnitude faster on a GPU than on a CPU. Ensure your environment is configured to use an available NVIDIA GPU (<code>device: cuda</code>) or Apple Silicon GPU (<code>device: mps</code>).</p>"},{"location":"concepts/technical/performance_optimization/#mixed-precision-traininginference","title":"Mixed-Precision Training/Inference","text":"<ul> <li>What it is: By default, models use 32-bit floating-point numbers (FP32). Mixed precision uses a combination of 16-bit floats (FP16 or BF16) and FP32. This can provide significant speedups and reduce memory usage by nearly half.</li> <li>How it works: Operations are performed in the faster, less precise FP16 format, while critical components like weight updates can be kept in FP32 to maintain stability.</li> <li>How to use in DNALLM: In your configuration file, set <code>use_fp16: true</code> or <code>use_bf16: true</code>.<ul> <li><code>fp16</code>: Widely supported, great for speed.</li> <li><code>bf16</code>: More numerically stable than FP16, but requires newer GPUs (NVIDIA Ampere or later).</li> </ul> </li> </ul>"},{"location":"concepts/technical/performance_optimization/#2-parameter-efficient-fine-tuning-peft","title":"2. Parameter-Efficient Fine-Tuning (PEFT)","text":"<p>As discussed in Fine-tuning Strategies, using PEFT methods like LoRA is a major optimization.</p> <ul> <li>Benefit: Instead of training billions of parameters, you might only train a few million. This drastically cuts down on:<ul> <li>GPU Memory Required: Allowing you to fine-tune larger models on smaller GPUs.</li> <li>Training Time: Fewer parameters mean faster updates.</li> <li>Storage Space: The resulting LoRA adapter is only a few megabytes, versus gigabytes for a fully fine-tuned model.</li> </ul> </li> </ul>"},{"location":"concepts/technical/performance_optimization/#3-efficient-data-loading","title":"3. Efficient Data Loading","text":"<p>The process of loading and preparing data can become a bottleneck, leaving your expensive GPU idle.</p> <ul> <li><code>num_workers</code>: This parameter in the configuration specifies how many parallel CPU processes to use for data loading.<ul> <li>Recommendation: Set this to a value greater than 0. A good starting point is half the number of your CPU cores. Increase it if you notice your GPU utilization is low.</li> </ul> </li> <li><code>pin_memory</code>: Setting this to <code>true</code> can speed up data transfer from the CPU to the GPU by pinning the data in a special memory region.</li> </ul>"},{"location":"concepts/technical/performance_optimization/#4-batch-size-and-gradient-accumulation","title":"4. Batch Size and Gradient Accumulation","text":""},{"location":"concepts/technical/performance_optimization/#batch-size","title":"Batch Size","text":"<ul> <li>The Goal: Use the largest <code>batch_size</code> that fits in your GPU's memory. Larger batches allow the GPU to perform computations more efficiently.</li> <li>How to find it: Start with a small batch size (e.g., 4 or 8) and double it until you get a \"CUDA out of memory\" error, then back off.</li> </ul>"},{"location":"concepts/technical/performance_optimization/#gradient-accumulation","title":"Gradient Accumulation","text":"<ul> <li>The Problem: What if the optimal batch size for model performance (e.g., 64) doesn't fit in your GPU memory, and you can only fit a batch size of 8?</li> <li>The Solution: Use <code>gradient_accumulation_steps</code>.<ul> <li>How it works: The trainer will process a small batch (e.g., size 8), calculate the gradients, but not update the model weights. It will repeat this for a specified number of steps (e.g., 8 steps), accumulating the gradients along the way. After 8 steps, it will sum the accumulated gradients and perform a single weight update.</li> <li>The Effect: This simulates a larger effective batch size. In our example, <code>batch_size=8</code> and <code>gradient_accumulation_steps=8</code> is equivalent to a single step with a <code>batch_size=64</code>.</li> <li>How to use: Set <code>gradient_accumulation_steps</code> in your training configuration.</li> </ul> </li> </ul>"},{"location":"concepts/technical/performance_optimization/#5-optimized-attention-mechanisms","title":"5. Optimized Attention Mechanisms","text":"<ul> <li>Flash Attention: As detailed in the Attention Mechanisms guide, Flash Attention is a highly optimized implementation that provides significant speed and memory improvements.</li> <li>How to use: DNALLM uses it automatically if it's installed and compatible. Ensure you have installed the extra dependencies for it. See the Installation Guide.</li> </ul>"},{"location":"concepts/technical/performance_optimization/#6-model-compilation","title":"6. Model Compilation","text":"<ul> <li><code>torch.compile</code>: For PyTorch 2.0 and later, you can use <code>torch.compile</code> to get a significant speedup. It uses a JIT (Just-In-Time) compiler to optimize the model's execution graph.</li> <li>How to use in DNALLM: In your training configuration, you can enable this feature (support may vary by model and version).</li> </ul> <p>By combining these techniques, you can dramatically reduce training times and inference latency, making your research and development cycles much more efficient.</p>"},{"location":"concepts/technical/transfer_learning/","title":"Transfer Learning in Genomics","text":"<p>Transfer learning is a machine learning paradigm that has become the cornerstone of modern Large Language Models (LLMs), including those used in DNALLM. The core idea is to leverage knowledge gained from solving one problem and apply it to a different but related problem.</p>"},{"location":"concepts/technical/transfer_learning/#1-what-is-transfer-learning","title":"1. What is Transfer Learning?","text":"<p>In traditional machine learning, models are trained from scratch for each specific task. This requires a large, task-specific labeled dataset and significant computational resources.</p> <p>Transfer learning revolutionizes this process. It involves two main stages:</p> <ol> <li> <p>Pre-training: A large, general-purpose model (a \"foundation model\") is trained on a massive, broad dataset. The goal of this stage is not to solve a specific task, but to learn general patterns, features, and representations of the data. For LLMs, this means learning grammar, syntax, and semantic relationships.</p> </li> <li> <p>Fine-tuning: The pre-trained model is then adapted to a specific, downstream task using a much smaller, task-specific labeled dataset. Instead of learning from scratch, the model \"transfers\" its pre-trained knowledge and refines it for the new task.</p> </li> </ol> <p> Image Credit: Mad Devs</p>"},{"location":"concepts/technical/transfer_learning/#2-transfer-learning-and-dna-language-models","title":"2. Transfer Learning and DNA Language Models","text":"<p>This two-stage process is perfectly suited for genomics. The \"language\" of DNA is universal, but its \"dialects\" (i.e., the functions of specific sequences) are diverse.</p>"},{"location":"concepts/technical/transfer_learning/#pre-training-in-dnallm","title":"Pre-training in DNALLM","text":"<ul> <li>The Data: A DNA foundation model like <code>DNABERT</code> or <code>HyenaDNA</code> is pre-trained on a vast corpus of genomic data, often spanning multiple species (e.g., the entire human reference genome).</li> <li>The Goal: During this phase, the model learns the fundamental \"grammar\" of DNA. It learns to recognize common motifs, understand codon structures, and capture long-range dependencies between different parts of a sequence, all without any specific functional labels. This is typically done through self-supervised objectives like Masked Language Modeling (MLM) or Causal Language Modeling (CLM).</li> </ul>"},{"location":"concepts/technical/transfer_learning/#fine-tuning-in-dnallm","title":"Fine-tuning in DNALLM","text":"<ul> <li>The Data: You provide a smaller, labeled dataset for a specific biological problem. For example, a CSV file with 5,000 sequences labeled as \"promoter\" or \"not promoter.\"</li> <li>The Goal: The <code>DNATrainer</code> takes the powerful, pre-trained foundation model and slightly adjusts its weights to specialize it for your task. Because the model already has a deep understanding of DNA structure, it can learn to classify promoters with much less data and in far less time than a model trained from scratch.</li> </ul>"},{"location":"concepts/technical/transfer_learning/#3-why-is-transfer-learning-so-effective","title":"3. Why is Transfer Learning so Effective?","text":"<ul> <li>Reduced Data Requirement: Fine-tuning requires significantly less labeled data, which is often expensive and time-consuming to acquire in biology.</li> <li>Faster Training: Since the model starts with a strong baseline of knowledge, the fine-tuning process converges much faster than training from zero.</li> <li>Improved Performance: Foundation models are pre-trained on datasets far larger and more diverse than any single task-specific dataset. This provides a powerful inductive bias, often leading to higher accuracy and better generalization on the downstream task.</li> </ul> <p>By using the models in the Model Zoo, you are directly benefiting from the power of transfer learning. Each model is a repository of generalized biological knowledge, ready to be specialized for your unique research question.</p> <p>Next: Learn the best practices for adapting these models in Fine-tuning Strategies.</p>"},{"location":"example/marimo/benchmark/benchmark_demo/","title":"Benchmark Demo","text":"In\u00a0[\u00a0]: Copied! <pre>import marimo\n</pre> import marimo In\u00a0[\u00a0]: Copied! <pre>__generated_with = \"0.11.17\"\napp = marimo.App(width=\"medium\")\n</pre> __generated_with = \"0.11.17\" app = marimo.App(width=\"medium\") In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(__file__):\n    import sys\n    # from os import path\n    # sys.path.append(path.abspath(path.join(path.dirname(__file__), '../../..')))\n    import marimo as mo\n    import pandas as pd\n    from dnallm import load_config, load_model_and_tokenizer, Benchmark\n\n    return sys, pd, mo, load_config, load_model_and_tokenizer, Benchmark\n</pre> @app.cell def __(__file__):     import sys     # from os import path     # sys.path.append(path.abspath(path.join(path.dirname(__file__), '../../..')))     import marimo as mo     import pandas as pd     from dnallm import load_config, load_model_and_tokenizer, Benchmark      return sys, pd, mo, load_config, load_model_and_tokenizer, Benchmark In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo):\n    title = mo.md(\n        \"&lt;center&gt;&lt;h2&gt;Benchmark of multiple DNA models&lt;/h2&gt;&lt;/center&gt;\"\n    )\n    config_text = mo.ui.text(value=\"config.yaml\", placeholder=\"config.yaml\",\n                             label=\"Config file (*.yaml)\", full_width=True)\n    datasets_text = mo.ui.text(value=\"test.csv\", placeholder=\"local dataset path\",\n                               label=\"Datasets file\", full_width=True)\n    source_text = mo.ui.dropdown(['local', 'huggingface', 'modelscope'], value=\"modelscope\",\n                                 label=\"Model source\", full_width=True)\n    number_text = mo.ui.dropdown(list(map(str, range(2, 13))), value=\"4\", label=\"Number of models\", full_width=True)\n    seq_col_text = mo.ui.text(value=\"sequence\", placeholder=\"sequence\", label=\"Sequence column name\", full_width=True)\n    label_col_text = mo.ui.text(value=\"labels\", placeholder=\"labels\", label=\"Label column name\", full_width=True)\n    input_stack = mo.hstack([config_text.style(width=\"35ch\"), datasets_text.style(width=\"55ch\")],\n                            align='center', justify='center')\n    option_stack = mo.hstack([seq_col_text.style(width=\"22.5ch\"), label_col_text.style(width=\"22.5ch\"),\n                              source_text.style(width=\"22ch\"), number_text.style(width=\"22ch\")],\n                             align='center', justify='center')\n    mo.vstack([title, input_stack, option_stack], align='center', justify='center')\n    return (config_text, datasets_text, source_text, seq_col_text, label_col_text,)\n</pre> @app.cell def __(mo):     title = mo.md(         \"Benchmark of multiple DNA models\"     )     config_text = mo.ui.text(value=\"config.yaml\", placeholder=\"config.yaml\",                              label=\"Config file (*.yaml)\", full_width=True)     datasets_text = mo.ui.text(value=\"test.csv\", placeholder=\"local dataset path\",                                label=\"Datasets file\", full_width=True)     source_text = mo.ui.dropdown(['local', 'huggingface', 'modelscope'], value=\"modelscope\",                                  label=\"Model source\", full_width=True)     number_text = mo.ui.dropdown(list(map(str, range(2, 13))), value=\"4\", label=\"Number of models\", full_width=True)     seq_col_text = mo.ui.text(value=\"sequence\", placeholder=\"sequence\", label=\"Sequence column name\", full_width=True)     label_col_text = mo.ui.text(value=\"labels\", placeholder=\"labels\", label=\"Label column name\", full_width=True)     input_stack = mo.hstack([config_text.style(width=\"35ch\"), datasets_text.style(width=\"55ch\")],                             align='center', justify='center')     option_stack = mo.hstack([seq_col_text.style(width=\"22.5ch\"), label_col_text.style(width=\"22.5ch\"),                               source_text.style(width=\"22ch\"), number_text.style(width=\"22ch\")],                              align='center', justify='center')     mo.vstack([title, input_stack, option_stack], align='center', justify='center')     return (config_text, datasets_text, source_text, seq_col_text, label_col_text,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, number_text):\n    model_texts = {}\n    name_texts = {}\n    # model_stacks = {}\n    number_of_models = int(number_text.value)\n    default_models = [[\"Plant DNABERT\", \"zhangtaolab/plant-dnabert-BPE-promoter\"],\n                      [\"Plant DNAGPT\", \"zhangtaolab/plant-dnagpt-BPE-promoter\"]] + [[\"\", \"\"]] * 10\n    model_texts = mo.ui.dictionary({\n        i: mo.ui.text(value=default_models[i][1], placeholder=default_models[i][1],\n                      label=f\"Model{i+1}\", full_width=True)\n        for i in range(number_of_models)\n    })\n    name_texts = mo.ui.dictionary({\n        i: mo.ui.text(value=default_models[i][0], placeholder=default_models[i][0],\n                      label=f\"Model{i+1} name\", full_width=True)\n        for i in range(number_of_models)\n    })\n    # model_stacks = mo.ui.dictionary({\n    #     i: mo.hstack([model_texts.value[i].style(width=\"60ch\"), name_texts.value[i].style(width=\"30ch\")],\n    #                  align='start', justify='center')\n    #     for i in range(number_of_models)\n    # })\n    # for i in range(int(number_of_models)):\n    #     if i == 0:\n    #         value1 = \"Plant DNABERT\"\n    #         value2 = \"zhangtaolab/plant-dnabert-BPE-promoter\"\n    #     elif i == 1:\n    #         value1 = \"Plant DNAGPT\"\n    #         value2 = \"zhangtaolab/plant-dnagpt-BPE-promoter\"\n    #     else:\n    #         value1 = \"\"\n    #         value2 = \"\"\n    #     model_texts[i] = mo.ui.text(value=value2, placeholder=\"zhangtaolab/plant-dnagpt-BPE\",\n    #                                 label=f\"Model{i+1}\", full_width=True)\n    #     name_texts[i] = mo.ui.text(value=value1, placeholder=\"Plant DNAGPT\",\n    #                                label=f\"Model{i+1} name\", full_width=True)\n    #     model_stacks[i] = mo.hstack([model_texts[i].style(width=\"60ch\"), name_texts[i].style(width=\"30ch\")],\n    #                                 align='start', justify='center')\n    # mo.vstack([model_stacks.value[i] for i in range(int(number_of_models))],\n    #           align='center', justify='center')\n    mo.hstack([model_texts.vstack(align='stretch', gap=0.5),\n               name_texts.vstack(align='stretch', gap=0.5)],\n              widths=[2, 1], align='stretch', gap=0.5)\n    return (number_of_models, model_texts, name_texts,)\n</pre> @app.cell def __(mo, number_text):     model_texts = {}     name_texts = {}     # model_stacks = {}     number_of_models = int(number_text.value)     default_models = [[\"Plant DNABERT\", \"zhangtaolab/plant-dnabert-BPE-promoter\"],                       [\"Plant DNAGPT\", \"zhangtaolab/plant-dnagpt-BPE-promoter\"]] + [[\"\", \"\"]] * 10     model_texts = mo.ui.dictionary({         i: mo.ui.text(value=default_models[i][1], placeholder=default_models[i][1],                       label=f\"Model{i+1}\", full_width=True)         for i in range(number_of_models)     })     name_texts = mo.ui.dictionary({         i: mo.ui.text(value=default_models[i][0], placeholder=default_models[i][0],                       label=f\"Model{i+1} name\", full_width=True)         for i in range(number_of_models)     })     # model_stacks = mo.ui.dictionary({     #     i: mo.hstack([model_texts.value[i].style(width=\"60ch\"), name_texts.value[i].style(width=\"30ch\")],     #                  align='start', justify='center')     #     for i in range(number_of_models)     # })     # for i in range(int(number_of_models)):     #     if i == 0:     #         value1 = \"Plant DNABERT\"     #         value2 = \"zhangtaolab/plant-dnabert-BPE-promoter\"     #     elif i == 1:     #         value1 = \"Plant DNAGPT\"     #         value2 = \"zhangtaolab/plant-dnagpt-BPE-promoter\"     #     else:     #         value1 = \"\"     #         value2 = \"\"     #     model_texts[i] = mo.ui.text(value=value2, placeholder=\"zhangtaolab/plant-dnagpt-BPE\",     #                                 label=f\"Model{i+1}\", full_width=True)     #     name_texts[i] = mo.ui.text(value=value1, placeholder=\"Plant DNAGPT\",     #                                label=f\"Model{i+1} name\", full_width=True)     #     model_stacks[i] = mo.hstack([model_texts[i].style(width=\"60ch\"), name_texts[i].style(width=\"30ch\")],     #                                 align='start', justify='center')     # mo.vstack([model_stacks.value[i] for i in range(int(number_of_models))],     #           align='center', justify='center')     mo.hstack([model_texts.vstack(align='stretch', gap=0.5),                name_texts.vstack(align='stretch', gap=0.5)],               widths=[2, 1], align='stretch', gap=0.5)     return (number_of_models, model_texts, name_texts,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(config_text, load_config):\n    configs = load_config(config_text.value)\n    return configs\n</pre> @app.cell def __(config_text, load_config):     configs = load_config(config_text.value)     return configs In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(configs, datasets_text, seq_col_text, label_col_text, \n       Benchmark):\n    benchmark = Benchmark(config=configs)\n    if datasets_text.value:\n        # Load the dataset\n        dataset = benchmark.get_dataset(datasets_text.value,\n                                        seq_col=seq_col_text.value,\n                                        label_col=label_col_text.value)\n    else:\n        dataset = None\n    return (dataset, benchmark)\n</pre> @app.cell def __(configs, datasets_text, seq_col_text, label_col_text,         Benchmark):     benchmark = Benchmark(config=configs)     if datasets_text.value:         # Load the dataset         dataset = benchmark.get_dataset(datasets_text.value,                                         seq_col=seq_col_text.value,                                         label_col=label_col_text.value)     else:         dataset = None     return (dataset, benchmark) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(model_texts, name_texts):\n    model_names = {\n        name_texts.value[i]: model_texts.value[i]\n        for i in range(len(model_texts.value))\n        if (model_texts.value[i] and name_texts.value[i])\n    }\n    return (model_names, )\n</pre> @app.cell def __(model_texts, name_texts):     model_names = {         name_texts.value[i]: model_texts.value[i]         for i in range(len(model_texts.value))         if (model_texts.value[i] and name_texts.value[i])     }     return (model_names, ) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, model_names, source_text, benchmark):\n    # Benchmark the models\n    predict_button = mo.ui.button(label=\"Start Benchmark\",\n                                    on_click=lambda value: benchmark.run(\n                                        model_names, source=source_text.value)\n                                    )\n    mo.hstack([predict_button], align='center', justify='center')\n    return (predict_button, )\n</pre> @app.cell def __(mo, model_names, source_text, benchmark):     # Benchmark the models     predict_button = mo.ui.button(label=\"Start Benchmark\",                                     on_click=lambda value: benchmark.run(                                         model_names, source=source_text.value)                                     )     mo.hstack([predict_button], align='center', justify='center')     return (predict_button, ) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(predict_button):\n    if predict_button.value:\n        results = predict_button.value\n    else:\n        results = None\n    results\n    return (results, )\n</pre> @app.cell def __(predict_button):     if predict_button.value:         results = predict_button.value     else:         results = None     results     return (results, ) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, ):\n    figure_size = mo.ui.number(start=200, stop=5120, step=10, label='Figure size',\n                            value = 800)\n    return (figure_size, )\n</pre> @app.cell def __(mo, ):     figure_size = mo.ui.number(start=200, stop=5120, step=10, label='Figure size',                             value = 800)     return (figure_size, ) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, figure_size, results, benchmark):\n    plot_button = mo.ui.button(label=\"Plot metrics\",\n                            on_click=lambda value: benchmark.plot(results, separate=True)\n                            )\n    mo.hstack([figure_size, plot_button], align='center', justify='center')\n    return (plot_button,)\n</pre> @app.cell def __(mo, figure_size, results, benchmark):     plot_button = mo.ui.button(label=\"Plot metrics\",                             on_click=lambda value: benchmark.plot(results, separate=True)                             )     mo.hstack([figure_size, plot_button], align='center', justify='center')     return (plot_button,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, model_names, plot_button, figure_size):\n    plot_out = plot_button.value\n    if plot_out:\n        num_models = len(model_names)\n        charts1 = mo.ui.tabs(\n                {\n                    metric: mo.ui.altair_chart(plot_out[0][metric]).properties(\n                        width=figure_size.value, height=figure_size.value * num_models / 10\n                        ) for metric in plot_out[0]\n                }, \n            )\n        charts2 = mo.ui.tabs(\n                {\n                    name: mo.ui.altair_chart(plot_out[1][name]).properties(\n                        width=figure_size.value, height=figure_size.value\n                        ) for name in plot_out[1]\n                }\n            )\n    else:\n        charts1 = \"\"\n        charts2 = \"\"\n    mo.vstack([charts1, charts2], align='center', justify='center')\n    return\n</pre> @app.cell def __(mo, model_names, plot_button, figure_size):     plot_out = plot_button.value     if plot_out:         num_models = len(model_names)         charts1 = mo.ui.tabs(                 {                     metric: mo.ui.altair_chart(plot_out[0][metric]).properties(                         width=figure_size.value, height=figure_size.value * num_models / 10                         ) for metric in plot_out[0]                 },              )         charts2 = mo.ui.tabs(                 {                     name: mo.ui.altair_chart(plot_out[1][name]).properties(                         width=figure_size.value, height=figure_size.value                         ) for name in plot_out[1]                 }             )     else:         charts1 = \"\"         charts2 = \"\"     mo.vstack([charts1, charts2], align='center', justify='center')     return In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    app.run()\n</pre> if __name__ == \"__main__\":     app.run()"},{"location":"example/marimo/finetune/finetune_demo/","title":"Fine-tuning Demo","text":"In\u00a0[\u00a0]: Copied! <pre>import marimo\n</pre> import marimo In\u00a0[\u00a0]: Copied! <pre>__generated_with = \"0.11.17\"\napp = marimo.App(width=\"medium\")\n</pre> __generated_with = \"0.11.17\" app = marimo.App(width=\"medium\") In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(__file__):\n    import sys\n    from os import path\n    sys.path.append(path.abspath(path.join(path.dirname(__file__), '../../..')))\n    import marimo as mo\n    import pandas as pd\n    from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n\n    return sys, pd, mo, load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n</pre> @app.cell def __(__file__):     import sys     from os import path     sys.path.append(path.abspath(path.join(path.dirname(__file__), '../../..')))     import marimo as mo     import pandas as pd     from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer      return sys, pd, mo, load_config, load_model_and_tokenizer, DNADataset, DNATrainer In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo):\n    title = mo.md(\n        \"&lt;center&gt;&lt;h2&gt;Finetune a DNA model with a custom dataset&lt;/h2&gt;&lt;/center&gt;\"\n    )\n    config_title = mo.md(\"&lt;h3&gt;Finetune configuration&lt;/h3&gt;\")\n    config_text = mo.ui.text(value=\"finetune_config.yaml\", placeholder=\"config.yaml\",\n                             label=\"Config file (*.yaml)\", full_width=True)\n    model_text = mo.ui.text(value=\"zhangtaolab/plant-dnagpt-BPE\", placeholder=\"zhangtaolab/plant-dnagpt-BPE\",\n                            label=\"Model name or path\", full_width=True)\n    source1_text = mo.ui.dropdown(['local', 'huggingface', 'modelscope'], value=\"modelscope\", label=\"Model source\", full_width=True)\n    datasets_text = mo.ui.text(value=\"zhangtaolab/plant-multi-species-core-promoters\",\n                               placeholder=\"zhangtaolab/plant-multi-species-core-promoters\",\n                               label=\"Datasets name or path\", full_width=True)\n    source2_text = mo.ui.dropdown(['local', 'huggingface', 'modelscope'], value=\"modelscope\", label=\"Dataset source\", full_width=True)\n    seq_col_text = mo.ui.text(value=\"sequence\", placeholder=\"sequence\",\n        label=\"Sequence column name\", full_width=True)\n    label_col_text = mo.ui.text(value=\"label\", placeholder=\"label\",\n        label=\"Label column name\", full_width=True)\n    maxlen_text = mo.ui.text(value=\"512\", placeholder=\"512\",\n        label=\"Max token length\", full_width=True)\n    mo.vstack([title, config_title, config_text.style(width=\"30ch\")],\n              align='center', justify='center')\n    return (\n        config_text,\n        model_text,\n        source1_text,\n        datasets_text,\n        source2_text,\n        seq_col_text,\n        label_col_text,\n        maxlen_text,\n        \n    )\n</pre> @app.cell def __(mo):     title = mo.md(         \"Finetune a DNA model with a custom dataset\"     )     config_title = mo.md(\"Finetune configuration\")     config_text = mo.ui.text(value=\"finetune_config.yaml\", placeholder=\"config.yaml\",                              label=\"Config file (*.yaml)\", full_width=True)     model_text = mo.ui.text(value=\"zhangtaolab/plant-dnagpt-BPE\", placeholder=\"zhangtaolab/plant-dnagpt-BPE\",                             label=\"Model name or path\", full_width=True)     source1_text = mo.ui.dropdown(['local', 'huggingface', 'modelscope'], value=\"modelscope\", label=\"Model source\", full_width=True)     datasets_text = mo.ui.text(value=\"zhangtaolab/plant-multi-species-core-promoters\",                                placeholder=\"zhangtaolab/plant-multi-species-core-promoters\",                                label=\"Datasets name or path\", full_width=True)     source2_text = mo.ui.dropdown(['local', 'huggingface', 'modelscope'], value=\"modelscope\", label=\"Dataset source\", full_width=True)     seq_col_text = mo.ui.text(value=\"sequence\", placeholder=\"sequence\",         label=\"Sequence column name\", full_width=True)     label_col_text = mo.ui.text(value=\"label\", placeholder=\"label\",         label=\"Label column name\", full_width=True)     maxlen_text = mo.ui.text(value=\"512\", placeholder=\"512\",         label=\"Max token length\", full_width=True)     mo.vstack([title, config_title, config_text.style(width=\"30ch\")],               align='center', justify='center')     return (         config_text,         model_text,         source1_text,         datasets_text,         source2_text,         seq_col_text,         label_col_text,         maxlen_text,              ) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, config_text, load_config):\n    if config_text.value:\n        raw_configs = load_config(config_text.value)\n        raw_task_configs = raw_configs['task']\n        raw_train_configs = raw_configs['finetune']\n        states = {}\n        states['label_separator'] = mo.state(',')\n        for att in dir(raw_task_configs):\n            if not att.startswith(\"_\"):\n                if att == \"label_names\":\n                    all_labels = getattr(raw_task_configs, att)\n                    print(all_labels)\n                    states[att] = mo.state(\",\".join(all_labels))\n                else:\n                    states[att] = mo.state(getattr(raw_task_configs, att))\n        for att in dir(raw_train_configs):\n            if not att.startswith(\"_\"):\n                states[att] = mo.state(getattr(raw_train_configs, att))\n        if raw_train_configs.bf16:\n            states['precision'] = mo.state('bf16')\n        elif raw_train_configs.fp16:\n            states['precision'] = mo.state('fp16')\n        else:\n            states['precision'] = mo.state('float32')\n        configs = raw_configs\n    else:\n        raw_task_configs = None\n        raw_train_configs = None\n        configs = None\n    return (raw_task_configs, raw_train_configs, states, configs, )\n</pre> @app.cell def __(mo, config_text, load_config):     if config_text.value:         raw_configs = load_config(config_text.value)         raw_task_configs = raw_configs['task']         raw_train_configs = raw_configs['finetune']         states = {}         states['label_separator'] = mo.state(',')         for att in dir(raw_task_configs):             if not att.startswith(\"_\"):                 if att == \"label_names\":                     all_labels = getattr(raw_task_configs, att)                     print(all_labels)                     states[att] = mo.state(\",\".join(all_labels))                 else:                     states[att] = mo.state(getattr(raw_task_configs, att))         for att in dir(raw_train_configs):             if not att.startswith(\"_\"):                 states[att] = mo.state(getattr(raw_train_configs, att))         if raw_train_configs.bf16:             states['precision'] = mo.state('bf16')         elif raw_train_configs.fp16:             states['precision'] = mo.state('fp16')         else:             states['precision'] = mo.state('float32')         configs = raw_configs     else:         raw_task_configs = None         raw_train_configs = None         configs = None     return (raw_task_configs, raw_train_configs, states, configs, ) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, model_text, source1_text, datasets_text, source2_text, seq_col_text, label_col_text,\n       maxlen_text, states):\n    config_dict = mo.ui.dictionary({\n        \"task_type\": mo.ui.dropdown(options=['mask', 'generation',\n                                             'binary', 'multiclass', 'multilabel', 'regression', 'token'],\n                                    value=states[\"task_type\"][0](),\n                                    on_change=states[\"task_type\"][1],\n                                    label=\"task_type\", full_width=True),\n        \"num_labels\": mo.ui.number(value=states[\"num_labels\"][0](),\n                                   on_change=states[\"num_labels\"][1],\n                                   start=0, step=1,\n                                   label=\"num_labels\", full_width=True),\n        \"label_separator\": mo.ui.dropdown(options=[',', ';', '|', '/', '&amp;'],\n                                          value=states[\"label_separator\"][0](),\n                                          on_change=states[\"label_separator\"][1],\n                                          label=\"label_separator\", full_width=True),\n        \"label_names\": mo.ui.text(value=states[\"label_names\"][0](),\n                                  on_change=states[\"label_names\"][1],\n                                  label=\"label_names\", full_width=True),\n        \"num_train_epochs\": mo.ui.number(value=states[\"num_train_epochs\"][0](),\n                                         on_change=states[\"num_train_epochs\"][1],\n                                         start=1, step=1,\n                                         label=\"num_train_epochs\", full_width=True),\n        \"per_device_train_batch_size\": mo.ui.number(value=states[\"per_device_train_batch_size\"][0](),\n                                                    on_change=states[\"per_device_train_batch_size\"][1],\n                                                    start=1, step=1,\n                                                    label=\"per_device_train_batch_size\", full_width=True),\n        \"per_device_eval_batch_size\": mo.ui.number(value=states[\"per_device_train_batch_size\"][0](),\n                                                   on_change=states[\"per_device_train_batch_size\"][1],\n                                                   start=1, step=1,\n                                                   label=\"per_device_eval_batch_size\", full_width=True),\n        \"gradient_accumulation_steps\": mo.ui.number(value=states[\"gradient_accumulation_steps\"][0](),\n                                                    on_change=states[\"gradient_accumulation_steps\"][1],\n                                                    start=1, step=1,\n                                                    label=\"gradient_accumulation_steps\", full_width=True),\n        \"logging_strategy\": mo.ui.dropdown(options=['steps', 'epoch'],\n                                           value=states[\"logging_strategy\"][0](),\n                                           on_change=states[\"logging_strategy\"][1],\n                                           label=\"logging_strategy\", full_width=True),\n        \"logging_steps\": mo.ui.number(value=states[\"logging_steps\"][0](),\n                                      on_change=states[\"logging_steps\"][1],\n                                      start=0, step=5,\n                                      label=\"logging_steps\", full_width=True),\n        \"eval_strategy\": mo.ui.dropdown(options=['steps', 'epoch'],\n                                        value=states[\"logging_strategy\"][0](),\n                                        on_change=states[\"logging_strategy\"][1],\n                                        label=\"eval_strategy\", full_width=True),\n        \"eval_steps\": mo.ui.number(value=states[\"eval_steps\"][0](),\n                                   on_change=states[\"eval_steps\"][1],\n                                   start=0, step=5,\n                                   label=\"eval_steps\", full_width=True),\n        \"save_strategy\": mo.ui.dropdown(options=['steps', 'epoch'],\n                                        value=states[\"logging_strategy\"][0](),\n                                        on_change=states[\"logging_strategy\"][1],\n                                        label=\"save_strategy\", full_width=True),\n        \"save_steps\": mo.ui.number(value=states[\"save_steps\"][0](),\n                                   on_change=states[\"save_steps\"][1],\n                                   start=0, step=5,\n                                   label=\"save_steps\", full_width=True),\n        \"save_total_limit\": mo.ui.number(value=states[\"save_total_limit\"][0](),\n                                         on_change=states[\"save_total_limit\"][1],\n                                         start=1, step=1,\n                                         label=\"save_total_limit\", full_width=True),\n        \"learning_rate\": mo.ui.number(value=states[\"learning_rate\"][0](),\n                                      on_change=states[\"learning_rate\"][1],\n                                      start=1e-10, stop=1, step=1e-6,\n                                      label=\"learning_rate\", full_width=True),\n        \"weight_decay\": mo.ui.number(value=states[\"weight_decay\"][0](),\n                                     on_change=states[\"weight_decay\"][1],\n                                     start=0.0, stop=1, step=0.005,\n                                     label=\"weight_decay\", full_width=True),\n        \"adam_beta1\": mo.ui.number(value=states[\"adam_beta1\"][0](),\n                                   on_change=states[\"adam_beta1\"][1],\n                                   start=0.0, stop=1.0, step=0.001,\n                                   label=\"adam_beta1\", full_width=True),\n        \"adam_beta2\": mo.ui.number(value=states[\"adam_beta2\"][0](),\n                                   on_change=states[\"adam_beta2\"][1],\n                                   start=0.0, stop=1.0, step=0.001,\n                                   label=\"adam_beta2\", full_width=True),\n        \"adam_epsilon\": mo.ui.number(value=states[\"adam_epsilon\"][0](),\n                                     on_change=states[\"adam_epsilon\"][1],\n                                     start=1e-10, stop=1.0, step=1e-8,\n                                     label=\"adam_epsilon\", full_width=True),\n        \"max_grad_norm\": mo.ui.number(value=states[\"max_grad_norm\"][0](),\n                                      on_change=states[\"max_grad_norm\"][1],\n                                      start=0.0, step=0.1,\n                                      label=\"max_grad_norm\", full_width=True),\n        \"warmup_ratio\": mo.ui.number(value=states[\"warmup_ratio\"][0](),\n                                     on_change=states[\"warmup_ratio\"][1],\n                                     start=0.0, step=0.01,\n                                     label=\"warmup_ratio\", full_width=True),\n        \"lr_scheduler_type\": mo.ui.dropdown(options=['linear', 'cosine', 'cosine_with_restarts',\n                                                     'polynomial', 'constant',\n                                                     'constant_with_warmup', 'inverse_sqrt'],\n                                            value=states[\"lr_scheduler_type\"][0](),\n                                            on_change=states[\"lr_scheduler_type\"][1],\n                                            label=\"lr_scheduler_type\", full_width=True),\n        \"precision\": mo.ui.dropdown(options=['float32', 'fp16', 'bf16'],\n                                    value=states[\"precision\"][0](),\n                                    on_change=states[\"precision\"][1],\n                                    label=\"precision\", full_width=True),\n        \"output_dir\": mo.ui.text(value=states[\"output_dir\"][0](),\n                                 on_change=states[\"output_dir\"][1],\n                                 label=\"output_dir\", full_width=True),\n    })\n    elems = list(config_dict.values())\n    rows = [\n        mo.hstack(\n            elems[i : i+4], widths=\"equal\", align=\"stretch\", gap=0.5\n        ) for i in range(0, len(elems), 4)\n    ]\n    config_stack = mo.vstack(rows, align=\"stretch\", gap=0.5)\n    model_title = mo.md(\"&lt;h3&gt;Model and dataset&lt;/h3&gt;\")\n    model_stack = mo.hstack([model_text.style(width=\"75ch\"), source1_text], align='center', justify='center')\n    datasets_stack = mo.hstack([datasets_text.style(width=\"75ch\"), source2_text], align='center', justify='center')\n    options_stack = mo.hstack(\n        [seq_col_text,\n        label_col_text,\n        maxlen_text],\n        align='center',\n        justify='center'\n    )\n    mo.vstack([config_stack, model_title, model_stack, datasets_stack, options_stack],\n              align='center', justify='center')\n    return (config_dict, )\n</pre> @app.cell def __(mo, model_text, source1_text, datasets_text, source2_text, seq_col_text, label_col_text,        maxlen_text, states):     config_dict = mo.ui.dictionary({         \"task_type\": mo.ui.dropdown(options=['mask', 'generation',                                              'binary', 'multiclass', 'multilabel', 'regression', 'token'],                                     value=states[\"task_type\"][0](),                                     on_change=states[\"task_type\"][1],                                     label=\"task_type\", full_width=True),         \"num_labels\": mo.ui.number(value=states[\"num_labels\"][0](),                                    on_change=states[\"num_labels\"][1],                                    start=0, step=1,                                    label=\"num_labels\", full_width=True),         \"label_separator\": mo.ui.dropdown(options=[',', ';', '|', '/', '&amp;'],                                           value=states[\"label_separator\"][0](),                                           on_change=states[\"label_separator\"][1],                                           label=\"label_separator\", full_width=True),         \"label_names\": mo.ui.text(value=states[\"label_names\"][0](),                                   on_change=states[\"label_names\"][1],                                   label=\"label_names\", full_width=True),         \"num_train_epochs\": mo.ui.number(value=states[\"num_train_epochs\"][0](),                                          on_change=states[\"num_train_epochs\"][1],                                          start=1, step=1,                                          label=\"num_train_epochs\", full_width=True),         \"per_device_train_batch_size\": mo.ui.number(value=states[\"per_device_train_batch_size\"][0](),                                                     on_change=states[\"per_device_train_batch_size\"][1],                                                     start=1, step=1,                                                     label=\"per_device_train_batch_size\", full_width=True),         \"per_device_eval_batch_size\": mo.ui.number(value=states[\"per_device_train_batch_size\"][0](),                                                    on_change=states[\"per_device_train_batch_size\"][1],                                                    start=1, step=1,                                                    label=\"per_device_eval_batch_size\", full_width=True),         \"gradient_accumulation_steps\": mo.ui.number(value=states[\"gradient_accumulation_steps\"][0](),                                                     on_change=states[\"gradient_accumulation_steps\"][1],                                                     start=1, step=1,                                                     label=\"gradient_accumulation_steps\", full_width=True),         \"logging_strategy\": mo.ui.dropdown(options=['steps', 'epoch'],                                            value=states[\"logging_strategy\"][0](),                                            on_change=states[\"logging_strategy\"][1],                                            label=\"logging_strategy\", full_width=True),         \"logging_steps\": mo.ui.number(value=states[\"logging_steps\"][0](),                                       on_change=states[\"logging_steps\"][1],                                       start=0, step=5,                                       label=\"logging_steps\", full_width=True),         \"eval_strategy\": mo.ui.dropdown(options=['steps', 'epoch'],                                         value=states[\"logging_strategy\"][0](),                                         on_change=states[\"logging_strategy\"][1],                                         label=\"eval_strategy\", full_width=True),         \"eval_steps\": mo.ui.number(value=states[\"eval_steps\"][0](),                                    on_change=states[\"eval_steps\"][1],                                    start=0, step=5,                                    label=\"eval_steps\", full_width=True),         \"save_strategy\": mo.ui.dropdown(options=['steps', 'epoch'],                                         value=states[\"logging_strategy\"][0](),                                         on_change=states[\"logging_strategy\"][1],                                         label=\"save_strategy\", full_width=True),         \"save_steps\": mo.ui.number(value=states[\"save_steps\"][0](),                                    on_change=states[\"save_steps\"][1],                                    start=0, step=5,                                    label=\"save_steps\", full_width=True),         \"save_total_limit\": mo.ui.number(value=states[\"save_total_limit\"][0](),                                          on_change=states[\"save_total_limit\"][1],                                          start=1, step=1,                                          label=\"save_total_limit\", full_width=True),         \"learning_rate\": mo.ui.number(value=states[\"learning_rate\"][0](),                                       on_change=states[\"learning_rate\"][1],                                       start=1e-10, stop=1, step=1e-6,                                       label=\"learning_rate\", full_width=True),         \"weight_decay\": mo.ui.number(value=states[\"weight_decay\"][0](),                                      on_change=states[\"weight_decay\"][1],                                      start=0.0, stop=1, step=0.005,                                      label=\"weight_decay\", full_width=True),         \"adam_beta1\": mo.ui.number(value=states[\"adam_beta1\"][0](),                                    on_change=states[\"adam_beta1\"][1],                                    start=0.0, stop=1.0, step=0.001,                                    label=\"adam_beta1\", full_width=True),         \"adam_beta2\": mo.ui.number(value=states[\"adam_beta2\"][0](),                                    on_change=states[\"adam_beta2\"][1],                                    start=0.0, stop=1.0, step=0.001,                                    label=\"adam_beta2\", full_width=True),         \"adam_epsilon\": mo.ui.number(value=states[\"adam_epsilon\"][0](),                                      on_change=states[\"adam_epsilon\"][1],                                      start=1e-10, stop=1.0, step=1e-8,                                      label=\"adam_epsilon\", full_width=True),         \"max_grad_norm\": mo.ui.number(value=states[\"max_grad_norm\"][0](),                                       on_change=states[\"max_grad_norm\"][1],                                       start=0.0, step=0.1,                                       label=\"max_grad_norm\", full_width=True),         \"warmup_ratio\": mo.ui.number(value=states[\"warmup_ratio\"][0](),                                      on_change=states[\"warmup_ratio\"][1],                                      start=0.0, step=0.01,                                      label=\"warmup_ratio\", full_width=True),         \"lr_scheduler_type\": mo.ui.dropdown(options=['linear', 'cosine', 'cosine_with_restarts',                                                      'polynomial', 'constant',                                                      'constant_with_warmup', 'inverse_sqrt'],                                             value=states[\"lr_scheduler_type\"][0](),                                             on_change=states[\"lr_scheduler_type\"][1],                                             label=\"lr_scheduler_type\", full_width=True),         \"precision\": mo.ui.dropdown(options=['float32', 'fp16', 'bf16'],                                     value=states[\"precision\"][0](),                                     on_change=states[\"precision\"][1],                                     label=\"precision\", full_width=True),         \"output_dir\": mo.ui.text(value=states[\"output_dir\"][0](),                                  on_change=states[\"output_dir\"][1],                                  label=\"output_dir\", full_width=True),     })     elems = list(config_dict.values())     rows = [         mo.hstack(             elems[i : i+4], widths=\"equal\", align=\"stretch\", gap=0.5         ) for i in range(0, len(elems), 4)     ]     config_stack = mo.vstack(rows, align=\"stretch\", gap=0.5)     model_title = mo.md(\"Model and dataset\")     model_stack = mo.hstack([model_text.style(width=\"75ch\"), source1_text], align='center', justify='center')     datasets_stack = mo.hstack([datasets_text.style(width=\"75ch\"), source2_text], align='center', justify='center')     options_stack = mo.hstack(         [seq_col_text,         label_col_text,         maxlen_text],         align='center',         justify='center'     )     mo.vstack([config_stack, model_title, model_stack, datasets_stack, options_stack],               align='center', justify='center')     return (config_dict, ) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(configs, config_dict):\n    if configs:\n        for arg in config_dict:\n            if arg in ['task_type', 'num_labels']:\n                setattr(configs['task'], arg, config_dict[arg].value)\n            if arg == \"label_names\":\n                sep = config_dict['label_separator'].value\n                setattr(\n                    configs['task'],\n                    arg,\n                    config_dict[arg].value.split(sep)\n                )\n            if arg in dir(configs['finetune']):\n                setattr(configs['finetune'], arg, config_dict[arg].value)\n            if arg == \"precision\":\n                if config_dict[arg].value == \"bf16\":\n                    configs['finetune'].bf16 = True\n                elif config_dict[arg].value == \"fp16\":\n                    configs['finetune'].fp16 = True\n                else:\n                    pass\n    print(configs)\n    return (configs, )\n</pre> @app.cell def __(configs, config_dict):     if configs:         for arg in config_dict:             if arg in ['task_type', 'num_labels']:                 setattr(configs['task'], arg, config_dict[arg].value)             if arg == \"label_names\":                 sep = config_dict['label_separator'].value                 setattr(                     configs['task'],                     arg,                     config_dict[arg].value.split(sep)                 )             if arg in dir(configs['finetune']):                 setattr(configs['finetune'], arg, config_dict[arg].value)             if arg == \"precision\":                 if config_dict[arg].value == \"bf16\":                     configs['finetune'].bf16 = True                 elif config_dict[arg].value == \"fp16\":                     configs['finetune'].fp16 = True                 else:                     pass     print(configs)     return (configs, ) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, configs, model_text, source1_text, load_model_and_tokenizer, \n       datasets_text, source2_text, seq_col_text, label_col_text, maxlen_text, DNADataset):\n    def prepare(configs, model_text, source1_text, load_model_and_tokenizer, \n        datasets_text, source2_text, seq_col_text, label_col_text, maxlen_text, DNADataset):\n        # Load model and tokenizer\n        model_name = model_text.value\n        source1 = source1_text.value\n        if model_name:\n            model, tokenizer = load_model_and_tokenizer(\n                model_name,\n                task_config=configs['task'],\n                source=source1\n            )\n        else:\n            model = None\n            tokenizer = None\n        # Load datasets\n        datasets_name = datasets_text.value\n        seq_col = seq_col_text.value\n        label_col = label_col_text.value\n        max_length = int(maxlen_text.value)\n        source2 = source2_text.value\n        print(datasets_name, source2)\n        if datasets_name:\n            if source2 == \"huggingface\":\n                datasets = DNADataset.from_huggingface(datasets_name, seq_col=seq_col, label_col=label_col,\n                                                    tokenizer=tokenizer, max_length=max_length)\n            elif source2 == \"modelscope\":\n                datasets = DNADataset.from_modelscope(datasets_name, seq_col=seq_col, label_col=label_col,\n                                                    tokenizer=tokenizer, max_length=max_length)\n            else:\n                datasets = DNADataset.load_local_data(datasets_name, seq_col=seq_col, label_col=label_col,\n                                                    tokenizer=tokenizer, max_length=max_length)\n        else:\n            datasets = None\n        # Process datasets\n        if datasets is not None:\n            datasets.encode_sequences(task=configs['task'].task_type, remove_unused_columns=True)\n            if isinstance(datasets.dataset, dict):\n                pass\n            else:\n                datasets.split_data()\n        else:\n            pass\n        \n        return (model, tokenizer, datasets,)\n    train_button = mo.ui.button(label=\"Start Training\", \n                                on_click=lambda _: prepare(\n                                    configs, model_text, source1_text, load_model_and_tokenizer,\n                                    datasets_text, source2_text, seq_col_text, label_col_text, maxlen_text, DNADataset\n                                ))\n    mo.vstack([train_button], align='center', justify='center')\n    return (train_button,)\n</pre> @app.cell def __(mo, configs, model_text, source1_text, load_model_and_tokenizer,         datasets_text, source2_text, seq_col_text, label_col_text, maxlen_text, DNADataset):     def prepare(configs, model_text, source1_text, load_model_and_tokenizer,          datasets_text, source2_text, seq_col_text, label_col_text, maxlen_text, DNADataset):         # Load model and tokenizer         model_name = model_text.value         source1 = source1_text.value         if model_name:             model, tokenizer = load_model_and_tokenizer(                 model_name,                 task_config=configs['task'],                 source=source1             )         else:             model = None             tokenizer = None         # Load datasets         datasets_name = datasets_text.value         seq_col = seq_col_text.value         label_col = label_col_text.value         max_length = int(maxlen_text.value)         source2 = source2_text.value         print(datasets_name, source2)         if datasets_name:             if source2 == \"huggingface\":                 datasets = DNADataset.from_huggingface(datasets_name, seq_col=seq_col, label_col=label_col,                                                     tokenizer=tokenizer, max_length=max_length)             elif source2 == \"modelscope\":                 datasets = DNADataset.from_modelscope(datasets_name, seq_col=seq_col, label_col=label_col,                                                     tokenizer=tokenizer, max_length=max_length)             else:                 datasets = DNADataset.load_local_data(datasets_name, seq_col=seq_col, label_col=label_col,                                                     tokenizer=tokenizer, max_length=max_length)         else:             datasets = None         # Process datasets         if datasets is not None:             datasets.encode_sequences(task=configs['task'].task_type, remove_unused_columns=True)             if isinstance(datasets.dataset, dict):                 pass             else:                 datasets.split_data()         else:             pass                  return (model, tokenizer, datasets,)     train_button = mo.ui.button(label=\"Start Training\",                                  on_click=lambda _: prepare(                                     configs, model_text, source1_text, load_model_and_tokenizer,                                     datasets_text, source2_text, seq_col_text, label_col_text, maxlen_text, DNADataset                                 ))     mo.vstack([train_button], align='center', justify='center')     return (train_button,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo):\n    text_output = mo.output\n    return text_output\n</pre> @app.cell def __(mo):     text_output = mo.output     return text_output In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, configs, train_button, DNATrainer, text_output):\n    from transformers import TrainerCallback\n    from math import ceil\n    def get_total_steps(trainer):\n        if trainer.args.max_steps and trainer.args.max_steps &gt; 0:\n            total_steps = trainer.args.max_steps\n        else:\n            # 2\ufe0f\u20e3 else compute from dataloader\n            train_dl = trainer.get_train_dataloader()\n            # number of optimizer updates per epoch\n            steps_per_epoch = ceil(\n                len(train_dl) / trainer.args.gradient_accumulation_steps\n            )\n            total_steps = steps_per_epoch * trainer.args.num_train_epochs\n        return total_steps\n\n    class MarimoCallback(TrainerCallback):\n        def __init__(self, text_out):\n            self.text_out = text_out\n            self.steps = []\n            self.epochs = []\n            self.all_logs = \"\"\n\n        def on_log(self, args, state, control, logs=None, **kwargs):\n            # logs might contain 'loss', 'eval_loss', 'eval_accuracy', etc.\n            step = state.global_step\n            self.steps.append(step)\n            increment = self.steps[-1] - self.steps[-2] if len(self.steps) &gt; 1 else 0\n            # update progress bar\n            # self.bar.update(increment=increment)\n\n            # collect\n            txt = ''\n            if \"loss\" in logs:\n                txt = f\"**Step {step}**&lt;br&gt;\" + \", \".join(\n                    f\"{k}: {v:.4f}\" for k, v in logs.items()\n                )\n            if \"eval_loss\" in logs:\n                txt = \", \".join(\n                    f\"{k}: {v:.4f}\" for k, v in logs.items()\n                )\n            if txt:\n                self.all_logs += txt + \"&lt;br&gt;\"\n            self.text_out.clear()\n            self.text_out.replace(mo.md(self.all_logs))\n\n    if train_button.value:\n        model, _, datasets = train_button.value\n        trainer = DNATrainer(\n            model=model,\n            config=configs,\n            datasets=datasets\n        )\n        trainer.trainer.add_callback(MarimoCallback(text_output))\n        trainer.train()\n    else:\n        trainer = None\n    return trainer\n</pre> @app.cell def __(mo, configs, train_button, DNATrainer, text_output):     from transformers import TrainerCallback     from math import ceil     def get_total_steps(trainer):         if trainer.args.max_steps and trainer.args.max_steps &gt; 0:             total_steps = trainer.args.max_steps         else:             # 2\ufe0f\u20e3 else compute from dataloader             train_dl = trainer.get_train_dataloader()             # number of optimizer updates per epoch             steps_per_epoch = ceil(                 len(train_dl) / trainer.args.gradient_accumulation_steps             )             total_steps = steps_per_epoch * trainer.args.num_train_epochs         return total_steps      class MarimoCallback(TrainerCallback):         def __init__(self, text_out):             self.text_out = text_out             self.steps = []             self.epochs = []             self.all_logs = \"\"          def on_log(self, args, state, control, logs=None, **kwargs):             # logs might contain 'loss', 'eval_loss', 'eval_accuracy', etc.             step = state.global_step             self.steps.append(step)             increment = self.steps[-1] - self.steps[-2] if len(self.steps) &gt; 1 else 0             # update progress bar             # self.bar.update(increment=increment)              # collect             txt = ''             if \"loss\" in logs:                 txt = f\"**Step {step}**\" + \", \".join(                     f\"{k}: {v:.4f}\" for k, v in logs.items()                 )             if \"eval_loss\" in logs:                 txt = \", \".join(                     f\"{k}: {v:.4f}\" for k, v in logs.items()                 )             if txt:                 self.all_logs += txt + \"\"             self.text_out.clear()             self.text_out.replace(mo.md(self.all_logs))      if train_button.value:         model, _, datasets = train_button.value         trainer = DNATrainer(             model=model,             config=configs,             datasets=datasets         )         trainer.trainer.add_callback(MarimoCallback(text_output))         trainer.train()     else:         trainer = None     return trainer <p>@app.cell def __(): return</p> In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    app.run()\n</pre> if __name__ == \"__main__\":     app.run()"},{"location":"example/marimo/inference/inference_demo/","title":"Inference Demo","text":"In\u00a0[\u00a0]: Copied! <pre>import marimo\n</pre> import marimo In\u00a0[\u00a0]: Copied! <pre>__generated_with = \"0.11.17\"\napp = marimo.App(width=\"medium\")\n</pre> __generated_with = \"0.11.17\" app = marimo.App(width=\"medium\") In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(__file__):\n    import sys\n    # from os import path\n    # sys.path.append(path.abspath(path.join(path.dirname(__file__),\n    # '../../..')))\n    import marimo as mo\n    import pandas as pd\n    from dnallm import load_config, load_model_and_tokenizer, DNAInference\n    return sys, pd, mo, load_config, load_model_and_tokenizer, DNAInference\n</pre> @app.cell def __(__file__):     import sys     # from os import path     # sys.path.append(path.abspath(path.join(path.dirname(__file__),     # '../../..')))     import marimo as mo     import pandas as pd     from dnallm import load_config, load_model_and_tokenizer, DNAInference     return sys, pd, mo, load_config, load_model_and_tokenizer, DNAInference In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(pd):\n    model_df = pd.read_excel(\"./plant_DNA_LLMs_finetune_list.xlsx\")\n    return (model_df,)\n</pre> @app.cell def __(pd):     model_df = pd.read_excel(\"./plant_DNA_LLMs_finetune_list.xlsx\")     return (model_df,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(model_df):\n    tasks = model_df.Task.unique()\n    print(\"Available tasks:\", tasks, sep=\"\\n\")\n    return (tasks,)\n</pre> @app.cell def __(model_df):     tasks = model_df.Task.unique()     print(\"Available tasks:\", tasks, sep=\"\\n\")     return (tasks,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, tasks):\n    task_dropdown = mo.ui.dropdown(\n        tasks,\n        value='open chromatin',\n        label='Predict Task'\n    )\n    return (task_dropdown,)\n</pre> @app.cell def __(mo, tasks):     task_dropdown = mo.ui.dropdown(         tasks,         value='open chromatin',         label='Predict Task'     )     return (task_dropdown,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(model_df):\n    models = model_df.Model.unique()\n    print(\"Available models:\", models, sep=\"\\n\")\n    return (models,)\n</pre> @app.cell def __(model_df):     models = model_df.Model.unique()     print(\"Available models:\", models, sep=\"\\n\")     return (models,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, models):\n    model_dropdown = mo.ui.dropdown(\n        models,\n        value='Plant DNABERT',\n        label='Model'\n    )\n    return (model_dropdown,)\n</pre> @app.cell def __(mo, models):     model_dropdown = mo.ui.dropdown(         models,         value='Plant DNABERT',         label='Model'     )     return (model_dropdown,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(model_df):\n    tokenizers = model_df.Tokenzier.unique()\n    print(\"Available models:\", tokenizers,sep=\"\\n\")\n    return (tokenizers,)\n</pre> @app.cell def __(model_df):     tokenizers = model_df.Tokenzier.unique()     print(\"Available models:\", tokenizers,sep=\"\\n\")     return (tokenizers,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, tokenizers):\n    tokenizer_dropdown = mo.ui.dropdown(\n        tokenizers,\n        value='BPE',\n        label='Tokenizer'\n    )\n    return (tokenizer_dropdown,)\n</pre> @app.cell def __(mo, tokenizers):     tokenizer_dropdown = mo.ui.dropdown(         tokenizers,         value='BPE',         label='Tokenizer'     )     return (tokenizer_dropdown,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo):\n    source_dropdown = mo.ui.dropdown({'modelscope':'modelscope',\n                                      'huggingface':'huggingface'\n                                     }, value='modelscope', label='Model Source')\n    return (source_dropdown,)\n</pre> @app.cell def __(mo):     source_dropdown = mo.ui.dropdown({'modelscope':'modelscope',                                       'huggingface':'huggingface'                                      }, value='modelscope', label='Model Source')     return (source_dropdown,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo):\n    placeholder = 'GGGCAGCGGTTACACCTTAATCGACACGACTCTCGGCAACGGATATCTCG\\\n    GCTCTTGCATCGATGAAGAACGTAGCAAAATGCGATACCTGGTGTGAATTGCAGAAT\\\n    CCCGCGAACCATCGAGTTTTTGAACGCAAGTTGCGCCCGAAGCCTTCTGACGGA\\\n    GGGCACGTCTGCCTGGGCGTCACGCCAAAAGACACTCCCAACACCCCCCCGCGGGGC\\\n    GAGGGACGTGGCGTCTGGCCCCCCGCGCTGCAGGGCGAGGTGGGCCGAAGCAGGGGCTGCC\\\n    GGCGAACCGCGTCGGACGCAACACGTGGTGGGCGACATCAAGTTGTTCTCGGTGCAGCGT\\\n    CCCGGCGCGCGGCCGGCCATTCGGCCCTAAGGACCCATCGAGCGACCGAGCTTGCCCTCG\\\n    GACCACGACCCCAGGTCAGTCGGGACTACCCGCTGAGTTTAAGCATATAAATAAGCGGAGGAG\\\n    AAGAAACTTACGAGGATTCCCCTAGTAACGGCGAGCGAACCGGGAGCAGCCCAGCTTGA\\\n    GAATCGGGCGGCCTCGCCGCCCGAATTGTAGTCTGGAGAGGCGT'\n    dnaseq_entry_box = mo.ui.text_area(\n        placeholder=placeholder,\n        full_width=True,\n        label='DNA Sequence:',\n        rows=5\n    )\n    return (dnaseq_entry_box, placeholder, )\n</pre> @app.cell def __(mo):     placeholder = 'GGGCAGCGGTTACACCTTAATCGACACGACTCTCGGCAACGGATATCTCG\\     GCTCTTGCATCGATGAAGAACGTAGCAAAATGCGATACCTGGTGTGAATTGCAGAAT\\     CCCGCGAACCATCGAGTTTTTGAACGCAAGTTGCGCCCGAAGCCTTCTGACGGA\\     GGGCACGTCTGCCTGGGCGTCACGCCAAAAGACACTCCCAACACCCCCCCGCGGGGC\\     GAGGGACGTGGCGTCTGGCCCCCCGCGCTGCAGGGCGAGGTGGGCCGAAGCAGGGGCTGCC\\     GGCGAACCGCGTCGGACGCAACACGTGGTGGGCGACATCAAGTTGTTCTCGGTGCAGCGT\\     CCCGGCGCGCGGCCGGCCATTCGGCCCTAAGGACCCATCGAGCGACCGAGCTTGCCCTCG\\     GACCACGACCCCAGGTCAGTCGGGACTACCCGCTGAGTTTAAGCATATAAATAAGCGGAGGAG\\     AAGAAACTTACGAGGATTCCCCTAGTAACGGCGAGCGAACCGGGAGCAGCCCAGCTTGA\\     GAATCGGGCGGCCTCGCCGCCCGAATTGTAGTCTGGAGAGGCGT'     dnaseq_entry_box = mo.ui.text_area(         placeholder=placeholder,         full_width=True,         label='DNA Sequence:',         rows=5     )     return (dnaseq_entry_box, placeholder, ) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(\n    mo,\n    dnaseq_entry_box,\n    model_dropdown,\n    source_dropdown,\n    task_dropdown,\n    tokenizer_dropdown,\n):\n    title = mo.md(\n        \"&lt;center&gt;&lt;h2&gt;Model inference&lt;/h2&gt;&lt;/center&gt;\"\n    )\n    hstack=mo.hstack(\n        [task_dropdown,\n        model_dropdown,\n        tokenizer_dropdown,\n        source_dropdown],\n        align='center',\n        justify='center'\n    )\n    mo.vstack([title, dnaseq_entry_box, hstack])\n    return (hstack,)\n</pre> @app.cell def __(     mo,     dnaseq_entry_box,     model_dropdown,     source_dropdown,     task_dropdown,     tokenizer_dropdown, ):     title = mo.md(         \"Model inference\"     )     hstack=mo.hstack(         [task_dropdown,         model_dropdown,         tokenizer_dropdown,         source_dropdown],         align='center',         justify='center'     )     mo.vstack([title, dnaseq_entry_box, hstack])     return (hstack,) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, model_df, model_dropdown, task_dropdown, tokenizer_dropdown):\n    try:\n        model_name = model_df[\n            (model_df.Task == task_dropdown.value)\n            &amp; (model_df.Model == model_dropdown.value)\n            &amp; (model_df.Tokenzier == tokenizer_dropdown.value)\n        ].Name.tolist()[0]\n        print(\"Current model:\", model_name, sep=\"\\n\")\n        callout = \"\"\n    except:\n        callout = mo.callout(\"Cannot found the model\", kind=\"warn\")\n        model_name = None\n    mo.vstack([callout], align=\"stretch\")\n    return (model_name,)\n</pre> @app.cell def __(mo, model_df, model_dropdown, task_dropdown, tokenizer_dropdown):     try:         model_name = model_df[             (model_df.Task == task_dropdown.value)             &amp; (model_df.Model == model_dropdown.value)             &amp; (model_df.Tokenzier == tokenizer_dropdown.value)         ].Name.tolist()[0]         print(\"Current model:\", model_name, sep=\"\\n\")         callout = \"\"     except:         callout = mo.callout(\"Cannot found the model\", kind=\"warn\")         model_name = None     mo.vstack([callout], align=\"stretch\")     return (model_name,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(dnaseq_entry_box, placeholder):\n    dnaseq = ''\n    if dnaseq_entry_box.value:\n        dnaseq = dnaseq_entry_box.value\n    else:\n        dnaseq = placeholder\n        print(\"No sequence found, use default sequence.\")\n    return (dnaseq,)\n</pre> @app.cell def __(dnaseq_entry_box, placeholder):     dnaseq = ''     if dnaseq_entry_box.value:         dnaseq = dnaseq_entry_box.value     else:         dnaseq = placeholder         print(\"No sequence found, use default sequence.\")     return (dnaseq,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(load_config):\n    configs = load_config(\"./inference_config.yaml\")\n    return configs\n</pre> @app.cell def __(load_config):     configs = load_config(\"./inference_config.yaml\")     return configs In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(task_dropdown, configs):\n    # Set task type\n    task = task_dropdown.value\n    if task in ['core promoter', 'sequence conservation', 'enhancer', \n                'H3K27ac', 'H3K27me3', 'H3K4me3', 'lncRNAs']:\n        data = task.split()[-1]\n        configs['task'].task_type = 'binary'\n        configs['task'].num_labels = 2\n        configs['task'].label_names = ['Not '+data, data.capitalize()]\n    elif task in ['open chromatin']:\n        configs['task'].task_type = 'multiclass'\n        configs['task'].num_labels = 3\n        configs['task'].label_names = ['Not '+task, 'Partial '+task, 'Full '+task]\n    elif task in ['promoter strength leaf', 'promoter strength protoplast']:\n        configs['task'].task_type = 'regression'\n        configs['task'].num_labels = 1\n        configs['task'].label_names = [task]\n    else:\n        pass\n    return (configs,)\n</pre> @app.cell def __(task_dropdown, configs):     # Set task type     task = task_dropdown.value     if task in ['core promoter', 'sequence conservation', 'enhancer',                  'H3K27ac', 'H3K27me3', 'H3K4me3', 'lncRNAs']:         data = task.split()[-1]         configs['task'].task_type = 'binary'         configs['task'].num_labels = 2         configs['task'].label_names = ['Not '+data, data.capitalize()]     elif task in ['open chromatin']:         configs['task'].task_type = 'multiclass'         configs['task'].num_labels = 3         configs['task'].label_names = ['Not '+task, 'Partial '+task, 'Full '+task]     elif task in ['promoter strength leaf', 'promoter strength protoplast']:         configs['task'].task_type = 'regression'         configs['task'].num_labels = 1         configs['task'].label_names = [task]     else:         pass     return (configs,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(\n    mo,\n    dnaseq,\n    model_name,\n    source_dropdown,\n    configs,\n    load_model_and_tokenizer,\n    DNAInference\n):\n    if model_name:\n        # Load the model and tokenizer\n        model, tokenizer = load_model_and_tokenizer(\n            model_name,\n            task_config=configs['task'],\n            source=source_dropdown.value\n        )\n        # Instantiate the inference engine\n        inference_engine = DNAInference(\n            model=model,\n            tokenizer=tokenizer,\n            config=configs\n        )\n        # Predict the sequence\n        predict_button = mo.ui.button(label=\"Predict\",\n                                      on_click=lambda value: inference_engine.infer_seqs(\n                                        dnaseq, output_attentions=True)\n                                     )\n    else:\n        predict_button = mo.ui.button(label=\"Predict\")\n        inference_engine = None\n    mo.hstack([predict_button], align='center', justify='center')\n    return (predict_button, inference_engine,)\n</pre> @app.cell def __(     mo,     dnaseq,     model_name,     source_dropdown,     configs,     load_model_and_tokenizer,     DNAInference ):     if model_name:         # Load the model and tokenizer         model, tokenizer = load_model_and_tokenizer(             model_name,             task_config=configs['task'],             source=source_dropdown.value         )         # Instantiate the inference engine         inference_engine = DNAInference(             model=model,             tokenizer=tokenizer,             config=configs         )         # Predict the sequence         predict_button = mo.ui.button(label=\"Predict\",                                       on_click=lambda value: inference_engine.infer_seqs(                                         dnaseq, output_attentions=True)                                      )     else:         predict_button = mo.ui.button(label=\"Predict\")         inference_engine = None     mo.hstack([predict_button], align='center', justify='center')     return (predict_button, inference_engine,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(predict_button):\n    if predict_button.value:\n        results = predict_button.value\n    else:\n        results = None\n    results\n    return (results, )\n</pre> @app.cell def __(predict_button):     if predict_button.value:         results = predict_button.value     else:         results = None     results     return (results, ) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, results, inference_engine):\n    if results:\n        seqs = len(inference_engine.sequences)\n        layers = len(inference_engine.embeddings['attentions'])\n        heads = inference_engine.embeddings['attentions'][0].shape[1]\n    else:\n        seqs = 1\n        layers = 12\n        heads = 12\n    seq_number = mo.ui.number(start=1, stop=seqs if seqs&gt;0 else 1, label=\"Sequence index\")\n    layer_slider = mo.ui.slider(start=1, stop=layers, step=1, label='Layer index',\n                                show_value=True)\n    head_slider = mo.ui.slider(start=1, stop=heads, step=1, label='Head index',\n                            show_value=True)\n    figure_size = mo.ui.number(start=200, stop=5120, step=10, label='Figure size',\n                            value = 800)\n    return (seq_number, layer_slider, head_slider, figure_size, )\n</pre> @app.cell def __(mo, results, inference_engine):     if results:         seqs = len(inference_engine.sequences)         layers = len(inference_engine.embeddings['attentions'])         heads = inference_engine.embeddings['attentions'][0].shape[1]     else:         seqs = 1         layers = 12         heads = 12     seq_number = mo.ui.number(start=1, stop=seqs if seqs&gt;0 else 1, label=\"Sequence index\")     layer_slider = mo.ui.slider(start=1, stop=layers, step=1, label='Layer index',                                 show_value=True)     head_slider = mo.ui.slider(start=1, stop=heads, step=1, label='Head index',                             show_value=True)     figure_size = mo.ui.number(start=200, stop=5120, step=10, label='Figure size',                             value = 800)     return (seq_number, layer_slider, head_slider, figure_size, ) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(\n    mo,\n    seq_number,\n    layer_slider,\n    head_slider,\n    figure_size,\n    inference_engine\n):\n    plot_button = mo.ui.button(label=\"Plot attention map\",\n                            on_click=lambda value: inference_engine.plot_attentions(\n                                seq_number.value-1, layer_slider.value-1, head_slider.value-1\n                                )\n                            )\n    plot_options = mo.hstack(\n        [seq_number,\n        layer_slider,\n        head_slider,\n        figure_size],\n        align='center',\n        justify='center'\n    )\n    mo.vstack([plot_options, plot_button], align='center', justify='center')\n    return (plot_button,)\n</pre> @app.cell def __(     mo,     seq_number,     layer_slider,     head_slider,     figure_size,     inference_engine ):     plot_button = mo.ui.button(label=\"Plot attention map\",                             on_click=lambda value: inference_engine.plot_attentions(                                 seq_number.value-1, layer_slider.value-1, head_slider.value-1                                 )                             )     plot_options = mo.hstack(         [seq_number,         layer_slider,         head_slider,         figure_size],         align='center',         justify='center'     )     mo.vstack([plot_options, plot_button], align='center', justify='center')     return (plot_button,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef __(mo, plot_button, figure_size):\n    plot_out = plot_button.value\n    if plot_out:\n        chart = mo.ui.altair_chart(plot_out).properties(\n            width=figure_size.value, height=figure_size.value\n            )\n    else:\n        chart = None\n    mo.vstack([chart], align='center', justify='center')\n    return\n</pre> @app.cell def __(mo, plot_button, figure_size):     plot_out = plot_button.value     if plot_out:         chart = mo.ui.altair_chart(plot_out).properties(             width=figure_size.value, height=figure_size.value             )     else:         chart = None     mo.vstack([chart], align='center', justify='center')     return <p>@app.cell def __(): return</p> In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    app.run()\n</pre> if __name__ == \"__main__\":     app.run()"},{"location":"example/mcp_example/mcp_client_ollama_langchain_agents/","title":"DNALLM MCP Client with LangChain Agents","text":"In\u00a0[\u00a0]: Copied! <pre># Install required dependencies for LangChain MCP integration\n# This cell installs the necessary packages to use LangChain with MCP (Model Context Protocol)\n# and Ollama for local LLM inference\n\n!uv pip install -U langchain                    # Core LangChain framework\n!uv pip install -U langchain-mcp-adapters       # MCP adapters for LangChain integration\n!uv pip install -U langchain-ollama            # Ollama integration for local LLM inference\n</pre> # Install required dependencies for LangChain MCP integration # This cell installs the necessary packages to use LangChain with MCP (Model Context Protocol) # and Ollama for local LLM inference  !uv pip install -U langchain                    # Core LangChain framework !uv pip install -U langchain-mcp-adapters       # MCP adapters for LangChain integration !uv pip install -U langchain-ollama            # Ollama integration for local LLM inference  <pre>Using Python 3.13.5 environment at: /Users/forrest/GitHub/DNALLM/.venv\nResolved 32 packages in 2.97s                                        \n\u2819 Preparing packages... (0/22)                                                  \n\u2819 Preparing packages... (0/22)-------------     0 B/69.34 KiB           \n\u2819 Preparing packages... (0/22)------------- 14.85 KiB/69.34 KiB         \n\u2819 Preparing packages... (0/22)------------- 30.85 KiB/69.34 KiB         \n\u2839 Preparing packages... (3/22)--------- 46.85 KiB/69.34 KiB         \n\u2839 Preparing packages... (3/22)--------- 46.85 KiB/69.34 KiB         \n\u2839 Preparing packages... (3/22)--------- 57.26 KiB/69.34 KiB         \n\u2839 Preparing packages... (3/22)--------- 57.26 KiB/69.34 KiB         \ntyping-inspection    ------------------------------     0 B/14.27 KiB\n\u2839 Preparing packages... (3/22)--------- 57.26 KiB/69.34 KiB         \ntyping-inspection    ------------------------------     0 B/14.27 KiB\n\u2839 Preparing packages... (3/22)--------- 57.26 KiB/69.34 KiB         \ntyping-inspection    ------------------------------     0 B/14.27 KiB\n\u2839 Preparing packages... (3/22)--------- 57.26 KiB/69.34 KiB         \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\n\u2839 Preparing packages... (3/22)-------------     0 B/388.03 KiB          \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\n\u2839 Preparing packages... (3/22)-------------     0 B/388.03 KiB          \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\n\u2839 Preparing packages... (3/22)-------------     0 B/388.03 KiB          \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\n\u2839 Preparing packages... (3/22)-------------     0 B/388.03 KiB          \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\nlangsmith            ------------------------------     0 B/388.03 KiB\n\u2839 Preparing packages... (3/22)-------------     0 B/456.17 KiB          \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\nlangsmith            ------------------------------     0 B/388.03 KiB\n\u2839 Preparing packages... (3/22)-------------     0 B/456.17 KiB          \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\nlangsmith            ------------------------------     0 B/388.03 KiB\n\u2839 Preparing packages... (3/22)-------------     0 B/456.17 KiB          \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\nlangsmith            ------------------------------     0 B/388.03 KiB\n\u2839 Preparing packages... (3/22)-------------     0 B/456.17 KiB          \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\nlangsmith            ------------------------------     0 B/388.03 KiB\n\u2839 Preparing packages... (3/22)-------------     0 B/456.17 KiB          \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\nanyio                ------------------------------     0 B/106.54 KiB\nlangsmith            ------------------------------     0 B/388.03 KiB\n\u2839 Preparing packages... (3/22)-------------     0 B/456.17 KiB          \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\nanyio                ------------------------------     0 B/106.54 KiB\nlangsmith            ------------------------------     0 B/388.03 KiB\n\u2839 Preparing packages... (3/22)-------------     0 B/456.17 KiB          \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nlanggraph-prebuilt   ------------------------------ 14.90 KiB/27.79 KiB\nxxhash               ------------------------------     0 B/30.10 KiB\nlanggraph-checkpoint ------------------------------     0 B/44.98 KiB\nlanggraph-sdk        ------------------------------ 14.91 KiB/55.42 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\nlangchain            ------------------------------     0 B/105.30 KiB\nanyio                ------------------------------     0 B/106.54 KiB\norjson               ------------------------------ 14.91 KiB/125.85 KiB\nlanggraph            ------------------------------     0 B/151.77 KiB\ncertifi              ------------------------------ 14.88 KiB/159.46 KiB\npyyaml               ------------------------------     0 B/169.19 KiB\ncharset-normalizer   ------------------------------     0 B/203.21 KiB\normsgpack            ------------------------------     0 B/359.87 KiB\nlangsmith            ------------------------------     0 B/388.03 KiB\npydantic             ------------------------------ 7.48 KiB/451.59 KiB\nlangchain-core       ------------------------------     0 B/456.17 KiB\nzstandard            ------------------------------     0 B/625.43 KiB\n\u2839 Preparing packages... (3/22)------------     0 B/1.82 MiB            \ntyping-inspection    ------------------------------     0 B/14.27 KiB\nlanggraph-prebuilt   ------------------------------ 14.90 KiB/27.79 KiB\nxxhash               ------------------------------     0 B/30.10 KiB\nlanggraph-checkpoint ------------------------------     0 B/44.98 KiB\nlanggraph-sdk        ------------------------------ 14.91 KiB/55.42 KiB\nidna                 ------------------------------ 57.26 KiB/69.34 KiB\nlangchain            ------------------------------ 14.85 KiB/105.30 KiB\nanyio                ------------------------------     0 B/106.54 KiB\norjson               ------------------------------ 14.91 KiB/125.85 KiB\nlanggraph            ------------------------------     0 B/151.77 KiB\ncertifi              ------------------------------ 14.88 KiB/159.46 KiB\npyyaml               ------------------------------     0 B/169.19 KiB\ncharset-normalizer   ------------------------------ 14.90 KiB/203.21 KiB\normsgpack            ------------------------------     0 B/359.87 KiB\nlangsmith            ------------------------------     0 B/388.03 KiB\npydantic             ------------------------------ 7.48 KiB/451.59 KiB\nlangchain-core       ------------------------------     0 B/456.17 KiB\nzstandard            ------------------------------     0 B/625.43 KiB\n\u2839 Preparing packages... (3/22)------------     0 B/1.82 MiB            \ntyping-inspection    ------------------------------ 8.77 KiB/14.27 KiB\nlanggraph-prebuilt   ------------------------------ 14.90 KiB/27.79 KiB\nxxhash               ------------------------------ 16.00 KiB/30.10 KiB\nlanggraph-checkpoint ------------------------------ 14.87 KiB/44.98 KiB\nlanggraph-sdk        ------------------------------ 14.91 KiB/55.42 KiB\nidna                 ------------------------------ 69.34 KiB/69.34 KiB\nlangchain            ------------------------------ 14.85 KiB/105.30 KiB\nanyio                ------------------------------ 16.00 KiB/106.54 KiB\norjson               ------------------------------ 14.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 16.00 KiB/151.77 KiB\ncertifi              ------------------------------ 14.88 KiB/159.46 KiB\npyyaml               ------------------------------ 16.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 14.90 KiB/203.21 KiB\normsgpack            ------------------------------ 16.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 16.00 KiB/388.03 KiB\npydantic             ------------------------------ 7.48 KiB/451.59 KiB\nlangchain-core       ------------------------------ 16.00 KiB/456.17 KiB\nzstandard            ------------------------------ 15.89 KiB/625.43 KiB\n\u2839 Preparing packages... (3/22)------------ 14.90 KiB/1.82 MiB          \ntyping-inspection    ------------------------------ 14.27 KiB/14.27 KiB\nlanggraph-prebuilt   ------------------------------ 14.90 KiB/27.79 KiB\nxxhash               ------------------------------ 16.00 KiB/30.10 KiB\nlanggraph-checkpoint ------------------------------ 14.87 KiB/44.98 KiB\nlanggraph-sdk        ------------------------------ 14.91 KiB/55.42 KiB\nlangchain            ------------------------------ 14.85 KiB/105.30 KiB\nanyio                ------------------------------ 16.00 KiB/106.54 KiB\norjson               ------------------------------ 14.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 16.00 KiB/151.77 KiB\ncertifi              ------------------------------ 30.88 KiB/159.46 KiB\npyyaml               ------------------------------ 16.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 14.90 KiB/203.21 KiB\normsgpack            ------------------------------ 16.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 16.00 KiB/388.03 KiB\npydantic             ------------------------------ 14.88 KiB/451.59 KiB\nlangchain-core       ------------------------------ 16.00 KiB/456.17 KiB\nzstandard            ------------------------------ 15.89 KiB/625.43 KiB\n\u2839 Preparing packages... (3/22)------------ 14.90 KiB/1.82 MiB          \nlanggraph-prebuilt   ------------------------------ 14.90 KiB/27.79 KiB\nxxhash               ------------------------------ 16.00 KiB/30.10 KiB\nlanggraph-checkpoint ------------------------------ 14.87 KiB/44.98 KiB\nlanggraph-sdk        ------------------------------ 14.91 KiB/55.42 KiB\nlangchain            ------------------------------ 14.85 KiB/105.30 KiB\nanyio                ------------------------------ 16.00 KiB/106.54 KiB\norjson               ------------------------------ 14.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 16.00 KiB/151.77 KiB\ncertifi              ------------------------------ 30.88 KiB/159.46 KiB\npyyaml               ------------------------------ 16.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 14.90 KiB/203.21 KiB\normsgpack            ------------------------------ 16.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 16.00 KiB/388.03 KiB\npydantic             ------------------------------ 14.88 KiB/451.59 KiB\nlangchain-core       ------------------------------ 16.00 KiB/456.17 KiB\nzstandard            ------------------------------ 15.89 KiB/625.43 KiB\n\u2839 Preparing packages... (3/22)------------ 14.90 KiB/1.82 MiB          \nxxhash               ------------------------------ 16.00 KiB/30.10 KiB\nlanggraph-checkpoint ------------------------------ 14.87 KiB/44.98 KiB\nlanggraph-sdk        ------------------------------ 30.91 KiB/55.42 KiB\nlangchain            ------------------------------ 14.85 KiB/105.30 KiB\nanyio                ------------------------------ 16.00 KiB/106.54 KiB\norjson               ------------------------------ 14.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 16.00 KiB/151.77 KiB\ncertifi              ------------------------------ 30.88 KiB/159.46 KiB\npyyaml               ------------------------------ 16.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 14.90 KiB/203.21 KiB\normsgpack            ------------------------------ 16.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 16.00 KiB/388.03 KiB\npydantic             ------------------------------ 14.88 KiB/451.59 KiB\nlangchain-core       ------------------------------ 16.00 KiB/456.17 KiB\nzstandard            ------------------------------ 15.89 KiB/625.43 KiB\n\u2838 Preparing packages... (6/22)------------ 14.90 KiB/1.82 MiB          \nxxhash               ------------------------------ 16.00 KiB/30.10 KiB\nlanggraph-checkpoint ------------------------------ 14.87 KiB/44.98 KiB\nlanggraph-sdk        ------------------------------ 30.91 KiB/55.42 KiB\nlangchain            ------------------------------ 14.85 KiB/105.30 KiB\nanyio                ------------------------------ 16.00 KiB/106.54 KiB\norjson               ------------------------------ 14.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 16.00 KiB/151.77 KiB\ncertifi              ------------------------------ 30.88 KiB/159.46 KiB\npyyaml               ------------------------------ 16.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 14.90 KiB/203.21 KiB\normsgpack            ------------------------------ 16.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 16.00 KiB/388.03 KiB\npydantic             ------------------------------ 14.88 KiB/451.59 KiB\nlangchain-core       ------------------------------ 16.00 KiB/456.17 KiB\nzstandard            ------------------------------ 15.89 KiB/625.43 KiB\n\u2838 Preparing packages... (6/22)------------ 14.90 KiB/1.82 MiB          \nxxhash               ------------------------------ 16.00 KiB/30.10 KiB\nlanggraph-checkpoint ------------------------------ 24.88 KiB/44.98 KiB\nlanggraph-sdk        ------------------------------ 30.91 KiB/55.42 KiB\nlangchain            ------------------------------ 30.85 KiB/105.30 KiB\nanyio                ------------------------------ 16.00 KiB/106.54 KiB\norjson               ------------------------------ 30.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 16.00 KiB/151.77 KiB\ncertifi              ------------------------------ 30.88 KiB/159.46 KiB\npyyaml               ------------------------------ 16.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 30.90 KiB/203.21 KiB\normsgpack            ------------------------------ 16.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 32.00 KiB/388.03 KiB\npydantic             ------------------------------ 14.88 KiB/451.59 KiB\nlangchain-core       ------------------------------ 32.00 KiB/456.17 KiB\nzstandard            ------------------------------ 15.89 KiB/625.43 KiB\n\u2838 Preparing packages... (6/22)------------ 30.90 KiB/1.82 MiB          \nlanggraph-checkpoint ------------------------------ 24.88 KiB/44.98 KiB\nlanggraph-sdk        ------------------------------ 30.91 KiB/55.42 KiB\nlangchain            ------------------------------ 30.85 KiB/105.30 KiB\nanyio                ------------------------------ 32.00 KiB/106.54 KiB\norjson               ------------------------------ 30.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 32.00 KiB/151.77 KiB\ncertifi              ------------------------------ 30.88 KiB/159.46 KiB\npyyaml               ------------------------------ 16.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 30.90 KiB/203.21 KiB\normsgpack            ------------------------------ 16.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 32.00 KiB/388.03 KiB\npydantic             ------------------------------ 14.88 KiB/451.59 KiB\nlangchain-core       ------------------------------ 32.00 KiB/456.17 KiB\nzstandard            ------------------------------ 15.89 KiB/625.43 KiB\n\u2838 Preparing packages... (6/22)------------ 30.90 KiB/1.82 MiB          \nlanggraph-checkpoint ------------------------------ 24.88 KiB/44.98 KiB\nlanggraph-sdk        ------------------------------ 30.91 KiB/55.42 KiB\nlangchain            ------------------------------ 30.85 KiB/105.30 KiB\nanyio                ------------------------------ 32.00 KiB/106.54 KiB\norjson               ------------------------------ 30.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 32.00 KiB/151.77 KiB\ncertifi              ------------------------------ 30.88 KiB/159.46 KiB\npyyaml               ------------------------------ 32.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 30.90 KiB/203.21 KiB\normsgpack            ------------------------------ 32.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 32.00 KiB/388.03 KiB\npydantic             ------------------------------ 14.88 KiB/451.59 KiB\nlangchain-core       ------------------------------ 32.00 KiB/456.17 KiB\nzstandard            ------------------------------ 15.89 KiB/625.43 KiB\n\u2838 Preparing packages... (6/22)------------ 30.90 KiB/1.82 MiB          \nlanggraph-checkpoint ------------------------------ 24.88 KiB/44.98 KiB\nlanggraph-sdk        ------------------------------ 46.91 KiB/55.42 KiB\nlangchain            ------------------------------ 46.85 KiB/105.30 KiB\nanyio                ------------------------------ 32.00 KiB/106.54 KiB\norjson               ------------------------------ 46.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 32.00 KiB/151.77 KiB\ncertifi              ------------------------------ 46.88 KiB/159.46 KiB\npyyaml               ------------------------------ 32.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 46.90 KiB/203.21 KiB\normsgpack            ------------------------------ 32.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 32.00 KiB/388.03 KiB\npydantic             ------------------------------ 14.88 KiB/451.59 KiB\nlangchain-core       ------------------------------ 32.00 KiB/456.17 KiB\nzstandard            ------------------------------ 17.68 KiB/625.43 KiB\n\u2838 Preparing packages... (6/22)------------ 30.90 KiB/1.82 MiB          \nlanggraph-checkpoint ------------------------------ 40.88 KiB/44.98 KiB\nlanggraph-sdk        ------------------------------ 46.91 KiB/55.42 KiB\nlangchain            ------------------------------ 46.85 KiB/105.30 KiB\nanyio                ------------------------------ 48.00 KiB/106.54 KiB\norjson               ------------------------------ 46.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 48.00 KiB/151.77 KiB\ncertifi              ------------------------------ 46.88 KiB/159.46 KiB\npyyaml               ------------------------------ 48.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 46.90 KiB/203.21 KiB\normsgpack            ------------------------------ 48.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 48.00 KiB/388.03 KiB\npydantic             ------------------------------ 30.88 KiB/451.59 KiB\nlangchain-core       ------------------------------ 48.00 KiB/456.17 KiB\nzstandard            ------------------------------ 33.68 KiB/625.43 KiB\n\u283c Preparing packages... (7/22)------------ 46.80 KiB/1.82 MiB          \nlanggraph-checkpoint ------------------------------ 40.88 KiB/44.98 KiB\nlangchain            ------------------------------ 46.85 KiB/105.30 KiB\nanyio                ------------------------------ 48.00 KiB/106.54 KiB\norjson               ------------------------------ 46.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 48.00 KiB/151.77 KiB\ncertifi              ------------------------------ 62.77 KiB/159.46 KiB\npyyaml               ------------------------------ 48.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 46.90 KiB/203.21 KiB\normsgpack            ------------------------------ 48.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 48.00 KiB/388.03 KiB\npydantic             ------------------------------ 30.88 KiB/451.59 KiB\nlangchain-core       ------------------------------ 48.00 KiB/456.17 KiB\nzstandard            ------------------------------ 33.68 KiB/625.43 KiB\n\u283c Preparing packages... (7/22)------------ 46.80 KiB/1.82 MiB          \nlangchain            ------------------------------ 62.85 KiB/105.30 KiB\nanyio                ------------------------------ 48.00 KiB/106.54 KiB\norjson               ------------------------------ 62.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 48.00 KiB/151.77 KiB\ncertifi              ------------------------------ 62.77 KiB/159.46 KiB\npyyaml               ------------------------------ 48.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 62.90 KiB/203.21 KiB\normsgpack            ------------------------------ 48.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 48.00 KiB/388.03 KiB\npydantic             ------------------------------ 30.88 KiB/451.59 KiB\nlangchain-core       ------------------------------ 48.00 KiB/456.17 KiB\nzstandard            ------------------------------ 33.68 KiB/625.43 KiB\n\u283c Preparing packages... (7/22)------------ 46.80 KiB/1.82 MiB          \nlangchain            ------------------------------ 62.85 KiB/105.30 KiB\nanyio                ------------------------------ 48.00 KiB/106.54 KiB\norjson               ------------------------------ 62.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 48.00 KiB/151.77 KiB\ncertifi              ------------------------------ 62.77 KiB/159.46 KiB\npyyaml               ------------------------------ 48.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 62.90 KiB/203.21 KiB\normsgpack            ------------------------------ 48.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 48.00 KiB/388.03 KiB\npydantic             ------------------------------ 46.88 KiB/451.59 KiB\nlangchain-core       ------------------------------ 48.00 KiB/456.17 KiB\nzstandard            ------------------------------ 33.68 KiB/625.43 KiB\n\u283c Preparing packages... (7/22)------------ 62.80 KiB/1.82 MiB          \nlangchain            ------------------------------ 78.85 KiB/105.30 KiB\nanyio                ------------------------------ 64.00 KiB/106.54 KiB\norjson               ------------------------------ 78.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 79.48 KiB/151.77 KiB\ncertifi              ------------------------------ 77.49 KiB/159.46 KiB\npyyaml               ------------------------------ 64.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 78.90 KiB/203.21 KiB\normsgpack            ------------------------------ 64.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 77.49 KiB/388.03 KiB\npydantic             ------------------------------ 66.25 KiB/451.59 KiB\nlangchain-core       ------------------------------ 76.18 KiB/456.17 KiB\nzstandard            ------------------------------ 48.00 KiB/625.43 KiB\n\u283c Preparing packages... (7/22)------------ 78.80 KiB/1.82 MiB          \nlangchain            ------------------------------ 94.85 KiB/105.30 KiB\nanyio                ------------------------------ 64.00 KiB/106.54 KiB\norjson               ------------------------------ 94.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 79.48 KiB/151.77 KiB\ncertifi              ------------------------------ 93.49 KiB/159.46 KiB\npyyaml               ------------------------------ 80.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 94.90 KiB/203.21 KiB\normsgpack            ------------------------------ 80.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 77.49 KiB/388.03 KiB\npydantic             ------------------------------ 66.25 KiB/451.59 KiB\nlangchain-core       ------------------------------ 76.18 KiB/456.17 KiB\nzstandard            ------------------------------ 57.47 KiB/625.43 KiB\n\u283c Preparing packages... (7/22)------------ 78.80 KiB/1.82 MiB          \nlangchain            ------------------------------ 94.85 KiB/105.30 KiB\nanyio                ------------------------------ 80.00 KiB/106.54 KiB\norjson               ------------------------------ 110.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 95.48 KiB/151.77 KiB\ncertifi              ------------------------------ 109.49 KiB/159.46 KiB\npyyaml               ------------------------------ 96.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 94.90 KiB/203.21 KiB\normsgpack            ------------------------------ 83.62 KiB/359.87 KiB\nlangsmith            ------------------------------ 93.49 KiB/388.03 KiB\npydantic             ------------------------------ 78.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 92.18 KiB/456.17 KiB\nzstandard            ------------------------------ 73.47 KiB/625.43 KiB\n\u283c Preparing packages... (7/22)------------ 78.90 KiB/1.82 MiB          \nanyio                ------------------------------ 80.00 KiB/106.54 KiB\norjson               ------------------------------ 110.91 KiB/125.85 KiB\nlanggraph            ------------------------------ 95.48 KiB/151.77 KiB\ncertifi              ------------------------------ 109.49 KiB/159.46 KiB\npyyaml               ------------------------------ 96.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 110.90 KiB/203.21 KiB\normsgpack            ------------------------------ 83.62 KiB/359.87 KiB\nlangsmith            ------------------------------ 109.49 KiB/388.03 KiB\npydantic             ------------------------------ 78.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 97.61 KiB/456.17 KiB\nzstandard            ------------------------------ 73.47 KiB/625.43 KiB\n\u2834 Preparing packages... (10/22)----------- 78.90 KiB/1.82 MiB          \nanyio                ------------------------------ 96.00 KiB/106.54 KiB\norjson               ------------------------------ 125.85 KiB/125.85 KiB\nlanggraph            ------------------------------ 111.48 KiB/151.77 KiB\ncertifi              ------------------------------ 125.38 KiB/159.46 KiB\npyyaml               ------------------------------ 112.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 110.90 KiB/203.21 KiB\normsgpack            ------------------------------ 99.62 KiB/359.87 KiB\nlangsmith            ------------------------------ 109.49 KiB/388.03 KiB\npydantic             ------------------------------ 94.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 97.61 KiB/456.17 KiB\nzstandard            ------------------------------ 89.47 KiB/625.43 KiB\n\u2834 Preparing packages... (10/22)----------- 94.90 KiB/1.82 MiB          \nanyio                ------------------------------ 96.00 KiB/106.54 KiB\nlanggraph            ------------------------------ 111.48 KiB/151.77 KiB\ncertifi              ------------------------------ 125.38 KiB/159.46 KiB\npyyaml               ------------------------------ 112.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 110.90 KiB/203.21 KiB\normsgpack            ------------------------------ 99.62 KiB/359.87 KiB\nlangsmith            ------------------------------ 109.49 KiB/388.03 KiB\npydantic             ------------------------------ 94.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 97.61 KiB/456.17 KiB\nzstandard            ------------------------------ 89.47 KiB/625.43 KiB\n\u2834 Preparing packages... (10/22)----------- 94.90 KiB/1.82 MiB          \nanyio                ------------------------------ 96.00 KiB/106.54 KiB\nlanggraph            ------------------------------ 127.48 KiB/151.77 KiB\ncertifi              ------------------------------ 125.38 KiB/159.46 KiB\npyyaml               ------------------------------ 112.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 126.90 KiB/203.21 KiB\normsgpack            ------------------------------ 99.62 KiB/359.87 KiB\nlangsmith            ------------------------------ 125.49 KiB/388.03 KiB\npydantic             ------------------------------ 94.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 113.61 KiB/456.17 KiB\nzstandard            ------------------------------ 89.47 KiB/625.43 KiB\n\u2834 Preparing packages... (10/22)----------- 94.90 KiB/1.82 MiB          \nlanggraph            ------------------------------ 127.48 KiB/151.77 KiB\ncertifi              ------------------------------ 141.38 KiB/159.46 KiB\npyyaml               ------------------------------ 112.95 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 142.90 KiB/203.21 KiB\normsgpack            ------------------------------ 128.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 125.49 KiB/388.03 KiB\npydantic             ------------------------------ 110.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 124.18 KiB/456.17 KiB\nzstandard            ------------------------------ 105.47 KiB/625.43 KiB\n\u2834 Preparing packages... (10/22)----------- 126.90 KiB/1.82 MiB         \nlanggraph            ------------------------------ 143.48 KiB/151.77 KiB\ncertifi              ------------------------------ 141.49 KiB/159.46 KiB\npyyaml               ------------------------------ 128.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 158.90 KiB/203.21 KiB\normsgpack            ------------------------------ 144.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 141.49 KiB/388.03 KiB\npydantic             ------------------------------ 142.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 140.18 KiB/456.17 KiB\nzstandard            ------------------------------ 112.00 KiB/625.43 KiB\n\u2834 Preparing packages... (10/22)----------- 142.90 KiB/1.82 MiB         \ncertifi              ------------------------------ 141.49 KiB/159.46 KiB\npyyaml               ------------------------------ 128.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 158.90 KiB/203.21 KiB\normsgpack            ------------------------------ 144.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 150.07 KiB/388.03 KiB\npydantic             ------------------------------ 142.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 140.18 KiB/456.17 KiB\nzstandard            ------------------------------ 112.00 KiB/625.43 KiB\n\u2834 Preparing packages... (10/22)------------ 142.90 KiB/1.82 MiB         \ncertifi              ------------------------------ 141.49 KiB/159.46 KiB\npyyaml               ------------------------------ 128.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 174.90 KiB/203.21 KiB\normsgpack            ------------------------------ 160.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 150.07 KiB/388.03 KiB\npydantic             ------------------------------ 142.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 156.18 KiB/456.17 KiB\nzstandard            ------------------------------ 112.00 KiB/625.43 KiB\n\u2826 Preparing packages... (13/22)------------ 142.90 KiB/1.82 MiB         \npyyaml               ------------------------------ 144.00 KiB/169.19 KiB\ncharset-normalizer   ------------------------------ 190.90 KiB/203.21 KiB\normsgpack            ------------------------------ 176.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 182.07 KiB/388.03 KiB\npydantic             ------------------------------ 149.99 KiB/451.59 KiB\nlangchain-core       ------------------------------ 172.18 KiB/456.17 KiB\nzstandard            ------------------------------ 144.00 KiB/625.43 KiB\n\u2826 Preparing packages... (13/22)------------ 158.90 KiB/1.82 MiB         \npyyaml               ------------------------------ 160.00 KiB/169.19 KiB\normsgpack            ------------------------------ 192.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 189.49 KiB/388.03 KiB\npydantic             ------------------------------ 163.90 KiB/451.59 KiB\nlangchain-core       ------------------------------ 188.18 KiB/456.17 KiB\nzstandard            ------------------------------ 160.00 KiB/625.43 KiB\n\u2826 Preparing packages... (13/22)------------ 174.90 KiB/1.82 MiB         \npyyaml               ------------------------------ 160.00 KiB/169.19 KiB\normsgpack            ------------------------------ 192.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 189.49 KiB/388.03 KiB\npydantic             ------------------------------ 163.90 KiB/451.59 KiB\nlangchain-core       ------------------------------ 188.18 KiB/456.17 KiB\nzstandard            ------------------------------ 160.00 KiB/625.43 KiB\n\u2826 Preparing packages... (13/22)------------ 190.90 KiB/1.82 MiB         \normsgpack            ------------------------------ 192.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 189.49 KiB/388.03 KiB\npydantic             ------------------------------ 179.90 KiB/451.59 KiB\nlangchain-core       ------------------------------ 188.18 KiB/456.17 KiB\nzstandard            ------------------------------ 162.95 KiB/625.43 KiB\n\u2826 Preparing packages... (13/22)------------ 190.90 KiB/1.82 MiB         \normsgpack            ------------------------------ 192.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 189.49 KiB/388.03 KiB\npydantic             ------------------------------ 195.90 KiB/451.59 KiB\nlangchain-core       ------------------------------ 188.18 KiB/456.17 KiB\nzstandard            ------------------------------ 162.95 KiB/625.43 KiB\n\u2826 Preparing packages... (13/22)------------ 206.90 KiB/1.82 MiB         \normsgpack            ------------------------------ 216.35 KiB/359.87 KiB\nlangsmith            ------------------------------ 221.49 KiB/388.03 KiB\npydantic             ------------------------------ 206.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 220.18 KiB/456.17 KiB\nzstandard            ------------------------------ 176.00 KiB/625.43 KiB\n\u2827 Preparing packages... (16/22)------------ 206.90 KiB/1.82 MiB         \normsgpack            ------------------------------ 232.35 KiB/359.87 KiB\nlangsmith            ------------------------------ 221.49 KiB/388.03 KiB\npydantic             ------------------------------ 222.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 236.18 KiB/456.17 KiB\nzstandard            ------------------------------ 176.00 KiB/625.43 KiB\n\u2827 Preparing packages... (16/22)------------ 222.90 KiB/1.82 MiB         \normsgpack            ------------------------------ 248.35 KiB/359.87 KiB\nlangsmith            ------------------------------ 253.49 KiB/388.03 KiB\npydantic             ------------------------------ 238.29 KiB/451.59 KiB\nlangchain-core       ------------------------------ 252.18 KiB/456.17 KiB\nzstandard            ------------------------------ 192.00 KiB/625.43 KiB\n\u2827 Preparing packages... (16/22)------------ 238.90 KiB/1.82 MiB         \normsgpack            ------------------------------ 256.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 253.49 KiB/388.03 KiB\npydantic             ------------------------------ 270.29 KiB/451.59 KiB\nlangchain-core       ------------------------------ 252.18 KiB/456.17 KiB\nzstandard            ------------------------------ 216.23 KiB/625.43 KiB\n\u2827 Preparing packages... (16/22)------------ 254.90 KiB/1.82 MiB         \normsgpack            ------------------------------ 272.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 269.49 KiB/388.03 KiB\npydantic             ------------------------------ 270.29 KiB/451.59 KiB\nlangchain-core       ------------------------------ 268.18 KiB/456.17 KiB\nzstandard            ------------------------------ 232.23 KiB/625.43 KiB\n\u2827 Preparing packages... (16/22)------------ 270.90 KiB/1.82 MiB         \normsgpack            ------------------------------ 288.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 293.49 KiB/388.03 KiB\npydantic             ------------------------------ 286.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 300.18 KiB/456.17 KiB\nzstandard            ------------------------------ 256.00 KiB/625.43 KiB\n\u2807 Preparing packages... (16/22)------------ 286.90 KiB/1.82 MiB         \normsgpack            ------------------------------ 304.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 309.49 KiB/388.03 KiB\npydantic             ------------------------------ 302.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 316.08 KiB/456.17 KiB\nzstandard            ------------------------------ 272.00 KiB/625.43 KiB\n\u2807 Preparing packages... (16/22)------------ 302.90 KiB/1.82 MiB         \normsgpack            ------------------------------ 336.00 KiB/359.87 KiB\nlangsmith            ------------------------------ 317.49 KiB/388.03 KiB\npydantic             ------------------------------ 334.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 316.18 KiB/456.17 KiB\nzstandard            ------------------------------ 295.77 KiB/625.43 KiB\n\u2807 Preparing packages... (16/22)------------ 334.90 KiB/1.82 MiB         \nlangsmith            ------------------------------ 333.49 KiB/388.03 KiB\npydantic             ------------------------------ 334.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 332.18 KiB/456.17 KiB\nzstandard            ------------------------------ 304.00 KiB/625.43 KiB\n\u2807 Preparing packages... (16/22)------------ 334.90 KiB/1.82 MiB         \nlangsmith            ------------------------------ 333.49 KiB/388.03 KiB\npydantic             ------------------------------ 350.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 348.18 KiB/456.17 KiB\nzstandard            ------------------------------ 304.00 KiB/625.43 KiB\n\u2807 Preparing packages... (16/22)------------ 350.90 KiB/1.82 MiB         \nlangsmith            ------------------------------ 365.49 KiB/388.03 KiB\npydantic             ------------------------------ 366.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 363.86 KiB/456.17 KiB\nzstandard            ------------------------------ 336.00 KiB/625.43 KiB\n\u280b Preparing packages... (17/22)------------ 382.90 KiB/1.82 MiB         \npydantic             ------------------------------ 398.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 395.86 KiB/456.17 KiB\nzstandard            ------------------------------ 352.00 KiB/625.43 KiB\n\u280b Preparing packages... (17/22)------------ 398.80 KiB/1.82 MiB         \npydantic             ------------------------------ 398.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 395.86 KiB/456.17 KiB\nzstandard            ------------------------------ 368.00 KiB/625.43 KiB\n\u280b Preparing packages... (17/22)------------ 398.80 KiB/1.82 MiB         \npydantic             ------------------------------ 430.40 KiB/451.59 KiB\nlangchain-core       ------------------------------ 412.18 KiB/456.17 KiB\nzstandard            ------------------------------ 400.00 KiB/625.43 KiB\n\u280b Preparing packages... (17/22)------------ 414.90 KiB/1.82 MiB         \nlangchain-core       ------------------------------ 444.08 KiB/456.17 KiB\nzstandard            ------------------------------ 416.00 KiB/625.43 KiB\n\u280b Preparing packages... (17/22)------------ 446.90 KiB/1.82 MiB         \nlangchain-core       ------------------------------ 444.08 KiB/456.17 KiB\nzstandard            ------------------------------ 432.00 KiB/625.43 KiB\n\u280b Preparing packages... (17/22)------------ 462.90 KiB/1.82 MiB         \nzstandard            ------------------------------ 432.00 KiB/625.43 KiB\n\u280b Preparing packages... (17/22)------------ 462.90 KiB/1.82 MiB         \nzstandard            ------------------------------ 496.00 KiB/625.43 KiB\n\u2819 Preparing packages... (20/22)------------ 510.90 KiB/1.82 MiB         \nzstandard            ------------------------------ 592.00 KiB/625.43 KiB\n\u2819 Preparing packages... (20/22)------------ 558.90 KiB/1.82 MiB         \n\u2819 Preparing packages... (20/22)------------ 654.90 KiB/1.82 MiB         \n\u2819 Preparing packages... (20/22)------------ 686.90 KiB/1.82 MiB         \n\u2819 Preparing packages... (20/22)------------ 814.90 KiB/1.82 MiB         \n\u2839 Preparing packages... (21/22)------------ 990.90 KiB/1.82 MiB         \n\u2839 Preparing packages... (21/22)------------ 1.09 MiB/1.82 MiB           \n\u2839 Preparing packages... (21/22)[2m--------- 1.22 MiB/1.82 MiB           \n\u2839 Preparing packages... (21/22)-------- 1.40 MiB/1.82 MiB           \n\u2838 Preparing packages... (21/22)-------- 1.54 MiB/1.82 MiB           \nPrepared 22 packages in 2.28s                                                \nUninstalled 9 packages in 39ms\nInstalled 22 packages in 16ms                               \n - anyio==4.10.0\n + anyio==4.11.0\n - certifi==2025.8.3\n + certifi==2025.10.5\n - charset-normalizer==3.4.3\n + charset-normalizer==3.4.4\n - idna==3.10\n + idna==3.11\n + jsonpatch==1.33\n + langchain==1.0.2\n + langchain-core==1.0.1\n + langgraph==1.0.1\n + langgraph-checkpoint==3.0.0\n + langgraph-prebuilt==1.0.1\n + langgraph-sdk==0.2.9\n + langsmith==0.4.38\n + orjson==3.11.4\n + ormsgpack==1.11.0\n - pydantic==2.11.7\n + pydantic==2.12.3\n - pydantic-core==2.33.2\n + pydantic-core==2.41.4\n - pyyaml==6.0.2\n + pyyaml==6.0.3\n + requests-toolbelt==1.0.0\n + tenacity==9.1.2\n - typing-inspection==0.4.1\n + typing-inspection==0.4.2\n - xxhash==3.5.0\n + xxhash==3.6.0\n + zstandard==0.25.0\nUsing Python 3.13.5 environment at: /Users/forrest/GitHub/DNALLM/.venv\nResolved 40 packages in 1.76s                                        \n\u2819 Preparing packages... (0/12)                                                  \n\u2819 Preparing packages... (0/12)------------------     0 B/18.00 KiB      \n\u2819 Preparing packages... (0/12)-------------- 14.86 KiB/18.00 KiB    \n\u2819 Preparing packages... (0/12)-------------- 14.86 KiB/18.00 KiB    \nhttpx-sse                 ------------------------------     0 B/8.75 KiB\n\u2819 Preparing packages... (0/12)-------------- 14.86 KiB/18.00 KiB    \nhttpx-sse                 ------------------------------ 8.75 KiB/8.75 KiB\n\u2819 Preparing packages... (0/12)-------------- 14.86 KiB/18.00 KiB    \nhttpx-sse                 ------------------------------ 8.75 KiB/8.75 KiB\n\u2819 Preparing packages... (0/12)-------------- 14.86 KiB/18.00 KiB    \n\u2819 Preparing packages... (0/12)-------------- 14.86 KiB/18.00 KiB    \n\u2819 Preparing packages... (0/12)-------------- 18.00 KiB/18.00 KiB    \n\u2819 Preparing packages... (0/12)-------------- 18.00 KiB/18.00 KiB    \njsonschema-specifications ------------------------------ 18.00 KiB/18.00 KiB\n\u2819 Preparing packages... (0/12)------------------     0 B/47.47 KiB      \njsonschema-specifications ------------------------------ 18.00 KiB/18.00 KiB\n\u2819 Preparing packages... (0/12)------------------     0 B/47.47 KiB      \njsonschema-specifications ------------------------------ 18.00 KiB/18.00 KiB\n\u2819 Preparing packages... (0/12)------------------     0 B/47.47 KiB      \njsonschema-specifications ------------------------------ 18.00 KiB/18.00 KiB\npydantic-settings         ------------------------------     0 B/47.47 KiB\n\u2819 Preparing packages... (0/12)------------------     0 B/72.44 KiB      \njsonschema-specifications ------------------------------ 18.00 KiB/18.00 KiB\npydantic-settings         ------------------------------     0 B/47.47 KiB\n\u2819 Preparing packages... (0/12)------------------     0 B/72.44 KiB      \npydantic-settings         ------------------------------     0 B/47.47 KiB\n\u2819 Preparing packages... (0/12)------------------     0 B/72.44 KiB      \npydantic-settings         ------------------------------ 14.90 KiB/47.47 KiB\n\u2819 Preparing packages... (0/12)------------------     0 B/72.44 KiB      \npydantic-settings         ------------------------------ 14.90 KiB/47.47 KiB\n\u2839 Preparing packages... (2/12)------------------ 14.88 KiB/72.44 KiB    \npydantic-settings         ------------------------------ 14.90 KiB/47.47 KiB\n\u2839 Preparing packages... (2/12)------------------ 14.88 KiB/72.44 KiB    \npydantic-settings         ------------------------------ 30.90 KiB/47.47 KiB\n\u2839 Preparing packages... (2/12)------------------ 14.88 KiB/72.44 KiB    \npydantic-settings         ------------------------------ 30.90 KiB/47.47 KiB\n\u2839 Preparing packages... (2/12)m----------------- 30.88 KiB/72.44 KiB    \npydantic-settings         ------------------------------ 38.81 KiB/47.47 KiB\n\u2839 Preparing packages... (2/12)m----------------- 30.88 KiB/72.44 KiB    \npydantic-settings         ------------------------------ 38.81 KiB/47.47 KiB\n\u2839 Preparing packages... (2/12)m----------------- 30.88 KiB/72.44 KiB    \npydantic-settings         ------------------------------ 38.81 KiB/47.47 KiB\n\u2839 Preparing packages... (2/12)m----------------- 30.88 KiB/72.44 KiB    \nlangchain-mcp-adapters    ------------------------------     0 B/15.38 KiB\npython-dotenv             ------------------------------     0 B/20.73 KiB\nreferencing               ------------------------------     0 B/26.14 KiB\npydantic-settings         ------------------------------ 38.81 KiB/47.47 KiB\nattrs                     ------------------------------     0 B/66.03 KiB\nuvicorn                   ------------------------------ 14.85 KiB/66.51 KiB\nstarlette                 ------------------------------ 46.88 KiB/72.44 KiB\nclick                     ------------------------------     0 B/104.78 KiB\nmcp                       ------------------------------     0 B/166.12 KiB\n\u2839 Preparing packages... (2/12)-----------------     0 B/340.08 KiB     \nlangchain-mcp-adapters    ------------------------------     0 B/15.38 KiB\npython-dotenv             ------------------------------     0 B/20.73 KiB\nreferencing               ------------------------------ 14.88 KiB/26.14 KiB\npydantic-settings         ------------------------------ 38.81 KiB/47.47 KiB\nattrs                     ------------------------------ 14.88 KiB/66.03 KiB\nuvicorn                   ------------------------------ 14.85 KiB/66.51 KiB\nstarlette                 ------------------------------ 46.88 KiB/72.44 KiB\nclick                     ------------------------------ 14.86 KiB/104.78 KiB\nmcp                       ------------------------------ 14.92 KiB/166.12 KiB\n\u2839 Preparing packages... (2/12)-----------------     0 B/340.08 KiB     \npython-dotenv             ------------------------------ 16.00 KiB/20.73 KiB\nreferencing               ------------------------------ 14.88 KiB/26.14 KiB\npydantic-settings         ------------------------------ 38.81 KiB/47.47 KiB\nattrs                     ------------------------------ 14.88 KiB/66.03 KiB\nuvicorn                   ------------------------------ 14.85 KiB/66.51 KiB\nstarlette                 ------------------------------ 46.88 KiB/72.44 KiB\nclick                     ------------------------------ 14.86 KiB/104.78 KiB\nmcp                       ------------------------------ 14.92 KiB/166.12 KiB\n\u2839 Preparing packages... (2/12)------------------ 2.39 KiB/340.08 KiB    \npython-dotenv             ------------------------------ 16.00 KiB/20.73 KiB\nreferencing               ------------------------------ 14.88 KiB/26.14 KiB\nattrs                     ------------------------------ 14.88 KiB/66.03 KiB\nuvicorn                   ------------------------------ 14.85 KiB/66.51 KiB\nstarlette                 ------------------------------ 46.88 KiB/72.44 KiB\nclick                     ------------------------------ 14.86 KiB/104.78 KiB\nmcp                       ------------------------------ 14.92 KiB/166.12 KiB\n\u2839 Preparing packages... (2/12)------------------ 2.39 KiB/340.08 KiB    \nreferencing               ------------------------------ 14.88 KiB/26.14 KiB\nattrs                     ------------------------------ 14.88 KiB/66.03 KiB\nuvicorn                   ------------------------------ 14.85 KiB/66.51 KiB\nstarlette                 ------------------------------ 62.88 KiB/72.44 KiB\nclick                     ------------------------------ 14.86 KiB/104.78 KiB\nmcp                       ------------------------------ 14.92 KiB/166.12 KiB\n\u2839 Preparing packages... (2/12)------------------ 2.39 KiB/340.08 KiB    \nreferencing               ------------------------------ 26.14 KiB/26.14 KiB\nattrs                     ------------------------------ 30.88 KiB/66.03 KiB\nuvicorn                   ------------------------------ 30.85 KiB/66.51 KiB\nstarlette                 ------------------------------ 62.88 KiB/72.44 KiB\nclick                     ------------------------------ 14.86 KiB/104.78 KiB\nmcp                       ------------------------------ 14.92 KiB/166.12 KiB\n\u2839 Preparing packages... (2/12)------------------ 18.39 KiB/340.08 KiB   \nattrs                     ------------------------------ 30.88 KiB/66.03 KiB\nuvicorn                   ------------------------------ 30.85 KiB/66.51 KiB\nstarlette                 ------------------------------ 62.88 KiB/72.44 KiB\nclick                     ------------------------------ 14.86 KiB/104.78 KiB\nmcp                       ------------------------------ 14.92 KiB/166.12 KiB\n\u2839 Preparing packages... (2/12)------------------ 18.39 KiB/340.08 KiB   \nattrs                     ------------------------------ 46.88 KiB/66.03 KiB\nuvicorn                   ------------------------------ 46.85 KiB/66.51 KiB\nclick                     ------------------------------ 46.86 KiB/104.78 KiB\nmcp                       ------------------------------ 46.92 KiB/166.12 KiB\n\u2839 Preparing packages... (2/12)------------------ 34.39 KiB/340.08 KiB   \nattrs                     ------------------------------ 60.44 KiB/66.03 KiB\nuvicorn                   ------------------------------ 62.85 KiB/66.51 KiB\nclick                     ------------------------------ 62.86 KiB/104.78 KiB\nmcp                       ------------------------------ 62.92 KiB/166.12 KiB\n\u2839 Preparing packages... (2/12)------------------ 50.39 KiB/340.08 KiB   \nattrs                     ------------------------------ 60.44 KiB/66.03 KiB\nclick                     ------------------------------ 62.86 KiB/104.78 KiB\nmcp                       ------------------------------ 62.92 KiB/166.12 KiB\n\u2838 Preparing packages... (8/12)------------------ 60.11 KiB/340.08 KiB   \nclick                     ------------------------------ 78.86 KiB/104.78 KiB\nmcp                       ------------------------------ 76.98 KiB/166.12 KiB\n\u2838 Preparing packages... (8/12)------------------ 60.11 KiB/340.08 KiB   \nmcp                       ------------------------------ 108.98 KiB/166.12 KiB\n\u2838 Preparing packages... (8/12)------------------ 108.11 KiB/340.08 KiB  \nmcp                       ------------------------------ 124.98 KiB/166.12 KiB\n\u2838 Preparing packages... (8/12)------------------ 108.11 KiB/340.08 KiB  \n\u2838 Preparing packages... (8/12)-------------- 188.11 KiB/340.08 KiB  \nPrepared 12 packages in 520ms                                                \nUninstalled 11 packages in 68ms\nInstalled 12 packages in 7ms                                \n - attrs==25.3.0\n + attrs==25.4.0\n - click==8.2.1\n + click==8.3.0\n - httpx-sse==0.4.1\n + httpx-sse==0.4.3\n - jsonschema-specifications==2025.4.1\n + jsonschema-specifications==2025.9.1\n + langchain-mcp-adapters==0.1.11\n - mcp==1.13.1\n + mcp==1.19.0\n - pydantic-settings==2.10.1\n + pydantic-settings==2.11.0\n - python-dotenv==1.1.1\n + python-dotenv==1.2.1\n - referencing==0.36.2\n + referencing==0.37.0\n - rpds-py==0.27.1\n + rpds-py==0.28.0\n - starlette==0.47.3\n + starlette==0.49.1\n - uvicorn==0.35.0\n + uvicorn==0.38.0\nUsing Python 3.13.5 environment at: /Users/forrest/GitHub/DNALLM/.venv\nResolved 27 packages in 1.22s                                        \n\u2819 Preparing packages... (0/2)                                                   \n\u2819 Preparing packages... (0/2)--------------     0 B/28.33 KiB           \n\u2819 Preparing packages... (0/2)--------------     0 B/28.33 KiB           \nollama               ------------------------------     0 B/13.80 KiB\n\u2819 Preparing packages... (0/2)--------------     0 B/28.33 KiB           \nollama               ------------------------------     0 B/13.80 KiB\n\u2819 Preparing packages... (0/2)-------------- 13.98 KiB/28.33 KiB         \nollama               ------------------------------ 13.80 KiB/13.80 KiB\n\u2819 Preparing packages... (0/2)-------------- 13.98 KiB/28.33 KiB         \nollama               ------------------------------ 13.80 KiB/13.80 KiB\n\u2819 Preparing packages... (0/2)-------------- 13.98 KiB/28.33 KiB         \n\u2819 Preparing packages... (0/2)-------------- 13.98 KiB/28.33 KiB         \n\u2819 Preparing packages... (0/2)---------- 28.33 KiB/28.33 KiB         \nPrepared 2 packages in 178ms                                                 \nUninstalled 1 package in 2ms\nInstalled 2 packages in 2ms                                 \n + langchain-ollama==1.0.0\n - ollama==0.5.3\n + ollama==0.6.0\n</pre> In\u00a0[\u00a0]: Copied! <pre># Start the DNALLM MCP server\n# This command starts the DNALLM MCP server with streamable HTTP transport\n# The server will be available at http://localhost:8000/mcp\n# Note: This cell should be run before the client connection cells\n\n!dnallm mcp-server --transport streamable-http\n</pre> # Start the DNALLM MCP server # This command starts the DNALLM MCP server with streamable HTTP transport # The server will be available at http://localhost:8000/mcp # Note: This cell should be run before the client connection cells  !dnallm mcp-server --transport streamable-http In\u00a0[\u00a0]: Copied! <pre># Configure asyncio for Jupyter notebook compatibility\n# nest_asyncio allows nested event loops, which is necessary for running async code in Jupyter\nimport nest_asyncio\nimport asyncio\nimport time\n\n# Apply nest_asyncio to enable nested event loops\nnest_asyncio.apply()\n</pre> # Configure asyncio for Jupyter notebook compatibility # nest_asyncio allows nested event loops, which is necessary for running async code in Jupyter import nest_asyncio import asyncio import time  # Apply nest_asyncio to enable nested event loops nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre># Initialize MCP client to connect to DNALLM server\n# This creates a connection to the DNALLM MCP server running on localhost:8000\nfrom langchain_mcp_adapters.client import MultiServerMCPClient  \nfrom langchain.agents import create_agent\n\n# Create MCP client with DNALLM server configuration\n# The server should be running on port 8000 with streamable HTTP transport\nclient = MultiServerMCPClient(  \n    {\n        \"dnallm\": {\n            \"transport\": \"streamable_http\",  # HTTP-based remote server transport\n            \"url\": \"http://localhost:8000/mcp\",  # DNALLM MCP server endpoint\n        }\n    }\n)\n</pre> # Initialize MCP client to connect to DNALLM server # This creates a connection to the DNALLM MCP server running on localhost:8000 from langchain_mcp_adapters.client import MultiServerMCPClient   from langchain.agents import create_agent  # Create MCP client with DNALLM server configuration # The server should be running on port 8000 with streamable HTTP transport client = MultiServerMCPClient(       {         \"dnallm\": {             \"transport\": \"streamable_http\",  # HTTP-based remote server transport             \"url\": \"http://localhost:8000/mcp\",  # DNALLM MCP server endpoint         }     } ) In\u00a0[\u00a0]: Copied! <pre># Create LangChain agent with MCP tools\n# This retrieves available tools from the DNALLM MCP server and creates a LangChain agent\n# that can use these tools for DNA sequence analysis\n\n# Get available tools from the MCP server\ntools = await client.get_tools()  \n\n# Create a LangChain agent using Ollama's Qwen3 model with MCP tools\n# The agent can now use DNALLM's DNA analysis capabilities through the MCP tools\nagent = create_agent(\n    \"ollama:qwen3:latest\",  # Local LLM model via Ollama\n    tools                   # MCP tools from DNALLM server\n)\n</pre> # Create LangChain agent with MCP tools # This retrieves available tools from the DNALLM MCP server and creates a LangChain agent # that can use these tools for DNA sequence analysis  # Get available tools from the MCP server tools = await client.get_tools()    # Create a LangChain agent using Ollama's Qwen3 model with MCP tools # The agent can now use DNALLM's DNA analysis capabilities through the MCP tools agent = create_agent(     \"ollama:qwen3:latest\",  # Local LLM model via Ollama     tools                   # MCP tools from DNALLM server ) In\u00a0[\u00a0]: Copied! <pre># Perform DNA sequence analysis using the LangChain agent\n# This demonstrates how to use the agent to analyze a DNA sequence using DNALLM's models\n# The agent will automatically select and use the appropriate MCP tools for analysis\n\n# Define the DNA sequence to analyze\ndna_sequence = \"\"\"AGAAAAAACATGACAAGAAATCGATAATAATACAAAAGCTATGATGGTGTGCAATGTCCGTGTGCATGCGTGCACGCATTGCAACCGGCCCAAATCAAGGCCCATCGATCAGTGAATACTCATGGGCCGGCGGCCCACCACCGCTTCATCTCCTCCTCCGACGACGGGAGCACCCCCGCCGCATCGCCACCGACGAGGAGGAGGCCATTGCCGGCGGCGCCCCCGGTGAGCCGCTGCACCACGTCCCTGA\"\"\"\n\n# Invoke the agent to analyze the DNA sequence\n# The agent will use DNALLM's specialized models to provide comprehensive analysis\ndnallm_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": f'What is the function of following DNA sequence? Please analyze it thoroughly using all available models:\\n{dna_sequence}'}]}\n)\n</pre> # Perform DNA sequence analysis using the LangChain agent # This demonstrates how to use the agent to analyze a DNA sequence using DNALLM's models # The agent will automatically select and use the appropriate MCP tools for analysis  # Define the DNA sequence to analyze dna_sequence = \"\"\"AGAAAAAACATGACAAGAAATCGATAATAATACAAAAGCTATGATGGTGTGCAATGTCCGTGTGCATGCGTGCACGCATTGCAACCGGCCCAAATCAAGGCCCATCGATCAGTGAATACTCATGGGCCGGCGGCCCACCACCGCTTCATCTCCTCCTCCGACGACGGGAGCACCCCCGCCGCATCGCCACCGACGAGGAGGAGGCCATTGCCGGCGGCGCCCCCGGTGAGCCGCTGCACCACGTCCCTGA\"\"\"  # Invoke the agent to analyze the DNA sequence # The agent will use DNALLM's specialized models to provide comprehensive analysis dnallm_response = await agent.ainvoke(     {\"messages\": [{\"role\": \"user\", \"content\": f'What is the function of following DNA sequence? Please analyze it thoroughly using all available models:\\n{dna_sequence}'}]} ) In\u00a0[\u00a0]: Copied! <pre># Display the analysis results\n# This prints the comprehensive DNA sequence analysis provided by the LangChain agent\n# The analysis includes insights from multiple DNALLM models (promoter, conservation, chromatin)\n# The results show detailed functional interpretation of the DNA sequence\n\nprint(dnallm_response['messages'][-1].content)\n</pre> # Display the analysis results # This prints the comprehensive DNA sequence analysis provided by the LangChain agent # The analysis includes insights from multiple DNALLM models (promoter, conservation, chromatin) # The results show detailed functional interpretation of the DNA sequence  print(dnallm_response['messages'][-1].content) <pre>The provided DNA sequence has been analyzed using three specialized models, revealing key functional insights:\n\n1. **Promoter Analysis**:\n   - **Core Promoter**: The sequence is confidently identified as a core promoter region (score: 94.0%). Core promoters are critical for initiating transcription by RNA polymerase.\n\n2. **Conservation Analysis**:\n   - **Conserved Region**: The sequence shows strong conservation across species (score: 92.0%), indicating evolutionary importance. This suggests it likely serves a regulatory function preserved through evolution.\n\n3. **Open Chromatin Analysis**:\n   - **Full Open Chromatin**: The sequence is classified as \"Full open\" (score: 94.6%), indicating it resides in an actively accessible chromatin state. Open chromatin is typically associated with enhancers, promoters, or regulatory elements.\n\n**Functional Interpretation**:\nThis sequence represents a **highly conserved core promoter region** in an **actively transcribed genomic locus**. The combination of:\n- Core promoter identification\n- Strong conservation\n- Open chromatin state\n\nStrongly suggests it is a **regulatory region controlling gene expression**, likely serving as both a transcription start site and a distal regulatory element. The near-perfect scores across all models indicate this is a biologically significant region with critical roles in gene regulation.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/mcp_example/mcp_client_ollama_langchain_agents/#dnallm-mcp-client-with-langchain-agents","title":"DNALLM MCP Client with LangChain Agents\u00b6","text":"<p>This notebook demonstrates how to integrate DNALLM (DNA Large Language Model Toolkit) with LangChain agents using the Model Context Protocol (MCP).</p>"},{"location":"example/mcp_example/mcp_client_ollama_langchain_agents/#overview","title":"Overview\u00b6","text":"<p>This example shows how to:</p> <ol> <li>Start a DNALLM MCP server with streamable HTTP transport</li> <li>Connect a LangChain client to the MCP server</li> <li>Create an agent that can use DNALLM's DNA analysis tools</li> <li>Perform comprehensive DNA sequence analysis using multiple specialized models</li> </ol>"},{"location":"example/mcp_example/mcp_client_ollama_langchain_agents/#key-components","title":"Key Components\u00b6","text":"<ul> <li>DNALLM MCP Server: Provides DNA analysis tools via MCP protocol</li> <li>LangChain MCP Adapters: Enables LangChain to communicate with MCP servers</li> <li>Ollama Integration: Uses local LLM (Qwen3) for natural language processing</li> <li>Multi-Model Analysis: Combines promoter, conservation, and chromatin analysis</li> </ul>"},{"location":"example/mcp_example/mcp_client_ollama_langchain_agents/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>DNALLM installed and configured</li> <li>Ollama running with Qwen3 model</li> <li>Required Python packages installed (see first cell)</li> </ul>"},{"location":"example/mcp_example/mcp_client_ollama_langchain_agents/#workflow","title":"Workflow\u00b6","text":"<ol> <li>Install dependencies</li> <li>Start DNALLM MCP server</li> <li>Configure async environment</li> <li>Initialize MCP client connection</li> <li>Create LangChain agent with MCP tools</li> <li>Analyze DNA sequence using the agent</li> <li>Display comprehensive results</li> </ol>"},{"location":"example/mcp_example/mcp_client_ollama_pydantic_ai/","title":"Pydantic AI with Ollama and MCP Server","text":"In\u00a0[1]: Copied! <pre>import nest_asyncio\nimport asyncio\nimport time\nnest_asyncio.apply()\n</pre> import nest_asyncio import asyncio import time nest_asyncio.apply() In\u00a0[2]: Copied! <pre>from pydantic import BaseModel\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ollama import OllamaProvider\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\n\n\n\n\nollama_model = OpenAIChatModel(\n    model_name='qwen3:latest',\n    provider=OllamaProvider(base_url='http://localhost:11434/v1'),\n)\n</pre> from pydantic import BaseModel from pydantic_ai import Agent from pydantic_ai.models.openai import OpenAIChatModel from pydantic_ai.providers.ollama import OllamaProvider from pydantic_ai.mcp import MCPServerStreamableHTTP      ollama_model = OpenAIChatModel(     model_name='qwen3:latest',     provider=OllamaProvider(base_url='http://localhost:11434/v1'), )  In\u00a0[3]: Copied! <pre># Create MCP server connection\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')\n</pre> # Create MCP server connection server = MCPServerStreamableHTTP('http://localhost:8000/mcp') In\u00a0[4]: Copied! <pre># Create agent with MCP server tools and proper system prompt\nagent_ollama = Agent(\n    ollama_model, \n    toolsets=[server],\n    system_prompt='''You are a DNA analysis assistant with access to specialized DNA analysis tools via MCP server.\n\nWhen analyzing a DNA sequence, you should:\n1. First call _list_loaded_models to see what models are available\n2. Then call _dna_multi_model_predict with the DNA sequence and appropriate model names\n3. Interpret and explain the results in a comprehensive way, not just only list the results\n\nAvailable tools should include:\n- _list_loaded_models: Lists available DNA analysis models\n- _dna_multi_model_predict: Predicts DNA sequence properties using multiple models\n\nAlways use the tools to provide accurate analysis. Based on the returned results, make reasonable inferences with comprehensive biological functions of this sequence.'''\n)\n</pre> # Create agent with MCP server tools and proper system prompt agent_ollama = Agent(     ollama_model,      toolsets=[server],     system_prompt='''You are a DNA analysis assistant with access to specialized DNA analysis tools via MCP server.  When analyzing a DNA sequence, you should: 1. First call _list_loaded_models to see what models are available 2. Then call _dna_multi_model_predict with the DNA sequence and appropriate model names 3. Interpret and explain the results in a comprehensive way, not just only list the results  Available tools should include: - _list_loaded_models: Lists available DNA analysis models - _dna_multi_model_predict: Predicts DNA sequence properties using multiple models  Always use the tools to provide accurate analysis. Based on the returned results, make reasonable inferences with comprehensive biological functions of this sequence.''' ) In\u00a0[5]: Copied! <pre># Analyze DNA sequence using MCP server with proper async context\nasync def analyze_dna_sequence():\n    async with agent_ollama:  # This ensures proper MCP server connection\n        result = await agent_ollama.run(\n            'What is the function of following DNA sequence? Please analyze it thoroughly using all available models: AGAAAAAACATGACAAGAAATCGATAATAATACAAAAGCTATGATGGTGTGCAATGTCCGTGTGCATGCGTGCACGCATTGCAACCGGCCCAAATCAAGGCCCATCGATCAGTGAATACTCATGGGCCGGCGGCCCACCACCGCTTCATCTCCTCCTCCGACGACGGGAGCACCCCCGCCGCATCGCCACCGACGAGGAGGAGGCCATTGCCGGCGGCGCCCCCGGTGAGCCGCTGCACCACGTCCCTGA'\n        )\n        return result\n\n# Run the analysis\nresult = await analyze_dna_sequence()\n\ntime.sleep(3)\n</pre> # Analyze DNA sequence using MCP server with proper async context async def analyze_dna_sequence():     async with agent_ollama:  # This ensures proper MCP server connection         result = await agent_ollama.run(             'What is the function of following DNA sequence? Please analyze it thoroughly using all available models: AGAAAAAACATGACAAGAAATCGATAATAATACAAAAGCTATGATGGTGTGCAATGTCCGTGTGCATGCGTGCACGCATTGCAACCGGCCCAAATCAAGGCCCATCGATCAGTGAATACTCATGGGCCGGCGGCCCACCACCGCTTCATCTCCTCCTCCGACGACGGGAGCACCCCCGCCGCATCGCCACCGACGAGGAGGAGGCCATTGCCGGCGGCGCCCCCGGTGAGCCGCTGCACCACGTCCCTGA'         )         return result  # Run the analysis result = await analyze_dna_sequence()  time.sleep(3) In\u00a0[6]: Copied! <pre># Display the comprehensive analysis result\nprint(\"=== DNA Sequence Analysis Result ===\")\nprint(result.output)\nprint(\"\\n=== Usage Statistics ===\")\nprint(result.usage())\n</pre> # Display the comprehensive analysis result print(\"=== DNA Sequence Analysis Result ===\") print(result.output) print(\"\\n=== Usage Statistics ===\") print(result.usage())  <pre>=== DNA Sequence Analysis Result ===\n\n\nThe analyzed DNA sequence exhibits characteristics of a **highly conserved regulatory region** with **promoter activity** and **open chromatin structure**, suggesting it plays a critical role in gene regulation. Here's the biological interpretation:\n\n---\n\n### **1. Promoter Activity (Core Promoter)**\n- **Model**: `promoter_model`\n- **Prediction**: Labeled as **\"Core promoter\"** with 94% confidence.\n- **Biological Significance**:\n  - This sequence is likely a **transcription start site (TSS)** or a **core promoter region** critical for initiating gene transcription.\n  - Core promoters typically include elements like the **TATA box**, **Nucleosome Remodeling Domain (NRD)**, or **Inr motifs**.\n  - The high score suggests this region is actively involved in **RNA polymerase II recruitment** and **cis-regulatory control**.\n\n---\n\n### **2. Evolutionary Conservation**\n- **Model**: `conservation_model`\n- **Prediction**: Labeled as **\"Conserved\"** with 92% confidence.\n- **Biological Significance**:\n  - The sequence is **evolutionarily conserved** across species, implying it is functionally important.\n  - Conservation here indicates **selective pressure** to maintain this region, likely due to its role in **developmental regulation** or **housekeeping gene expression**.\n  - Such regions are often part of **cis-regulatory modules (CRMs)** or **enhancer regions** that control gene expression patterns.\n\n---\n\n### **3. Open Chromatin Structure**\n- **Model**: `open_chromatin_model`\n- **Prediction**: Labeled as **\"Full open\"** with 94.6% confidence.\n- **Biological Significance**:\n  - The sequence corresponds to an **open chromatin region** (e.g., **enhancer**, **promoter**, or **regulatory island**).\n  - Open chromatin is typically **accessible to transcription factors (TFs)** and **epigenetic modifiers**, enabling **gene activation**.\n  - This suggests the region is in an **active transcriptional state** and may interact with **distal regulatory elements** or **enhancer-promoter loops**.\n\n---\n\n### **Integrated Biological Function**\nThe sequence is a **highly conserved regulatory element** with three key features:\n1. **Promoter activity** (core promoter).\n2. **Evolutionary conservation** (functional constraint).\n3. **Open chromatin** (active regulatory state).\n\nThis combination strongly suggests it is a **cis-regulatory module** involved in:\n- **Gene regulation** (e.g., enhancer-promoter interaction).\n- **Developmental control** (due to conservation and open chromatin).\n- **Transcriptional activation** (via open chromatin and promoter elements).\n\nSuch regions are critical for **fine-tuning gene expression** in response to environmental signals or developmental cues. Further experiments (e.g., chromatin immunoprecipitation, reporter assays) would validate its functional role.\n\n=== Usage Statistics ===\nRunUsage(input_tokens=9299, output_tokens=1922, requests=3, tool_calls=2)\n</pre> In\u00a0[7]: Copied! <pre># Alternative approach: Test individual tool calls to debug\nasync def test_individual_tools():\n    async with agent_ollama:\n        # First, let's try to list available tools\n        print(\"=== Testing tool availability ===\")\n        \n        # Try to get the agent to use the list models tool\n        list_result = await agent_ollama.run(\n            'Please list all the available DNA analysis models using the _list_loaded_models tool.'\n        )\n        print(\"List models result:\")\n        print(list_result.output)\n        \n        return list_result\n\n# Run individual tool test\nlist_result = await test_individual_tools()\n</pre> # Alternative approach: Test individual tool calls to debug async def test_individual_tools():     async with agent_ollama:         # First, let's try to list available tools         print(\"=== Testing tool availability ===\")                  # Try to get the agent to use the list models tool         list_result = await agent_ollama.run(             'Please list all the available DNA analysis models using the _list_loaded_models tool.'         )         print(\"List models result:\")         print(list_result.output)                  return list_result  # Run individual tool test list_result = await test_individual_tools() <pre>=== Testing tool availability ===\nList models result:\n\n\nHere are the three DNA analysis models currently loaded on the server:\n\n1. **Promoter Prediction Model**\n   - **Task Type**: Binary classification\n   - **Labels**: \"Not promoter\" vs \"Core promoter\"\n   - **Performance**: 85% accuracy, 82% F1 score\n   - **Architecture**: DNABERT (plant-specific)\n   - **Memory Usage**: ~351.7MB parameters, 369.3MB total\n   - **Use Case**: Identifies promoter regions in plant genomes\n\n2. **Conservation Prediction Model**\n   - **Task Type**: Binary classification\n   - **Labels**: \"Not conserved\" vs \"Conserved\"\n   - **Performance**: 88% accuracy, 85% F1 score\n   - **Architecture**: DNABERT (plant-specific)\n   - **Memory Usage**: Same as promoter model\n   - **Use Case**: Detects conserved genomic regions across species\n\n3. **Open Chromatin State Model**\n   - **Task Type**: Multiclass classification\n   - **Labels**: \"Not open\", \"Full open\", \"Partial open\"\n   - **Performance**: 82% accuracy, 79% F1 score\n   - **Architecture**: DNAMamba (plant-specific)\n   - **Memory Usage**: Slightly higher at 368.8MB parameters\n   - **Use Case**: Characterizes chromatin accessibility states\n\nAll models are optimized for plant genome analysis and use BPE tokenization. The DNAMamba architecture shows promising performance for chromatin state prediction. Would you like to analyze a specific DNA sequence with any of these models?\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/mcp_example/mcp_client_ollama_pydantic_ai/#pydantic-ai-with-ollama-and-mcp-server","title":"Pydantic AI with Ollama and MCP Server\u00b6","text":"<p>This notebook demonstrates how to use Pydantic AI with Ollama models and MCP servers for DNA sequence analysis.</p>"},{"location":"example/mcp_example/mcp_client_ollama_pydantic_ai/#install-dependencies","title":"Install dependencies\u00b6","text":"<pre>uv pip install pydantic-ai\nuv pip install nest-asyncio\n</pre>"},{"location":"example/mcp_example/mcp_client_ollama_pydantic_ai/#ollama-model-is-available","title":"Ollama Model is available\u00b6","text":"<pre>ollama models list\n</pre> <p>Please make sure qwen3:latest is available. if not, please run <code>ollama pull qwen3:latest</code>. Or choose another model and modify the model name in the code.</p>"},{"location":"example/notebooks/embedding_attention/","title":"Embedding","text":"In\u00a0[1]: Copied! <pre>#!pip install seaborn\n</pre> #!pip install seaborn In\u00a0[2]: Copied! <pre>import sys\nimport random\nimport inspect\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import softmax\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom umap import UMAP\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport torch\n</pre> import sys import random import inspect import numpy as np import pandas as pd from scipy.special import softmax import matplotlib.pyplot as plt import seaborn as sns from sklearn.decomposition import PCA from sklearn.manifold import TSNE from umap import UMAP from transformers import AutoTokenizer, AutoModelForMaskedLM import torch In\u00a0[3]: Copied! <pre>model_path = \"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\"\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nmodel = AutoModelForMaskedLM.from_pretrained(model_path, trust_remote_code=True)\nmax_length = tokenizer.model_max_length\n</pre> model_path = \"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\" tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True) model = AutoModelForMaskedLM.from_pretrained(model_path, trust_remote_code=True) max_length = tokenizer.model_max_length <pre>model.safetensors:   0%|          | 0.00/224M [00:00&lt;?, ?B/s]</pre> In\u00a0[4]: Copied! <pre>def random_generate_sequences(minl, maxl=0, samples=1, with_N=False, padding_size=0, gc=0, seed=None):\n    sequences = []\n    basemap = [\"A\", \"C\", \"G\", \"T\"]\n    if with_N:\n        basemap.append(\"N\")\n    baseidx = len(basemap) - 1\n    if seed:\n        random.seed(seed)\n    if maxl:\n        for i in range(samples):\n            length = random.randint(minl, maxl)\n            if padding_size:\n                length = (length // padding_size + 1) * padding_size if length % padding_size else length\n                if length &gt; maxl:\n                    length -= padding_size\n            seq = \"\".join([basemap[random.randint(0,baseidx)] for _ in range(length)])\n            sequences.append(seq)\n    else:\n        for i in range(samples):\n            seq = \"\".join([basemap[random.randint(0,baseidx)] for _ in range(minl)])\n            sequences.append(seq)\n    \n    return sequences\n</pre> def random_generate_sequences(minl, maxl=0, samples=1, with_N=False, padding_size=0, gc=0, seed=None):     sequences = []     basemap = [\"A\", \"C\", \"G\", \"T\"]     if with_N:         basemap.append(\"N\")     baseidx = len(basemap) - 1     if seed:         random.seed(seed)     if maxl:         for i in range(samples):             length = random.randint(minl, maxl)             if padding_size:                 length = (length // padding_size + 1) * padding_size if length % padding_size else length                 if length &gt; maxl:                     length -= padding_size             seq = \"\".join([basemap[random.randint(0,baseidx)] for _ in range(length)])             sequences.append(seq)     else:         for i in range(samples):             seq = \"\".join([basemap[random.randint(0,baseidx)] for _ in range(minl)])             sequences.append(seq)          return sequences In\u00a0[5]: Copied! <pre># sequences = [\"ATTCCGATTCCGATTCCG\", \"ATTTCTCTCTCTCTCTGAGATCGATCGATCGAT\"]\nsequences = random_generate_sequences(30, 500, 100, padding_size=6)\nsequences[:10]\n</pre> # sequences = [\"ATTCCGATTCCGATTCCG\", \"ATTTCTCTCTCTCTCTGAGATCGATCGATCGAT\"] sequences = random_generate_sequences(30, 500, 100, padding_size=6) sequences[:10] Out[5]: <pre>['AATCTCTGTCGACAGTTCTCGAGCTCTAGCGAGTTCCCTGATCTGGAAGTAGCGACAATCTTCCGGGTCTCATCTTTTCAAACTCGTGATCGTTGTTGGGGTCTGTGATTAGGGATGGAACCTCATACCTGCGCGTCGGTGTGTGACCTTCAATAGCTGGGAAGGCCGGTAAGGGTCTATGAGAATATATGTGTACTACGGAGAGTCGGCCACTGGTCGAGGCGACAATAATTGGGCCCTAGGATCACGACAGATCTCGTAAATAGCAGCAGAGTTACCAGGCCCATGACGTGATCCGCGTAGAAGATGATACCCGTGAATTAGGTGTTACTCTTA',\n 'CGAGTTTCGTATGGTAACCATCACGGGCTTAATTACCCGCAGCGCATAGTACCGTCTACCGTGACTCTAAGCACGGGTCCCCGCTTTACTATTGCCTCAGCGATTTGCACCCTGGGATGAGCCATATTACGGCAACTTGGGTCGGCAAGTATGTTCAATAGCGTACTCCTTCACTCAGTGATTGCTATGTCCAACGTGCATTTTGTCGCGGAGAGAGTCGTACAGAATTCTCGA',\n 'TTTGAATCTTTTATCCCTCTATTGCGCTTTAAATTGTAAGGATATTCCGTCAGACTCCCAAACATAGGCTCGATGCTGAGGTAGATTGTCATTATCTATCTTACGCAATGGTTAACCATGAGCAGTTTACTCTACGAA',\n 'TAGTCCCACATGCGAACATGACCCGCAGTGCGATGAGCGAGTGCTGCATGTGTCGTAAATGGAAATAAAGTGGCCTCGGCTGAGCATTAAAATGCTCAACCCAGGGTGCCTACGCTAAAATCCGAACAGCGAAGAATTACGTCGTCGCCG',\n 'CCATCAGAGGGAGCAAGTGGGCACAGACCACTAATTTCGTCGAATTCCGGCTCATATATGGGGATACCAGTCGCTCTTCGTGCCCATCATTATTTGCTTAGAGACTTTTCGACTGTCTCGCGGATAAGCTCAAAAGACATTAAGGGTTGTTTCCCACGACCAAGTTCGTAGAACCGCCGCATGAGATCAACTGTTGGCTACTAGGTATGGGTCAAGGGATTATGCCTTACTCTAAGTTGTAAAAAGCCCCTAAGGCCATGGCACGCTCCGTGCACCTGAAATAGCGCGTCTTGGCGGTACGAGAGACACACCCAGACGTGGCGATAGATTTGAATGTTCTCTTGATTCGCACAGCAACCTCCCGACAAATTTTGCCACTGCTTGCCCGCATGGGGTAGACATCTCACA',\n 'ACATAATAAGGGGGGACACACGGGAACTTGAGCTCCCAGCAGGATGCCTGGTAGGGTGCAATATAAAAAGGCCTGCAATAATTGAACTAGCCCATCTTCTTGGCGAAAGTCCAGCTATACCTCACAAGTACGTTCGGTTTTGCCCTCTGCATGGCAGGCCAG',\n 'GGGACGTGGTACATATTGGTCAGACGCAACCTGCGGTTTCCCTTAAGACCTATGTAGAAGGCCATAACTAGTTTTTTTTAGGGCCGGGATATCCTTAGTACTAATTCCGCAGTATTCCCAATGTTTCCCAGATCGAAGGATAGCCGTGATGCAGTATGCGTGTGATAAGGGGCCAAGTGTGTATCCCCATACTAATTATGCGCTATCGTGCCCCTTAGGCCCAAACTACGTAAATGCCCCTTAAAAAGCGCGCGCACAAAGCCGCGTATTAATATAACGAGCCCGATGCCTAGCCTATATTTTCATACGAATTTGATCAACTCAAAGGCGACAGGGGACCGTCTTTCTATAGTACCTGGATGGTTTCCTCAGCGCAGT',\n 'GTCCATCACGAGGGATATGTTACGGCACAGAAGGGCATATCGTTAAAACCGCTTGACGACTACCAGACTATGTAGGCTAGCTCTGTCCGGCGGCTTTCAAACGACAAACTCTTGACCACCGAGGACGACTACGTGAGGCACCTCGCCCTA',\n 'TGGAATCATGGTCTAGGCTTTAGGGTTGTAGCATAAGGCTCTTATTCACCTCTTATTTGGTGTGATGAGCGGAAGTTATAATAAAGAACTCCAATAACTGTGCTGCCATCGCCGCTGACGCATACCTAGGTCAGGATAGAGGATATTGGTTACGTACCCTCAGTTCTGCGTACACGTCACGATTCTAAGCTTAGTAGGTACCACATGGATTAGCTATGAGGTTATAATGGTTCCTGTGGCATTGCATTAGGGATAGGCGCCGCCACTGACACGCCAAAAGCTACGCTGCGATTCCCTATATAATTGGCGGACTGAAACTGTGCCGCCAATACTGGGGGGTCTATAATGACCATCCCTGTTAACGAGATCACACGTCTGGGAAGTGCTTGTCATATGATGCCACTACAGAGATGGGGAACTAGTCCAGACGTAGGGATGTAACAGGTTTTTGAAAACATTTTTTTAACTCAATGAGCCGATATCATCTACCCGGCGGTC',\n 'TAGGGTCTTATTTCCGAATTCAGACCCACGAAGCCTTTGCCCGTGTCCCCGCGTATTAGC']</pre> In\u00a0[6]: Copied! <pre>inputs = tokenizer(\n    sequences,\n    truncation=True, padding='longest',\n    max_length=max_length,\n    return_tensors=\"pt\"\n)\ntokens_ids = inputs['input_ids'].detach()\ntokens_str = [b.split() for b in tokenizer.batch_decode(tokens_ids)]\ntokens_idx = [[False if s in tokenizer.all_special_tokens else True for i, s in enumerate(tokens)] for tokens in tokens_str]\n</pre> inputs = tokenizer(     sequences,     truncation=True, padding='longest',     max_length=max_length,     return_tensors=\"pt\" ) tokens_ids = inputs['input_ids'].detach() tokens_str = [b.split() for b in tokenizer.batch_decode(tokens_ids)] tokens_idx = [[False if s in tokenizer.all_special_tokens else True for i, s in enumerate(tokens)] for tokens in tokens_str] In\u00a0[7]: Copied! <pre># Predict\nsig = inspect.signature(model.forward)\nparams = sig.parameters\nif \"output_attentions\" in params:\n    outputs = model(\n        **inputs,\n        output_attentions=True,\n        output_hidden_states=True\n    )\nelse:\n    outputs = model(\n        **inputs,\n        output_hidden_states=True\n    )\n</pre> # Predict sig = inspect.signature(model.forward) params = sig.parameters if \"output_attentions\" in params:     outputs = model(         **inputs,         output_attentions=True,         output_hidden_states=True     ) else:     outputs = model(         **inputs,         output_hidden_states=True     ) In\u00a0[8]: Copied! <pre>def plot_attention_map(attentions, tokens_str, tokens_idx, layer=-1, idx=0, ncols=3, scale_width=5, scale_height=4):\n    tokens = [tokens_str[idx][i] for i,b in enumerate(tokens_idx[idx]) if b]\n    n_heads = len(attentions)\n    nrows = (n_heads + ncols - 1) // ncols\n    figsize = (ncols * scale_width, nrows * scale_height)\n    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n    if n_heads == 1:\n        axes = [axes]\n    else:\n        axes = axes.flatten()\n    for i, data in enumerate(attentions):\n        data = data[layer][idx].detach().numpy()\n        data = [[data[j][jj] for jj,bb in enumerate(tokens_idx[idx]) if bb]\n                             for j,b in enumerate(tokens_idx[idx]) if b]\n        sns.heatmap(\n            data,\n            ax=axes[i], cmap=\"viridis\",\n            xticklabels=tokens,\n            yticklabels=tokens\n        )\n        axes[i].set_title(f\"Head {i+1}\")\n    for j in range(i+1, len(axes)):\n        fig.delaxes(axes[j])\n\n    fig.tight_layout()\n    plt.show()\n</pre> def plot_attention_map(attentions, tokens_str, tokens_idx, layer=-1, idx=0, ncols=3, scale_width=5, scale_height=4):     tokens = [tokens_str[idx][i] for i,b in enumerate(tokens_idx[idx]) if b]     n_heads = len(attentions)     nrows = (n_heads + ncols - 1) // ncols     figsize = (ncols * scale_width, nrows * scale_height)     fig, axes = plt.subplots(nrows, ncols, figsize=figsize)     if n_heads == 1:         axes = [axes]     else:         axes = axes.flatten()     for i, data in enumerate(attentions):         data = data[layer][idx].detach().numpy()         data = [[data[j][jj] for jj,bb in enumerate(tokens_idx[idx]) if bb]                              for j,b in enumerate(tokens_idx[idx]) if b]         sns.heatmap(             data,             ax=axes[i], cmap=\"viridis\",             xticklabels=tokens,             yticklabels=tokens         )         axes[i].set_title(f\"Head {i+1}\")     for j in range(i+1, len(axes)):         fig.delaxes(axes[j])      fig.tight_layout()     plt.show() In\u00a0[9]: Copied! <pre># Attention map\nif hasattr(outputs, 'attentions'):\n    attentions = outputs.attentions  # ((seq_num, heads, max_token_len, max_token_len) x layers)\n    plot_attention_map(attentions, tokens_str, tokens_idx, layer=-1, idx=1, ncols=3)\n</pre> # Attention map if hasattr(outputs, 'attentions'):     attentions = outputs.attentions  # ((seq_num, heads, max_token_len, max_token_len) x layers)     plot_attention_map(attentions, tokens_str, tokens_idx, layer=-1, idx=1, ncols=3) In\u00a0[10]: Copied! <pre>def plot_layer_embeddings(hidden_states, attention_mask, layers=[0,1], labels=None, reducer=\"t-SNE\", ncols=4, scale_width=5, scale_height=4):\n    if reducer.lower() == \"pca\":\n        dim_reducer = PCA(n_components=2)\n    elif reducer.lower() == \"t-sne\":\n        dim_reducer = TSNE(n_components=2)\n    elif reducer.lower() == \"umap\":\n        dim_reducer = UMAP(n_components=2)\n    else:\n        raise(\"Unsupported dim reducer, please try PCA, t-SNE or UMAP.\")\n    n_layers = len(layers)\n    nrows = (n_layers + ncols - 1) // ncols\n    figsize = (ncols * scale_width, nrows * scale_height)\n    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n    if n_layers == 1:\n        axes = [axes]\n    else:\n        axes = axes.flatten()\n    for i,layer_i in enumerate(layers):\n        embeddings = hidden_states[layer_i].detach().numpy()\n        mean_sequence_embeddings = torch.sum(attention_mask*embeddings, axis=-2) / torch.sum(attention_mask, axis=1)\n        layer_dim_reduced_vectors = dim_reducer.fit_transform(mean_sequence_embeddings.detach().numpy())\n        if not labels:\n            labels = [\"Uncategorized\"] * layer_dim_reduced_vectors.shape[0]\n        dataframe = {\n            'Dimension 1': layer_dim_reduced_vectors[:,0],\n            'Dimension 2': layer_dim_reduced_vectors[:,1],\n            'labels': labels\n            }\n        df = pd.DataFrame.from_dict(dataframe)\n        sns.scatterplot(\n            data=df,\n            x='Dimension 1',\n            y='Dimension 2',\n            hue='labels', ax=axes[i]\n        )\n        axes[i].set_title(f\"Layer {layer_i+1}\")\n    for j in range(i+1, len(axes)):\n        fig.delaxes(axes[j])\n\n    fig.tight_layout()\n    plt.show()\n</pre> def plot_layer_embeddings(hidden_states, attention_mask, layers=[0,1], labels=None, reducer=\"t-SNE\", ncols=4, scale_width=5, scale_height=4):     if reducer.lower() == \"pca\":         dim_reducer = PCA(n_components=2)     elif reducer.lower() == \"t-sne\":         dim_reducer = TSNE(n_components=2)     elif reducer.lower() == \"umap\":         dim_reducer = UMAP(n_components=2)     else:         raise(\"Unsupported dim reducer, please try PCA, t-SNE or UMAP.\")     n_layers = len(layers)     nrows = (n_layers + ncols - 1) // ncols     figsize = (ncols * scale_width, nrows * scale_height)     fig, axes = plt.subplots(nrows, ncols, figsize=figsize)     if n_layers == 1:         axes = [axes]     else:         axes = axes.flatten()     for i,layer_i in enumerate(layers):         embeddings = hidden_states[layer_i].detach().numpy()         mean_sequence_embeddings = torch.sum(attention_mask*embeddings, axis=-2) / torch.sum(attention_mask, axis=1)         layer_dim_reduced_vectors = dim_reducer.fit_transform(mean_sequence_embeddings.detach().numpy())         if not labels:             labels = [\"Uncategorized\"] * layer_dim_reduced_vectors.shape[0]         dataframe = {             'Dimension 1': layer_dim_reduced_vectors[:,0],             'Dimension 2': layer_dim_reduced_vectors[:,1],             'labels': labels             }         df = pd.DataFrame.from_dict(dataframe)         sns.scatterplot(             data=df,             x='Dimension 1',             y='Dimension 2',             hue='labels', ax=axes[i]         )         axes[i].set_title(f\"Layer {layer_i+1}\")     for j in range(i+1, len(axes)):         fig.delaxes(axes[j])      fig.tight_layout()     plt.show() In\u00a0[11]: Copied! <pre># Get the layer embeddings\nhidden_states = outputs['hidden_states']\nattention_mask = torch.unsqueeze(torch.tensor(tokens_idx), dim=-1)\n\nplot_layer_embeddings(hidden_states, attention_mask, layers=range(6), labels=None, reducer=\"t-SNE\", ncols=3)\n</pre> # Get the layer embeddings hidden_states = outputs['hidden_states'] attention_mask = torch.unsqueeze(torch.tensor(tokens_idx), dim=-1)  plot_layer_embeddings(hidden_states, attention_mask, layers=range(6), labels=None, reducer=\"t-SNE\", ncols=3) In\u00a0[12]: Copied! <pre>def get_token_probability(probabilities, idx=0, top_k=5):\n    tokens_probs = []\n    probas = probabilities[idx]\n    for pos, probs in enumerate(probas):\n        sorted_positions = np.argsort(-probs)\n        sorted_probs = probs[sorted_positions]\n        token_probs = {}\n        for k in range(top_k):\n            predicted_token = tokenizer.id_to_token(int(sorted_positions[k]))\n            prob = sorted_probs[k]\n            # print(f\"seq_id: {idx}, token_position: {pos}, k: {k}, token: {predicted_token}, probability: {prob * 100:.2f}%\")\n            token_probs[predicted_token] = prob\n        tokens_probs.append(token_probs)\n    return tokens_probs\n</pre> def get_token_probability(probabilities, idx=0, top_k=5):     tokens_probs = []     probas = probabilities[idx]     for pos, probs in enumerate(probas):         sorted_positions = np.argsort(-probs)         sorted_probs = probs[sorted_positions]         token_probs = {}         for k in range(top_k):             predicted_token = tokenizer.id_to_token(int(sorted_positions[k]))             prob = sorted_probs[k]             # print(f\"seq_id: {idx}, token_position: {pos}, k: {k}, token: {predicted_token}, probability: {prob * 100:.2f}%\")             token_probs[predicted_token] = prob         tokens_probs.append(token_probs)     return tokens_probs In\u00a0[13]: Copied! <pre>logits = outputs['logits'].detach().numpy()\nprobabilities = []\n# get probabilities separately for each seq as they have different lengths\nfor idx in range(logits.shape[0]):\n    logits_seq = logits[idx]\n    logits_seq = [logits_seq[i] for i,b in enumerate(tokens_idx[idx]) if b]\n    probs = softmax(logits_seq, axis=-1)  # use softmax to transform logits into probabilities\n    probabilities.append(probs)\n\ntokens_probs = get_token_probability(probabilities, idx=1, top_k=5)\ntokens_probs\n</pre> logits = outputs['logits'].detach().numpy() probabilities = [] # get probabilities separately for each seq as they have different lengths for idx in range(logits.shape[0]):     logits_seq = logits[idx]     logits_seq = [logits_seq[i] for i,b in enumerate(tokens_idx[idx]) if b]     probs = softmax(logits_seq, axis=-1)  # use softmax to transform logits into probabilities     probabilities.append(probs)  tokens_probs = get_token_probability(probabilities, idx=1, top_k=5) tokens_probs Out[13]: <pre>[{'CGAGTT': 0.17259638,\n  'ATTTTG': 0.074336775,\n  'AATTTT': 0.03158132,\n  'ACTTTG': 0.029733788,\n  'ACTTTT': 0.027789203},\n {'TCGTAT': 0.9961033,\n  'CCGTAT': 0.002380367,\n  'TTGTAT': 0.00013929073,\n  'TCGCAT': 0.00012747609,\n  'TCATAT': 8.166215e-05},\n {'GGTAAC': 0.99959,\n  'GGTAAT': 0.00017227474,\n  'GGGTAA': 2.4163122e-05,\n  'GGTAAA': 2.2399232e-05,\n  'TGGTAA': 2.146018e-05},\n {'CATCAC': 0.97606814,\n  'ATCCCC': 0.005534643,\n  'ATCACC': 0.0022881867,\n  'CGTCAC': 0.0019009738,\n  'ATTCCC': 0.0010483828},\n {'GGGCTT': 0.99871385,\n  'GGACTT': 0.0001330908,\n  'GGGTTT': 0.000121900695,\n  'AGGCTT': 0.00011995096,\n  'CGGGTT': 7.997676e-05},\n {'AATTAC': 0.9997267,\n  'GATTAC': 4.0375737e-05,\n  'ACTTAC': 2.550763e-05,\n  'ATTTAC': 1.6145128e-05,\n  'AACTAC': 1.5840844e-05},\n {'CCGCAG': 0.999956,\n  'CCGCAC': 1.18452435e-05,\n  'CCACAG': 6.178044e-06,\n  'CTGCAG': 3.992571e-06,\n  'CCGCGG': 3.4355671e-06},\n {'CGCATA': 0.99955136,\n  'TGCATA': 0.00012145034,\n  'AGCGTA': 4.034968e-05,\n  'AGCATA': 3.7026664e-05,\n  'CGCGTA': 3.6516085e-05},\n {'GTACCG': 0.9998487,\n  'TACCGG': 2.1025691e-05,\n  'GTACCA': 2.0927728e-05,\n  'GGACCG': 2.0473371e-05,\n  'GTTCCG': 1.207671e-05},\n {'TCTACC': 0.9964585,\n  'TCTATC': 0.0021104612,\n  'TCTACT': 0.0005306293,\n  'TCTGCC': 0.0003090096,\n  'TCCACC': 0.0002797095},\n {'GTGACT': 0.99964905,\n  'GCGACT': 6.814439e-05,\n  'GTGACA': 4.108843e-05,\n  'ATGACT': 3.0910054e-05,\n  'CTGACT': 2.6076928e-05},\n {'CTAAGC': 0.9998443,\n  'CTAAGT': 3.1842705e-05,\n  'CCAAGC': 2.6327254e-05,\n  'GTAAGC': 1.9758221e-05,\n  'CTGAGC': 1.924362e-05},\n {'ACGGGT': 0.99969983,\n  'GCGGGT': 7.628152e-05,\n  'ACGGGC': 5.7914764e-05,\n  'ACAGGT': 4.4192246e-05,\n  'ATGGGT': 1.8808125e-05},\n {'CCCCGC': 0.9998889,\n  'CCCAGC': 2.2182645e-05,\n  'CCCTGC': 2.1937427e-05,\n  'CCCGCC': 1.8683259e-05,\n  'TCCCGC': 1.6611688e-05},\n {'TTTACT': 0.9978835,\n  'TTTAGT': 0.00022872274,\n  'TTTATT': 0.00021545753,\n  'TTTAGA': 0.00012321863,\n  'TTTCCT': 0.00010700531},\n {'ATTGCC': 0.9996586,\n  'ATTGTC': 7.325049e-05,\n  'ATTGCT': 6.860292e-05,\n  'ATTGCG': 2.8827857e-05,\n  'ATTGCA': 2.580576e-05},\n {'TCAGCG': 0.9995969,\n  'TCAACG': 0.0001207212,\n  'TCAGTG': 6.348614e-05,\n  'TCAGCA': 2.96002e-05,\n  'TCAGAG': 2.6319307e-05},\n {'ATTTGC': 0.9996542,\n  'ATTTGT': 9.031595e-05,\n  'ATTCGC': 3.39291e-05,\n  'ACTTGC': 1.8995961e-05,\n  'ATTGGC': 1.5615282e-05},\n {'ACCCTG': 0.9983924,\n  'ACCCCG': 0.00037356524,\n  'GCCCTG': 0.00019969455,\n  'ACGCTG': 0.00016784917,\n  'ACTCTG': 8.21346e-05},\n {'GGATGA': 0.98345125,\n  'AGATGA': 0.0083533535,\n  'AGATAA': 0.0011463522,\n  'GGATAA': 0.0009950879,\n  'GGACGA': 0.0007692377},\n {'GCCATA': 0.99653244,\n  'GCCGTA': 0.0017881092,\n  'CCCATA': 0.0005162992,\n  'GCTGTA': 0.00028362343,\n  'ACCATA': 0.00013441472},\n {'TTACGG': 0.9990121,\n  'TTACAA': 0.00045957882,\n  'TTACGA': 0.00017892425,\n  'TTACAG': 0.00012856742,\n  'TTATGG': 3.4307715e-05},\n {'CAACTT': 0.9993787,\n  'TAACTT': 7.4429176e-05,\n  'CAACTG': 4.3916898e-05,\n  'CAATTT': 3.8595674e-05,\n  'CAACTC': 3.402011e-05},\n {'GGGTCG': 0.9997696,\n  'GGGTCA': 4.0975166e-05,\n  'GGGTTG': 3.3173714e-05,\n  'GGGTCT': 1.3578625e-05,\n  'GGTCGG': 1.3248425e-05},\n {'GCAAGT': 0.99595284,\n  'ACAAGT': 0.0010641017,\n  'GCGAGT': 0.00079922826,\n  'GCAAGC': 0.0002992736,\n  'TCAAGT': 0.00022391975},\n {'ATGTTC': 0.9975695,\n  'ACGTTC': 0.0007419577,\n  'GTGTTC': 0.0002995831,\n  'ATATTC': 0.0002683154,\n  'ATGTTG': 0.00021825168},\n {'AATAGC': 0.9996363,\n  'AATAGT': 7.465532e-05,\n  'AATAGA': 4.515491e-05,\n  'CATAGC': 3.187673e-05,\n  'AATGGC': 2.12161e-05},\n {'GTACTC': 0.9995332,\n  'GTACTT': 0.00040913644,\n  'GTACGC': 1.0108596e-05,\n  'GTATTC': 9.164296e-06,\n  'GTACCC': 6.7496317e-06},\n {'CTTCAC': 0.99902916,\n  'CTTCAG': 0.0005663351,\n  'TTTCAC': 0.00010902189,\n  'CTTCAT': 3.6280082e-05,\n  'CATCAC': 2.6068687e-05},\n {'TCAGTG': 0.99983084,\n  'CCAGTG': 2.8872284e-05,\n  'TCATTG': 1.5589605e-05,\n  'TCACTG': 1.1398803e-05,\n  'TCAGAG': 1.0691216e-05},\n {'ATTGCT': 0.99978036,\n  'ATTGCC': 7.3094234e-05,\n  'GTTGCT': 4.9736147e-05,\n  'ATTGTT': 2.7429634e-05,\n  'ATTACT': 1.4872187e-05},\n {'ATGTCC': 0.99388593,\n  'ATGCCC': 0.002409024,\n  'ACGTCC': 0.0010945916,\n  'ATGTCT': 0.0004333326,\n  'ATATCC': 0.0004030408},\n {'AACGTG': 0.9973693,\n  'AACATG': 0.0002474348,\n  'GACGTG': 0.00018803342,\n  'AACGTC': 0.00018510975,\n  'AATGTG': 0.0001479922},\n {'CATTTT': 0.9995813,\n  'CGTTTT': 3.7846006e-05,\n  'AAGTTT': 3.2062322e-05,\n  'TTTTTT': 2.970213e-05,\n  'AATTTT': 2.6819005e-05},\n {'GTCGCG': 0.99959356,\n  'GTCGTG': 0.000115663956,\n  'CGCGGG': 3.4483375e-05,\n  'GCGGCG': 3.386643e-05,\n  'GTCACG': 2.5877345e-05},\n {'GAGAGA': 0.998789,\n  'AGAGAG': 0.00019560783,\n  'GAGAGG': 0.00014330231,\n  'GGGAGA': 9.80927e-05,\n  'AAGAGA': 7.373096e-05},\n {'GTCGTA': 0.99971396,\n  'GTTGTA': 7.912869e-05,\n  'GCCGTA': 7.065815e-05,\n  'GTCATA': 3.3755936e-05,\n  'GTCGTG': 2.5411036e-05},\n {'CAGAAT': 0.99983215,\n  'GAGAAT': 7.2248535e-05,\n  'CAGGAT': 1.6000364e-05,\n  'CAAAAT': 1.3472809e-05,\n  'CAGAAC': 9.629115e-06},\n {'TCTCGA': 0.999401,\n  'TCTTGA': 0.0002367876,\n  'TCGCGA': 0.0001701102,\n  'TTTCGA': 3.5704878e-05,\n  'TCTCAA': 1.5103107e-05}]</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/embedding_attention/#attention-map","title":"Attention Map\u00b6","text":""},{"location":"example/notebooks/embedding_attention/#embedding-visualization","title":"Embedding visualization\u00b6","text":""},{"location":"example/notebooks/embedding_attention/#probability-of-token","title":"Probability of token\u00b6","text":""},{"location":"example/notebooks/overview/","title":"Jupyter Notebook Examples","text":"<p>This section contains interactive Jupyter notebooks demonstrating various DNALLM features.</p>"},{"location":"example/notebooks/overview/#prerequisites","title":"Prerequisites","text":"<p>Before running notebooks, ensure you have:</p> <ul> <li>Installed DNALLM with <code>uv pip install -e '.[base,notebook,cuda124]'</code></li> <li>Downloaded required models from Hugging Face/ModelScope</li> <li>Prepared data files in the expected locations</li> </ul>"},{"location":"example/notebooks/overview/#running-notebooks","title":"Running Notebooks","text":""},{"location":"example/notebooks/overview/#view-in-browser","title":"View in Browser","text":"<p>Browse notebooks directly in this documentation (rendered via mkdocs-jupyter).</p>"},{"location":"example/notebooks/overview/#run-locally","title":"Run Locally","text":"<pre><code># Clone repository\ngit clone https://github.com/zhangtaolab/DNALLM.git\ncd DNALLM\n\n# Install dependencies\nuv pip install -e '.[base,notebook,cuda124]'\n\n# Start Jupyter\njupyter lab example/notebooks/\n</code></pre>"},{"location":"example/notebooks/overview/#notebook-categories","title":"Notebook Categories","text":""},{"location":"example/notebooks/overview/#fine-tuning-notebooks","title":"Fine-Tuning Notebooks","text":"<p>Learn how to fine-tune DNA language models for specific tasks.</p> <ul> <li>Binary Classification - Train a binary classifier for promoter prediction</li> <li>Multi-Label Classification - Predict multiple labels per sequence</li> <li>NER Task - Named Entity Recognition for genomic sequences</li> <li>Fine-tuning: Token classification training</li> <li>Data Generation: Creating training data for NER</li> <li>Custom Head - Define custom classification architectures</li> <li>Generation - Fine-tune causal language models for sequence generation</li> <li>LoRA Fine-tuning - Parameter-efficient fine-tuning with LoRA</li> <li>Fine-tuning: Training with LoRA adapters</li> <li>Inference: Running inference with LoRA models</li> </ul>"},{"location":"example/notebooks/overview/#inference-notebooks","title":"Inference Notebooks","text":"<p>Run inference with pre-trained models.</p> <ul> <li>Basic Inference - Single sequence prediction</li> <li>EVO Models - Causal model inference with EVO-1/EVO-2</li> <li>MegaDNA Models - Specialized model inference</li> <li>Sequence Generation - Generate DNA sequences de novo</li> <li>tRNA Inference - tRNA-specific predictions</li> </ul>"},{"location":"example/notebooks/overview/#analysis-notebooks","title":"Analysis Notebooks","text":"<p>Analyze model behavior and predictions.</p> <ul> <li>In Silico Mutagenesis - Saturation mutation analysis</li> <li>Model Interpretation - Attention and embedding analysis</li> <li>Embedding &amp; Attention - Feature visualization</li> </ul>"},{"location":"example/notebooks/overview/#benchmarking","title":"Benchmarking","text":"<ul> <li>Benchmark Evaluation - Compare multiple models on the same dataset</li> </ul>"},{"location":"example/notebooks/overview/#data-preparation","title":"Data Preparation","text":"<ul> <li>Fine-tuning Data - Prepare training data from various sources</li> <li>Prediction Data - Prepare data for inference</li> </ul>"},{"location":"example/notebooks/overview/#mcp-examples","title":"MCP Examples","text":"<ul> <li>LangChain Agents - Using DNALLM MCP server with LangChain</li> <li>Pydantic AI - Using DNALLM MCP server with Pydantic AI</li> </ul>"},{"location":"example/notebooks/overview/#tips","title":"Tips","text":"<ul> <li>Notebooks expect data in specific locations (check each notebook)</li> <li>Adjust model paths in configuration files as needed</li> <li>GPU is recommended for most notebooks</li> <li>Clear cell outputs before committing: <code>nbstripout *.ipynb</code></li> </ul>"},{"location":"example/notebooks/overview/#troubleshooting","title":"Troubleshooting","text":"<p>Out of Memory: Reduce batch size in config files</p> <p>Model Download Issues: - Use ModelScope as alternative source - Check Hugging Face token for gated models</p> <p>Import Errors: Verify all dependencies installed with <code>uv pip list</code></p> <p>Notebook Won't Execute: Make sure you've installed Jupyter with <code>uv pip install -e '.[notebook]'</code></p>"},{"location":"example/notebooks/benchmark/benchmark/","title":"Benchmark","text":"In\u00a0[1]: Copied! <pre>from dnallm import load_config\nfrom dnallm import Benchmark\n</pre> from dnallm import load_config from dnallm import Benchmark In\u00a0[2]: Copied! <pre># Load configurations\nconfigs = load_config(\"./benchmark_config.yaml\")\n</pre> # Load configurations configs = load_config(\"./benchmark_config.yaml\") In\u00a0[3]: Copied! <pre># Initialize benchmark\nbenchmark = Benchmark(config=configs)\n</pre> # Initialize benchmark benchmark = Benchmark(config=configs) <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> <pre>Format labels:   0%|          | 0/500 [00:00&lt;?, ? examples/s]</pre> In\u00a0[4]: Copied! <pre># Run benchmark\nresults = benchmark.run()\n</pre> # Run benchmark results = benchmark.run() <pre>Dataset name: promoter_data\nModel name: Plant DNABERT\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnabert-BPE-promoter\n14:32:32 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnabert-BPE-promoter\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnabert-BPE-promoter\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnabert-BPE-promoter\n</pre> <pre>Encoding inputs:   0%|          | 0/500 [00:00&lt;?, ? examples/s]</pre> <pre>14:32:34 - dnallm.utils.support - INFO - Using device: mps\n</pre> <pre>Inferring: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:38&lt;00:00,  2.42s/it]\n</pre> <pre>Model name: Plant DNAGPT\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE-promoter\n14:33:14 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE-promoter\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE-promoter\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE-promoter\n</pre> <pre>Encoding inputs:   0%|          | 0/500 [00:00&lt;?, ? examples/s]</pre> <pre>14:33:16 - dnallm.utils.support - INFO - Using device: mps\n</pre> <pre>Inferring: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:41&lt;00:00,  2.57s/it]\n</pre> <pre>Model name: Nucleotide Transformer\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/nucleotide-transformer-v2-100m-promoter\n</pre> <pre>2025-12-28 14:33:59,334 - modelscope - INFO - Got 9 files, start to download ...\n</pre> <pre>Processing 9 items:   0%|          | 0.00/9.00 [00:00&lt;?, ?it/s]</pre> <pre>Downloading [config.json]:   0%|          | 0.00/1.19k [00:00&lt;?, ?B/s]</pre> <pre>Downloading [configuration.json]:   0%|          | 0.00/42.0 [00:00&lt;?, ?B/s]</pre> <pre>Downloading [README.md]:   0%|          | 0.00/2.90k [00:00&lt;?, ?B/s]</pre> <pre>Downloading [special_tokens_map.json]:   0%|          | 0.00/101 [00:00&lt;?, ?B/s]</pre> <pre>Downloading [esm_config.py]:   0%|          | 0.00/14.5k [00:00&lt;?, ?B/s]</pre> <pre>Downloading [model.safetensors]:   0%|          | 0.00/365M [00:00&lt;?, ?B/s]</pre> <pre>Downloading [modeling_esm.py]:   0%|          | 0.00/56.8k [00:00&lt;?, ?B/s]</pre> <pre>Downloading [tokenizer_config.json]:   0%|          | 0.00/927 [00:00&lt;?, ?B/s]</pre> <pre>Downloading [vocab.txt]:   0%|          | 0.00/28.0k [00:00&lt;?, ?B/s]</pre> <pre>2025-12-28 14:34:22,796 - modelscope - INFO - Download model 'zhangtaolab/nucleotide-transformer-v2-100m-promoter' successfully.\n</pre> <pre>14:34:22 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/nucleotide-transformer-v2-100m-promoter\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/nucleotide-transformer-v2-100m-promoter\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/nucleotide-transformer-v2-100m-promoter\n</pre> <pre>Encoding inputs:   0%|          | 0/500 [00:00&lt;?, ? examples/s]</pre> <pre>14:34:25 - dnallm.utils.support - INFO - Using device: mps\n</pre> <pre>Inferring: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:33&lt;00:00,  2.10s/it]\n</pre> In\u00a0[5]: Copied! <pre># Display results\nfor dataset_name, dataset_results in results.items():\n    print(f\"\\n{dataset_name}:\")\n    for model_name, metrics in dataset_results.items():\n        print(f\"  {model_name}:\")\n        for metric, value in metrics.items():\n            if metric not in [\"curve\", \"scatter\"]:\n                print(f\"    {metric}: {value:.4f}\")\n</pre> # Display results for dataset_name, dataset_results in results.items():     print(f\"\\n{dataset_name}:\")     for model_name, metrics in dataset_results.items():         print(f\"  {model_name}:\")         for metric, value in metrics.items():             if metric not in [\"curve\", \"scatter\"]:                 print(f\"    {metric}: {value:.4f}\") <pre>\npromoter_data:\n  Plant DNABERT:\n    accuracy: 0.7840\n    precision: 0.7569\n    recall: 0.8516\n    f1: 0.8015\n    mcc: 0.5712\n    AUROC: 0.8593\n    AUPRC: 0.8505\n    TPR: 0.8516\n    TNR: 0.7131\n    FPR: 0.2869\n    FNR: 0.1484\n  Plant DNAGPT:\n    accuracy: 0.8000\n    precision: 0.7708\n    recall: 0.8672\n    f1: 0.8162\n    mcc: 0.6035\n    AUROC: 0.8783\n    AUPRC: 0.8751\n    TPR: 0.8672\n    TNR: 0.7295\n    FPR: 0.2705\n    FNR: 0.1328\n  Nucleotide Transformer:\n    accuracy: 0.7680\n    precision: 0.7632\n    recall: 0.7930\n    f1: 0.7778\n    mcc: 0.5357\n    AUROC: 0.8510\n    AUPRC: 0.8422\n    TPR: 0.7930\n    TNR: 0.7418\n    FPR: 0.2582\n    FNR: 0.2070\n</pre> In\u00a0[6]: Copied! <pre># Plot metrics\n# pbar: bar chart for all the scores, pline: ROC curve\npbar, pline = benchmark.plot(results, save_path=\"plot.pdf\")\n</pre> # Plot metrics # pbar: bar chart for all the scores, pline: ROC curve pbar, pline = benchmark.plot(results, save_path=\"plot.pdf\") <pre>Metrics bar charts saved to plot_metrics.pdf\nROC/PR curves saved to plot_roc.pdf\n</pre> In\u00a0[7]: Copied! <pre>pbar.show()\npline.show()\n</pre> pbar.show() pline.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/benchmark/benchmark/#models-benchmark","title":"Models benchmark\u00b6","text":""},{"location":"example/notebooks/data_prepare/finetune/finetune_data/","title":"Fine-tuning Data","text":"In\u00a0[1]: Copied! <pre>from dnallm.datahandling import *\n</pre> from dnallm.datahandling import * In\u00a0[2]: Copied! <pre># Display preset datasets\nshow_preset_dataset()\n</pre> # Display preset datasets show_preset_dataset() Out[2]: <pre>{'nucleotide_transformer_downstream_tasks': {'name': 'lgq12697/nucleotide_transformer_downstream_tasks',\n  'description': 'A collection of nucleotidetransformer downstream tasks datasets from NT.',\n  'reference': 'https://doi.org/10.1038/s41592-024-02523-z',\n  'tasks': ['enhancers',\n   'enhancers_types',\n   'H2AFZ',\n   'H3K27ac',\n   'H3K27me3',\n   'H3K36me3',\n   'H3K4me1',\n   'H3K4me2',\n   'H3K4me3',\n   'H3K9ac',\n   'H3K9me3',\n   'H4K20me1',\n   'promoter_all',\n   'promoter_no_tata',\n   'promoter_tata',\n   'splice_sites_acceptors',\n   'splice_sites_all',\n   'splice_sites_donors'],\n  'default_task': 'enhancers',\n  'format': 'csv',\n  'sequence': 'sequence',\n  'label': 'label',\n  'separator': ',',\n  'multi_separator': ';'},\n 'GUE': {'name': 'lgq12697/GUE',\n  'description': 'A dataset for GenomeUnderstanding Evaluation (GUE) tasks from DNABERT-2.',\n  'reference': 'https://doi.org/10.48550/arXiv.2306.15006',\n  'tasks': ['emp_H3',\n   'emp_H3K14ac',\n   'emp_H3K36me3',\n   'emp_H3K4me1',\n   'emp_H3K4me2',\n   'emp_H3K4me3',\n   'emp_H3K79me3',\n   'emp_H3K9ac',\n   'emp_H4',\n   'emp_H4ac',\n   'EPI_GM12878',\n   'EPI_HeLa-S3',\n   'EPI_HUVEC',\n   'EPI_IMR90',\n   'EPI_K562',\n   'EPI_NHEK',\n   'fungi_species_20',\n   'human_tf_0',\n   'human_tf_1',\n   'human_tf_2',\n   'human_tf_3',\n   'human_tf_4',\n   'mouse_0',\n   'mouse_1',\n   'mouse_2',\n   'mouse_3',\n   'mouse_4',\n   'phage_fragments',\n   'prom_300_all',\n   'prom_300_notata',\n   'prom_300_tata',\n   'prom_core_all',\n   'prom_core_notata',\n   'prom_core_tata',\n   'splice_reconstructed',\n   'virus_covid',\n   'virus_species_40'],\n  'default_task': 'prom_core_all',\n  'format': 'csv',\n  'sequence': 'sequence',\n  'label': 'label',\n  'separator': ',',\n  'multi_separator': ';'},\n 'plant-genomic-benchmark': {'name': 'lgq12697/plant-genomic-benchmark',\n  'description': 'A benchmark dataset forplant genomic tasks from AgroNT.',\n  'reference': 'https://doi.org/10.1038/s42003-024-06465-2',\n  'tasks': ['poly_a.arabidopsis_thaliana',\n   'poly_a.oryza_sativa_indica',\n   'poly_a.oryza_sativa_japonica',\n   'poly_a.chlamydomonas_reinhardtii',\n   'poly_a.medicago_truncatula',\n   'poly_a.trifolium_pratense',\n   'splicing.arabidopsis_thaliana_acceptor',\n   'splicing.arabidopsis_thaliana_donor',\n   'lncrna.m_esculenta',\n   'lncrna.z_mays',\n   'lncrna.g_max',\n   'lncrna.s_lycopersicum',\n   'lncrna.t_aestivum',\n   'lncrna.s_bicolor',\n   'promoter_strength.leaf',\n   'promoter_strength.protoplast',\n   'terminator_strength.leaf',\n   'terminator_strength.protoplast',\n   'gene_exp.arabidopsis_thaliana',\n   'gene_exp.oryza_sativa',\n   'gene_exp.zea_mays',\n   'gene_exp.glycine_max',\n   'gene_exp.solanum_lycopersicum',\n   'pro_seq.m_esculenta',\n   'chromatin_access.arabidopis_thaliana',\n   'chromatin_access.oryza_sativa_MH63',\n   'chromatin_access.oryza_sativa_ZS97',\n   'chromatin_access.brachypodium_distachyon',\n   'chromatin_access.zea_mays',\n   'chromatin_access.sorghum_bicolor',\n   'chromatin_access.setaria_italica'],\n  'default_task': 'poly_a.arabidopsis_thaliana',\n  'format': 'csv',\n  'sequence': 'sequence',\n  'label': 'label',\n  'separator': ',',\n  'multi_separator': ';'}}</pre> In\u00a0[3]: Copied! <pre># Load a preset dataset\ndataset = load_preset_dataset(dataset_name='plant-genomic-benchmark', task='promoter_strength.leaf')\n</pre> # Load a preset dataset dataset = load_preset_dataset(dataset_name='plant-genomic-benchmark', task='promoter_strength.leaf') <pre>Loading dataset: lgq12697/plant-genomic-benchmark ...\npromoter_strength.leaf\n</pre> <pre>Downloading [README.md]: 0.00B [00:00, ?B/s]</pre> <pre>2025-12-28 14:35:58,663 - modelscope - INFO - storing https://www.modelscope.cn/api/v1/datasets/lgq12697/plant-genomic-benchmark/repo?Source=SDK&amp;Revision=master&amp;FilePath=README.md&amp;View=False in cache at /Users/forrest/.cache/modelscope/hub/datasets/c59000af909fc22889b4786d4a362c0d0dc6b8e4b07094c6c0c358cfcb435f02\n2025-12-28 14:35:58,664 - modelscope - INFO - creating metadata file for /Users/forrest/.cache/modelscope/hub/datasets/c59000af909fc22889b4786d4a362c0d0dc6b8e4b07094c6c0c358cfcb435f02\n</pre> <pre>Downloading data:   0%|          | 0.00/12.1M [00:00&lt;?, ?B/s]</pre> <pre>2025-12-28 14:36:08,124 - modelscope - INFO - storing https://www.modelscope.cn/api/v1/datasets/lgq12697/plant-genomic-benchmark/repo?Source=SDK&amp;Revision=master&amp;FilePath=promoter_strength.leaf%2Ftrain.csv in cache at /Users/forrest/.cache/modelscope/hub/datasets/downloads/5d18dc4a80980450245c31009650f82a2db8cb9914c761a892e252f6733bb673\n2025-12-28 14:36:08,125 - modelscope - INFO - creating metadata file for /Users/forrest/.cache/modelscope/hub/datasets/downloads/5d18dc4a80980450245c31009650f82a2db8cb9914c761a892e252f6733bb673\n</pre> <pre>Downloading data:   0%|          | 0.00/1.49M [00:00&lt;?, ?B/s]</pre> <pre>2025-12-28 14:36:09,658 - modelscope - INFO - storing https://www.modelscope.cn/api/v1/datasets/lgq12697/plant-genomic-benchmark/repo?Source=SDK&amp;Revision=master&amp;FilePath=promoter_strength.leaf%2Ftest.csv in cache at /Users/forrest/.cache/modelscope/hub/datasets/downloads/7162681f0376bc615bdfcb777bd3306e4a622398e48c5cd637d44cf1a662d854\n2025-12-28 14:36:09,659 - modelscope - INFO - creating metadata file for /Users/forrest/.cache/modelscope/hub/datasets/downloads/7162681f0376bc615bdfcb777bd3306e4a622398e48c5cd637d44cf1a662d854\n</pre> <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> <pre>Generating test split: 0 examples [00:00, ? examples/s]</pre> In\u00a0[4]: Copied! <pre># Display the dataset samples\ndataset.show(head=1)\n</pre> # Display the dataset samples dataset.show(head=1) <pre>Dataset: train\n{0: {'name': 'ENSRNA049481990_Sb',\n     'sequence': 'CGTTTGGGTATGGACATTTAGACTTGTCGTGTTCCTGATGCCTCCCATTCCTATGGTTCTTAGGTGCTCCTTCCTCTTCCTTTCGCTAGCGCAATTGATTTAGTGATGAACACAATATACATTCCAAAGCACATAGTTAGATGAGAGCCTGATGGCAATTGGCAAGTCAG',\n     'labels': -0.2166846610255508}}\nDataset: test\n{0: {'name': 'AT5G03425_At',\n     'sequence': 'TGAGTGAAGGCAGAATTGACCCATGCAGCTTCCTTTCTTTCACCACTCACTTGCTAGGAAACTACAAAAATAGAAAAAGAAAACTCACGGCAACCAAAAACGCGAACTCCTAGAGGGTTTCGAACACTTTGAAATTTGTATCAGACATCAAATGAAATCTTTAACTTCTT',\n     'labels': -0.5374512291279694}}\n</pre> In\u00a0[5]: Copied! <pre># Display dataset statistics\ndataset.statistics()\n</pre> # Display dataset statistics dataset.statistics() Out[5]: <pre>{'train': {'data_type': 'regression',\n  'n_samples': 58179,\n  'min_len': 170,\n  'max_len': 170,\n  'mean_len': 170.0,\n  'median_len': 170.0},\n 'test': {'data_type': 'regression',\n  'n_samples': 7154,\n  'min_len': 170,\n  'max_len': 170,\n  'mean_len': 170.0,\n  'median_len': 170.0}}</pre> In\u00a0[6]: Copied! <pre># Visualize dataset statistics\ndataset.plot_statistics()\n</pre> # Visualize dataset statistics dataset.plot_statistics() <pre>Successfully plotted dataset statistics.\n</pre> In\u00a0[7]: Copied! <pre>## Pre-processing of dataset\nfrom dnallm import load_config, load_model_and_tokenizer\n\n# Load configuration\nconfigs = load_config(\"finetune_config.yaml\")\n\n# Load model and tokenizer\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\", \n    task_config=configs[\"task\"], \n    source=\"modelscope\"\n)\n\n# Tokenize the dataset\ndataset.encode_sequences(tokenizer=tokenizer)\n</pre> ## Pre-processing of dataset from dnallm import load_config, load_model_and_tokenizer  # Load configuration configs = load_config(\"finetune_config.yaml\")  # Load model and tokenizer model, tokenizer = load_model_and_tokenizer(     \"zhangtaolab/plant-dnabert-BPE\",      task_config=configs[\"task\"],      source=\"modelscope\" )  # Tokenize the dataset dataset.encode_sequences(tokenizer=tokenizer) <pre>Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnabert-BPE\n14:36:12 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnabert-BPE\n</pre> <pre>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnabert-BPE and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> <pre>Encoding inputs:   0%|          | 0/58179 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/7154 [00:00&lt;?, ? examples/s]</pre> In\u00a0[8]: Copied! <pre>## Start fine-tuning\n# from dnallm.finetune import DNATrainer\n\n# trainer = DNATrainer(\n#     config=configs,\n#     model=model,\n#     datasets=dataset\n# )\n# trainer.train()\n</pre> ## Start fine-tuning # from dnallm.finetune import DNATrainer  # trainer = DNATrainer( #     config=configs, #     model=model, #     datasets=dataset # ) # trainer.train() In\u00a0[9]: Copied! <pre># Load tokenizer for demonstration\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"zhangtaolab/plant-dnabert-BPE\")\n</pre> # Load tokenizer for demonstration from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained(\"zhangtaolab/plant-dnabert-BPE\") In\u00a0[10]: Copied! <pre># Load dataset from Hugging Face\ndataset = DNADataset.from_huggingface(\n    \"zhangtaolab/plant-multi-species-core-promoters\", \n    seq_col=\"sequence\", \n    label_col=\"label\", \n    tokenizer=tokenizer, \n    max_length=512\n)\n</pre> # Load dataset from Hugging Face dataset = DNADataset.from_huggingface(     \"zhangtaolab/plant-multi-species-core-promoters\",      seq_col=\"sequence\",      label_col=\"label\",      tokenizer=tokenizer,      max_length=512 ) In\u00a0[11]: Copied! <pre># Load dataset from ModelScope\ndataset = DNADataset.from_modelscope(\n    \"zhangtaolab/plant-multi-species-core-promoters\", \n    seq_col=\"sequence\", \n    label_col=\"label\", \n    tokenizer=tokenizer, \n    max_length=512\n)\n</pre> # Load dataset from ModelScope dataset = DNADataset.from_modelscope(     \"zhangtaolab/plant-multi-species-core-promoters\",      seq_col=\"sequence\",      label_col=\"label\",      tokenizer=tokenizer,      max_length=512 ) In\u00a0[12]: Copied! <pre># Load single dataset\ndataset = DNADataset.load_local_data(\n    \"../../../../tests/test_data/regression/train.csv\", \n    seq_col=\"sequence\", \n    label_col=\"label\",\n    tokenizer=tokenizer, \n    max_length=512\n)\n</pre> # Load single dataset dataset = DNADataset.load_local_data(     \"../../../../tests/test_data/regression/train.csv\",      seq_col=\"sequence\",      label_col=\"label\",     tokenizer=tokenizer,      max_length=512 ) <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> <pre>Format labels:   0%|          | 0/500 [00:00&lt;?, ? examples/s]</pre> In\u00a0[13]: Copied! <pre># Load multiple files (e.g., pre-split datasets)\ndataset = DNADataset.load_local_data(\n    {\n        \"train\": \"../../../../tests/test_data/regression/train.csv\", \n        \"test\": \"../../../../tests/test_data/regression/test.csv\", \n        \"validation\": \"../../../../tests/test_data/regression/dev.csv\"\n    },\n    seq_col=\"sequence\", \n    label_col=\"label\",\n    tokenizer=tokenizer, \n    max_length=512\n)\n</pre> # Load multiple files (e.g., pre-split datasets) dataset = DNADataset.load_local_data(     {         \"train\": \"../../../../tests/test_data/regression/train.csv\",          \"test\": \"../../../../tests/test_data/regression/test.csv\",          \"validation\": \"../../../../tests/test_data/regression/dev.csv\"     },     seq_col=\"sequence\",      label_col=\"label\",     tokenizer=tokenizer,      max_length=512 ) <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> <pre>Format labels:   0%|          | 0/100 [00:00&lt;?, ? examples/s]</pre> <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> <pre>Format labels:   0%|          | 0/100 [00:00&lt;?, ? examples/s]</pre> In\u00a0[14]: Copied! <pre>## Binary classification example\n# For binary classification, labels should be 0 and 1\n# Data format: two columns, \"sequence\" and \"label\"\n'''\nsequence,label\nATCGATCGATCG,1\nGCTAGCTAGCTA,0\nTATATATATATA,1\n'''\n</pre> ## Binary classification example # For binary classification, labels should be 0 and 1 # Data format: two columns, \"sequence\" and \"label\" ''' sequence,label ATCGATCGATCG,1 GCTAGCTAGCTA,0 TATATATATATA,1 ''' Out[14]: <pre>'\\nsequence,label\\nATCGATCGATCG,1\\nGCTAGCTAGCTA,0\\nTATATATATATA,1\\n'</pre> In\u00a0[15]: Copied! <pre>## Multi-class classification example\n# For multi-class classification, labels should be integers starting from 0\n# Data format: two columns, \"sequence\" and \"label\"\n'''\nsequence,label\nATCGATCGATCG,0\nGCTAGCTAGCTA,1\nTATATATATATA,2\nCGCGCGCGCGCG,3\n'''\n</pre> ## Multi-class classification example # For multi-class classification, labels should be integers starting from 0 # Data format: two columns, \"sequence\" and \"label\" ''' sequence,label ATCGATCGATCG,0 GCTAGCTAGCTA,1 TATATATATATA,2 CGCGCGCGCGCG,3 ''' Out[15]: <pre>'\\nsequence,label\\nATCGATCGATCG,0\\nGCTAGCTAGCTA,1\\nTATATATATATA,2\\nCGCGCGCGCGCG,3\\n'</pre> In\u00a0[16]: Copied! <pre>## Multi-label classification example\n# For multi-label classification, labels should be a list of integers separated by specific delimiters (e.g., \";\")\n# Data format: two columns, \"sequence\" and \"label\"\n'''\nsequence,label\nATCGATCGATCG,1;0;1;0;0\nGCTAGCTAGCTA,0;1;0;1;0\nTATATATATATA,1;1;0;0;1\n'''\n</pre> ## Multi-label classification example # For multi-label classification, labels should be a list of integers separated by specific delimiters (e.g., \";\") # Data format: two columns, \"sequence\" and \"label\" ''' sequence,label ATCGATCGATCG,1;0;1;0;0 GCTAGCTAGCTA,0;1;0;1;0 TATATATATATA,1;1;0;0;1 ''' Out[16]: <pre>'\\nsequence,label\\nATCGATCGATCG,1;0;1;0;0\\nGCTAGCTAGCTA,0;1;0;1;0\\nTATATATATATA,1;1;0;0;1\\n'</pre> In\u00a0[17]: Copied! <pre>## Regression example\n# For regression, labels should be float numbers\n# Data format: two columns, \"sequence\" and \"label\"\n'''\nsequence,label\nATCGATCGATCG,0.85\nGCTAGCTAGCTA,0.23\nTATATATATATA,0.67\n'''\n</pre> ## Regression example # For regression, labels should be float numbers # Data format: two columns, \"sequence\" and \"label\" ''' sequence,label ATCGATCGATCG,0.85 GCTAGCTAGCTA,0.23 TATATATATATA,0.67 ''' Out[17]: <pre>'\\nsequence,label\\nATCGATCGATCG,0.85\\nGCTAGCTAGCTA,0.23\\nTATATATATATA,0.67\\n'</pre> In\u00a0[18]: Copied! <pre>## Token classification example / Named Entity Recognition (NER)\n# For token classification, please refer to example/notebooks/finetune_NER_task/data_generation_and_inference.ipynb\n</pre> ## Token classification example / Named Entity Recognition (NER) # For token classification, please refer to example/notebooks/finetune_NER_task/data_generation_and_inference.ipynb In\u00a0[19]: Copied! <pre>## Masked language modeling (MLM) example\n# For MLM, It only needs one column \"sequence\" without labels since this is a self-supervised task.\n# Sequences can be extracted from FASTA files from genomic data.\n'''\nsequence\nATCGATCGATCG\nGCTAGCTAGCTA\nTATATATATATA\n'''\n</pre> ## Masked language modeling (MLM) example # For MLM, It only needs one column \"sequence\" without labels since this is a self-supervised task. # Sequences can be extracted from FASTA files from genomic data. ''' sequence ATCGATCGATCG GCTAGCTAGCTA TATATATATATA ''' Out[19]: <pre>'\\nsequence\\nATCGATCGATCG\\nGCTAGCTAGCTA\\nTATATATATATA\\n'</pre> In\u00a0[20]: Copied! <pre>## Generation example / Causal Language Modeling (CLM)\n# For CLM, It needs two columns \"input_sequence\" and \"output_sequence\"\n# Sequences can be extracted from FASTA files from genomic data.\n'''\nsequence\nATCGATCGATCG\nGCTAGCTAGCTA\nTATATATATATA\n'''\n</pre> ## Generation example / Causal Language Modeling (CLM) # For CLM, It needs two columns \"input_sequence\" and \"output_sequence\" # Sequences can be extracted from FASTA files from genomic data. ''' sequence ATCGATCGATCG GCTAGCTAGCTA TATATATATATA ''' Out[20]: <pre>'\\nsequence\\nATCGATCGATCG\\nGCTAGCTAGCTA\\nTATATATATATA\\n'</pre> In\u00a0[21]: Copied! <pre>## These sequences and labels can be saved as a CSV file for loading.\n# dataset = DNADataset.load_local_data(\n#     \"data.csv\", \n#     seq_col=\"sequence\", \n#     label_col=\"label\",\n#     tokenizer=tokenizer, \n#     max_length=512\n# )\n</pre> ## These sequences and labels can be saved as a CSV file for loading. # dataset = DNADataset.load_local_data( #     \"data.csv\",  #     seq_col=\"sequence\",  #     label_col=\"label\", #     tokenizer=tokenizer,  #     max_length=512 # ) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/data_prepare/finetune/finetune_data/#use-the-preset-dataset-to-quickly-try-the-finetuning-pipeline","title":"Use the preset dataset to quickly try the finetuning pipeline\u00b6","text":""},{"location":"example/notebooks/data_prepare/finetune/finetune_data/#load-datasets-from-hugging-face-or-modelscope","title":"Load datasets from Hugging Face or ModelScope\u00b6","text":""},{"location":"example/notebooks/data_prepare/finetune/finetune_data/#load-datasets-from-local-paths","title":"Load datasets from local paths\u00b6","text":""},{"location":"example/notebooks/data_prepare/finetune/finetune_data/#manually-prepare-dataset-for-fine-tuning","title":"Manually prepare dataset for fine-tuning\u00b6","text":""},{"location":"example/notebooks/data_prepare/predict/predict_data/","title":"Prediction Data Preparation","text":""},{"location":"example/notebooks/data_prepare/predict/predict_data/#prediction-data-preparation","title":"Prediction Data Preparation\u00b6","text":"<p>This notebook demonstrates how to prepare data for inference/prediction tasks.</p>"},{"location":"example/notebooks/data_prepare/predict/predict_data/#overview","title":"Overview\u00b6","text":"<p>For inference tasks, you need to prepare your input sequences in the correct format expected by the DNALLM inference pipeline.</p>"},{"location":"example/notebooks/data_prepare/predict/predict_data/#expected-input-format","title":"Expected Input Format\u00b6","text":"<p>DNALLM inference accepts:</p> <ul> <li>Plain text files (<code>.txt</code>, <code>.fasta</code>) with one sequence per line</li> <li>CSV files with sequence and label columns</li> <li>JSON files with structured data</li> </ul>"},{"location":"example/notebooks/data_prepare/predict/predict_data/#example","title":"Example\u00b6","text":"<pre>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.inference import DNAInference\n\n# Load model and configuration\nconfigs = load_config(\"path/to/config.yaml\")\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name=\"zhangtaolab/plant-dnabert-BPE\",\n    task_config=configs['task']\n)\n\n# Initialize inference\ninferencer = DNAInference(\n    model=model,\n    tokenizer=tokenizer,\n    device=\"cuda\"\n)\n\n# Run inference\nresults = inferencer.infer(\n    data_path=\"path/to/sequences.txt\",\n    batch_size=32\n)\n</pre>"},{"location":"example/notebooks/data_prepare/predict/predict_data/#data-requirements","title":"Data Requirements\u00b6","text":"<ul> <li>Sequences should be in standard nucleotide format (A, T, C, G)</li> <li>Remove or replace non-standard characters (N, R, Y, etc.)</li> <li>Ensure consistent sequence lengths if required by your model</li> <li>Use appropriate file encoding (UTF-8 recommended)</li> </ul>"},{"location":"example/notebooks/finetune_NER_task/data_generation_and_inference/","title":"NER Data Generation","text":"In\u00a0[1]: Copied! <pre># First install two dependencies for generating NER datasets, and make sure you have installed bedtools in your system\n# please make sure you have installed bedtools in your system\n# !conda install -c bioconda bedtools\n# or in macos, you can use brew install bedtools\n!brew install bedtools\n\n# Omit it if you have already installed these two packages\n!uv pip install pyfastx pybedtools\n</pre> # First install two dependencies for generating NER datasets, and make sure you have installed bedtools in your system # please make sure you have installed bedtools in your system # !conda install -c bioconda bedtools # or in macos, you can use brew install bedtools !brew install bedtools  # Omit it if you have already installed these two packages !uv pip install pyfastx pybedtools <pre>==&gt; Auto-updating Homebrew...\nAdjust how often this is run with `$HOMEBREW_AUTO_UPDATE_SECS` or disable with\n`$HOMEBREW_NO_AUTO_UPDATE=1`. Hide these hints with `$HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/portable-ruby/blobs/sha256:1c98fa49eacc935640a6f8e10a2bf33f14cfc276804b71ddb658ea45ba99d167\n######################################################################### 100.0%41.5%            50.3%                        57.0%              58.8%                           59.0%   59.8%          62.0%              63.1%             65.3%         66.3%  67.8%8.5%             71.2%1.9%              72.6%######                      72.8%.0% 75.1%           75.6%   76.3%                   76.6%##                   76.9%###################################                   77.1%               78.2%             78.4%###########                  78.9%%80.1% 80.1% 80.8%#######              84.5% 85.3%            85.6%###             85.7%##            86.6%   87.3%#####           89.0%       91.9% 93.0%     93.1%###############       94.4%###############    97.5%#####################    98.0%   98.7%\n==&gt; Pouring portable-ruby-3.4.8.arm64_big_sur.bottle.tar.gz\n==&gt; Auto-updated Homebrew!\n==&gt; Updated Homebrew from 5.0.3 (c3790fc6e4) to 5.0.7 (d61f229fd2).\nUpdated 2 taps (homebrew/core and homebrew/cask).\n==&gt; New Formulae\nastra: Command-Line Interface for DataStax Astra\nbookokrat: Terminal EPUB Book Reader\ncalm-cli: CLI allows you to interact with the Common Architecture Language Model (CALM)\ncarl: Calendar for the command-line\ncinecli: Browse, inspect, and launch movie torrents directly from your terminal\nctre: Compile-time PCRE-compatible regular expression matcher for C++\ndepot: Build your Docker images in the cloud\ndnspyre: CLI tool for a high QPS DNS benchmark\ndocker-language-server: Language server for Dockerfiles, Compose files, and Bake files\nflecs: Fast entity component system for C &amp; C++\ngarage: S3 object store so reliable you can run it outside datacenters\ngit-get: Better way to clone, organize and manage multiple git repositories\ngit-pages: Scalable static site server for Git forges\ngit-pages-cli: Tool for publishing a site to a git-pages server\ngoat: General purpose AT Protocol CLI in Go\ngup: Update binaries installed by go install\nhayagriva: Bibliography management tool\njsonfmt: Like gofmt, but for JSON files\nkhaos: Kafka traffic simulator for observability and chaos engineering\nklog: Command-line tool for time tracking in a human-readable, plain-text file format\nkubernetes-cli@1.34: Kubernetes command-line interface\nkyua: Testing framework for infrastructure software\nlibevdev: Wrapper library for evdev devices\nlispkit: Scheme framework for extension and scripting languages on macOS and iOS\nmacchanger: Change your mac address, for macOS\nmapscii: Whole World In Your Console\nmcp-scan: Constrain, log and scan your MCP connections for security vulnerabilities\nmistral-vibe: Minimal CLI coding agent\nmole: Deep clean and optimize your Mac\nmq: Jq-like command-line tool for markdown processing\nmufetch: Neofetch-style music cli\nneo4j-mcp: Neo4j official Model Context Protocol server for AI tools\nnetshow: Interactive network connection monitor with friendly service names\nnkt: TUI for fast and simple interacting with your BibLaTeX database\noctodns: Tools for managing DNS across multiple providers\npapis: Powerful command-line document and bibliography manager\nphantom: CLI tool for seamless parallel development with Git worktrees\npixlet: App runtime and UX toolkit for pixel-based apps\npony-language-server: Language server for Pony\nrad: Modern CLI scripts made easy\nredu: Ncdu for your restic repository\nrmrfrs: Filesystem cleaning tool\nrockcraft: Tool to create OCI images using the language from Snapcraft and Charmcraft\nruby@3.4: Powerful, clean, object-oriented scripting language\nslicot: Fortran subroutines library for systems and control\nsnitch: Prettier way to inspect network connections\nsuperseedr: BitTorrent Client in your Terminal\nsvu: Semantic version utility\ntalm: Manage Talos Linux configurations the GitOps way\ntfclean: Remove applied moved block, import block, etc\ntree-sitter@0.25: Incremental parsing library\ntronbyt-server: Manage your apps on your Tronbyt (flashed Tidbyt) completely locally\nty: Extremely fast Python type checker, written in Rust\nvacuum: World's fastest OpenAPI &amp; Swagger linter\nwasm-bindgen: Facilitating high-level interactions between Wasm modules and JavaScript\nwifitui: Fast featureful friendly wifi terminal UI\nwitr: Why is this running?\nwuchale: Protobuf-like i18n from plain code\n==&gt; New Casks\n8bitdo-ultimate-software-v2: Control every piece of your controller\nairscroll: Smooth mouse scrolling utility\naks-desktop: Azure Kubernetes Service desktop application\nalma: AI chat application\nappbox: iOS app distribution tool\nathas: Lightweight code editor\ncardinal-search: Fastest file searching tool\ncomet: Web browser with integrated AI assistant\nconar: AI-powered database and data management tool\ncopilot-cli: Brings the power of Copilot coding agent directly to your terminal\ncopilot-cli@prerelease: Brings the power of Copilot coding agent directly to your terminal\ndatadog-security-cli: Datadog Security Product CLI\ndigiexam: Academic testing platform with device lockdown\ndnclient: Peer-to-peer VPN client for managed nebula networks\nelgato-studio: Capture and manage Elgato devices for content creation\nfont-amarna\nfont-bbh-bartle\nfont-bbh-bogle\nfont-bbh-hegarty\nfont-cause\nfont-geom\nfont-guguru-sans-code\nfont-guguru-sans-code-nf\nfont-gveret-levin\nfont-line-seed-jp\nfont-sekuya\nfontra-pak: Browser-based font editor\nglkvm: App for controlling GL.iNet KVM devices\nhyperwhisper: AI-powered speech-to-text transcription\nlocu: Daily planner and focus timer\nm32-edit: Remote control for Midas M32 audio consoles\nmacdown-3000: Markdown editor with live preview and syntax highlighting\nmace: Simplify compliance baseline creation, auditing, and management\nmaestro: AI agent command center\nmaru-jan: Play japanese mahjong online\nmonocle-app: Window dimming utility\nmotionik: Screen recording software\nmountmate: Menubar app to easily manage external drives\nmozregression-gui: Interactive regression range finder for Firefox and other Mozilla products\nmpluginmanager: Installer for MeldaProduction audio plugins\nnanoleaf: Control your Nanoleaf lights\nnessie-app: Knowledge base from AI chats\nopencode-desktop: AI coding agent desktop client\noracle-data-modeler: Graphical tool for data modeling tasks\npapercut-mobility-print-client: Client for printing to PaperCut Mobility Print queues\nportalbox: Share a region of your screen in video calls\nrocketman-choices-packager: Utility for customising installer package choices\nsmartsheet: Spreadsheet-style project management solution\nsnapmaker-orca: Slicing software for Snapmaker 3D printers, a fork of OrcaSlicer\nsourcegit: Git GUI client\nstirling-pdf: PDF utility\nstremio@beta: Open-source media center\nsupport: Menu bar app for user and help desk support\nswiftdialog: Admin utility that presents custom dialogs or messages from shell scripts\ntypeless: AI voice dictation that turns speech into polished text\nuuremote: NetEase UU remote desktop access and control tool\nvcamapp: Face-tracking virtual avatar app\nvisualdiffer: Visually compare folders and files\nvocaster-hub: Interface controller for Focusrite Vocaster One and Two\nwailbrew: Manage Homebrew packages with a UI\nwireless-workbench: Desktop app for RF coordination and wireless system management\nxmlmind-editor: Strictly validating near WYSIWYG XML editor\nyingfu-online: Education app for teens\nyoink: Drag and drop utility\nzo: Friendly personal server\n\nYou have 46 outdated formulae installed.\n\n\nThe 5.0.7 changelog can be found at:\n  https://github.com/Homebrew/brew/releases/tag/5.0.7\n==&gt; Fetching downloads for: bedtools\n\u280b Bottle Manifest bedtools (2.31.1)\u280b Bottle Manifest bedtools (2.31.1)\u2819 Bottle Manifest bedtools (2.31.1)\u2819 Bottle Manifest bedtools (2.31.1)\u281a Bottle Manifest bedtools (2.31.1)\u281a Bottle Manifest bedtools (2.31.1)\u281e Bottle Manifest bedtools (2.31.1)\u281e Bottle Manifest bedtools (2.31.1)\u2816 Bottle Manifest bedtools (2.31.1)\u2816 Bottle Manifest bedtools (2.31.1)\u2826 Bottle Manifest bedtools (2.31.1)\u2826 Bottle Manifest bedtools (2.31.1)\u2834 Bottle Manifest bedtools (2.31.1)\u2834 Bottle Manifest bedtools (2.31.1) ---------------- Downloading   4.1KB/-------\u2832 Bottle Manifest bedtools (2.31.1) ---------------- Downloading   4.1KB/-------\u2714\ufe0e Bottle Manifest bedtools (2.31.1)                  Downloaded   13.6KB/ 13.6KB\n\u2833 Bottle bedtools (2.31.1)\u2833 Bottle bedtools (2.31.1)\u2813 Bottle bedtools (2.31.1)\u2813 Bottle bedtools (2.31.1)\u280b Bottle bedtools (2.31.1)\u280b Bottle bedtools (2.31.1)\u2819 Bottle bedtools (2.31.1)\u2819 Bottle bedtools (2.31.1)\u281a Bottle bedtools (2.31.1)\u281a Bottle bedtools (2.31.1)\u281e Bottle bedtools (2.31.1)\u281e Bottle bedtools (2.31.1)\u2816 Bottle bedtools (2.31.1)\u2816 Bottle bedtools (2.31.1)\u2826 Bottle bedtools (2.31.1)\u2826 Bottle bedtools (2.31.1)\u2834 Bottle bedtools (2.31.1)\u2834 Bottle bedtools (2.31.1)\u2832 Bottle bedtools (2.31.1)\u2832 Bottle bedtools (2.31.1)\u2833 Bottle bedtools (2.31.1)\u2833 Bottle bedtools (2.31.1)\u2813 Bottle bedtools (2.31.1)\u2813 Bottle bedtools (2.31.1)\u280b Bottle bedtools (2.31.1) ------------------------- Downloading  12.3KB/633.1KB\u280b Bottle bedtools (2.31.1) #------------------------ Downloading  24.6KB/633.1KB\u2819 Bottle bedtools (2.31.1) #------------------------ Downloading  24.6KB/633.1KB\u2819 Bottle bedtools (2.31.1) #------------------------ Downloading  24.6KB/633.1KB\u281a Bottle bedtools (2.31.1) #------------------------ Downloading  24.6KB/633.1KB\u281a Bottle bedtools (2.31.1) ###---------------------- Downloading  77.8KB/633.1KB\u281e Bottle bedtools (2.31.1) ####--------------------- Downloading  98.3KB/633.1KB\u281e Bottle bedtools (2.31.1) #####-------------------- Downloading 131.1KB/633.1KB\u2816 Bottle bedtools (2.31.1) #####-------------------- Downloading 131.1KB/633.1KB\u2816 Bottle bedtools (2.31.1) #####-------------------- Downloading 131.1KB/633.1KB\u2826 Bottle bedtools (2.31.1) #####-------------------- Downloading 139.3KB/633.1KB\u2826 Bottle bedtools (2.31.1) #####-------------------- Downloading 139.3KB/633.1KB\u2834 Bottle bedtools (2.31.1) #####-------------------- Downloading 139.3KB/633.1KB\u2834 Bottle bedtools (2.31.1) ########----------------- Downloading 196.6KB/633.1KB\u2832 Bottle bedtools (2.31.1) ########----------------- Downloading 208.9KB/633.1KB\u2832 Bottle bedtools (2.31.1) #########---------------- Downloading 217.1KB/633.1KB\u2833 Bottle bedtools (2.31.1) #########---------------- Downloading 217.1KB/633.1KB\u2833 Bottle bedtools (2.31.1) #########---------------- Downloading 217.1KB/633.1KB\u2813 Bottle bedtools (2.31.1) #########---------------- Downloading 237.6KB/633.1KB\u2813 Bottle bedtools (2.31.1) #########---------------- Downloading 237.6KB/633.1KB\u280b Bottle bedtools (2.31.1) ##########--------------- Downloading 258.0KB/633.1KB\u280b Bottle bedtools (2.31.1) ##########--------------- Downloading 258.0KB/633.1KB\u2819 Bottle bedtools (2.31.1) ##########--------------- Downloading 262.1KB/633.1KB\u2819 Bottle bedtools (2.31.1) ###########-------------- Downloading 270.3KB/633.1KB\u281a Bottle bedtools (2.31.1) ###########-------------- Downloading 278.5KB/633.1KB\u281a Bottle bedtools (2.31.1) ###########-------------- Downloading 278.5KB/633.1KB\u281e Bottle bedtools (2.31.1) ###########-------------- Downloading 278.5KB/633.1KB\u281e Bottle bedtools (2.31.1) ############------------- Downloading 303.1KB/633.1KB\u2816 Bottle bedtools (2.31.1) ############------------- Downloading 307.2KB/633.1KB\u2816 Bottle bedtools (2.31.1) ############------------- Downloading 307.2KB/633.1KB\u2826 Bottle bedtools (2.31.1) #############------------ Downloading 319.5KB/633.1KB\u2826 Bottle bedtools (2.31.1) #############------------ Downloading 319.5KB/633.1KB\u2834 Bottle bedtools (2.31.1) #############------------ Downloading 331.8KB/633.1KB\u2834 Bottle bedtools (2.31.1) #############------------ Downloading 335.9KB/633.1KB\u2832 Bottle bedtools (2.31.1) ##############----------- Downloading 344.1KB/633.1KB\u2832 Bottle bedtools (2.31.1) ##############----------- Downloading 352.3KB/633.1KB\u2833 Bottle bedtools (2.31.1) ##############----------- Downloading 360.4KB/633.1KB\u2833 Bottle bedtools (2.31.1) ###############---------- Downloading 368.6KB/633.1KB\u2813 Bottle bedtools (2.31.1) ###############---------- Downloading 372.7KB/633.1KB\u2813 Bottle bedtools (2.31.1) ###############---------- Downloading 372.7KB/633.1KB\u280b Bottle bedtools (2.31.1) ###############---------- Downloading 385.0KB/633.1KB\u280b Bottle bedtools (2.31.1) ################--------- Downloading 393.2KB/633.1KB\u2819 Bottle bedtools (2.31.1) ################--------- Downloading 401.4KB/633.1KB\u2819 Bottle bedtools (2.31.1) ################--------- Downloading 405.5KB/633.1KB\u281a Bottle bedtools (2.31.1) ################--------- Downloading 417.8KB/633.1KB\u281a Bottle bedtools (2.31.1) #################-------- Downloading 426.0KB/633.1KB\u281e Bottle bedtools (2.31.1) #################-------- Downloading 434.2KB/633.1KB\u281e Bottle bedtools (2.31.1) ##################------- Downloading 446.5KB/633.1KB\u2816 Bottle bedtools (2.31.1) ##################------- Downloading 446.5KB/633.1KB\u2816 Bottle bedtools (2.31.1) ##################------- Downloading 462.8KB/633.1KB\u2826 Bottle bedtools (2.31.1) ##################------- Downloading 462.8KB/633.1KB\u2826 Bottle bedtools (2.31.1) ###################------ Downloading 471.0KB/633.1KB\u2834 Bottle bedtools (2.31.1) ###################------ Downloading 483.3KB/633.1KB\u2834 Bottle bedtools (2.31.1) ###################------ Downloading 491.5KB/633.1KB\u2832 Bottle bedtools (2.31.1) ####################----- Downloading 499.7KB/633.1KB\u2832 Bottle bedtools (2.31.1) ####################----- Downloading 503.8KB/633.1KB\u2833 Bottle bedtools (2.31.1) ####################----- Downloading 503.8KB/633.1KB\u2833 Bottle bedtools (2.31.1) #####################---- Downloading 528.4KB/633.1KB\u2813 Bottle bedtools (2.31.1) #####################---- Downloading 528.4KB/633.1KB\u2813 Bottle bedtools (2.31.1) #####################---- Downloading 540.7KB/633.1KB\u280b Bottle bedtools (2.31.1) #####################---- Downloading 540.7KB/633.1KB\u280b Bottle bedtools (2.31.1) ######################--- Downloading 548.9KB/633.1KB\u2819 Bottle bedtools (2.31.1) ######################--- Downloading 561.2KB/633.1KB\u2819 Bottle bedtools (2.31.1) ######################--- Downloading 569.3KB/633.1KB\u281a Bottle bedtools (2.31.1) ######################--- Downloading 569.3KB/633.1KB\u281a Bottle bedtools (2.31.1) ######################--- Downloading 569.3KB/633.1KB\u281e Bottle bedtools (2.31.1) #######################-- Downloading 585.7KB/633.1KB\u281e Bottle bedtools (2.31.1) #######################-- Downloading 585.7KB/633.1KB\u2816 Bottle bedtools (2.31.1) ########################- Downloading 602.1KB/633.1KB\u2816 Bottle bedtools (2.31.1) ########################- Downloading 602.1KB/633.1KB\u2826 Bottle bedtools (2.31.1) ########################- Downloading 602.1KB/633.1KB\u2826 Bottle bedtools (2.31.1) ########################- Downloading 602.1KB/633.1KB\u2834 Bottle bedtools (2.31.1) ########################- Downloading 610.3KB/633.1KB\u2834 Bottle bedtools (2.31.1) ######################### Downloading 622.6KB/633.1KB\u2832 Bottle bedtools (2.31.1) ######################### Downloading 622.6KB/633.1KB\u2832 Bottle bedtools (2.31.1) ######################### Downloading 626.7KB/633.1KB\u2833 Bottle bedtools (2.31.1)                           Extracting  633.1KB/633.1KB\u2714\ufe0e Bottle bedtools (2.31.1)                           Downloaded  633.1KB/633.1KB\n==&gt; Pouring bedtools--2.31.1.arm64_tahoe.bottle.tar.gz\n\ud83c\udf7a  /opt/homebrew/Cellar/bedtools/2.31.1: 43 files, 1.7MB\n==&gt; Running `brew cleanup bedtools`...\nDisable this behaviour by setting `HOMEBREW_NO_INSTALL_CLEANUP=1`.\nHide these hints with `HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n==&gt; `brew cleanup` has not been run in the last 30 days, running now...\nDisable this behaviour by setting `HOMEBREW_NO_INSTALL_CLEANUP=1`.\nHide these hints with `HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\nRemoving: /Users/forrest/Library/Caches/Homebrew/portable-ruby-3.4.7.arm64_big_sur.bottle.tar.gz... (12.2MB)\nRemoving: /Users/forrest/Library/Caches/Homebrew/bootsnap/e480d26faefcc0649aee4a1b4bb5df1bf6ea887486b0f1485789bf3d26f5e27a... (650 files, 5.6MB)\nRemoving: /Users/forrest/Library/Caches/Homebrew/bootsnap/42e939983ed75547f42207cad9f1e0fde134291f63f94bcb8df8abbd25416d42... (635 files, 5.5MB)\nRemoving: /Users/forrest/Library/Logs/Homebrew/ca-certificates... (64B)\nUsing Python 3.12.8 environment at: /Users/forrest/GitHub/DNALLM/.venv\nResolved 9 packages in 984ms                                         \nInstalled 3 packages in 17ms                                \n + pybedtools==0.12.0\n + pyfastx==2.2.0\n + pysam==0.23.3\n</pre> In\u00a0[2]: Copied! <pre># verify  intersectBed\n!which intersectBed\n</pre> # verify  intersectBed !which intersectBed <pre>/Users/forrest/miniconda3/bin/intersectBed\n</pre> In\u00a0[3]: Copied! <pre>import gzip\nimport random\nimport numpy as np\nfrom pyfastx import Fasta\nfrom pybedtools import BedTool\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport pickle\n\nfrom dnallm import load_config, load_model_and_tokenizer\n</pre> import gzip import random import numpy as np from pyfastx import Fasta from pybedtools import BedTool from tqdm import tqdm from collections import defaultdict import pickle  from dnallm import load_config, load_model_and_tokenizer In\u00a0[4]: Copied! <pre># Set random seed\nrandom.seed(42)\n</pre> # Set random seed random.seed(42) In\u00a0[5]: Copied! <pre># Set minimum and maximum extend length around a gene\nmin_ext = 50\nmax_ext = 100\next_list = [[random.randint(min_ext, max_ext), random.randint(min_ext, max_ext)] for x in range(60000)]\n</pre> # Set minimum and maximum extend length around a gene min_ext = 50 max_ext = 100 ext_list = [[random.randint(min_ext, max_ext), random.randint(min_ext, max_ext)] for x in range(60000)] In\u00a0[6]: Copied! <pre># Define Named Entity Recognition (NER) tags and corresponding id\n# NER includes IO, IOB, IOE, IOBES, BI, IE and BIES schemes, here we use IOB scheme\n# Example:\n# ..........[ exon1 ]-----[ exon2 ]-------[ exon3 ]........\n# 000000000012222222234444122222222344444412222222200000000\nnamed_entities = {\n    'intergenic': 'O',\n    'exon0': 'B-EXON',\n    'exon1': 'I-EXON',\n    'intron0': 'B-INTRON',\n    'intron1': 'I-INTRON',\n}\ntags_id = {\n    'O': 0,\n    'B-EXON': 1,\n    'I-EXON': 2,\n    'B-INTRON': 3,\n    'I-INTRON': 4,\n}\n</pre> # Define Named Entity Recognition (NER) tags and corresponding id # NER includes IO, IOB, IOE, IOBES, BI, IE and BIES schemes, here we use IOB scheme # Example: # ..........[ exon1 ]-----[ exon2 ]-------[ exon3 ]........ # 000000000012222222234444122222222344444412222222200000000 named_entities = {     'intergenic': 'O',     'exon0': 'B-EXON',     'exon1': 'I-EXON',     'intron0': 'B-INTRON',     'intron1': 'I-INTRON', } tags_id = {     'O': 0,     'B-EXON': 1,     'I-EXON': 2,     'B-INTRON': 3,     'I-INTRON': 4, } In\u00a0[7]: Copied! <pre>def get_gene_annotation(gene_anno):\n    cnt = 0\n    gene_info = {}\n    for gene in gene_anno:\n        gene_info[gene] = []\n        chrom = gene_anno[gene][\"chrom\"]\n        start = gene_anno[gene][\"start\"]\n        end = gene_anno[gene][\"end\"]\n        strand = gene_anno[gene][\"strand\"]\n        isoforms = gene_anno[gene][\"isoform\"]\n        # Get representative isoform\uff08longest\uff09\n        if not isoforms:\n            continue\n        lso_lens = [(iso, sum([(x[2]-x[1]) for x in isoforms[iso]])) for iso in isoforms]\n        representative = sorted(lso_lens, key=lambda x:x[1])[-1][0]\n        isoform_info = isoforms[representative]\n        iso_start = min([x[1] for x in isoform_info])\n        iso_end = max([x[2] for x in isoform_info])\n\n        if iso_start == start and iso_end == end:\n            is_reverse = False if strand == \"+\" else True\n            # Get intron annotation\n            last = 0\n            for region in sorted(isoform_info, key=lambda x:x[1], reverse=is_reverse):\n                if strand == \"+\":\n                    if last:\n                        intron = [chrom, last, region[1], \"intron\", strand]\n                        if intron[1] &lt; intron[2]:\n                            gene_info[gene].append(intron)\n                    last = region[2]\n                else:\n                    if last:\n                        intron = [chrom, region[2], last, \"intron\", strand]\n                        if intron[1] &lt; intron[2]:\n                            gene_info[gene].append(intron)\n                    last = region[1]\n                gene_info[gene].append([chrom, region[1], region[2], region[0], strand])\n        cnt += 1\n\n    return gene_info\n</pre> def get_gene_annotation(gene_anno):     cnt = 0     gene_info = {}     for gene in gene_anno:         gene_info[gene] = []         chrom = gene_anno[gene][\"chrom\"]         start = gene_anno[gene][\"start\"]         end = gene_anno[gene][\"end\"]         strand = gene_anno[gene][\"strand\"]         isoforms = gene_anno[gene][\"isoform\"]         # Get representative isoform\uff08longest\uff09         if not isoforms:             continue         lso_lens = [(iso, sum([(x[2]-x[1]) for x in isoforms[iso]])) for iso in isoforms]         representative = sorted(lso_lens, key=lambda x:x[1])[-1][0]         isoform_info = isoforms[representative]         iso_start = min([x[1] for x in isoform_info])         iso_end = max([x[2] for x in isoform_info])          if iso_start == start and iso_end == end:             is_reverse = False if strand == \"+\" else True             # Get intron annotation             last = 0             for region in sorted(isoform_info, key=lambda x:x[1], reverse=is_reverse):                 if strand == \"+\":                     if last:                         intron = [chrom, last, region[1], \"intron\", strand]                         if intron[1] &lt; intron[2]:                             gene_info[gene].append(intron)                     last = region[2]                 else:                     if last:                         intron = [chrom, region[2], last, \"intron\", strand]                         if intron[1] &lt; intron[2]:                             gene_info[gene].append(intron)                     last = region[1]                 gene_info[gene].append([chrom, region[1], region[2], region[0], strand])         cnt += 1      return gene_info In\u00a0[8]: Copied! <pre>def tokenization(genome, gene_anno, gene_info, tokenizer, outfile, ext_list, sampling=1e7):\n    \"\"\"\n    For each gene in `gene_anno`, extract the annotated exonic (and flanking) DNA subsequences,\n    run the tokenizer once per subsequence with offset_mapping enabled, \n    and record the (genomic_start, genomic_end, token) tuples for all non-special tokens.\n\n    - genome: dict mapping chromosome \u2192 SeqRecord (so that genome[chrom][start:end].seq gives a Seq)\n    - gene_anno: dict mapping gene_name \u2192 { \"chrom\": str, \"strand\": \"+\" or \"-\", ... }\n    - gene_info: dict mapping gene_name \u2192 list of (feature_id, exon_start, exon_end) or similar\n    - tokenizer: a HuggingFace\u2010style tokenizer that supports return_offsets_mapping\n    - outfile: (unused here, but you can write token_pos to it later)\n    - ext_list: list of (left_extension, right_extension) tuples parallel to gene_anno order\n    - sampling: random sampling the given number of genes for tokenization\n    \"\"\"\n    # 1) Precompute special-tokens set for O(1) membership checks\n    sp_tokens = set(tokenizer.special_tokens_map.values())\n\n    token_pos = {}\n    # Since gene_anno is likely a dict, we need a stable way of iterating + indexing ext_list.\n    # We'll assume ext_list[i] corresponds to the i-th gene in `list(gene_anno.keys())`.\n    gene_list = list(gene_anno.keys())\n    if len(gene_list) &gt; sampling:\n        gene_list = random.sample(gene_list, int(sampling))\n\n    for num, gene in enumerate(tqdm(gene_list, desc=\"Genes\")):\n        chrom = gene_anno[gene][\"chrom\"]\n        strand = gene_anno[gene][\"strand\"]\n\n        # Skip genes not in gene_info or with empty annotation\n        if gene not in gene_info or not gene_info[gene]:\n            continue\n\n        # Determine exon\u2010range and extended boundaries\n        exon_coords = gene_info[gene]\n        # start = minimum exon_start; end = maximum exon_end\n        start = min(exon[1] for exon in exon_coords)\n        end   = max(exon[2] for exon in exon_coords)\n\n        left_ext, right_ext = ext_list[num]\n        ext_start = max(0, start - left_ext)\n        ext_end   = end + right_ext\n\n        # Shortcut: grab the full chromosome record once\n        chrom_record = genome[chrom]\n\n        # Build a list of (genomic_anchor, seq_string) for \"+\" or \"-\" strand\n        seqinfo = []\n        if strand == \"+\":\n            #  1) upstream flank\n            try:\n                upstream_seq = chrom_record[ext_start:start].seq\n            except Exception:\n                # If slicing fails, log and skip\n                print(f\"ERROR: {chrom}\\t{ext_start}\\t{start}\")\n                upstream_seq = \"\"\n            seqinfo.append((ext_start, str(upstream_seq)))\n\n            #  2) each exon\n            for feature in exon_coords:\n                exon_start = feature[1]\n                exon_end   = feature[2]\n                if exon_start &gt;= exon_end:\n                    continue\n                seq = chrom_record[exon_start:exon_end].seq\n                seqinfo.append((exon_start, str(seq)))\n\n            #  3) downstream flank\n            downstream_seq = chrom_record[end:ext_end].seq\n            seqinfo.append((end, str(downstream_seq)))\n\n        else:  # strand == \"-\"\n            # On the reverse\u2010strand, we want the reverse complement (\"antisense\").\n            # Note: .antisense == .reverse_complement() for most SeqRecord slicing.\n            # We still record the genomic anchor as if it were the left index on the + strand.\n            # But because the sequence is reversed, offset_mapping will need to be mapped differently.\n\n            #  1) \u201cupstream\u201d on reverse strand = (end \u2192 ext_end) in forward coords, but take antisense\n            try:\n                flank_seq = chrom_record[end:ext_end].antisense\n            except Exception:\n                print(f\"ERROR (rev): {chrom}\\t{end}\\t{ext_end}\")\n                flank_seq = \"\"\n            seqinfo.append((ext_end, str(flank_seq)))\n\n            #  2) each exon (reverse\u2010complement)\n            for feature in exon_coords:\n                exon_start = feature[1]\n                exon_end   = feature[2]\n                if exon_start &gt;= exon_end:\n                    continue\n                seq = chrom_record[exon_start:exon_end].antisense\n                # For mapping, we\u2019ll anchor each token by the 5\u2032-most position on the minus strand,\n                # but because the sequence is reversed, the \u201cfirst character\u201d of seq actually corresponds\n                # to genomic position = exon_end - 1 in forward coordinates, and the \u201clast character\u201d \u21a6 exon_start.\n                seqinfo.append((exon_end, str(seq)))\n\n            #  3) downstream on reverse strand = (ext_start \u2192 start) in forward coords, but antisense\n            flank_seq = chrom_record[ext_start:start].antisense\n            seqinfo.append((start, str(flank_seq)))\n\n        # Initialize the list for this gene\n        token_pos[gene] = []\n\n        # For each (anchor, raw_seq), run a single tokenizer(...) call\n        for anchor, raw_seq in seqinfo:\n            if not raw_seq:\n                continue\n\n            # 1) Tokenize with offsets (add_special_tokens=False so we skip [CLS], [SEP], etc.)\n            #    \u201coffset_mapping\u201d is a list of (char_start, char_end) for each token in raw_seq.\n            # encoding = tokenizer(\n            #     raw_seq,\n            #     return_offsets_mapping=True,\n            #     add_special_tokens=False\n            # )\n            # offsets = encoding[\"offset_mapping\"]\n            # token_ids = encoding[\"input_ids\"]\n            token_ids = tokenizer.encode(raw_seq, add_special_tokens=False)\n            tok_strs = tokenizer.convert_ids_to_tokens(token_ids)\n            offsets = []\n            cursor  = 0\n            for tok in tok_strs:\n                char_start = cursor\n                char_end   = cursor + len(tok)\n                offsets.append((char_start, char_end))\n                cursor = char_end\n            if len(offsets) != len(token_ids):\n                # This should never happen in a well\u2010formed tokenizer, but just in case:\n                raise RuntimeError(\"Offset mapping length \u2260 token_ids length\")\n\n            # 2) Iterate through each token + offset, skip special tokens, then map back to genome coords\n            for idx, (token_id, (char_start, char_end)) in enumerate(zip(token_ids, offsets)):\n                token_str = tokenizer.convert_ids_to_tokens(token_id)\n\n                # Skip if it\u2019s one of the special tokens (\u201c[PAD]\u201d, \u201c[CLS]\u201d, etc.)\n                if token_str in sp_tokens:\n                    continue\n\n                if strand == \"+\":\n                    # On the forward strand, raw_seq[0] \u21a6 genomic position \u201canchor\u201d.\n                    # So any token covering raw_seq[char_start:char_end] \u21a6 genome positions [anchor+char_start : anchor+char_end]\n                    g_start = anchor + char_start\n                    g_end   = anchor + char_end\n\n                else:\n                    # On the reverse strand, raw_seq was already antisense (reverse), and \u201canchor\u201d is the forward\u2010strand coordinate\n                    # of the first character in raw_seq.  That first character of raw_seq is actually genome position (anchor-1),\n                    # and the last character of raw_seq is genome position (anchor - len(raw_seq)).\n                    # More generally, for raw_seq index i, the corresponding forward\u2010strand position is:\n                    #     g_pos = anchor - 1 - i\n                    #\n                    # Thus, if the token covers raw_seq[char_start:char_end] (i.e. from i = char_start to i = char_end-1),\n                    # its genomic coordinates (inclusive\u2010exclusive) on the forward strand are:\n                    #   g_end = (anchor - 1 - char_start) + 1  = anchor - char_start\n                    #   g_start = (anchor - 1 - (char_end - 1))  = anchor - char_end\n                    #\n                    # We want to store them as [g_start, g_end] with g_start &lt; g_end.  So:\n                    g_start = anchor - char_end\n                    g_end   = anchor - char_start\n\n                token_pos[gene].append([g_start, g_end, token_str])\n\n    # save sequences and tokens\n    with open(outfile, \"w\") as outf:\n        for gene in tqdm(token_pos, desc=\"Save token positions\"):\n            chrom = gene_anno[gene][\"chrom\"]\n            strand = gene_anno[gene][\"strand\"]\n            for token in token_pos[gene]:\n                print(chrom, token[0], token[1], token[2], gene, strand, sep=\"\\t\", file=outf)\n\n    return token_pos\n</pre> def tokenization(genome, gene_anno, gene_info, tokenizer, outfile, ext_list, sampling=1e7):     \"\"\"     For each gene in `gene_anno`, extract the annotated exonic (and flanking) DNA subsequences,     run the tokenizer once per subsequence with offset_mapping enabled,      and record the (genomic_start, genomic_end, token) tuples for all non-special tokens.      - genome: dict mapping chromosome \u2192 SeqRecord (so that genome[chrom][start:end].seq gives a Seq)     - gene_anno: dict mapping gene_name \u2192 { \"chrom\": str, \"strand\": \"+\" or \"-\", ... }     - gene_info: dict mapping gene_name \u2192 list of (feature_id, exon_start, exon_end) or similar     - tokenizer: a HuggingFace\u2010style tokenizer that supports return_offsets_mapping     - outfile: (unused here, but you can write token_pos to it later)     - ext_list: list of (left_extension, right_extension) tuples parallel to gene_anno order     - sampling: random sampling the given number of genes for tokenization     \"\"\"     # 1) Precompute special-tokens set for O(1) membership checks     sp_tokens = set(tokenizer.special_tokens_map.values())      token_pos = {}     # Since gene_anno is likely a dict, we need a stable way of iterating + indexing ext_list.     # We'll assume ext_list[i] corresponds to the i-th gene in `list(gene_anno.keys())`.     gene_list = list(gene_anno.keys())     if len(gene_list) &gt; sampling:         gene_list = random.sample(gene_list, int(sampling))      for num, gene in enumerate(tqdm(gene_list, desc=\"Genes\")):         chrom = gene_anno[gene][\"chrom\"]         strand = gene_anno[gene][\"strand\"]          # Skip genes not in gene_info or with empty annotation         if gene not in gene_info or not gene_info[gene]:             continue          # Determine exon\u2010range and extended boundaries         exon_coords = gene_info[gene]         # start = minimum exon_start; end = maximum exon_end         start = min(exon[1] for exon in exon_coords)         end   = max(exon[2] for exon in exon_coords)          left_ext, right_ext = ext_list[num]         ext_start = max(0, start - left_ext)         ext_end   = end + right_ext          # Shortcut: grab the full chromosome record once         chrom_record = genome[chrom]          # Build a list of (genomic_anchor, seq_string) for \"+\" or \"-\" strand         seqinfo = []         if strand == \"+\":             #  1) upstream flank             try:                 upstream_seq = chrom_record[ext_start:start].seq             except Exception:                 # If slicing fails, log and skip                 print(f\"ERROR: {chrom}\\t{ext_start}\\t{start}\")                 upstream_seq = \"\"             seqinfo.append((ext_start, str(upstream_seq)))              #  2) each exon             for feature in exon_coords:                 exon_start = feature[1]                 exon_end   = feature[2]                 if exon_start &gt;= exon_end:                     continue                 seq = chrom_record[exon_start:exon_end].seq                 seqinfo.append((exon_start, str(seq)))              #  3) downstream flank             downstream_seq = chrom_record[end:ext_end].seq             seqinfo.append((end, str(downstream_seq)))          else:  # strand == \"-\"             # On the reverse\u2010strand, we want the reverse complement (\"antisense\").             # Note: .antisense == .reverse_complement() for most SeqRecord slicing.             # We still record the genomic anchor as if it were the left index on the + strand.             # But because the sequence is reversed, offset_mapping will need to be mapped differently.              #  1) \u201cupstream\u201d on reverse strand = (end \u2192 ext_end) in forward coords, but take antisense             try:                 flank_seq = chrom_record[end:ext_end].antisense             except Exception:                 print(f\"ERROR (rev): {chrom}\\t{end}\\t{ext_end}\")                 flank_seq = \"\"             seqinfo.append((ext_end, str(flank_seq)))              #  2) each exon (reverse\u2010complement)             for feature in exon_coords:                 exon_start = feature[1]                 exon_end   = feature[2]                 if exon_start &gt;= exon_end:                     continue                 seq = chrom_record[exon_start:exon_end].antisense                 # For mapping, we\u2019ll anchor each token by the 5\u2032-most position on the minus strand,                 # but because the sequence is reversed, the \u201cfirst character\u201d of seq actually corresponds                 # to genomic position = exon_end - 1 in forward coordinates, and the \u201clast character\u201d \u21a6 exon_start.                 seqinfo.append((exon_end, str(seq)))              #  3) downstream on reverse strand = (ext_start \u2192 start) in forward coords, but antisense             flank_seq = chrom_record[ext_start:start].antisense             seqinfo.append((start, str(flank_seq)))          # Initialize the list for this gene         token_pos[gene] = []          # For each (anchor, raw_seq), run a single tokenizer(...) call         for anchor, raw_seq in seqinfo:             if not raw_seq:                 continue              # 1) Tokenize with offsets (add_special_tokens=False so we skip [CLS], [SEP], etc.)             #    \u201coffset_mapping\u201d is a list of (char_start, char_end) for each token in raw_seq.             # encoding = tokenizer(             #     raw_seq,             #     return_offsets_mapping=True,             #     add_special_tokens=False             # )             # offsets = encoding[\"offset_mapping\"]             # token_ids = encoding[\"input_ids\"]             token_ids = tokenizer.encode(raw_seq, add_special_tokens=False)             tok_strs = tokenizer.convert_ids_to_tokens(token_ids)             offsets = []             cursor  = 0             for tok in tok_strs:                 char_start = cursor                 char_end   = cursor + len(tok)                 offsets.append((char_start, char_end))                 cursor = char_end             if len(offsets) != len(token_ids):                 # This should never happen in a well\u2010formed tokenizer, but just in case:                 raise RuntimeError(\"Offset mapping length \u2260 token_ids length\")              # 2) Iterate through each token + offset, skip special tokens, then map back to genome coords             for idx, (token_id, (char_start, char_end)) in enumerate(zip(token_ids, offsets)):                 token_str = tokenizer.convert_ids_to_tokens(token_id)                  # Skip if it\u2019s one of the special tokens (\u201c[PAD]\u201d, \u201c[CLS]\u201d, etc.)                 if token_str in sp_tokens:                     continue                  if strand == \"+\":                     # On the forward strand, raw_seq[0] \u21a6 genomic position \u201canchor\u201d.                     # So any token covering raw_seq[char_start:char_end] \u21a6 genome positions [anchor+char_start : anchor+char_end]                     g_start = anchor + char_start                     g_end   = anchor + char_end                  else:                     # On the reverse strand, raw_seq was already antisense (reverse), and \u201canchor\u201d is the forward\u2010strand coordinate                     # of the first character in raw_seq.  That first character of raw_seq is actually genome position (anchor-1),                     # and the last character of raw_seq is genome position (anchor - len(raw_seq)).                     # More generally, for raw_seq index i, the corresponding forward\u2010strand position is:                     #     g_pos = anchor - 1 - i                     #                     # Thus, if the token covers raw_seq[char_start:char_end] (i.e. from i = char_start to i = char_end-1),                     # its genomic coordinates (inclusive\u2010exclusive) on the forward strand are:                     #   g_end = (anchor - 1 - char_start) + 1  = anchor - char_start                     #   g_start = (anchor - 1 - (char_end - 1))  = anchor - char_end                     #                     # We want to store them as [g_start, g_end] with g_start &lt; g_end.  So:                     g_start = anchor - char_end                     g_end   = anchor - char_start                  token_pos[gene].append([g_start, g_end, token_str])      # save sequences and tokens     with open(outfile, \"w\") as outf:         for gene in tqdm(token_pos, desc=\"Save token positions\"):             chrom = gene_anno[gene][\"chrom\"]             strand = gene_anno[gene][\"strand\"]             for token in token_pos[gene]:                 print(chrom, token[0], token[1], token[2], gene, strand, sep=\"\\t\", file=outf)      return token_pos In\u00a0[9]: Copied! <pre>def tokens_to_nerdata(tokens_bed, annotation_bed, outfile, named_entities, tags_id):\n    \"\"\"\n    Build a token\u2010level NER dataset by intersecting `tokens_bed` with `annotation_bed`.\n    Returns a dict: { 'id': [...geneIDs...], 'sequence': [[token1, token2, \u2026], \u2026],\n                     'labels': [[label1, label2, \u2026], \u2026] } \n    and also writes two files:\n      1) \u201coutfile\u201d as a pickle of ner_info,\n      2) \u201c&lt;outfile&gt;.token_sizes\u201d containing \u201cgene&lt;TAB&gt;token_count\u201d for each gene.\n    \"\"\"\n\n    ne = named_entities\n    # Build a map from \u201cbaseName + '0' \u2192 named_entities[...] \u2192 tags_id[...]\u201d\n    zero_map = {}\n    one_map  = {}\n    for base_name, ner_label in ne.items():\n        # \u201cintergenic\u201d maps to 'O' no matter whether we\u2019re at a \u201cstart\u201d or \u201cinside\u201d \u2014\n        # so we do it for both 'intergenic0' and 'intergenic1'.\n        if base_name == \"intergenic\":\n            zero_map[\"intergenic0\"] = ner_label\n            one_map[\"intergenic1\"] = ner_label\n            continue\n\n        # base_name will be something like \u201cexon0\u201d or \u201cexon1\u201d, \u201cintron0\u201d, \u201cintron1\u201d\n        # We want to know, whenever the token\u2019s name is exactly \u201cexon\u201d and we\u2019re at a \u201cstart\u201d boundary,\n        # pick the B-EXON label.  If the name is \u201cexon\u201d but it matched the previous gene-level \u201cname\u201d,\n        # then we call named_entities[\"exon1\"] to get \u201cI-EXON\u201d.\n        if base_name.endswith(\"0\"):\n            zero_map[base_name] = ner_label\n        else:\n            one_map[base_name]  = ner_label\n\n    # 2) Perform the intersection once (Loj = \u201cleft outer join\u201d) so we keep every token\n    intersection = BedTool(tokens_bed).intersect(annotation_bed, loj=True)\n\n    # 3) Prepare our output containers\n    ner_info = {\n        \"id\":       [],  # list of gene IDs (in the same order as we append)\n        \"sequence\": [],  # each element is a list-of-strings (tokens)\n        \"labels\":   []   # each element is a list-of-ints (NER tags)\n    }\n\n    # We'll accumulate (gene, token_count) pairs in-memory, then write them in bulk\n    sizes_buffer = []\n\n    # 4) Use defaultdict(set) to track \u201cwhich token\u2010IDs we\u2019ve already seen for each gene\u201d\n    token_seen = defaultdict(set)\n\n    current_gene = None\n    tokens_list  = []\n    labels_list  = []\n    last_name    = None  # to know if \u201cname == last_name\u201d (inside vs start)\n\n    # 5) Iterate through every interval from the intersection\n    #    We rely on the fact that BedTool.intersect(...) returns results in ascending\n    #    genomic order, and within each gene, that will appear \u201cin order of token positions.\u201d\n    for iv in intersection:\n        # Instead of \u201cstr(iv).split('\\t')\u201d, do:\n        chrom   = iv.chrom\n        start   = iv.start   # integer\n        end     = iv.end     # integer\n        token   = iv.name    # 4th column of tokens_bed\n        gene    = iv.fields[4]   # 5th column of tokens_bed (original gene ID)\n        gene2   = iv.fields[9]   # 10th field (unused here, but was in your code)\n        name    = iv.fields[10]  # 11th field = the annotation \u201cname\u201d\n        # Build a unique\u2010ID for this token instance\n        token_id = (start, end)\n\n        # 6) When we see a new gene (i.e. \u201cgene != current_gene\u201d), we flush the previous gene\u2019s data\n        if gene != current_gene:\n            # flush old gene if it exists\n            if current_gene is not None:\n                # Only append if we actually collected \u22651 token for current_gene\n                if tokens_list:\n                    sizes_buffer.append((current_gene, len(tokens_list)))\n                    ner_info[\"id\"].append(current_gene)\n                    ner_info[\"sequence\"].append(tokens_list)\n                    ner_info[\"labels\"].append(labels_list)\n                    count = len(ner_info[\"id\"])\n                    if count % 100 == 0:\n                        print(count)\n            # Reset for the new gene\n            current_gene = gene\n            tokens_list  = []\n            labels_list  = []\n            last_name    = None\n\n        # 7) If we\u2019ve already seen this exact (start,end) \u201ctoken_id\u201d for this gene, skip\n        if token_id in token_seen[gene]:\n            continue\n        token_seen[gene].add(token_id)\n\n        # 8) Determine the correct NER\u2010tag (integer) for this token\n        #    - If name == \"-1\" \u2192 treat as \u201cintergenic\u201d\n        #    - If name == last_name \u2192 we pick \u201cinside\u201d (use one_map[name + \"1\"])\n        #    - else \u2192 we pick \u201cstart\u201d   (use zero_map[name + \"0\"])\n        if name == \"-1\":\n            base_name = \"intergenic\"\n            ner_label = ne[base_name]          # always \u201cO\u201d\n        else:\n            # If it matched the previous token\u2019s annotation name, choose inside\n            if name == last_name:\n                lookup_key = name + \"1\"       # e.g. \u201cexon1\u201d \u2192 I-EXON\n                ner_label  = one_map.get(lookup_key)\n                # If somehow it\u2019s missing, fall back to \u201cstart\u201d logic\n                if ner_label is None:\n                    ner_label = zero_map[name + \"0\"]\n            else:\n                # new annotation segment \u2192 start\n                lookup_key = name + \"0\"       # e.g. \u201cexon0\u201d \u2192 B-EXON\n                ner_label  = zero_map.get(lookup_key)\n                # If it\u2019s missing, fall back to \u201cintergenic\u201d\n                if ner_label is None:\n                    ner_label = ne[\"intergenic\"]\n\n        ner_tag = tags_id[ner_label]\n        last_name = name\n\n        # 9) Append the token string + numeric label\n        tokens_list.append(token)\n        labels_list.append(ner_tag)\n\n    # 10) Don\u2019t forget to flush the final gene once the loop ends\n    if current_gene is not None and tokens_list:\n        sizes_buffer.append((current_gene, len(tokens_list)))\n        ner_info[\"id\"].append(current_gene)\n        ner_info[\"sequence\"].append(tokens_list)\n        ner_info[\"labels\"].append(labels_list)\n        print(\".\", end=\"\")\n\n    # 11) Write out the token_sizes file in one go\n    sizes_file = outfile.rsplit(\".\", 1)[0] + \".token_sizes\"\n    with open(sizes_file, \"w\") as tsf:\n        for gene_name, count in sizes_buffer:\n            tsf.write(f\"{gene_name}\\t{count}\\n\")\n\n    # 12) Finally, pickle\u2010dump ner_info\n    with open(outfile, \"wb\") as handle:\n        pickle.dump(ner_info, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    return ner_info\n</pre> def tokens_to_nerdata(tokens_bed, annotation_bed, outfile, named_entities, tags_id):     \"\"\"     Build a token\u2010level NER dataset by intersecting `tokens_bed` with `annotation_bed`.     Returns a dict: { 'id': [...geneIDs...], 'sequence': [[token1, token2, \u2026], \u2026],                      'labels': [[label1, label2, \u2026], \u2026] }      and also writes two files:       1) \u201coutfile\u201d as a pickle of ner_info,       2) \u201c.token_sizes\u201d containing \u201cgenetoken_count\u201d for each gene.     \"\"\"      ne = named_entities     # Build a map from \u201cbaseName + '0' \u2192 named_entities[...] \u2192 tags_id[...]\u201d     zero_map = {}     one_map  = {}     for base_name, ner_label in ne.items():         # \u201cintergenic\u201d maps to 'O' no matter whether we\u2019re at a \u201cstart\u201d or \u201cinside\u201d \u2014         # so we do it for both 'intergenic0' and 'intergenic1'.         if base_name == \"intergenic\":             zero_map[\"intergenic0\"] = ner_label             one_map[\"intergenic1\"] = ner_label             continue          # base_name will be something like \u201cexon0\u201d or \u201cexon1\u201d, \u201cintron0\u201d, \u201cintron1\u201d         # We want to know, whenever the token\u2019s name is exactly \u201cexon\u201d and we\u2019re at a \u201cstart\u201d boundary,         # pick the B-EXON label.  If the name is \u201cexon\u201d but it matched the previous gene-level \u201cname\u201d,         # then we call named_entities[\"exon1\"] to get \u201cI-EXON\u201d.         if base_name.endswith(\"0\"):             zero_map[base_name] = ner_label         else:             one_map[base_name]  = ner_label      # 2) Perform the intersection once (Loj = \u201cleft outer join\u201d) so we keep every token     intersection = BedTool(tokens_bed).intersect(annotation_bed, loj=True)      # 3) Prepare our output containers     ner_info = {         \"id\":       [],  # list of gene IDs (in the same order as we append)         \"sequence\": [],  # each element is a list-of-strings (tokens)         \"labels\":   []   # each element is a list-of-ints (NER tags)     }      # We'll accumulate (gene, token_count) pairs in-memory, then write them in bulk     sizes_buffer = []      # 4) Use defaultdict(set) to track \u201cwhich token\u2010IDs we\u2019ve already seen for each gene\u201d     token_seen = defaultdict(set)      current_gene = None     tokens_list  = []     labels_list  = []     last_name    = None  # to know if \u201cname == last_name\u201d (inside vs start)      # 5) Iterate through every interval from the intersection     #    We rely on the fact that BedTool.intersect(...) returns results in ascending     #    genomic order, and within each gene, that will appear \u201cin order of token positions.\u201d     for iv in intersection:         # Instead of \u201cstr(iv).split('\\t')\u201d, do:         chrom   = iv.chrom         start   = iv.start   # integer         end     = iv.end     # integer         token   = iv.name    # 4th column of tokens_bed         gene    = iv.fields[4]   # 5th column of tokens_bed (original gene ID)         gene2   = iv.fields[9]   # 10th field (unused here, but was in your code)         name    = iv.fields[10]  # 11th field = the annotation \u201cname\u201d         # Build a unique\u2010ID for this token instance         token_id = (start, end)          # 6) When we see a new gene (i.e. \u201cgene != current_gene\u201d), we flush the previous gene\u2019s data         if gene != current_gene:             # flush old gene if it exists             if current_gene is not None:                 # Only append if we actually collected \u22651 token for current_gene                 if tokens_list:                     sizes_buffer.append((current_gene, len(tokens_list)))                     ner_info[\"id\"].append(current_gene)                     ner_info[\"sequence\"].append(tokens_list)                     ner_info[\"labels\"].append(labels_list)                     count = len(ner_info[\"id\"])                     if count % 100 == 0:                         print(count)             # Reset for the new gene             current_gene = gene             tokens_list  = []             labels_list  = []             last_name    = None          # 7) If we\u2019ve already seen this exact (start,end) \u201ctoken_id\u201d for this gene, skip         if token_id in token_seen[gene]:             continue         token_seen[gene].add(token_id)          # 8) Determine the correct NER\u2010tag (integer) for this token         #    - If name == \"-1\" \u2192 treat as \u201cintergenic\u201d         #    - If name == last_name \u2192 we pick \u201cinside\u201d (use one_map[name + \"1\"])         #    - else \u2192 we pick \u201cstart\u201d   (use zero_map[name + \"0\"])         if name == \"-1\":             base_name = \"intergenic\"             ner_label = ne[base_name]          # always \u201cO\u201d         else:             # If it matched the previous token\u2019s annotation name, choose inside             if name == last_name:                 lookup_key = name + \"1\"       # e.g. \u201cexon1\u201d \u2192 I-EXON                 ner_label  = one_map.get(lookup_key)                 # If somehow it\u2019s missing, fall back to \u201cstart\u201d logic                 if ner_label is None:                     ner_label = zero_map[name + \"0\"]             else:                 # new annotation segment \u2192 start                 lookup_key = name + \"0\"       # e.g. \u201cexon0\u201d \u2192 B-EXON                 ner_label  = zero_map.get(lookup_key)                 # If it\u2019s missing, fall back to \u201cintergenic\u201d                 if ner_label is None:                     ner_label = ne[\"intergenic\"]          ner_tag = tags_id[ner_label]         last_name = name          # 9) Append the token string + numeric label         tokens_list.append(token)         labels_list.append(ner_tag)      # 10) Don\u2019t forget to flush the final gene once the loop ends     if current_gene is not None and tokens_list:         sizes_buffer.append((current_gene, len(tokens_list)))         ner_info[\"id\"].append(current_gene)         ner_info[\"sequence\"].append(tokens_list)         ner_info[\"labels\"].append(labels_list)         print(\".\", end=\"\")      # 11) Write out the token_sizes file in one go     sizes_file = outfile.rsplit(\".\", 1)[0] + \".token_sizes\"     with open(sizes_file, \"w\") as tsf:         for gene_name, count in sizes_buffer:             tsf.write(f\"{gene_name}\\t{count}\\n\")      # 12) Finally, pickle\u2010dump ner_info     with open(outfile, \"wb\") as handle:         pickle.dump(ner_info, handle, protocol=pickle.HIGHEST_PROTOCOL)      return ner_info In\u00a0[10]: Copied! <pre># Download genome and gene annotation (make sure you have wget command in your path)\n!wget -c https://rice.uga.edu/osa1r7_download/osa1_r7.asm.fa.gz\n!wget -c https://rice.uga.edu/osa1r7_download/osa1_r7.all_models.gff3.gz\n</pre> # Download genome and gene annotation (make sure you have wget command in your path) !wget -c https://rice.uga.edu/osa1r7_download/osa1_r7.asm.fa.gz !wget -c https://rice.uga.edu/osa1r7_download/osa1_r7.all_models.gff3.gz <pre>--2025-12-28 14:42:11--  https://rice.uga.edu/osa1r7_download/osa1_r7.asm.fa.gz\nResolving rice.uga.edu (rice.uga.edu)... 128.192.162.131\nConnecting to rice.uga.edu (rice.uga.edu)|128.192.162.131|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 115858754 (110M) [application/x-gzip]\nSaving to: \u2018osa1_r7.asm.fa.gz\u2019\n\nosa1_r7.asm.fa.gz   100%[===================&gt;] 110.49M   290KB/s    in 3m 0s   \n\n2025-12-28 14:45:13 (629 KB/s) - \u2018osa1_r7.asm.fa.gz\u2019 saved [115858754/115858754]\n\n--2025-12-28 14:45:13--  https://rice.uga.edu/osa1r7_download/osa1_r7.all_models.gff3.gz\nResolving rice.uga.edu (rice.uga.edu)... 128.192.162.131\nConnecting to rice.uga.edu (rice.uga.edu)|128.192.162.131|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8901387 (8.5M) [application/x-gzip]\nSaving to: \u2018osa1_r7.all_models.gff3.gz\u2019\n\nosa1_r7.all_models. 100%[===================&gt;]   8.49M  1.49MB/s    in 10s     \n\n2025-12-28 14:45:25 (850 KB/s) - \u2018osa1_r7.all_models.gff3.gz\u2019 saved [8901387/8901387]\n\n</pre> In\u00a0[11]: Copied! <pre># Load genome sequence\ngenome_file = \"osa1_r7.asm.fa.gz\"\ngenome = Fasta(genome_file)\n# Load annotation\ngene_anno = {}\nwith gzip.open(\"osa1_r7.all_models.gff3.gz\", \"rt\") as infile:\n    for line in tqdm(infile):\n        if line.startswith(\"#\") or line.startswith(\"\\n\"):\n            continue\n        info = line.strip().split(\"\\t\")\n        chrom = info[0]\n        datatype = info[2]\n        start = int(info[3]) - 1\n        end = int(info[4])\n        strand = info[6]\n        description = info[8].split(\";\")\n        if datatype == \"gene\":\n            for item in description:\n                if item.startswith(\"Name=\"):\n                    gene = item[5:]\n            if gene not in gene_anno:\n                gene_anno[gene] = {}\n                gene_anno[gene][\"chrom\"] = chrom\n                gene_anno[gene][\"start\"] = start\n                gene_anno[gene][\"end\"] = end\n                gene_anno[gene][\"strand\"] = strand\n                gene_anno[gene][\"isoform\"] = {}\n        elif datatype in [\"exon\"]:\n            for item in description:\n                if item.startswith(\"Parent=\"):\n                    isoform = item[7:].split(',')[0]\n            if isoform not in gene_anno[gene][\"isoform\"]:\n                gene_anno[gene][\"isoform\"][isoform] = []\n            gene_anno[gene][\"isoform\"][isoform].append([datatype, start, end])\n\n# Get full gene annotation information and save\ngene_info = get_gene_annotation(gene_anno)\nannotation_bed = \"rice_annotation.bed\"\nwith open(annotation_bed, \"w\") as outf:\n    for gene in sorted(gene_anno, key=lambda x: (gene_anno[x][\"chrom\"], gene_anno[x][\"start\"])):\n        chrom = gene_anno[gene][\"chrom\"]\n        strand = gene_anno[gene][\"strand\"]\n        if strand == \"+\":\n            for item in gene_info[gene]:\n                print(item[0], item[1], item[2], gene, item[3], item[4], sep=\"\\t\", file=outf)\n        else:\n            for item in gene_info[gene][::-1]:\n                print(item[0], item[1], item[2], gene, item[3], item[4], sep=\"\\t\", file=outf)\n</pre> # Load genome sequence genome_file = \"osa1_r7.asm.fa.gz\" genome = Fasta(genome_file) # Load annotation gene_anno = {} with gzip.open(\"osa1_r7.all_models.gff3.gz\", \"rt\") as infile:     for line in tqdm(infile):         if line.startswith(\"#\") or line.startswith(\"\\n\"):             continue         info = line.strip().split(\"\\t\")         chrom = info[0]         datatype = info[2]         start = int(info[3]) - 1         end = int(info[4])         strand = info[6]         description = info[8].split(\";\")         if datatype == \"gene\":             for item in description:                 if item.startswith(\"Name=\"):                     gene = item[5:]             if gene not in gene_anno:                 gene_anno[gene] = {}                 gene_anno[gene][\"chrom\"] = chrom                 gene_anno[gene][\"start\"] = start                 gene_anno[gene][\"end\"] = end                 gene_anno[gene][\"strand\"] = strand                 gene_anno[gene][\"isoform\"] = {}         elif datatype in [\"exon\"]:             for item in description:                 if item.startswith(\"Parent=\"):                     isoform = item[7:].split(',')[0]             if isoform not in gene_anno[gene][\"isoform\"]:                 gene_anno[gene][\"isoform\"][isoform] = []             gene_anno[gene][\"isoform\"][isoform].append([datatype, start, end])  # Get full gene annotation information and save gene_info = get_gene_annotation(gene_anno) annotation_bed = \"rice_annotation.bed\" with open(annotation_bed, \"w\") as outf:     for gene in sorted(gene_anno, key=lambda x: (gene_anno[x][\"chrom\"], gene_anno[x][\"start\"])):         chrom = gene_anno[gene][\"chrom\"]         strand = gene_anno[gene][\"strand\"]         if strand == \"+\":             for item in gene_info[gene]:                 print(item[0], item[1], item[2], gene, item[3], item[4], sep=\"\\t\", file=outf)         else:             for item in gene_info[gene][::-1]:                 print(item[0], item[1], item[2], gene, item[3], item[4], sep=\"\\t\", file=outf) <pre>813791it [00:01, 467221.72it/s]\n</pre> In\u00a0[12]: Copied! <pre># Load configs, model and tokenizer\nconfigs = load_config(\"./ner_task_config.yaml\")\nmodel_name = \"zhangtaolab/plant-dnagpt-6mer\"\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\n</pre> # Load configs, model and tokenizer configs = load_config(\"./ner_task_config.yaml\") model_name = \"zhangtaolab/plant-dnagpt-6mer\" model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") <pre>Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-6mer\n</pre> <pre>2025-12-28 14:45:31,636 - modelscope - INFO - Got 8 files, start to download ...\n</pre> <pre>Processing 8 items:   0%|          | 0.00/8.00 [00:00&lt;?, ?it/s]</pre> <pre>Downloading [configuration.json]:   0%|          | 0.00/42.0 [00:00&lt;?, ?B/s]</pre> <pre>Downloading [config.json]:   0%|          | 0.00/910 [00:00&lt;?, ?B/s]</pre> <pre>Downloading [tokenizer_config.json]:   0%|          | 0.00/1.07k [00:00&lt;?, ?B/s]</pre> <pre>Downloading [README.md]:   0%|          | 0.00/2.73k [00:00&lt;?, ?B/s]</pre> <pre>Downloading [generation_config.json]:   0%|          | 0.00/111 [00:00&lt;?, ?B/s]</pre> <pre>Downloading [special_tokens_map.json]:   0%|          | 0.00/581 [00:00&lt;?, ?B/s]</pre> <pre>Downloading [vocab.txt]:   0%|          | 0.00/28.0k [00:00&lt;?, ?B/s]</pre> <pre>Downloading [model.safetensors]:   0%|          | 0.00/340M [00:00&lt;?, ?B/s]</pre> <pre>2025-12-28 14:46:04,983 - modelscope - INFO - Download model 'zhangtaolab/plant-dnagpt-6mer' successfully.\n</pre> <pre>14:46:04 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-6mer\n</pre> <pre>Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-6mer and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[13]: Copied! <pre># Tokenize gene sequences and generate NER dataset format\nprint(\"# Performing sequence tokenization...\")\ntokens_bed = \"rice_genes_tokens.bed\"\n\ntoken_pos = tokenization(genome, gene_anno, gene_info, tokenizer, tokens_bed, ext_list, sampling=2000)\n</pre> # Tokenize gene sequences and generate NER dataset format print(\"# Performing sequence tokenization...\") tokens_bed = \"rice_genes_tokens.bed\"  token_pos = tokenization(genome, gene_anno, gene_info, tokenizer, tokens_bed, ext_list, sampling=2000) <pre># Performing sequence tokenization...\n</pre> <pre>Genes:   0%|          | 5/2000 [00:00&lt;00:44, 45.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (606 &gt; 512). Running this sequence through the model will result in indexing errors\nGenes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:30&lt;00:00, 64.84it/s] \nSave token positions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1926/1926 [00:01&lt;00:00, 1243.50it/s]\n</pre> In\u00a0[14]: Copied! <pre>print(\"# Generate NER dataset...\")\n\ndataset = 'rice_gene_ner.pkl'\nner_info = tokens_to_nerdata(tokens_bed, annotation_bed, dataset, named_entities, tags_id)\n</pre>  print(\"# Generate NER dataset...\")  dataset = 'rice_gene_ner.pkl' ner_info = tokens_to_nerdata(tokens_bed, annotation_bed, dataset, named_entities, tags_id)  <pre># Generate NER dataset...\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n.</pre> In\u00a0[15]: Copied! <pre>from dnallm import DNADataset, DNATrainer\n</pre> from dnallm import DNADataset, DNATrainer In\u00a0[16]: Copied! <pre># Load the datasets\ndatasets = DNADataset.load_local_data(\"./rice_gene_ner.pkl\", seq_col=\"sequence\", label_col=\"labels\", tokenizer=tokenizer, max_length=1024)\n\n# Encode the sequences with given task's data collator\ndatasets.encode_sequences(task=configs['task'].task_type, remove_unused_columns=True)\n\n# Split the dataset into train, test, and validation sets\ndatasets.split_data()\n</pre> # Load the datasets datasets = DNADataset.load_local_data(\"./rice_gene_ner.pkl\", seq_col=\"sequence\", label_col=\"labels\", tokenizer=tokenizer, max_length=1024)  # Encode the sequences with given task's data collator datasets.encode_sequences(task=configs['task'].task_type, remove_unused_columns=True)  # Split the dataset into train, test, and validation sets datasets.split_data() <pre>Format labels:   0%|          | 0/1926 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/1926 [00:00&lt;?, ? examples/s]</pre> In\u00a0[17]: Copied! <pre># check the dataset\nif hasattr(datasets.dataset, 'keys'):\n    for split_name in datasets.dataset.keys():\n        print(f\"{split_name}: {len(datasets.dataset[split_name])} samples\")\n</pre> # check the dataset if hasattr(datasets.dataset, 'keys'):     for split_name in datasets.dataset.keys():         print(f\"{split_name}: {len(datasets.dataset[split_name])} samples\") <pre>train: 1348 samples\ntest: 385 samples\nval: 193 samples\n</pre> In\u00a0[18]: Copied! <pre># Initialize the trainer\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=datasets\n)\n</pre> # Initialize the trainer trainer = DNATrainer(     model=model,     config=configs,     datasets=datasets ) In\u00a0[19]: Copied! <pre># Start training\nmetrics = trainer.train()\nprint(metrics)\n</pre> # Start training metrics = trainer.train() print(metrics)        [1011/1011 47:50, Epoch 3/3]      Step Training Loss Validation Loss Accuracy Precision Recall F1 500 0.494600 0.288843 0.905063 0.585748 0.515507 0.548387 1000 0.180700 0.193303 0.931075 0.598708 0.574776 0.586498 <pre>{'train_runtime': 2882.0198, 'train_samples_per_second': 1.403, 'train_steps_per_second': 0.351, 'total_flos': 2113463702642688.0, 'train_loss': 0.33529989231234136, 'epoch': 3.0}\n</pre> In\u00a0[20]: Copied! <pre># Do prediction on the test set\npredictions = trainer.infer()\nprint(predictions.metrics)\n</pre> # Do prediction on the test set predictions = trainer.infer() print(predictions.metrics) <pre>{'test_loss': 0.21177801489830017, 'test_accuracy': 0.9327967958079203, 'test_precision': 0.6445709638064254, 'test_recall': 0.6024325351577346, 'test_f1': 0.6227897838899803, 'test_runtime': 75.823, 'test_samples_per_second': 5.078, 'test_steps_per_second': 0.33}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/finetune_NER_task/finetune_NER_task/","title":"NER Fine-tuning","text":"In\u00a0[1]: Copied! <pre>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n</pre> from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer In\u00a0[2]: Copied! <pre># Load the config file\nconfigs = load_config(\"./ner_task_config.yaml\")\n</pre> # Load the config file configs = load_config(\"./ner_task_config.yaml\") In\u00a0[3]: Copied! <pre># Load the model and tokenizer\nmodel_name = \"zhangtaolab/plant-nucleotide-transformer-BPE\"\n# from Hugging Face\n# model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n# from ModelScope\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\n</pre> # Load the model and tokenizer model_name = \"zhangtaolab/plant-nucleotide-transformer-BPE\" # from Hugging Face # model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\") # from ModelScope model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") <pre>Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-nucleotide-transformer-BPE\n14:37:38 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-nucleotide-transformer-BPE\n</pre> <pre>Some weights of EsmForTokenClassification were not initialized from the model checkpoint at /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-nucleotide-transformer-BPE and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[4]: Copied! <pre># Load the datasets\ndatasets = DNADataset.load_local_data(\"./rice_gene_ner_BPE.pkl\", seq_col=\"sequence\", label_col=\"labels\", tokenizer=tokenizer, max_length=1024)\n\n# Encode the sequences with given task's data collator\ndatasets.encode_sequences(task=configs['task'].task_type, remove_unused_columns=True)\n\n# Split the dataset into train, test, and validation sets\ndatasets.split_data()\n</pre> # Load the datasets datasets = DNADataset.load_local_data(\"./rice_gene_ner_BPE.pkl\", seq_col=\"sequence\", label_col=\"labels\", tokenizer=tokenizer, max_length=1024)  # Encode the sequences with given task's data collator datasets.encode_sequences(task=configs['task'].task_type, remove_unused_columns=True)  # Split the dataset into train, test, and validation sets datasets.split_data() <pre>Format labels:   0%|          | 0/2000 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/2000 [00:00&lt;?, ? examples/s]</pre> <pre>Token indices sequence length is longer than the specified maximum sequence length for this model (2067 &gt; 2048). Running this sequence through the model will result in indexing errors\n</pre> In\u00a0[5]: Copied! <pre># Initialize the trainer\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=datasets\n)\n</pre> # Initialize the trainer trainer = DNATrainer(     model=model,     config=configs,     datasets=datasets ) In\u00a0[6]: Copied! <pre># Start training\nmetrics = trainer.train()\nprint(metrics)\n</pre> # Start training metrics = trainer.train() print(metrics)        [1050/1050 56:13, Epoch 3/3]      Step Training Loss Validation Loss Accuracy Precision Recall F1 500 0.572600 0.312247 0.901479 0.611558 0.566396 0.588111 1000 0.164400 0.187498 0.945075 0.767223 0.716802 0.741156 <pre>{'train_runtime': 3382.1381, 'train_samples_per_second': 1.241, 'train_steps_per_second': 0.31, 'total_flos': 2381880829722624.0, 'train_loss': 0.35612290246146067, 'epoch': 3.0}\n</pre> In\u00a0[7]: Copied! <pre># Do prediction on the test set\ntrainer.infer()\n</pre> # Do prediction on the test set trainer.infer() Out[7]: <pre>PredictionOutput(predictions=array([[[ 2.323482  , -1.5099787 , -0.8836714 , ..., -0.927853  ,\n         -1.4103745 ,  0.04891092],\n        [ 5.6943636 , -1.5035043 , -0.9116931 , ..., -1.6362175 ,\n         -2.0631716 , -0.94190305],\n        [ 5.8905754 , -0.913214  , -1.2376132 , ..., -1.6290904 ,\n         -1.4768428 , -1.1819894 ],\n        ...,\n        [-0.30754197, -1.0372043 ,  0.7798816 , ...,  0.88006437,\n         -1.8100628 ,  1.5709177 ],\n        [-0.9096149 , -1.5369973 ,  1.5390666 , ...,  2.3269818 ,\n         -1.854856  ,  1.2788448 ],\n        [-1.1789548 , -1.90242   ,  1.672185  , ...,  3.0487711 ,\n         -2.1100445 ,  1.5636654 ]],\n\n       [[ 2.1773806 , -1.733074  , -0.8279599 , ..., -1.1372741 ,\n         -1.5287644 ,  0.13668065],\n        [ 5.831868  , -0.84064394, -1.1946642 , ..., -1.2554147 ,\n         -1.5157167 , -1.4430664 ],\n        [ 6.172637  , -1.1160768 , -0.8910407 , ..., -1.0433854 ,\n         -2.0278275 , -1.6230795 ],\n        ...,\n        [ 0.20084313, -1.9085996 ,  1.306694  , ...,  2.1570723 ,\n         -2.0906959 , -1.3304782 ],\n        [-0.43361968, -1.7822204 ,  1.5568638 , ...,  2.6296084 ,\n         -2.3139496 , -1.7256203 ],\n        [ 0.08428355, -1.0843365 ,  0.08792825, ...,  1.1610407 ,\n         -2.0241451 , -1.6648997 ]],\n\n       [[ 4.004428  , -1.5304439 , -1.0645057 , ..., -1.3070977 ,\n         -1.7046654 , -0.42329815],\n        [ 6.164284  , -0.4061684 , -1.1873906 , ..., -1.3393153 ,\n         -1.8861045 , -1.8528885 ],\n        [ 5.9722977 , -1.1154335 , -1.0116359 , ..., -1.1730963 ,\n         -1.6184071 , -1.5292833 ],\n        ...,\n        [ 1.4968075 , -2.1300478 ,  1.2883517 , ...,  0.30521736,\n         -2.9978743 ,  3.9086025 ],\n        [ 0.68947285, -1.6700606 ,  0.48152518, ..., -0.09665348,\n         -2.489622  ,  5.368163  ],\n        [ 0.49167556, -2.3265865 ,  1.2546384 , ...,  1.6504503 ,\n         -2.937198  ,  3.5238042 ]],\n\n       ...,\n\n       [[ 2.0136719 , -1.5443933 , -0.995857  , ..., -0.7935262 ,\n         -1.5587214 ,  0.04592375],\n        [ 5.949845  , -1.0519358 , -1.2473067 , ..., -1.1787896 ,\n         -1.6000899 , -1.3036001 ],\n        [ 5.193037  , -1.1930175 , -1.4470754 , ..., -1.2377715 ,\n         -2.272998  , -1.2173096 ],\n        ...,\n        [ 0.09787848, -1.9046811 ,  0.22073314, ...,  2.8915255 ,\n         -2.6773348 , -0.22627246],\n        [-0.41955495, -2.02986   ,  0.10492338, ...,  3.935906  ,\n         -2.8089907 , -0.45451915],\n        [-0.20144568, -1.564522  , -0.06979136, ...,  3.2189605 ,\n         -2.692558  , -0.7473492 ]],\n\n       [[ 2.5997348 , -1.6728938 , -0.63443017, ..., -1.4162369 ,\n         -1.3144252 ,  0.01173805],\n        [ 6.1485314 , -0.36608234, -0.7258667 , ..., -2.0770552 ,\n         -1.9250768 , -1.4549558 ],\n        [ 6.220311  , -0.77784175, -0.66847754, ..., -1.7634381 ,\n         -1.8343551 , -1.441422  ],\n        ...,\n        [-1.7147992 , -0.7445232 ,  3.0627747 , ...,  1.5302951 ,\n         -1.556146  , -0.7908939 ],\n        [-2.1494386 , -0.53000575,  2.9308417 , ...,  1.7755798 ,\n         -1.8375412 , -0.41812512],\n        [-2.2338183 , -0.894239  ,  3.5817401 , ...,  1.9765705 ,\n         -1.8537011 , -0.3381852 ]],\n\n       [[ 4.1081896 , -1.7863655 , -0.8371127 , ..., -1.3999596 ,\n         -2.0466268 , -0.22405335],\n        [ 6.374647  , -0.9495045 , -0.9802384 , ..., -1.1920446 ,\n         -2.0436976 , -1.8929298 ],\n        [ 5.9258685 , -0.13790183, -0.7697776 , ..., -1.2913331 ,\n         -2.1232731 , -1.9614186 ],\n        ...,\n        [-1.3141325 , -2.1083286 , -0.5923042 , ...,  6.754025  ,\n         -2.5073946 ,  0.9992453 ],\n        [-1.3384458 , -2.1396263 , -0.395907  , ...,  6.7997427 ,\n         -2.4598    ,  0.7980067 ],\n        [-1.3597232 , -2.2289631 , -0.71087015, ...,  6.658898  ,\n         -2.3441703 ,  1.0996859 ]]], dtype=float32), label_ids=array([[-100,    0,    0, ..., -100, -100, -100],\n       [-100,    0,    0, ..., -100, -100, -100],\n       [-100,    0,    0, ...,    6,    6,    6],\n       ...,\n       [-100,    0,    0, ..., -100, -100, -100],\n       [-100,    0,    0, ..., -100, -100, -100],\n       [-100,    0,    0, ..., -100, -100, -100]]), metrics={'test_loss': 0.191721111536026, 'test_accuracy': 0.9447842876634394, 'test_precision': 0.7677760968229954, 'test_recall': 0.723965763195435, 'test_f1': 0.7452276064610867, 'test_runtime': 101.2282, 'test_samples_per_second': 3.951, 'test_steps_per_second': 0.247})</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/finetune_binary/finetune_binary/","title":"Binary Classification","text":"In\u00a0[1]: Copied! <pre>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n</pre> from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer In\u00a0[2]: Copied! <pre># Load the config file\nconfigs = load_config(\"./finetune_config.yaml\")\n</pre> # Load the config file configs = load_config(\"./finetune_config.yaml\") In\u00a0[3]: Copied! <pre># Load the model and tokenizer\nmodel_name = \"zhangtaolab/plant-dnabert-BPE\"\n# from Hugging Face\n# model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n# from ModelScope\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\n</pre> # Load the model and tokenizer model_name = \"zhangtaolab/plant-dnabert-BPE\" # from Hugging Face # model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\") # from ModelScope model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") <pre>Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnabert-BPE\n14:36:28 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnabert-BPE\n</pre> <pre>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnabert-BPE and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[4]: Copied! <pre># Load the datasets\ndata_name = \"zhangtaolab/plant-multi-species-core-promoters\"\n# from Hugging Face\n# datasets = DNADataset.from_huggingface(data_name, seq_col=\"sequence\", label_col=\"label\", tokenizer=tokenizer, max_length=512)\n# from ModelScope\ndatasets = DNADataset.from_modelscope(data_name, seq_col=\"sequence\", label_col=\"label\", tokenizer=tokenizer, max_length=512)\n\n# Encode the datasets\ndatasets.encode_sequences()\n\n# sample datasets\nsampled_datasets = datasets.sampling(0.05, overwrite=True)\n</pre> # Load the datasets data_name = \"zhangtaolab/plant-multi-species-core-promoters\" # from Hugging Face # datasets = DNADataset.from_huggingface(data_name, seq_col=\"sequence\", label_col=\"label\", tokenizer=tokenizer, max_length=512) # from ModelScope datasets = DNADataset.from_modelscope(data_name, seq_col=\"sequence\", label_col=\"label\", tokenizer=tokenizer, max_length=512)  # Encode the datasets datasets.encode_sequences()  # sample datasets sampled_datasets = datasets.sampling(0.05, overwrite=True) In\u00a0[5]: Copied! <pre># Initialize the trainer\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=sampled_datasets\n)\n</pre> # Initialize the trainer trainer = DNATrainer(     model=model,     config=configs,     datasets=sampled_datasets ) In\u00a0[6]: Copied! <pre># Start training\nmetrics = trainer.train()\nprint(metrics)\n</pre> # Start training metrics = trainer.train() print(metrics)        [624/624 32:45, Epoch 3/3]      Step Training Loss Validation Loss <pre>{'train_runtime': 1970.2993, 'train_samples_per_second': 5.067, 'train_steps_per_second': 0.317, 'total_flos': 2626900776714240.0, 'train_loss': 0.49973121056189906, 'epoch': 3.0}\n</pre> In\u00a0[7]: Copied! <pre># Do prediction on the test set\ntrainer.infer()\n</pre> # Do prediction on the test set trainer.infer() Out[7]: <pre>PredictionOutput(predictions=array([[ 0.5577329 , -0.8030548 ],\n       [ 0.9570494 , -1.1918997 ],\n       [-0.9938638 ,  0.9575563 ],\n       [-0.60064745,  0.6426938 ],\n       [ 1.4832035 , -1.9161403 ],\n       [ 1.3016005 , -1.5200926 ],\n       [ 1.5945574 , -1.6393385 ],\n       [-1.4026142 ,  1.4884773 ],\n       [-1.3183881 ,  1.2857653 ],\n       [-0.09730295, -0.11398756],\n       [-0.2546597 ,  0.17048775],\n       [ 1.3844563 , -1.6691169 ],\n       [-0.48660016,  0.3201029 ],\n       [-1.294498  ,  1.3028102 ],\n       [ 1.325114  , -1.6558539 ],\n       [-0.2022911 ,  0.31325665],\n       [ 1.423259  , -1.6940306 ],\n       [-1.3596604 ,  1.3692466 ],\n       [ 0.80646116, -1.0766298 ],\n       [-0.84406507,  0.71761286],\n       [ 0.54542243, -0.6185697 ],\n       [ 0.16049805, -0.2285097 ],\n       [ 1.5807096 , -1.9499329 ],\n       [-1.2979336 ,  1.3735614 ],\n       [-1.2418863 ,  1.1945035 ],\n       [-1.4102248 ,  1.4493538 ],\n       [ 1.5520118 , -1.9036473 ],\n       [ 1.0173329 , -1.1798364 ],\n       [-0.11771775, -0.10202907],\n       [ 1.6252443 , -1.8320519 ],\n       [ 0.3479699 , -0.51021934],\n       [-0.5696614 ,  0.5631565 ],\n       [-0.11787229,  0.01062225],\n       [-0.83115715,  0.7247779 ],\n       [ 1.6075097 , -1.8506739 ],\n       [-1.3573443 ,  1.3718733 ],\n       [-0.58971155,  0.5022862 ],\n       [ 0.4714074 , -0.6349477 ],\n       [ 1.4791216 , -1.6792916 ],\n       [ 1.1500984 , -1.4210988 ],\n       [ 1.5822346 , -2.0107975 ],\n       [ 1.3554505 , -1.4193162 ],\n       [ 1.5690008 , -1.861536  ],\n       [-0.84422225,  0.8461284 ],\n       [ 0.00745145, -0.22164413],\n       [ 1.1027538 , -1.29434   ],\n       [ 1.2410516 , -1.5359365 ],\n       [ 1.4366279 , -1.4913076 ],\n       [ 1.5612892 , -1.8441374 ],\n       [-0.99949676,  0.9770712 ],\n       [ 1.2095952 , -1.4052038 ],\n       [-1.2406192 ,  1.2911683 ],\n       [-1.0312954 ,  0.9276224 ],\n       [-0.8732899 ,  0.760186  ],\n       [-0.5695674 ,  0.4935085 ],\n       [-1.3672153 ,  1.3892132 ],\n       [-1.1820761 ,  1.2388614 ],\n       [ 0.8144074 , -0.82929546],\n       [-0.9985524 ,  0.9819247 ],\n       [ 1.4472567 , -1.7250379 ],\n       [-1.2314817 ,  1.2551306 ],\n       [-0.7306664 ,  0.6941302 ],\n       [-0.6863005 ,  0.70132214],\n       [-1.079128  ,  1.1422242 ],\n       [-0.6269436 ,  0.60497206],\n       [-0.26599976,  0.2267803 ],\n       [-0.825573  ,  0.7718678 ],\n       [ 0.03657643, -0.24321091],\n       [-1.2272748 ,  1.2231656 ],\n       [-1.2708217 ,  1.3129302 ],\n       [-0.26195097,  0.16955447],\n       [-1.3458587 ,  1.4039825 ],\n       [-1.1734842 ,  1.1932706 ],\n       [-0.9798685 ,  1.0740572 ],\n       [-1.3917636 ,  1.4167433 ],\n       [ 1.502997  , -1.7267826 ],\n       [ 0.37218732, -0.6693345 ],\n       [-0.8302911 ,  0.79802185],\n       [ 0.9518308 , -1.1267493 ],\n       [-0.6255062 ,  0.57823926],\n       [-1.3481553 ,  1.3887287 ],\n       [ 1.3758191 , -1.4774593 ],\n       [-0.5602606 ,  0.5290985 ],\n       [ 1.3468207 , -1.303733  ],\n       [-1.2037784 ,  1.1528097 ],\n       [ 1.7095612 , -1.9561986 ],\n       [-1.1514832 ,  1.0992239 ],\n       [ 1.5686915 , -1.8688035 ],\n       [ 0.42763752, -0.51843286],\n       [-1.3540958 ,  1.3385452 ],\n       [-1.2284757 ,  1.2295631 ],\n       [-1.1457762 ,  1.0981886 ],\n       [-1.2776201 ,  1.2983266 ],\n       [-1.3212379 ,  1.2867109 ],\n       [ 1.7022113 , -1.7515687 ],\n       [-1.1499294 ,  1.2604824 ],\n       [-0.8432434 ,  0.87342083],\n       [-0.2900778 ,  0.24339008],\n       [-0.38514715,  0.23084594],\n       [-1.4251684 ,  1.4629778 ],\n       [ 0.6050388 , -0.80519634],\n       [ 1.4799398 , -1.8512454 ],\n       [-0.47195566,  0.36394602],\n       [-1.0522511 ,  0.95883656],\n       [ 1.5833713 , -1.836211  ],\n       [-1.2626321 ,  1.230873  ],\n       [-1.1727291 ,  1.1579808 ],\n       [-1.3536657 ,  1.3644755 ],\n       [-0.39937246,  0.21839345],\n       [ 1.0680312 , -1.2066556 ],\n       [-0.7543539 ,  0.70291096],\n       [ 1.5970832 , -1.677627  ],\n       [-0.92718506,  0.82606965],\n       [-0.6040784 ,  0.5762423 ],\n       [ 0.4636594 , -0.58932215],\n       [ 0.09789699, -0.05052497],\n       [ 1.4964664 , -1.5802745 ],\n       [ 1.3499347 , -1.3258331 ],\n       [ 1.6447695 , -1.8706449 ],\n       [-0.985332  ,  0.96924824],\n       [ 0.20887558, -0.35321534],\n       [-0.05957422, -0.19362457],\n       [-1.0171176 ,  0.96157575],\n       [-0.4644188 ,  0.40253037],\n       [ 1.5823988 , -1.4988681 ],\n       [-0.31100056,  0.09334096],\n       [ 1.5717999 , -1.6346072 ],\n       [ 1.796017  , -1.7547585 ],\n       [-0.40313178,  0.2804762 ],\n       [ 1.6515217 , -1.881707  ],\n       [-1.2736226 ,  1.3228364 ],\n       [-0.26445752,  0.04009939],\n       [ 0.40440384, -0.5677159 ],\n       [-1.1390237 ,  1.1724001 ],\n       [ 0.494136  , -0.5247938 ],\n       [ 0.9650662 , -1.2135478 ],\n       [-0.7476247 ,  0.6455683 ],\n       [-1.015749  ,  1.01013   ],\n       [-1.2159853 ,  1.2823149 ],\n       [ 1.0993272 , -1.3466582 ],\n       [ 0.98879   , -1.0664445 ],\n       [-0.36989778,  0.52961004],\n       [-0.57509124,  0.5194055 ],\n       [-1.2165301 ,  1.2243818 ],\n       [-0.24992532,  0.06832168],\n       [ 0.20572607, -0.34993288],\n       [-0.73381203,  0.7251406 ],\n       [-0.90065974,  0.7981563 ],\n       [-0.5190124 ,  0.45966852],\n       [ 1.6118833 , -1.9382305 ],\n       [-1.0084198 ,  0.915203  ],\n       [ 0.66755706, -0.789568  ],\n       [-0.8382204 ,  0.7918621 ],\n       [-0.99696606,  0.97336346],\n       [ 0.21140781, -0.33548838],\n       [-1.0679529 ,  1.0950316 ],\n       [-1.189418  ,  1.1952361 ],\n       [-0.8177872 ,  0.74131304],\n       [-1.2991964 ,  1.3310165 ],\n       [ 1.4556478 , -1.7494483 ],\n       [-1.1688458 ,  1.2045152 ],\n       [ 0.5536668 , -0.78122836],\n       [ 1.361622  , -1.6234909 ],\n       [ 0.7649532 , -0.8133715 ],\n       [-0.01458968, -0.07631731],\n       [ 0.8659127 , -0.7840119 ],\n       [ 0.63314635, -0.80083853],\n       [ 1.6893884 , -1.8274938 ],\n       [-0.96642077,  0.99818146],\n       [ 0.7272222 , -1.0515829 ],\n       [-0.86757225,  0.77184767],\n       [-1.1105474 ,  1.171454  ],\n       [-1.389122  ,  1.3928033 ],\n       [-0.5306263 ,  0.45501995],\n       [ 0.00532693, -0.17311484],\n       [ 1.5873389 , -1.927431  ],\n       [-1.2789222 ,  1.3592643 ],\n       [-0.42477873,  0.39500433],\n       [-0.63694185,  0.48672152],\n       [ 0.41364676, -0.7547719 ],\n       [-1.2107717 ,  1.1538314 ],\n       [-0.5199976 ,  0.36353183],\n       [ 0.8498854 , -0.9862545 ],\n       [-0.07518784,  0.02111368],\n       [-1.0819017 ,  1.0662978 ],\n       [-0.8103624 ,  0.78190905],\n       [ 0.03135213, -0.02362744],\n       [-0.5873022 ,  0.58831584],\n       [-1.3325663 ,  1.2818176 ],\n       [-1.3739865 ,  1.4229722 ],\n       [-0.42308006,  0.30201465],\n       [-0.7269806 ,  0.81624883],\n       [ 0.9002553 , -1.178957  ],\n       [ 1.7332679 , -1.9236602 ],\n       [-0.8217513 ,  0.8404052 ],\n       [ 0.5108718 , -0.6016149 ],\n       [ 0.94424814, -1.0327394 ],\n       [-0.6843086 ,  0.63677686],\n       [ 0.4226529 , -0.7417124 ],\n       [-1.1976591 ,  1.1869377 ],\n       [ 1.2547488 , -1.4446388 ],\n       [ 0.1772547 , -0.32688025],\n       [-1.0880431 ,  1.0344006 ],\n       [-0.85587543,  0.98312926],\n       [-0.28562692,  0.2751907 ],\n       [-1.26179   ,  1.3049685 ],\n       [-0.570008  ,  0.6790961 ],\n       [-1.1346258 ,  1.1528231 ],\n       [-1.0644034 ,  1.117829  ],\n       [-0.6452114 ,  0.6957346 ],\n       [ 1.4879336 , -1.7165645 ],\n       [-0.26838493, -0.03449854],\n       [ 1.2421948 , -1.4744865 ],\n       [ 1.6533617 , -1.8518062 ],\n       [-1.2761068 ,  1.3455716 ],\n       [-0.2525769 ,  0.1322923 ],\n       [ 1.5002793 , -1.8049845 ],\n       [ 1.2922493 , -1.4359244 ],\n       [-1.1557776 ,  1.1496897 ],\n       [-0.8210451 ,  0.8799413 ],\n       [-1.1786658 ,  1.1697284 ],\n       [ 1.5982404 , -1.582562  ],\n       [-1.2301971 ,  1.258272  ],\n       [-0.73068285,  0.73771787],\n       [-1.1131903 ,  1.1523852 ],\n       [-0.31618655,  0.16451481],\n       [-0.6307304 ,  0.6571413 ],\n       [-0.9559965 ,  0.87205756],\n       [ 1.2460501 , -1.5037402 ],\n       [-0.50554425,  0.4592195 ],\n       [-1.1698605 ,  1.1423081 ],\n       [-1.4411846 ,  1.4663855 ],\n       [ 1.5138999 , -1.5124294 ],\n       [-1.1834826 ,  1.1537012 ],\n       [ 0.79589194, -0.78574675],\n       [ 1.0967437 , -1.227151  ],\n       [-0.6829153 ,  0.53623897],\n       [ 1.3922092 , -1.5305232 ],\n       [ 0.4359655 , -0.5606684 ],\n       [-0.22898187,  0.23115505],\n       [ 0.33569926, -0.56453013],\n       [-0.66180474,  0.6605379 ],\n       [ 1.3518492 , -1.3521231 ],\n       [ 0.59174436, -0.715198  ],\n       [-0.6936844 ,  0.60279775],\n       [-1.4122216 ,  1.4706935 ],\n       [-0.4734665 ,  0.43554315],\n       [-1.3261245 ,  1.3227925 ],\n       [-1.1183566 ,  1.2143539 ],\n       [ 1.5324315 , -1.6837435 ],\n       [ 0.08614116, -0.27294338],\n       [ 0.6863317 , -0.95060056],\n       [ 1.2068276 , -1.3238548 ],\n       [ 1.697599  , -1.9149207 ],\n       [ 1.5546368 , -1.6088763 ],\n       [-0.523581  ,  0.61582714],\n       [-1.2198628 ,  1.2916564 ],\n       [-1.0805902 ,  1.0977618 ],\n       [ 1.216892  , -1.4714797 ],\n       [-0.73172367,  0.7032055 ],\n       [-1.2984093 ,  1.2936101 ],\n       [-1.2466395 ,  1.3180121 ],\n       [-0.39185238,  0.27262735],\n       [-1.3931319 ,  1.421331  ],\n       [-0.8177087 ,  0.8800821 ],\n       [-1.125269  ,  1.134355  ],\n       [ 1.7540283 , -1.9865215 ],\n       [ 0.07949524, -0.19705625],\n       [-0.9389173 ,  0.94795847],\n       [ 0.56559855, -0.41471455],\n       [-1.2744968 ,  1.330278  ],\n       [-1.1891031 ,  1.193532  ],\n       [ 0.05332426, -0.24565436],\n       [-1.062723  ,  0.9855904 ],\n       [-0.04422983, -0.09029736],\n       [-0.2265503 ,  0.06257492],\n       [-0.82969874,  0.75241905],\n       [-0.72241527,  0.75683933],\n       [ 1.3615862 , -1.427906  ],\n       [-1.3504695 ,  1.3124349 ],\n       [ 0.53724414, -0.7809214 ],\n       [-0.45657033,  0.3229495 ],\n       [-1.2515299 ,  1.276889  ],\n       [-0.97517055,  1.082663  ],\n       [-0.4341408 ,  0.256033  ],\n       [ 1.2664717 , -1.521618  ],\n       [ 0.7300044 , -0.9394987 ],\n       [-0.5673488 ,  0.43745005],\n       [-1.405819  ,  1.3792373 ],\n       [ 0.2428019 , -0.43220258],\n       [-0.09925749,  0.05682614],\n       [-1.0530088 ,  1.0433487 ],\n       [-1.1487026 ,  1.1373123 ],\n       [ 1.1027614 , -1.3764333 ],\n       [-0.84106135,  0.96369165],\n       [-0.94482905,  0.95197755],\n       [ 1.5302075 , -1.787095  ],\n       [ 0.11337625, -0.26553828],\n       [ 1.3584012 , -1.7496812 ],\n       [ 1.6934212 , -1.9382876 ],\n       [ 1.2942636 , -1.5303822 ],\n       [ 1.0669235 , -1.3576771 ],\n       [-1.2527634 ,  1.2186801 ],\n       [-1.0050036 ,  1.0154649 ],\n       [-1.1998129 ,  1.2176453 ],\n       [-1.3041458 ,  1.3380841 ],\n       [-1.3520285 ,  1.3832189 ],\n       [-0.9372749 ,  0.9113778 ],\n       [-0.2345313 ,  0.29747623],\n       [ 1.6521133 , -1.8190899 ],\n       [ 1.6075076 , -1.775444  ],\n       [ 1.7293898 , -1.9653217 ],\n       [-0.92414427,  0.9611327 ],\n       [ 1.1550691 , -1.3208699 ],\n       [ 0.5349592 , -0.7314057 ],\n       [-1.1760349 ,  1.1673583 ],\n       [ 0.8511173 , -1.1448234 ],\n       [-0.4309797 ,  0.24988866],\n       [-0.94850856,  0.9259827 ],\n       [-1.1192988 ,  1.1273329 ],\n       [ 0.21793678, -0.47305   ],\n       [-1.1826305 ,  1.2146287 ],\n       [ 0.73061085, -0.9866113 ],\n       [-1.351041  ,  1.418126  ],\n       [-1.2654577 ,  1.20268   ],\n       [-0.05933864, -0.17659986],\n       [ 1.6260041 , -1.9416031 ],\n       [-1.1156518 ,  1.1050879 ],\n       [-1.1453948 ,  1.0875494 ],\n       [-1.1021256 ,  1.1232965 ],\n       [ 1.1123554 , -1.3207425 ],\n       [-0.7780799 ,  0.76053244],\n       [-0.46665007,  0.45093125],\n       [-1.0185397 ,  0.9815777 ],\n       [-1.3853211 ,  1.4102417 ],\n       [ 1.0734968 , -1.2083229 ],\n       [-1.3982979 ,  1.3957694 ],\n       [-0.5815403 ,  0.512447  ],\n       [ 0.88721687, -1.1894358 ],\n       [ 0.44434276, -0.53479385],\n       [-0.47004694,  0.34366703],\n       [ 1.5649613 , -1.7546021 ],\n       [-0.7523135 ,  0.732053  ],\n       [ 1.6071392 , -1.8448821 ],\n       [ 0.6281331 , -0.90578204],\n       [ 0.24052589, -0.38822764],\n       [ 1.4331508 , -1.5123495 ],\n       [ 0.09660292, -0.39895368],\n       [-0.26327515,  0.1333119 ],\n       [-1.3925685 ,  1.3932649 ],\n       [-0.86779255,  0.7629286 ],\n       [ 0.5935859 , -0.61984646],\n       [-1.2782537 ,  1.3463581 ],\n       [ 0.94731647, -1.0377201 ],\n       [-1.1538966 ,  1.1813294 ],\n       [ 0.57126933, -0.79344994],\n       [ 1.8104352 , -1.9474521 ],\n       [-1.2555505 ,  1.3041471 ],\n       [ 1.1934108 , -1.4009365 ],\n       [-0.39384183,  0.4257695 ],\n       [ 1.6122043 , -1.7356795 ],\n       [ 0.34323618, -0.5432874 ],\n       [ 0.6424754 , -0.82168   ],\n       [-0.6096074 ,  0.48168054],\n       [ 1.2125766 , -1.5653802 ],\n       [-1.0322951 ,  1.010977  ],\n       [ 1.37907   , -1.6104273 ],\n       [-1.3398412 ,  1.3727248 ],\n       [-1.3224226 ,  1.3238128 ],\n       [-0.75143903,  0.6346068 ],\n       [-0.8448062 ,  0.8183308 ],\n       [-0.8441712 ,  0.8117836 ],\n       [ 1.0812001 , -1.3846493 ],\n       [ 1.7432698 , -2.0456288 ],\n       [ 1.3448184 , -1.2538617 ],\n       [-0.92661667,  0.8846802 ],\n       [ 1.085209  , -1.3600862 ],\n       [ 0.7873263 , -0.9047324 ],\n       [ 0.09210203, -0.3161428 ],\n       [-1.2893046 ,  1.3230072 ],\n       [-1.1746594 ,  1.2350492 ],\n       [ 1.4256618 , -1.6675484 ],\n       [ 1.4060736 , -1.7232703 ],\n       [ 1.447527  , -1.7386421 ],\n       [-1.0773181 ,  1.0621217 ],\n       [-1.059279  ,  1.0186865 ],\n       [ 1.1942095 , -1.3816328 ],\n       [ 1.083388  , -1.3679458 ],\n       [-0.93872064,  0.8314673 ],\n       [-0.5958398 ,  0.62500936],\n       [ 1.6514406 , -1.5800294 ],\n       [ 0.9539915 , -1.2396554 ],\n       [-1.2231072 ,  1.2656891 ],\n       [-1.2012442 ,  1.1845726 ],\n       [-0.8104535 ,  0.9064877 ],\n       [-1.1194602 ,  1.1714758 ],\n       [ 1.4919053 , -1.614102  ],\n       [-0.7094151 ,  0.68993574],\n       [-1.2707471 ,  1.2356404 ],\n       [-1.3438381 ,  1.3274558 ],\n       [-1.3180305 ,  1.328517  ],\n       [-1.0589381 ,  1.0890217 ],\n       [ 1.4062606 , -1.7911521 ],\n       [-0.03953973, -0.1970173 ],\n       [-0.351996  ,  0.12677348],\n       [-1.0855893 ,  1.0397857 ],\n       [ 0.80288625, -1.0463302 ],\n       [-1.1802422 ,  1.2334666 ],\n       [-1.3390526 ,  1.3797063 ],\n       [-1.4089551 ,  1.3949007 ],\n       [ 1.3641564 , -1.4658877 ],\n       [ 0.09185684, -0.33491716],\n       [-1.2480863 ,  1.2653872 ],\n       [ 0.902401  , -1.0304167 ],\n       [-0.66132396,  0.58375436],\n       [ 0.7220566 , -0.8485579 ]], dtype=float32), label_ids=array([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n       0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n       0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n       1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n       0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n       1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n       1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n       1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n       0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0]), metrics={'test_loss': 0.5033698678016663, 'test_accuracy': 0.7644230769230769, 'test_precision': 0.7184873949579832, 'test_recall': 0.8465346534653465, 'test_f1': 0.7772727272727272, 'test_mcc': 0.5388628601153327, 'test_AUROC': 0.8568289071897844, 'test_AUPRC': 0.8378719062491448, 'test_TPR': 0.8465346534653465, 'test_TNR': 0.6869158878504673, 'test_FPR': 0.3130841121495327, 'test_FNR': 0.15346534653465346, 'test_runtime': 34.816, 'test_samples_per_second': 11.949, 'test_steps_per_second': 0.747})</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/finetune_custom_head/finetune/","title":"Custom Head","text":"In\u00a0[1]: Copied! <pre>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n</pre> from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer In\u00a0[2]: Copied! <pre># Load the config file\nconfigs = load_config(\"./finetune_config.yaml\")\n</pre> # Load the config file configs = load_config(\"./finetune_config.yaml\") In\u00a0[3]: Copied! <pre># Load the model and tokenizer\nmodel_name = \"zhangtaolab/plant-dnagpt-BPE\"\n# from ModelScope\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\n</pre> # Load the model and tokenizer model_name = \"zhangtaolab/plant-dnagpt-BPE\" # from ModelScope model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") <pre>Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE\n14:36:48 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE\n14:36:50 - dnallm.utils.support - WARNING - Warning: Could not determine model type, falling back to 'mean' pooling.\n14:36:50 - dnallm.utils.support - INFO - Using mean pooling strategy.\n</pre> In\u00a0[4]: Copied! <pre># Load the datasets\ndata_name = \"zhangtaolab/plant-multi-species-core-promoters\"\n# from Hugging Face\n# datasets = DNADataset.from_huggingface(data_name, seq_col=\"sequence\", label_col=\"label\", tokenizer=tokenizer, max_length=512)\n# from ModelScope\ndatasets = DNADataset.from_modelscope(data_name, seq_col=\"sequence\", label_col=\"label\", tokenizer=tokenizer, max_length=512)\n\n# sample datasets\nsampled_datasets = datasets.sampling(0.1, overwrite=True)\n\n# Encode the datasets\nsampled_datasets.encode_sequences()\n</pre> # Load the datasets data_name = \"zhangtaolab/plant-multi-species-core-promoters\" # from Hugging Face # datasets = DNADataset.from_huggingface(data_name, seq_col=\"sequence\", label_col=\"label\", tokenizer=tokenizer, max_length=512) # from ModelScope datasets = DNADataset.from_modelscope(data_name, seq_col=\"sequence\", label_col=\"label\", tokenizer=tokenizer, max_length=512)  # sample datasets sampled_datasets = datasets.sampling(0.1, overwrite=True)  # Encode the datasets sampled_datasets.encode_sequences() <pre>Encoding inputs:   0%|          | 0/6656 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/832 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/832 [00:00&lt;?, ? examples/s]</pre> In\u00a0[5]: Copied! <pre># Initialize the trainer\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=sampled_datasets\n)\n</pre> # Initialize the trainer trainer = DNATrainer(     model=model,     config=configs,     datasets=sampled_datasets ) In\u00a0[6]: Copied! <pre># Start training\nmetrics = trainer.train()\nprint(metrics)\n</pre> # Start training metrics = trainer.train() print(metrics) <pre>We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n</pre>        [417/417 1:11:52, Epoch 3/3]      Step Training Loss Validation Loss Accuracy Precision Recall F1 Mcc Auroc Auprc Tpr Tnr Fpr Fnr 100 0.564600 0.477763 0.771635 0.808933 0.742597 0.774347 0.546099 0.856063 0.856556 0.742597 0.804071 0.195929 0.257403 200 0.446100 0.466926 0.782452 0.753937 0.872437 0.808870 0.567577 0.864404 0.859758 0.872437 0.681934 0.318066 0.127563 300 0.414200 0.460737 0.786058 0.772443 0.842825 0.806100 0.571194 0.868826 0.865918 0.842825 0.722646 0.277354 0.157175 400 0.361500 0.469775 0.776442 0.784270 0.794989 0.789593 0.551212 0.864665 0.860681 0.794989 0.755725 0.244275 0.205011 <pre>{'train_runtime': 4322.1369, 'train_samples_per_second': 4.62, 'train_steps_per_second': 0.096, 'total_flos': 5298226681872384.0, 'train_loss': 0.44137911144778025, 'epoch': 3.0}\n</pre> In\u00a0[7]: Copied! <pre># Do prediction on the test set\nresults = trainer.infer()\nresults.metrics\n</pre> # Do prediction on the test set results = trainer.infer() results.metrics Out[7]: <pre>{'test_loss': 0.4892328679561615,\n 'test_accuracy': 0.7608173076923077,\n 'test_precision': 0.7344632768361582,\n 'test_recall': 0.8705357142857143,\n 'test_f1': 0.7967313585291114,\n 'test_mcc': 0.522206944653848,\n 'test_AUROC': 0.8532772972470237,\n 'test_AUPRC': 0.8636071401874013,\n 'test_TPR': 0.8705357142857143,\n 'test_TNR': 0.6328125,\n 'test_FPR': 0.3671875,\n 'test_FNR': 0.12946428571428573,\n 'test_runtime': 24.9902,\n 'test_samples_per_second': 33.293,\n 'test_steps_per_second': 0.72}</pre> In\u00a0[8]: Copied! <pre># Change head config in the config file\nconfigs['task'].head_config.head = \"megadna\"\n# Change saved model path\nconfigs['finetune'].output_dir = \"./outputs_megadna\"\n</pre> # Change head config in the config file configs['task'].head_config.head = \"megadna\" # Change saved model path configs['finetune'].output_dir = \"./outputs_megadna\" In\u00a0[9]: Copied! <pre># Load the model and tokenizer\nmodel_name = \"lingxusb/megaDNA_updated\"\n# from Hugging Face\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n# from ModelScope\n# model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\n</pre> # Load the model and tokenizer model_name = \"lingxusb/megaDNA_updated\" # from Hugging Face model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\") # from ModelScope # model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") <pre>Fetching 3 files:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>megaDNA_phage_145M.pt:   0%|          | 0.00/582M [00:00&lt;?, ?B/s]</pre> <pre>README.md:   0%|          | 0.00/29.0 [00:00&lt;?, ?B/s]</pre> <pre>.gitattributes: 0.00B [00:00, ?B/s]</pre> <pre>15:51:13 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/huggingface/hub/models--lingxusb--megaDNA_updated/snapshots/ed298be539e1667b52a1181a6472528a34dd2ef9\n</pre> <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nFile ~/GitHub/DNALLM/dnallm/models/special/megadna.py:128, in _handle_megadna_models(model_name, source, head_config, extra)\n    125 downloaded_model_path = os.path.join(\n    126     downloaded_model_path, full_model_name\n    127 )\n--&gt; 128 megadna_model = torch.load(\n    129     downloaded_model_path, weights_only=False\n    130 )\n    131 megadna_tokenizer = DNATokenizer()\n\nFile ~/GitHub/DNALLM/.venv/lib/python3.12/site-packages/torch/serialization.py:1471, in load(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\n   1470                 raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n-&gt; 1471         return _load(\n   1472             opened_zipfile,\n   1473             map_location,\n   1474             pickle_module,\n   1475             overall_storage=overall_storage,\n   1476             **pickle_load_args,\n   1477         )\n   1478 if mmap:\n\nFile ~/GitHub/DNALLM/.venv/lib/python3.12/site-packages/torch/serialization.py:1964, in _load(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\n   1963 _serialization_tls.map_location = map_location\n-&gt; 1964 result = unpickler.load()\n   1965 _serialization_tls.map_location = None\n\nFile ~/GitHub/DNALLM/.venv/lib/python3.12/site-packages/torch/serialization.py:1953, in _load.&lt;locals&gt;.UnpicklerWrapper.find_class(self, mod_name, name)\n   1952 mod_name = load_module_mapping.get(mod_name, mod_name)\n-&gt; 1953 return super().find_class(mod_name, name)\n\nModuleNotFoundError: No module named 'megaDNA'\n\nThe above exception was the direct cause of the following exception:\n\nImportError                               Traceback (most recent call last)\nCell In[9], line 4\n      2 model_name = \"lingxusb/megaDNA_updated\"\n      3 # from Hugging Face\n----&gt; 4 model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n      5 # from ModelScope\n      6 # model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\n\nFile ~/GitHub/DNALLM/dnallm/models/model.py:855, in load_model_and_tokenizer(model_name, task_config, source, use_mirror, revision, custom_tokenizer)\n    852 _ = _handle_gpn_models(model_name)\n    854 # Handle special case for megaDNA models\n--&gt; 855 megadna_result = _handle_megadna_models(model_name, source, head_config)\n    856 if megadna_result is not None:\n    857     return megadna_result\n\nFile ~/GitHub/DNALLM/dnallm/models/special/megadna.py:147, in _handle_megadna_models(model_name, source, head_config, extra)\n    142                 megadna_model = DNALLMforSequenceClassification(\n    143                     config=model_config,\n    144                     custom_model=megadna_model,\n    145                 )\n    146         except ImportError as e:\n--&gt; 147             raise ImportError(\n    148                 f\"megaDNA package is required for \"\n    149                 f\"{model_name} but not installed. \"\n    150                 \"Please install it following the instructions at: \"\n    151                 \"https://github.com/lingxusb/megaDNA\"\n    152             ) from e\n    154         return megadna_model, megadna_tokenizer\n    156 return None\n\nImportError: megaDNA package is required for lingxusb/megaDNA_updated but not installed. Please install it following the instructions at: https://github.com/lingxusb/megaDNA</pre> In\u00a0[\u00a0]: Copied! <pre># Load the datasets\ndatasets = DNADataset.from_modelscope(data_name, seq_col=\"sequence\", label_col=\"label\", tokenizer=tokenizer, max_length=1024)\nsampled_datasets = datasets.sampling(0.1, overwrite=True)\nsampled_datasets.encode_sequences()\n</pre> # Load the datasets datasets = DNADataset.from_modelscope(data_name, seq_col=\"sequence\", label_col=\"label\", tokenizer=tokenizer, max_length=1024) sampled_datasets = datasets.sampling(0.1, overwrite=True) sampled_datasets.encode_sequences() <pre>Encoding inputs:   0%|          | 0/6656 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/832 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/832 [00:00&lt;?, ? examples/s]</pre> In\u00a0[\u00a0]: Copied! <pre># Initialize the trainer\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=sampled_datasets\n)\n</pre> # Initialize the trainer trainer = DNATrainer(     model=model,     config=configs,     datasets=sampled_datasets ) In\u00a0[\u00a0]: Copied! <pre># Start training\nmetrics = trainer.train()\nprint(metrics)\n</pre> # Start training metrics = trainer.train() print(metrics) <pre>/home/liuguanqing/miniforge3/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n</pre>        [417/417 01:54, Epoch 3/3]      Step Training Loss Validation Loss Accuracy Precision Recall F1 Mcc Auroc Auprc Tpr Tnr Fpr Fnr 100 0.699400 0.674930 0.576923 0.584821 0.612150 0.598174 0.152141 0.606470 0.613311 0.612150 0.539604 0.460396 0.387850 200 0.675700 0.667276 0.587740 0.592191 0.637850 0.614173 0.173450 0.626203 0.625642 0.637850 0.534653 0.465347 0.362150 300 0.646800 0.674051 0.597356 0.662021 0.443925 0.531469 0.214306 0.664928 0.658847 0.443925 0.759901 0.240099 0.556075 400 0.618900 0.667258 0.610577 0.668831 0.481308 0.559783 0.236859 0.673285 0.666627 0.481308 0.747525 0.252475 0.518692 <pre>/home/liuguanqing/miniforge3/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n/home/liuguanqing/miniforge3/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n/home/liuguanqing/miniforge3/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n/home/liuguanqing/miniforge3/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n</pre> <pre>{'train_runtime': 114.7143, 'train_samples_per_second': 174.067, 'train_steps_per_second': 3.635, 'total_flos': 1.8043379178799104e+16, 'train_loss': 0.6572908154494471, 'epoch': 3.0}\n</pre> In\u00a0[\u00a0]: Copied! <pre># Do prediction on the test set\nresults = trainer.infer()\nresults.metrics\n</pre> # Do prediction on the test set results = trainer.infer() results.metrics <pre>/home/liuguanqing/miniforge3/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n</pre> Out[\u00a0]: <pre>{'test_loss': 0.6516980528831482,\n 'test_accuracy': 0.6213942307692307,\n 'test_precision': 0.6920289855072463,\n 'test_recall': 0.45368171021377673,\n 'test_f1': 0.5480631276901005,\n 'test_mcc': 0.26214204444019135,\n 'test_AUROC': 0.6950950985661528,\n 'test_AUPRC': 0.6844861256476427,\n 'test_TPR': 0.45368171021377673,\n 'test_TNR': 0.7931873479318735,\n 'test_FPR': 0.20681265206812652,\n 'test_FNR': 0.5463182897862233,\n 'test_runtime': 1.2112,\n 'test_samples_per_second': 686.915,\n 'test_steps_per_second': 14.861}</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/finetune_custom_head/finetune/#finetune-with-a-custom-classification-head-for-binary-classification","title":"Finetune with a custom classification head (for binary classification)\u00b6","text":""},{"location":"example/notebooks/finetune_custom_head/finetune/#model-that-is-not-compatible-with-the-transformer-library-megadna","title":"Model that is not compatible with the Transformer library (megaDNA)\u00b6","text":""},{"location":"example/notebooks/finetune_generation/finetune_generation/","title":"Generation","text":"In\u00a0[\u00a0]: Copied! <pre># !wget -c https://ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-62/fasta/arabidopsis_thaliana/cds/Arabidopsis_thaliana.TAIR10.cds.all.fa.gz\n\n# !pip install pyfastx\n</pre> # !wget -c https://ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-62/fasta/arabidopsis_thaliana/cds/Arabidopsis_thaliana.TAIR10.cds.all.fa.gz  # !pip install pyfastx In\u00a0[2]: Copied! <pre>from pyfastx import Fasta\n\ngenome = Fasta(\"Arabidopsis_thaliana.TAIR10.cds.all.fa.gz\")\nwith open(\"ath_cds.csv\", \"w\") as f:\n    print(\"seq_id,sequence\", file=f)\n    for seq in genome:\n        print(f\"{seq.name},{seq.seq}\", file=f)\n</pre> from pyfastx import Fasta  genome = Fasta(\"Arabidopsis_thaliana.TAIR10.cds.all.fa.gz\") with open(\"ath_cds.csv\", \"w\") as f:     print(\"seq_id,sequence\", file=f)     for seq in genome:         print(f\"{seq.name},{seq.seq}\", file=f) In\u00a0[3]: Copied! <pre>import copy\nfrom dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n</pre> import copy from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer In\u00a0[4]: Copied! <pre># Load the datasets\ndata_path = \"ath_cds.csv\"\ndatasets = DNADataset.load_local_data(data_path, seq_col=\"sequence\", sep=\",\")\n\n# Sampling the datasets\ndatasets.sampling(0.1, seed=42, overwrite=True)\ndatasets.split_data(seed=42)\n</pre> # Load the datasets data_path = \"ath_cds.csv\" datasets = DNADataset.load_local_data(data_path, seq_col=\"sequence\", sep=\",\")  # Sampling the datasets datasets.sampling(0.1, seed=42, overwrite=True) datasets.split_data(seed=42) <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> In\u00a0[5]: Copied! <pre>seq = datasets.dataset[\"test\"][10][\"sequence\"]\nprompt = seq[:10]\nprint(\"Length:\", len(seq))\nprint(\"Prompt sequence:\", prompt)\nprint(\"Full sequence:  \", seq)\n</pre> seq = datasets.dataset[\"test\"][10][\"sequence\"] prompt = seq[:10] print(\"Length:\", len(seq)) print(\"Prompt sequence:\", prompt) print(\"Full sequence:  \", seq) <pre>Length: 207\nPrompt sequence: ATGACTTGCA\nFull sequence:   ATGACTTGCACGACAGAGATAGATATTTTGAAGTGGACAGTGAGGTATTGTTCGAGTTTAGCTGCACACCTTCTAACTCCTACGAGATTGTTCAAATATGAAATTCAACAACAGAGCGATTTGAGAAATGCAACTGAAAACAAAACTGAAAAATATATTTCTGACGACGTCGGTCATTGTAGACATACATACATGCAAATCAGATAA\n</pre> In\u00a0[6]: Copied! <pre># Load the config file\nconfigs = load_config(\"./finetune_config.yaml\")\nconfigs[\"finetune\"].output_dir = \"./outputs_dnagpt\"\n</pre> # Load the config file configs = load_config(\"./finetune_config.yaml\") configs[\"finetune\"].output_dir = \"./outputs_dnagpt\" In\u00a0[7]: Copied! <pre># Load the model and tokenizer\nmodel_name = \"zhangtaolab/plant-dnagpt-singlebase\"\n# from Hugging Face\n# model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n# from ModelScope\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\ntokenizer.model_max_length = 2048\n</pre> # Load the model and tokenizer model_name = \"zhangtaolab/plant-dnagpt-singlebase\" # from Hugging Face # model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\") # from ModelScope model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") tokenizer.model_max_length = 2048 <pre>17:50:15 - dnallm.utils.support - WARNING - Generation task does not require num_labels, but got 1. Setting to 0.\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-singlebase\n</pre> <pre>2025-12-28 17:50:16,908 - modelscope - INFO - Got 1 files, start to download ...\n</pre> <pre>Processing 1 items:   0%|          | 0.00/1.00 [00:00&lt;?, ?it/s]</pre> <pre>Downloading [model.safetensors]:   0%|          | 0.00/328M [00:00&lt;?, ?B/s]</pre> <pre>2025-12-28 17:50:28,037 - modelscope - INFO - Download model 'zhangtaolab/plant-dnagpt-singlebase' successfully.\n</pre> <pre>17:50:28 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-singlebase\n</pre> In\u00a0[8]: Copied! <pre># Encode the datasets\ndata = copy.deepcopy(datasets)\ndata.encode_sequences(tokenizer=tokenizer)\n</pre> # Encode the datasets data = copy.deepcopy(datasets) data.encode_sequences(tokenizer=tokenizer) <pre>Encoding inputs:   0%|          | 0/3382 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/966 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/484 [00:00&lt;?, ? examples/s]</pre> In\u00a0[9]: Copied! <pre># Initialize the trainer\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=data\n)\n</pre> # Initialize the trainer trainer = DNATrainer(     model=model,     config=configs,     datasets=data ) In\u00a0[10]: Copied! <pre># Start training\nmetrics = trainer.train()\nprint(metrics)\n</pre> # Start training metrics = trainer.train() print(metrics) <pre>`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n</pre>        [424/424 06:00, Epoch 2/2]      Step Training Loss Validation Loss 200 1.289200 1.278937 400 1.272300 1.276405 <pre>There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n</pre> <pre>{'train_runtime': 361.7112, 'train_samples_per_second': 18.7, 'train_steps_per_second': 1.172, 'total_flos': 1767379304448000.0, 'train_loss': 1.2800271870954982, 'epoch': 2.0}\n</pre> In\u00a0[11]: Copied! <pre>model.eval()\n\ntokenizer.pad_token = tokenizer.eos_token\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=len(seq)+5, num_return_sequences=5, do_sample=True, top_k=50, top_p=0.95, temperature=1.0)\n</pre> model.eval()  tokenizer.pad_token = tokenizer.eos_token inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device) outputs = model.generate(**inputs, max_length=len(seq)+5, num_return_sequences=5, do_sample=True, top_k=50, top_p=0.95, temperature=1.0) <pre>Setting `pad_token_id` to `eos_token_id`:9 for open-end generation.\n</pre> In\u00a0[12]: Copied! <pre>print(\"Prompt:               \", prompt)\nfor i, out in enumerate(outputs):\n    out_seq = tokenizer.decode(out, skip_special_tokens=True)\n    print(f\"Generated sequence {i}: \", out_seq.replace(\" \", \"\"))\nprint(\"Raw sequence:         \", seq)\n</pre> print(\"Prompt:               \", prompt) for i, out in enumerate(outputs):     out_seq = tokenizer.decode(out, skip_special_tokens=True)     print(f\"Generated sequence {i}: \", out_seq.replace(\" \", \"\")) print(\"Raw sequence:         \", seq) <pre>Prompt:                ATGACTTGCA\nGenerated sequence 0:  ATGACTTGCATGTGGTCGTTGGAATAGGAGGTCACTATGTGGTTTTGAACCCAAGATTCTCATTTGATGGCTTATCAATCTCCATGCTTCTAACATTAGGTTTTCTCTCGTTCTTCTTCTTCTTTTCCTCTTATGGCGGCGGCGGCTCTCCAGTAGCTTGTGTGGAATCTGGAAAGGCATATTGTAGACCAAGGAATCTATCTCCAGCTA\nGenerated sequence 1:  ATGACTTGCAGATATACGTCACGAAGAAACCAAAATTCGACCTGACAGAAGGGAATCAAGCTGGTGAGGTTGAAGAACTCGCTATCTTCAGGTCTAACAGTATACTCCTCCAAAGGAAAGAAACGCTCTTCTTCCACCCTTTCTCCGTCGATGATGGTGTCGTTGAGAAGGAAATCAGAGCAGTTAAAGAGGTTAGACCGAGGTTAGCGT\nGenerated sequence 2:  ATGACTTGCATTCTCTTTTCGAAGCTGTGTTTTATCTTCAGATCTCAAGATTTCGATGTGGGATTTTGAACCAGCTGATTGATAAAGCTGGTTCAGAGTCTGGTTCTGGCCAACAAGAGGATTCAGCTTTGATGATTTTGGGAGCAGATTGCTCTACCTCAAGAGTATGGTTACATCGGCTTTCCATGATTGATGTAAAAGTTCTTGACA\nGenerated sequence 3:  ATGACTTGCAATTACGGTCTGCGAGGACACACTCCTCGAAGCTCGTCTCCGACCCTAACGTCGAGTCCAACTCCGAGTACAACCCCTTCGATTTGCCCAAAAGCCGGATGGATGCTTTTGGAGCAATCAAAGGAGAGCAGACAGAGGCGTTACACCTAACACCCTACCTTACAGGCCAGTTCTGCGCCGAGCTTCAAGAATTGAAAAAAA\nGenerated sequence 4:  ATGACTTGCACAAACAAAAAGCCTTAAACGCTTACGCCGGAAACGAACTCAGTCATTATGTCTTCACGGAAGATTCAAAAGTCGTGGAGTTTAGGGGAGAAATCTCAGATGGGCTTTTCATGGTGGATTCAAAAGCTAAAGATGATGATGATGTTCCAAATGTTGCTCTGGAAACCCGGGTTAAAACATCTGCTTACCTAAGATCTGTGG\nRaw sequence:          ATGACTTGCACGACAGAGATAGATATTTTGAAGTGGACAGTGAGGTATTGTTCGAGTTTAGCTGCACACCTTCTAACTCCTACGAGATTGTTCAAATATGAAATTCAACAACAGAGCGATTTGAGAAATGCAACTGAAAACAAAACTGAAAAATATATTTCTGACGACGTCGGTCATTGTAGACATACATACATGCAAATCAGATAA\n</pre> In\u00a0[13]: Copied! <pre># Load the config file\nconfigs = load_config(\"./finetune_config.yaml\")\nconfigs[\"task\"].task_type = \"embedding\"\nconfigs[\"finetune\"].output_dir = \"./outputs_megadna\"\n</pre> # Load the config file configs = load_config(\"./finetune_config.yaml\") configs[\"task\"].task_type = \"embedding\" configs[\"finetune\"].output_dir = \"./outputs_megadna\" In\u00a0[\u00a0]: Copied! <pre># !git clone https://github.com/lingxusb/megaDNA.git\n# !cd megaDNA\n# !pip install .\n</pre> # !git clone https://github.com/lingxusb/megaDNA.git # !cd megaDNA # !pip install . In\u00a0[15]: Copied! <pre># Load the model and tokenizer\nmodel_name = \"lingxusb/megaDNA_updated\"\n# from Hugging Face\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n# from ModelScope\n# model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\ntokenizer.model_max_length = 2048\n</pre> # Load the model and tokenizer model_name = \"lingxusb/megaDNA_updated\" # from Hugging Face model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\") # from ModelScope # model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") tokenizer.model_max_length = 2048 <pre>17:59:18 - dnallm.utils.support - WARNING - Embedding task does not require num_labels, but got 1. Setting to 0.\n</pre> <pre>Fetching 3 files:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>17:59:18 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/huggingface/hub/models--lingxusb--megaDNA_updated/snapshots/ed298be539e1667b52a1181a6472528a34dd2ef9\n</pre> In\u00a0[16]: Copied! <pre># Encode the datasets\ndata = copy.deepcopy(datasets)\ndata.encode_sequences(tokenizer=tokenizer)\n</pre> # Encode the datasets data = copy.deepcopy(datasets) data.encode_sequences(tokenizer=tokenizer) <pre>Encoding inputs:   0%|          | 0/3382 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/966 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/484 [00:00&lt;?, ? examples/s]</pre> In\u00a0[17]: Copied! <pre># Specific processing for MEGA-DNA\ndata.dataset = data.dataset.remove_columns([\"seq_id\", \"sequence\", \"token_type_ids\", \"attention_mask\"])\ndata.dataset = data.dataset.rename_column(\"input_ids\", \"ids\")\ndata.dataset\n</pre> # Specific processing for MEGA-DNA data.dataset = data.dataset.remove_columns([\"seq_id\", \"sequence\", \"token_type_ids\", \"attention_mask\"]) data.dataset = data.dataset.rename_column(\"input_ids\", \"ids\") data.dataset Out[17]: <pre>DatasetDict({\n    train: Dataset({\n        features: ['ids'],\n        num_rows: 3382\n    })\n    test: Dataset({\n        features: ['ids'],\n        num_rows: 966\n    })\n    val: Dataset({\n        features: ['ids'],\n        num_rows: 484\n    })\n})</pre> In\u00a0[18]: Copied! <pre># Initialize the trainer\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=data\n)\n</pre> # Initialize the trainer trainer = DNATrainer(     model=model,     config=configs,     datasets=data ) In\u00a0[19]: Copied! <pre># Define a custom trainer for MEGA-DNA\nclass MegaDNATrainer(type(trainer.trainer)):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        loss = model(**inputs, return_value = \"loss\")\n        if return_outputs:\n            logits = model(**inputs, return_value = \"logits\")\n            return (loss, logits)\n        \n        return loss\n\ntrainer.customize_trainer(MegaDNATrainer)\ntrainer.trainer.can_return_loss = True\n</pre> # Define a custom trainer for MEGA-DNA class MegaDNATrainer(type(trainer.trainer)):     def compute_loss(self, model, inputs, return_outputs=False, **kwargs):         loss = model(**inputs, return_value = \"loss\")         if return_outputs:             logits = model(**inputs, return_value = \"logits\")             return (loss, logits)                  return loss  trainer.customize_trainer(MegaDNATrainer) trainer.trainer.can_return_loss = True In\u00a0[20]: Copied! <pre># Start training\nmetrics = trainer.train()\nprint(metrics)\n</pre> # Start training metrics = trainer.train() print(metrics) <pre>/Users/forrest/miniconda3/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n</pre>        [424/424 01:46, Epoch 2/2]      Step Training Loss Validation Loss 200 1.322100 1.306477 400 1.305000 1.301980 <pre>/Users/forrest/miniconda3/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n/Users/forrest/miniconda3/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[20], line 2\n      1 # Start training\n----&gt; 2 metrics = trainer.train()\n      3 print(metrics)\n\nFile ~/GitHub/DNALLM/dnallm/finetune/trainer.py:261, in DNATrainer.train(self, save_tokenizer)\n    259 # Save the model\n    260 self.trainer.save_model()\n--&gt; 261 self.model.save_pretrained(\n    262     self.train_config.output_dir,\n    263     safe_serialization=self.trainer.args.save_safetensors,\n    264 )\n    265 if save_tokenizer:\n    266     self.datasets.tokenizer.save_pretrained(\n    267         self.train_config.output_dir\n    268     )\n\nFile ~/GitHub/DNALLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1928, in Module.__getattr__(self, name)\n   1926     if name in modules:\n   1927         return modules[name]\n-&gt; 1928 raise AttributeError(\n   1929     f\"'{type(self).__name__}' object has no attribute '{name}'\"\n   1930 )\n\nAttributeError: 'MEGADNA' object has no attribute 'save_pretrained'</pre> In\u00a0[\u00a0]: Copied! <pre>model.eval()\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = [model.generate(inputs[\"input_ids\"], seq_len=len(seq)+5, temperature=0.95, filter_thres=0.0) for _ in range(5)]\n</pre> model.eval()  inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") outputs = [model.generate(inputs[\"input_ids\"], seq_len=len(seq)+5, temperature=0.95, filter_thres=0.0) for _ in range(5)] <pre>  0%|          | 0/202 [00:00&lt;?, ?it/s]</pre> <pre>/home/liuguanqing/miniforge3/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n</pre> <pre>  0%|          | 0/202 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/202 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/202 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/202 [00:00&lt;?, ?it/s]</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"Prompt:               \", prompt)\nfor i, out in enumerate(outputs):\n    out_seq = tokenizer.decode(out[0], skip_special_tokens=True)\n    print(f\"Generated sequence {i}: \", out_seq.replace(\" \", \"\"))\nprint(\"Raw sequence:         \", seq)\n</pre> print(\"Prompt:               \", prompt) for i, out in enumerate(outputs):     out_seq = tokenizer.decode(out[0], skip_special_tokens=True)     print(f\"Generated sequence {i}: \", out_seq.replace(\" \", \"\")) print(\"Raw sequence:         \", seq) <pre>Prompt:                ATGACTTGCA\nGenerated sequence 0:  ATGACTTGCATGGCATCGAGCAATCACGAGTGCTCGAGTAGTTGGTGGCAGTCAGCCCATAGTGGATGCTCCACTAGTCTTGGGTTGACCTCCTCTGATTGGAAGTCTATGATTGTTGGACCATCCCCGTTTGGATCCCCATCTCTGGCTGGCTTTAGTACTAACTGGATCACTAGGACTCCTAATCATTCATCAGGTCTCGGGACCTGTGC\nGenerated sequence 1:  ATGACTTGCAAAAGGAGAGTATTTCTTGGCTGCCTCTCTGCCGAACCAAACATTCAAGAACCTCCCGAAATTGCTCGTGAAACTGTAACGCTCGGTATCAAAAACCCGAAATCAAGAAGGGAATATCTTACTCTCTACAAAAAACGAAGGGGAAAGATCTTTGTTCATCCGAGCGCTGATGTGCACATTATGGAACTCGAGATGGGTTTTCA\nGenerated sequence 2:  ATGACTTGCATGTTCTTCCATTCTTTCTCCTCACCTTGTCTTATCCGTAGCCCCCTGCTGCTTCAGGACTTTCGGTCTCTCCTGCTCTTTCTCCTGCTCCTGCTCTCTCTCACCGGGGATCTTCCCACATTTCTGACGCTGCCAGAAGTGGTGAAGCTGCTGGGCTTCCTCCCCTTCGTGGAGTTTCCTTTCTCCGCAGCCCGCCCATGTTG\nGenerated sequence 3:  ATGACTTGCATTTCCAGAGAAGACGAAATGCAAGCAATCCTCCACGAAGAGCGGGAAGAGATCAACGAGCTTCGCATTGAAGATGAAGAAGATGAAGGTGAACATGTTACCTCTTACAAGAAGAATGAATCGCTCACCACTCATGATGATCTGCTGGATATCGTTCTTGATGAGCTCAAGAAAGAGCGGATTGGTAATGAAGAAGCTGAGAT\nGenerated sequence 4:  ATGACTTGCAAATCTAGTCCAGAGGCTGCAAAGCTGACCAATGACACCAAGAAGACAAAGAAAGGACAAGATGAAGAGAGTTCGAGGAGACCTGTTCTTCCGATTGTTGAGGAGAAGAACACGTTCAACGGAGTTGAGATTCATGTTCTTCTTGATAACGAATCGATCGAGGTCGAGATCGAAGCGCATGTTGACAAGTTTAAACTTACTCA\nRaw sequence:          ATGACTTGCACGACAGAGATAGATATTTTGAAGTGGACAGTGAGGTATTGTTCGAGTTTAGCTGCACACCTTCTAACTCCTACGAGATTGTTCAAATATGAAATTCAACAACAGAGCGATTTGAGAAATGCAACTGAAAACAAAACTGAAAAATATATTTCTGACGACGTCGGTCATTGTAGACATACATACATGCAAATCAGATAA\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/finetune_generation/finetune_generation/#preparation","title":"Preparation\u00b6","text":""},{"location":"example/notebooks/finetune_generation/finetune_generation/#dnagpt","title":"DNAGPT\u00b6","text":""},{"location":"example/notebooks/finetune_generation/finetune_generation/#megadna","title":"MegaDNA\u00b6","text":""},{"location":"example/notebooks/finetune_multi_labels/finetune_multi_labels/","title":"Multi-Label Classification","text":"In\u00a0[1]: Copied! <pre>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n</pre> from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer In\u00a0[2]: Copied! <pre># Load the config file\nconfigs = load_config(\"./multi_labels_config.yaml\")\n</pre> # Load the config file configs = load_config(\"./multi_labels_config.yaml\") In\u00a0[3]: Copied! <pre># Load the model and tokenizer\nmodel_name = \"zhangtaolab/plant-dnagpt-BPE\"\n# from Hugging Face\n# model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n# from ModelScope\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\n</pre> # Load the model and tokenizer model_name = \"zhangtaolab/plant-dnagpt-BPE\" # from Hugging Face # model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\") # from ModelScope model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") <pre>Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE\n14:37:17 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE\n</pre> <pre>Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[4]: Copied! <pre># Load the datasets\n## multiple labels are separated by ','\ndatasets = DNADataset.load_local_data(\"./maize_test.tsv\", seq_col=\"sequence\", label_col=\"labels\", multi_label_sep=\",\", tokenizer=tokenizer, max_length=512)\n\n# Encode the sequences with given task's data collator\ndatasets.encode_sequences(task=configs['task'].task_type, remove_unused_columns=True)\n\n# Split the dataset into train, test, and validation sets\ndatasets.split_data()\n</pre> # Load the datasets ## multiple labels are separated by ',' datasets = DNADataset.load_local_data(\"./maize_test.tsv\", seq_col=\"sequence\", label_col=\"labels\", multi_label_sep=\",\", tokenizer=tokenizer, max_length=512)  # Encode the sequences with given task's data collator datasets.encode_sequences(task=configs['task'].task_type, remove_unused_columns=True)  # Split the dataset into train, test, and validation sets datasets.split_data() <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> <pre>Format labels:   0%|          | 0/10000 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/10000 [00:00&lt;?, ? examples/s]</pre> In\u00a0[5]: Copied! <pre># Initialize the trainer\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=datasets\n)\n</pre> # Initialize the trainer trainer = DNATrainer(     model=model,     config=configs,     datasets=datasets ) In\u00a0[6]: Copied! <pre># Start training\nmetrics = trainer.train()\nprint(metrics)\n</pre> # Start training metrics = trainer.train() print(metrics)        [2625/2625 1:16:47, Epoch 3/3]      Step Training Loss Validation Loss Accuracy Precision Recall F1 Precision Micro Recall Micro F1 Micro Precision Weighted Recall Weighted F1 Weighted Precision Samples Recall Samples F1 Samples Mcc Auroc Auprc Tpr Tnr Fpr Fnr 500 0.280200 0.155547 0.762238 0.211572 0.158561 0.158172 0.473950 0.226870 0.306855 0.280166 0.226870 0.220705 0.081927 0.048246 0.047847 0.149856 0.918668 0.361795 0.158561 0.981821 0.018179 0.841439 1000 0.146100 0.170772 0.800200 0.441545 0.164471 0.227142 0.585973 0.208367 0.307418 0.514883 0.208367 0.282476 0.048261 0.035697 0.031921 0.236231 0.909454 0.399514 0.164471 0.989507 0.010493 0.835529 1500 0.138600 0.151820 0.800200 0.638011 0.133174 0.204163 0.641509 0.164119 0.261371 0.674379 0.164119 0.245527 0.038927 0.023819 0.023730 0.251443 0.929347 0.463728 0.133174 0.993440 0.006560 0.866826 2000 0.121000 0.145796 0.790210 0.576422 0.286482 0.362327 0.539597 0.323411 0.404427 0.546884 0.323411 0.393154 0.053367 0.047860 0.039968 0.362768 0.932501 0.474062 0.286482 0.980389 0.019611 0.713518 2500 0.111900 0.145860 0.796204 0.643390 0.252621 0.347409 0.597938 0.279968 0.381370 0.612762 0.279968 0.371620 0.046154 0.037964 0.033512 0.366503 0.935172 0.497237 0.252621 0.986607 0.013393 0.747379 <pre>/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n</pre> <pre>{'train_runtime': 4613.7613, 'train_samples_per_second': 4.551, 'train_steps_per_second': 0.569, 'total_flos': 5487290020528128.0, 'train_loss': 0.1565389651343936, 'epoch': 3.0}\n</pre> In\u00a0[7]: Copied! <pre># Do prediction on the test set\ntrainer.infer()\n</pre> # Do prediction on the test set trainer.infer() <pre>/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/Users/forrest/GitHub/DNALLM/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n</pre> Out[7]: <pre>PredictionOutput(predictions=array([[-5.443179  , -5.6372533 , -5.552704  , ..., -6.5272865 ,\n        -7.5997458 , -5.74388   ],\n       [-1.7592301 , -3.8711321 , -4.3161707 , ..., -2.0341883 ,\n        -5.250122  , -1.8616908 ],\n       [-6.446937  , -6.779642  , -6.8808575 , ..., -6.401059  ,\n        -7.6829267 , -7.433517  ],\n       ...,\n       [-0.9382036 , -1.0705391 , -0.59772235, ..., -1.354423  ,\n        -2.1761355 , -0.2642503 ],\n       [-5.504851  , -6.3220463 , -5.644334  , ..., -6.8325057 ,\n        -7.718524  , -6.2338963 ],\n       [-5.6954155 , -5.9499183 , -6.267033  , ..., -5.7308164 ,\n        -7.274815  , -6.108817  ]], dtype=float32), label_ids=array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [1., 1., 1., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), metrics={'test_loss': 0.13551659882068634, 'test_accuracy': 0.798, 'test_precision': 0.5518252754926944, 'test_recall': 0.311220457107487, 'test_f1': 0.3903308188112585, 'test_precision_micro': 0.5606896551724138, 'test_recall_micro': 0.3433277027027027, 'test_f1_micro': 0.4258774227344159, 'test_precision_weighted': 0.5585796433599094, 'test_recall_weighted': 0.3433277027027027, 'test_f1_weighted': 0.4181393571382143, 'test_precision_samples': 0.055576365449565915, 'test_recall_samples': 0.04742432422668491, 'test_f1_samples': 0.04118593701834425, 'test_mcc': 0.3822012575166047, 'test_AUROC': 0.9374013561554215, 'test_AUPRC': 0.47993831500493955, 'test_TPR': 0.311220457107487, 'test_TNR': 0.9818909959471704, 'test_FPR': 0.018109004052829662, 'test_FNR': 0.688779542892513, 'test_runtime': 34.2167, 'test_samples_per_second': 58.451, 'test_steps_per_second': 3.653})</pre>"},{"location":"example/notebooks/generation/inference/","title":"Sequence Generation","text":"In\u00a0[1]: Copied! <pre>from dnallm import load_config\nfrom dnallm import load_model_and_tokenizer, DNAInference\n</pre> from dnallm import load_config from dnallm import load_model_and_tokenizer, DNAInference In\u00a0[2]: Copied! <pre># Load configurations\nconfigs = load_config(\"./generation_config.yaml\")\n</pre> # Load configurations configs = load_config(\"./generation_config.yaml\") In\u00a0[\u00a0]: Copied! <pre>model_name = \"zhangtaolab/plant-dnagpt-BPE\"\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\n</pre> model_name = \"zhangtaolab/plant-dnagpt-BPE\" model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") <pre>Downloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE\n02:16:42 - dnallm.models.model - INFO - Model files are stored in /home/liuguanqing/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE\n02:16:42 - dnallm.models.model - WARNING - Generation task does not require num_labels, but got 1. Setting to 0.\nDownloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE\nDownloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE\n</pre> In\u00a0[4]: Copied! <pre># Create inference engine\ninference_engine = DNAInference(\n    model=model,\n    tokenizer=tokenizer,\n    config=configs\n)\n</pre> # Create inference engine inference_engine = DNAInference(     model=model,     tokenizer=tokenizer,     config=configs ) <pre>02:16:44 - dnallm.models.model - INFO - Using device: cuda\n</pre> In\u00a0[5]: Copied! <pre>output = inference_engine.generate([\"ACGT\"], n_tokens=512, temperature=0.8, top_p=0.9)\n</pre> output = inference_engine.generate([\"ACGT\"], n_tokens=512, temperature=0.8, top_p=0.9) <pre>Setting `pad_token_id` to `eos_token_id`:8000 for open-end generation.\n</pre> In\u00a0[6]: Copied! <pre>for seq in output:\n    print(f\"Input Sequence: {seq['Prompt']}\")\n    print(f\"Generated Sequence: {seq['Output']}\")\n    print()\n</pre> for seq in output:     print(f\"Input Sequence: {seq['Prompt']}\")     print(f\"Generated Sequence: {seq['Output']}\")     print() <pre>Input Sequence: ACGT\nGenerated Sequence: ACGTAGTAAAAAAGAAAGAAGGAAAGGGAAAAAGAGAAAGAGAAGGAAAAGGAAAAAGGAGAAAGGAAAGAAAGGGGAAAGAAGAAAAAAGGAAAGAAGAAAGAAAAAAAAGAAGGAAGAAAAAAAAAGGAGAAGGAGGGGGGAAAAAAAAGAAAGAAAAAAAAAGAAGAAAAAAAAAGAAAAAGAAAAAAAGAGAAAAAAAAAAAAAAAGAGGGAAAAAGAAAAAAGGAAGAGAAAGAGGAAAAAAGAAAAGAAGAAAAGGGAGAAGAGAAAAAAGAAAAAGGAAAGAAGAAAAGGAGAGAAAGGAAAAAAAAAAAGAAGGAAAAAAAAGAAGGAAGAAGAAAAAAAAAAAAAAAGAGAAAAGAAGAAGAAGGAAAAAGGAAAAGGGAAAAAAAGAAAAAGAAGGAAAGAAAAAAAAAAAAAGAAAGAAGAAGGAGAGAGAAAAAAGAGAGAAAGAAGAAAAAGAAAAAGGGAGAAAAAAGGGAAAAGGAAAAAAAAAAGAAAAAAGAGAAAAAGAA\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/generation_evo_models/inference/","title":"EVO Models","text":"In\u00a0[\u00a0]: Copied! <pre>## Make sure you have installed the dependencies for Evo2 model\n## (Install flash_attn and transformer-engine require compile, which may take a while)\n# !uv pip install \"flash_attn&lt;=2.7.4.post1\" --no-build-isolation --no-cache-dir\n# !uv pip install \"transformer-engine[pytorch]==2.3.0\" --no-build-isolation --no-cache-dir\n# !uv pip install evo2==0.3.0\n</pre> ## Make sure you have installed the dependencies for Evo2 model ## (Install flash_attn and transformer-engine require compile, which may take a while) # !uv pip install \"flash_attn&lt;=2.7.4.post1\" --no-build-isolation --no-cache-dir # !uv pip install \"transformer-engine[pytorch]==2.3.0\" --no-build-isolation --no-cache-dir # !uv pip install evo2==0.3.0 In\u00a0[1]: Copied! <pre>from dnallm import load_config\nfrom dnallm import load_model_and_tokenizer, DNAInference\n</pre> from dnallm import load_config from dnallm import load_model_and_tokenizer, DNAInference In\u00a0[2]: Copied! <pre># Load configurations\nconfigs = load_config(\"./inference_evo_config.yaml\")\n</pre> # Load configurations configs = load_config(\"./inference_evo_config.yaml\") In\u00a0[\u00a0]: Copied! <pre>model_name = \"arcinstitute/evo2_1b_base\"\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n</pre> model_name = \"arcinstitute/evo2_1b_base\" model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\") <pre>[09/17/25 11:13:04] INFO     root - INFO - Could not find package transformer_engine_jax. Install    __init__.py:58\n                             transformer-engine using 'pip3 install                                                \n                             transformer-engine[jax]==VERSION'                                                     \n</pre> <pre>11:13:05 - dnallm.models.model - WARNING - Current device compute capability is 8.0, which does not support FP8.\n</pre> <pre>[09/17/25 11:13:05] \u001b[33mWARNING\u001b[0m dnallm.models.model - \u001b[33mWARNING\u001b[0m - Current device compute       logger.py:76\n                                   capability is 8.0, which does not support FP8.                                  \n</pre> <pre>Downloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/evo2_1b_base\n11:13:06 - dnallm.models.model - INFO - Model files are stored in /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/evo2_1b_base\n</pre> <pre>[09/17/25 11:13:06] \u001b[36mINFO\u001b[0m dnallm.models.model - \u001b[36mINFO\u001b[0m - Model files are stored in          logger.py:72\n                                /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/evo2_1b_base               \n</pre> <pre>Loading model from /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/evo2_1b_base/evo2_1b_base.pt...\nLoading config from /home/liuguanqing/github/dnallm_dev/DNALLM/dnallm/configuration/evo/evo2-1b-8k-noFP8.yml...\n</pre> <pre>                    INFO     StripedHyena - INFO - Initializing StripedHyena with config:              model.py:616\n                             {'model_name': 'shc-evo2-1b-8k-2T-v2', 'vocab_size': 512, 'hidden_size':              \n                             1920, 'num_filters': 1920, 'attn_layer_idxs': [3, 10, 17, 24],                        \n                             'hcl_layer_idxs': [2, 6, 9, 13, 16, 20, 23], 'hcm_layer_idxs': [1, 5, 8,              \n                             12, 15, 19, 22], 'hcs_layer_idxs': [0, 4, 7, 11, 14, 18, 21],                         \n                             'hcm_filter_length': 128, 'hcl_filter_groups': 1920, 'hcm_filter_groups':             \n                             128, 'hcs_filter_groups': 128, 'hcs_filter_length': 7, 'num_layers': 25,              \n                             'short_filter_length': 3, 'num_attention_heads': 15, 'short_filter_bias':             \n                             False, 'mlp_init_method': 'torch.nn.init.zeros_',                                     \n                             'mlp_output_init_method': 'torch.nn.init.zeros_', 'eps': 1e-06,                       \n                             'state_size': 16, 'rotary_emb_base': 10000,                                           \n                             'make_vocab_size_divisible_by': 8, 'inner_size_multiple_of': 16,                      \n                             'inner_mlp_size': 5120, 'log_intermediate_values': False, 'proj_groups':              \n                             1, 'hyena_filter_groups': 1, 'column_split_hyena': False, 'column_split':             \n                             True, 'interleave': True, 'evo2_style_activations': True,                             \n                             'model_parallel_size': 1, 'pipe_parallel_size': 1, 'tie_embeddings':                  \n                             True, 'mha_out_proj_bias': True, 'hyena_out_proj_bias': True,                         \n                             'hyena_flip_x1x2': False, 'qkv_proj_bias': False,                                     \n                             'use_fp8_input_projections': False, 'max_seqlen': 8192, 'max_batch_size':             \n                             1, 'final_norm': True, 'use_flash_attn': True, 'use_flash_rmsnorm':                   \n                             False, 'use_flash_depthwise': False, 'use_flashfft': False,                           \n                             'use_laughing_hyena': False, 'inference_mode': True, 'tokenizer_type':                \n                             'CharLevelTokenizer', 'prefill_style': 'fft', 'mlp_activation': 'gelu',               \n                             'print_activations': False}                                                           \n</pre> <pre>                    INFO     StripedHyena - INFO - Initializing 25 blocks...                           model.py:635\n</pre> <pre>                    INFO     StripedHyena - INFO - Distributing across 1 GPUs, approximately 25 layers model.py:642\n                             per GPU                                                                               \n</pre> <pre>  0%|          | 0/25 [00:00&lt;?, ?it/s]</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=0 to device='cuda:0'             model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 0: 44260736               model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=1 to device='cuda:0'             model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 1: 44278144               model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=2 to device='cuda:0'             model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 2: 44323200               model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=3 to device='cuda:0'             model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 3: 44242560               model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=4 to device='cuda:0'             model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 4: 44260736               model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=5 to device='cuda:0'             model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 5: 44278144               model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=6 to device='cuda:0'             model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 6: 44323200               model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=7 to device='cuda:0'             model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 7: 44260736               model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=8 to device='cuda:0'             model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 8: 44278144               model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=9 to device='cuda:0'             model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 9: 44323200               model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=10 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 10: 44242560              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=11 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 11: 44260736              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=12 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 12: 44278144              model.py:661\n</pre> <pre> 52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 13/25 [00:00&lt;00:00, 129.92it/s]</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=13 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 13: 44323200              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=14 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 14: 44260736              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=15 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 15: 44278144              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=16 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 16: 44323200              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=17 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 17: 44242560              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=18 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 18: 44260736              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=19 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 19: 44278144              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=20 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 20: 44323200              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=21 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 21: 44260736              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=22 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 22: 44278144              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=23 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 23: 44323200              model.py:661\n</pre> <pre>                    INFO     StripedHyena - INFO - Assigned layer_idx=24 to device='cuda:0'            model.py:660\n</pre> <pre>                    INFO     StripedHyena - INFO - Parameter count for block 24: 44242560              model.py:661\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00&lt;00:00, 158.67it/s]\n</pre> <pre>                    INFO     StripedHyena - INFO - Initialized model                                   model.py:680\n</pre> <pre>                    INFO     vortex.model.utils - INFO - Loading                                        utils.py:92\n                             /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/evo2_1b_base/evo2_            \n                             1b_base.pt                                                                            \n</pre> <pre>Extra keys in state_dict: {'unembed.weight', 'blocks.10.mixer.dense._extra_state', 'blocks.17.mixer.dense._extra_state', 'blocks.24.mixer.dense._extra_state', 'blocks.3.mixer.dense._extra_state'}\n</pre> <pre>                    INFO     StripedHyena - INFO - Adjusting Wqkv for column split (permuting rows)    model.py:964\n</pre> In\u00a0[4]: Copied! <pre># Create inference engine\ninference_engine = DNAInference(\n    model=model,\n    tokenizer=tokenizer,\n    config=configs\n)\n</pre> # Create inference engine inference_engine = DNAInference(     model=model,     tokenizer=tokenizer,     config=configs ) <pre>11:13:06 - dnallm.models.model - INFO - Using device: cuda\n</pre> <pre>                    \u001b[36mINFO\u001b[0m dnallm.models.model - \u001b[36mINFO\u001b[0m - Using device: cuda                 logger.py:72\n</pre> In\u00a0[5]: Copied! <pre>output = inference_engine.generate([\"@\", \"ATG\"])\n</pre> output = inference_engine.generate([\"@\", \"ATG\"]) <pre>WARNING: Batched generation is turned off.\n</pre> <pre>Initializing inference params with max_seqlen=401\n</pre> <pre>[09/17/25 11:13:07] INFO     vortex.model.utils - INFO - Fixup applied: Allocating cublas workspace    utils.py:174\n                             for device=0                                                                          \n</pre> <pre>/home/liuguanqing/github/dnallm_dev/DNALLM/.venv/lib/python3.12/site-packages/vortex/model/engine.py:559: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /pytorch/aten/src/ATen/native/Copy.cpp:308.)\n  inference_params.state_dict[layer_idx] = state[..., L - 1].to(dtype=state_dtype)\n</pre> <pre>Initializing inference params with max_seqlen=403\nPrompt: \"@\",\tOutput: \"AGTTGCTGTGGGCAGCTAAAGAAGCTTTCTGGTATACGCTAAGACCCCCAATGTTCACCGAACCGTTAGAGGGACCTCATCCGCCGAAGTAATTGTTGATGAAACAATATTCAATTCATCCGGTTTCACAGTGTGGCTAGTAAGTCATGAATTTACCACCCTGCGGACGGTGTGGGCCACCAGGCGGCCTGGCCATCACCAAGGTCTTATCACGACGAAGATCGCCGTTAAAAGTTTGTACAAAGGAAGCAGTTTTGGTCGTTTCAGCAGTCTGGTGGGCTCAATCATCCAAAACTTGCACGTTCGCTACAGATACGCCGAGCCTAAGGTCTTCATAGTCATCACCACAACGCATGGCGACAAAATGGAATTGTACGCGTACCGCAGCAGTTCATGTGTA\",\tScore: -1.3838366270065308\nPrompt: \"ATG\",\tOutput: \"CGTGACATTCGCATTTTTTTTTTGCTTTTAGATTTTGTTTTTTAAAATACTGTTTTCTACTTATGTCGCGGCGTCTTTGTCTTTTACGCAACCCCGGAGTGCAAGGGCATCATCCTAGTTGTGTTCACCGCGGAAGAGAGCGAAACCATTTTAGAATCTCTTATACACCACCCCAGCTTTTTCTACGTAACAAACAGACTAATTAACGACAATGCACGACCTTATTACCTACAGTAAATGACTTATGCACCGACCATGTTTCGAGTGTGACAGTACGGCTTCACCACCCGCAATATTTTCCAGCCAACATCCAGATCAACGAACTTTCAAGTTAAACGGACGTAAGCTCCTACAAGAAGCAGGCCTGCCCCGAGACACGGCTAGTCATTCGCGGTCCCCA\",\tScore: -1.334855318069458\n</pre> In\u00a0[6]: Copied! <pre>for seq in output:\n    print(f\"Input Sequence: {seq['Prompt']}\")\n    print(f\"Generated Sequence: {seq['Output']}\")\n    print(f\"Score: {seq['Score']}\")\n    print()\n</pre> for seq in output:     print(f\"Input Sequence: {seq['Prompt']}\")     print(f\"Generated Sequence: {seq['Output']}\")     print(f\"Score: {seq['Score']}\")     print() <pre>Input Sequence: @\nGenerated Sequence: AGTTGCTGTGGGCAGCTAAAGAAGCTTTCTGGTATACGCTAAGACCCCCAATGTTCACCGAACCGTTAGAGGGACCTCATCCGCCGAAGTAATTGTTGATGAAACAATATTCAATTCATCCGGTTTCACAGTGTGGCTAGTAAGTCATGAATTTACCACCCTGCGGACGGTGTGGGCCACCAGGCGGCCTGGCCATCACCAAGGTCTTATCACGACGAAGATCGCCGTTAAAAGTTTGTACAAAGGAAGCAGTTTTGGTCGTTTCAGCAGTCTGGTGGGCTCAATCATCCAAAACTTGCACGTTCGCTACAGATACGCCGAGCCTAAGGTCTTCATAGTCATCACCACAACGCATGGCGACAAAATGGAATTGTACGCGTACCGCAGCAGTTCATGTGTA\nScore: -1.3838366270065308\n\nInput Sequence: ATG\nGenerated Sequence: CGTGACATTCGCATTTTTTTTTTGCTTTTAGATTTTGTTTTTTAAAATACTGTTTTCTACTTATGTCGCGGCGTCTTTGTCTTTTACGCAACCCCGGAGTGCAAGGGCATCATCCTAGTTGTGTTCACCGCGGAAGAGAGCGAAACCATTTTAGAATCTCTTATACACCACCCCAGCTTTTTCTACGTAACAAACAGACTAATTAACGACAATGCACGACCTTATTACCTACAGTAAATGACTTATGCACCGACCATGTTTCGAGTGTGACAGTACGGCTTCACCACCCGCAATATTTTCCAGCCAACATCCAGATCAACGAACTTTCAAGTTAAACGGACGTAAGCTCCTACAAGAAGCAGGCCTGCCCCGAGACACGGCTAGTCATTCGCGGTCCCCA\nScore: -1.334855318069458\n\n</pre> In\u00a0[7]: Copied! <pre>scores = inference_engine.scoring([\"ATCCGCATG\", \"ATGCGCATG\"])\nfor res in scores:\n    print(f\"Input Sequence: {res['Input']}\")\n    print(f\"Score: {res['Score']}\")\n    print()\n</pre> scores = inference_engine.scoring([\"ATCCGCATG\", \"ATGCGCATG\"]) for res in scores:     print(f\"Input Sequence: {res['Input']}\")     print(f\"Score: {res['Score']}\")     print() <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00, 49.34it/s]\n</pre> <pre>Input Sequence: ATCCGCATG\nScore: -1.4130859375\n\nInput Sequence: ATGCGCATG\nScore: -1.4072265625\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>## Make sure you have installed the dependencies for Evo-1 model\n# !uv pip install \"flash_attn&lt;=2.7.4.post1\" --no-build-isolation --no-cache-dir\n# !uv pip install evo-model==0.4\n</pre> ## Make sure you have installed the dependencies for Evo-1 model # !uv pip install \"flash_attn&lt;=2.7.4.post1\" --no-build-isolation --no-cache-dir # !uv pip install evo-model==0.4 In\u00a0[\u00a0]: Copied! <pre>model_name = \"togethercomputer/evo-1-131k-base\"\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\", use_mirror=True)\n</pre> model_name = \"togethercomputer/evo-1-131k-base\" model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\", use_mirror=True) <pre>11:13:18 - dnallm.models.model - INFO - Using HuggingFace mirror at hf-mirror.com\n</pre> <pre>[09/17/25 11:13:18] \u001b[36mINFO\u001b[0m dnallm.models.model - \u001b[36mINFO\u001b[0m - Using HuggingFace mirror at        logger.py:72\n                                hf-mirror.com                                                                      \n</pre> <pre>11:15:30 - dnallm.models.model - INFO - Model files are stored in /home/liuguanqing/.cache/huggingface/hub/models--togethercomputer--evo-1-131k-base/snapshots/78c715ab81852e02ec3b1c7e795dc7250d8c7625\n</pre> <pre>[09/17/25 11:15:30] \u001b[36mINFO\u001b[0m dnallm.models.model - \u001b[36mINFO\u001b[0m - Model files are stored in          logger.py:72\n                                /home/liuguanqing/.cache/huggingface/hub/models--togethercomputer--evo             \n                                -1-131k-base/snapshots/78c715ab81852e02ec3b1c7e795dc7250d8c7625                    \n</pre> <pre>Loading checkpoint shards:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> In\u00a0[9]: Copied! <pre># Create inference engine\ninference_engine = DNAInference(\n    model=model,\n    tokenizer=tokenizer,\n    config=configs\n)\n</pre> # Create inference engine inference_engine = DNAInference(     model=model,     tokenizer=tokenizer,     config=configs ) <pre>11:16:46 - dnallm.models.model - INFO - Using device: cuda\n</pre> <pre>[09/17/25 11:16:46] \u001b[36mINFO\u001b[0m dnallm.models.model - \u001b[36mINFO\u001b[0m - Using device: cuda                 logger.py:72\n</pre> In\u00a0[10]: Copied! <pre>output = inference_engine.generate([\"@\", \"ACGT\"])\n</pre> output = inference_engine.generate([\"@\", \"ACGT\"]) <pre>Note: Prompts are of different lengths.\nNote: Will not do batched generation.\n</pre> <pre>Prompt: \"@\",\tOutput: \"TGCGCCGCAAAAACTTAACAAACTAAAAAAGCCCAACACGTCAAAGATAAACAAGCCAGCCCAAGAAAAAAATAAACCCACACAAAACTCCACGATGCGAATCAGCCCCACCGCGCAACGTCACCTAGCGGGGTTCTAACAACTGCGAATCAAGCGCTTTTTACGTATCATACCGGCGCACGTACAGACCAAACAAACATAAATCCGCCTAAGGTAAAACAAAACGAACCCCACCCTGCATCATAGGCCAAAGCTTTCCCGTGCCCATAGACGCTAATATTGATCCATCAGCAGATTGTTTATTCCCAACAAGGTTGCCGTGTGTTTCAGCCGGGTTGTTCAAACGTTCAGCCATCATCATCATCAATCTTTACTCATTTTATAAACAACTAACAAACGT\",\tScore: -1.4032673835754395\nPrompt: \"ACGT\",\tOutput: \"GAAACTAAGAAAAACTACAAAAAAACCAAACAAAGAAACAAAAACAACAACACCCGAAAACCGCTCAAACCAGCAGCTGTTTCCCGCAACAGCTGCACCAGAACTGCAACACCCCTTCACCACAAAATAACGCCAGAAACTCACCAGACCTTCGCCACAAATCGCAAAAAAACATCACCAACACCGTTCACTTTCTTCAGCAAACCAGCAACAGCGCGACCCGGGCTCCAACACCGTGAAAAAATTTTTAAAATAAATCTGAAACTGGATCGCTGCGTTGCCAGCAGCGGTTGCGTCCCGCGTGCAGCGCGCAGGCAGATCACAAAATAAAATCACGAAGCGCCGCGCGGCGCGCAAACATCGCCAGCATCATTTTCGCCATCTCGGTCTTAGCGTGTTT\",\tScore: -1.8380001783370972\n</pre> In\u00a0[11]: Copied! <pre>for seq in output:\n    print(f\"Input Sequence: {seq['Prompt']}\")\n    print(f\"Generated Sequence: {seq['Output']}\")\n    print(f\"Score: {seq['Score']}\")\n    print()\n</pre> for seq in output:     print(f\"Input Sequence: {seq['Prompt']}\")     print(f\"Generated Sequence: {seq['Output']}\")     print(f\"Score: {seq['Score']}\")     print() <pre>Input Sequence: @\nGenerated Sequence: TGCGCCGCAAAAACTTAACAAACTAAAAAAGCCCAACACGTCAAAGATAAACAAGCCAGCCCAAGAAAAAAATAAACCCACACAAAACTCCACGATGCGAATCAGCCCCACCGCGCAACGTCACCTAGCGGGGTTCTAACAACTGCGAATCAAGCGCTTTTTACGTATCATACCGGCGCACGTACAGACCAAACAAACATAAATCCGCCTAAGGTAAAACAAAACGAACCCCACCCTGCATCATAGGCCAAAGCTTTCCCGTGCCCATAGACGCTAATATTGATCCATCAGCAGATTGTTTATTCCCAACAAGGTTGCCGTGTGTTTCAGCCGGGTTGTTCAAACGTTCAGCCATCATCATCATCAATCTTTACTCATTTTATAAACAACTAACAAACGT\nScore: -1.4032673835754395\n\nInput Sequence: ACGT\nGenerated Sequence: GAAACTAAGAAAAACTACAAAAAAACCAAACAAAGAAACAAAAACAACAACACCCGAAAACCGCTCAAACCAGCAGCTGTTTCCCGCAACAGCTGCACCAGAACTGCAACACCCCTTCACCACAAAATAACGCCAGAAACTCACCAGACCTTCGCCACAAATCGCAAAAAAACATCACCAACACCGTTCACTTTCTTCAGCAAACCAGCAACAGCGCGACCCGGGCTCCAACACCGTGAAAAAATTTTTAAAATAAATCTGAAACTGGATCGCTGCGTTGCCAGCAGCGGTTGCGTCCCGCGTGCAGCGCGCAGGCAGATCACAAAATAAAATCACGAAGCGCCGCGCGGCGCGCAAACATCGCCAGCATCATTTTCGCCATCTCGGTCTTAGCGTGTTT\nScore: -1.8380001783370972\n\n</pre> In\u00a0[12]: Copied! <pre>scores = inference_engine.scoring([\"ATCCGCATG\", \"ATGCGCATG\"])\nfor res in scores:\n    print(f\"Input Sequence: {res['Input']}\")\n    print(f\"Score: {res['Score']}\")\n    print()\n</pre> scores = inference_engine.scoring([\"ATCCGCATG\", \"ATGCGCATG\"]) for res in scores:     print(f\"Input Sequence: {res['Input']}\")     print(f\"Score: {res['Score']}\")     print() <pre>Input Sequence: ATCCGCATG\nScore: -3.7654080390930176\n\nInput Sequence: ATGCGCATG\nScore: -3.502007484436035\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/generation_evo_models/inference/#evo2","title":"EVO2\u00b6","text":""},{"location":"example/notebooks/generation_evo_models/inference/#evo1","title":"EVO1\u00b6","text":""},{"location":"example/notebooks/generation_megaDNA/inference/","title":"MegaDNA Models","text":"In\u00a0[1]: Copied! <pre>## Make sure you have installed the dependencies for megaDNA model\n# !git clone https://github.com/lingxusb/megaDNA\n# !cd megaDNA\n# !uv pip install -e .\n</pre> ## Make sure you have installed the dependencies for megaDNA model # !git clone https://github.com/lingxusb/megaDNA # !cd megaDNA # !uv pip install -e . In\u00a0[2]: Copied! <pre>from dnallm import load_config\nfrom dnallm import load_model_and_tokenizer, DNAInference\n</pre> from dnallm import load_config from dnallm import load_model_and_tokenizer, DNAInference In\u00a0[3]: Copied! <pre># Load configurations\nconfigs = load_config(\"./inference_megaDNA_config.yaml\")\n</pre> # Load configurations configs = load_config(\"./inference_megaDNA_config.yaml\") In\u00a0[\u00a0]: Copied! <pre>model_name = \"lingxusb/megaDNA_updated\"\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n</pre> model_name = \"lingxusb/megaDNA_updated\" model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\") <pre>2025-10-10 02:38:15,906 - modelscope - INFO - Not logged-in, you can login for uploadingor accessing controlled entities.\n</pre> <pre>Downloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/megaDNA_updated\n02:38:16 - dnallm.models.model - INFO - Model files are stored in /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/megaDNA_updated\n</pre> In\u00a0[5]: Copied! <pre># Create inference engine\ninference_engine = DNAInference(\n    model=model,\n    tokenizer=tokenizer,\n    config=configs\n)\n</pre> # Create inference engine inference_engine = DNAInference(     model=model,     tokenizer=tokenizer,     config=configs ) <pre>02:38:17 - dnallm.models.model - INFO - Using device: cuda\n</pre> In\u00a0[6]: Copied! <pre>output = inference_engine.generate([\"ACGT\"], n_tokens=1024, temperature=0.95, top_p=0.1)\n</pre> output = inference_engine.generate([\"ACGT\"], n_tokens=1024, temperature=0.95, top_p=0.1) <pre>  0%|          | 0/1020 [00:00&lt;?, ?it/s]</pre> <pre>/home/liuguanqing/miniforge3/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n</pre> In\u00a0[7]: Copied! <pre>for seq in output:\n    print(f\"Input Sequence: {seq['Prompt']}\")\n    print(f\"Generated Sequence: {seq['Output']}\")\n    print()\n</pre> for seq in output:     print(f\"Input Sequence: {seq['Prompt']}\")     print(f\"Generated Sequence: {seq['Output']}\")     print() <pre>Input Sequence: ACGT\nGenerated Sequence: ACGTGTGGCTCGAATATGGGCAGATTGAGCAGCTTGAGCGGCTCGTAAATGAAAGCGGAATTGACATTGCAAATACCGACGACCACTATTACTTGAAAATTGATGAGGTGTAAATGATGGACGCTACAATGAACTATAACACGTTGCCCTATCTTTCCAAGGCGCAGAACGGAGTGACTGAGAGGCTCCAAAAGGGAGACATCACGCAGAAAGATATCTTGCAGGACATTTTGTCAGACGGTTTCTTTTTTTTGAAGATTAAAGGGCGGAGCTTTGCGGATGCGTCTGAAAAACTGGCTCATCATGGGCGAAAAAAGACAGTCGCTAATATTGACATCTCAAAGTTGTTAAATGCGGCGGAAGATAAGGGCTTTATGAACCGTTACCTAAAGCTCGTGGATTCGCTACAGAGAATAATCAAGGGAGGGGATGTTGGAATCTTGGACGGCTATCAAAACGGCACTGAATTCGACAGCGTATTGGCTAGATGCATTGACAAGTTTAATATACAAGACTTCATGAGACCTGCAAACGAGGGAAGAGGCGTGGAAAGTATATCTTTGACGGCTTTGCTTACCCGTCAAAAGCAAAAAGCCAAGTATTCCAAGATGCTTCTGCGAATGCCGGCTGCGGAATTGAAAAATGAATTGGCAAAGCTGTGCCCCGACTGCAAAGATAGAAAAGAGAAATACAAATTACCCAGATGGTTAGCTTCAAATAATAAAGATGCATGGCTTTATTCTTATATGCCTGATGTGGGCAAACGTTGGAATGTCAATAATGCGACGCTTGTAGAATGAGGAATTAACAATATCAGAGGGAGGATTCCCGTAGCCGCCTTTCTTATTTCTTTCGATACCGAATGAAATCGATGATCTCACCGGGCTTGAGCTTGACGATGCAGGACTGGATTTTGGAGATTCCCTGTCGGCGCATGATTCCGACATTGAGGCAGCGCAGTTTTTCCCGGCGCATTTCTTCAACAGGTTTGCGCCCGGCCTTGGCCTCCGCCGTAAGATAGTTC\n\n</pre> In\u00a0[8]: Copied! <pre>scores = inference_engine.scoring([\"ATCCGCATG\", \"ATGCGCATG\"])\nfor res in scores:\n    print(f\"Input Sequence: {res['Input']}\")\n    print(f\"Score: {res['Score']}\")\n    print()\n</pre> scores = inference_engine.scoring([\"ATCCGCATG\", \"ATGCGCATG\"]) for res in scores:     print(f\"Input Sequence: {res['Input']}\")     print(f\"Score: {res['Score']}\")     print() <pre>Input Sequence: ATCCGCATG\nScore: 1.4011831283569336\n\nInput Sequence: ATGCGCATG\nScore: 1.4185738563537598\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/in_silico_mutagenesis/in_silico_mutagenesis/","title":"Mutagenesis","text":"In\u00a0[1]: Copied! <pre>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm import Mutagenesis\n</pre> from dnallm import load_config, load_model_and_tokenizer from dnallm import Mutagenesis In\u00a0[2]: Copied! <pre># Load configuration file\nconfigs = load_config(\"./inference_config.yaml\")\n</pre> # Load configuration file configs = load_config(\"./inference_config.yaml\") In\u00a0[3]: Copied! <pre># Load model and tokenizer\nmodel_name = \"zhangtaolab/plant-dnagpt-BPE-promoter_strength_protoplast\"\n# from Hugging Face\n# model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n# from ModelScope\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\n</pre> # Load model and tokenizer model_name = \"zhangtaolab/plant-dnagpt-BPE-promoter_strength_protoplast\" # from Hugging Face # model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\") # from ModelScope model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") <pre>Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE-promoter_strength_protoplast\nModel files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE-promoter_strength_protoplast\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE-promoter_strength_protoplast\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE-promoter_strength_protoplast\n</pre> In\u00a0[4]: Copied! <pre># Initialize in-silico mutagenesis analyzer\nmutagenesis = Mutagenesis(config=configs, model=model, tokenizer=tokenizer)\n</pre> # Initialize in-silico mutagenesis analyzer mutagenesis = Mutagenesis(config=configs, model=model, tokenizer=tokenizer) In\u00a0[6]: Copied! <pre># Input sequence for saturation mutagenesis\nsequence = \"AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC\"\nmutagenesis.mutate_sequence(sequence, replace_mut=True)\nprint(mutagenesis.sequences)\n</pre> # Input sequence for saturation mutagenesis sequence = \"AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC\" mutagenesis.mutate_sequence(sequence, replace_mut=True) print(mutagenesis.sequences) <pre>Encoding inputs:   0%|          | 0/511 [00:00&lt;?, ? examples/s]</pre> <pre>{'name': ['raw', 'mut_0_A_C', 'mut_0_A_G', 'mut_0_A_T', 'mut_1_A_C', 'mut_1_A_G', 'mut_1_A_T', 'mut_2_T_A', 'mut_2_T_C', 'mut_2_T_G', 'mut_3_A_C', 'mut_3_A_G', 'mut_3_A_T', 'mut_4_T_A', 'mut_4_T_C', 'mut_4_T_G', 'mut_5_A_C', 'mut_5_A_G', 'mut_5_A_T', 'mut_6_T_A', 'mut_6_T_C', 'mut_6_T_G', 'mut_7_T_A', 'mut_7_T_C', 'mut_7_T_G', 'mut_8_T_A', 'mut_8_T_C', 'mut_8_T_G', 'mut_9_A_C', 'mut_9_A_G', 'mut_9_A_T', 'mut_10_A_C', 'mut_10_A_G', 'mut_10_A_T', 'mut_11_T_A', 'mut_11_T_C', 'mut_11_T_G', 'mut_12_C_A', 'mut_12_C_G', 'mut_12_C_T', 'mut_13_G_A', 'mut_13_G_C', 'mut_13_G_T', 'mut_14_G_A', 'mut_14_G_C', 'mut_14_G_T', 'mut_15_T_A', 'mut_15_T_C', 'mut_15_T_G', 'mut_16_G_A', 'mut_16_G_C', 'mut_16_G_T', 'mut_17_T_A', 'mut_17_T_C', 'mut_17_T_G', 'mut_18_A_C', 'mut_18_A_G', 'mut_18_A_T', 'mut_19_T_A', 'mut_19_T_C', 'mut_19_T_G', 'mut_20_A_C', 'mut_20_A_G', 'mut_20_A_T', 'mut_21_A_C', 'mut_21_A_G', 'mut_21_A_T', 'mut_22_T_A', 'mut_22_T_C', 'mut_22_T_G', 'mut_23_T_A', 'mut_23_T_C', 'mut_23_T_G', 'mut_24_T_A', 'mut_24_T_C', 'mut_24_T_G', 'mut_25_C_A', 'mut_25_C_G', 'mut_25_C_T', 'mut_26_T_A', 'mut_26_T_C', 'mut_26_T_G', 'mut_27_G_A', 'mut_27_G_C', 'mut_27_G_T', 'mut_28_T_A', 'mut_28_T_C', 'mut_28_T_G', 'mut_29_G_A', 'mut_29_G_C', 'mut_29_G_T', 'mut_30_A_C', 'mut_30_A_G', 'mut_30_A_T', 'mut_31_A_C', 'mut_31_A_G', 'mut_31_A_T', 'mut_32_G_A', 'mut_32_G_C', 'mut_32_G_T', 'mut_33_A_C', 'mut_33_A_G', 'mut_33_A_T', 'mut_34_T_A', 'mut_34_T_C', 'mut_34_T_G', 'mut_35_C_A', 'mut_35_C_G', 'mut_35_C_T', 'mut_36_C_A', 'mut_36_C_G', 'mut_36_C_T', 'mut_37_T_A', 'mut_37_T_C', 'mut_37_T_G', 'mut_38_C_A', 'mut_38_C_G', 'mut_38_C_T', 'mut_39_G_A', 'mut_39_G_C', 'mut_39_G_T', 'mut_40_A_C', 'mut_40_A_G', 'mut_40_A_T', 'mut_41_T_A', 'mut_41_T_C', 'mut_41_T_G', 'mut_42_A_C', 'mut_42_A_G', 'mut_42_A_T', 'mut_43_C_A', 'mut_43_C_G', 'mut_43_C_T', 'mut_44_T_A', 'mut_44_T_C', 'mut_44_T_G', 'mut_45_T_A', 'mut_45_T_C', 'mut_45_T_G', 'mut_46_C_A', 'mut_46_C_G', 'mut_46_C_T', 'mut_47_A_C', 'mut_47_A_G', 'mut_47_A_T', 'mut_48_T_A', 'mut_48_T_C', 'mut_48_T_G', 'mut_49_A_C', 'mut_49_A_G', 'mut_49_A_T', 'mut_50_T_A', 'mut_50_T_C', 'mut_50_T_G', 'mut_51_A_C', 'mut_51_A_G', 'mut_51_A_T', 'mut_52_A_C', 'mut_52_A_G', 'mut_52_A_T', 'mut_53_G_A', 'mut_53_G_C', 'mut_53_G_T', 'mut_54_A_C', 'mut_54_A_G', 'mut_54_A_T', 'mut_55_G_A', 'mut_55_G_C', 'mut_55_G_T', 'mut_56_A_C', 'mut_56_A_G', 'mut_56_A_T', 'mut_57_T_A', 'mut_57_T_C', 'mut_57_T_G', 'mut_58_T_A', 'mut_58_T_C', 'mut_58_T_G', 'mut_59_T_A', 'mut_59_T_C', 'mut_59_T_G', 'mut_60_T_A', 'mut_60_T_C', 'mut_60_T_G', 'mut_61_G_A', 'mut_61_G_C', 'mut_61_G_T', 'mut_62_A_C', 'mut_62_A_G', 'mut_62_A_T', 'mut_63_G_A', 'mut_63_G_C', 'mut_63_G_T', 'mut_64_A_C', 'mut_64_A_G', 'mut_64_A_T', 'mut_65_G_A', 'mut_65_G_C', 'mut_65_G_T', 'mut_66_A_C', 'mut_66_A_G', 'mut_66_A_T', 'mut_67_G_A', 'mut_67_G_C', 'mut_67_G_T', 'mut_68_A_C', 'mut_68_A_G', 'mut_68_A_T', 'mut_69_G_A', 'mut_69_G_C', 'mut_69_G_T', 'mut_70_A_C', 'mut_70_A_G', 'mut_70_A_T', 'mut_71_G_A', 'mut_71_G_C', 'mut_71_G_T', 'mut_72_A_C', 'mut_72_A_G', 'mut_72_A_T', 'mut_73_G_A', 'mut_73_G_C', 'mut_73_G_T', 'mut_74_A_C', 'mut_74_A_G', 'mut_74_A_T', 'mut_75_A_C', 'mut_75_A_G', 'mut_75_A_T', 'mut_76_C_A', 'mut_76_C_G', 'mut_76_C_T', 'mut_77_C_A', 'mut_77_C_G', 'mut_77_C_T', 'mut_78_A_C', 'mut_78_A_G', 'mut_78_A_T', 'mut_79_A_C', 'mut_79_A_G', 'mut_79_A_T', 'mut_80_T_A', 'mut_80_T_C', 'mut_80_T_G', 'mut_81_T_A', 'mut_81_T_C', 'mut_81_T_G', 'mut_82_T_A', 'mut_82_T_C', 'mut_82_T_G', 'mut_83_T_A', 'mut_83_T_C', 'mut_83_T_G', 'mut_84_C_A', 'mut_84_C_G', 'mut_84_C_T', 'mut_85_G_A', 'mut_85_G_C', 'mut_85_G_T', 'mut_86_A_C', 'mut_86_A_G', 'mut_86_A_T', 'mut_87_A_C', 'mut_87_A_G', 'mut_87_A_T', 'mut_88_T_A', 'mut_88_T_C', 'mut_88_T_G', 'mut_89_G_A', 'mut_89_G_C', 'mut_89_G_T', 'mut_90_G_A', 'mut_90_G_C', 'mut_90_G_T', 'mut_91_G_A', 'mut_91_G_C', 'mut_91_G_T', 'mut_92_T_A', 'mut_92_T_C', 'mut_92_T_G', 'mut_93_G_A', 'mut_93_G_C', 'mut_93_G_T', 'mut_94_A_C', 'mut_94_A_G', 'mut_94_A_T', 'mut_95_G_A', 'mut_95_G_C', 'mut_95_G_T', 'mut_96_T_A', 'mut_96_T_C', 'mut_96_T_G', 'mut_97_T_A', 'mut_97_T_C', 'mut_97_T_G', 'mut_98_G_A', 'mut_98_G_C', 'mut_98_G_T', 'mut_99_G_A', 'mut_99_G_C', 'mut_99_G_T', 'mut_100_C_A', 'mut_100_C_G', 'mut_100_C_T', 'mut_101_A_C', 'mut_101_A_G', 'mut_101_A_T', 'mut_102_A_C', 'mut_102_A_G', 'mut_102_A_T', 'mut_103_A_C', 'mut_103_A_G', 'mut_103_A_T', 'mut_104_G_A', 'mut_104_G_C', 'mut_104_G_T', 'mut_105_T_A', 'mut_105_T_C', 'mut_105_T_G', 'mut_106_A_C', 'mut_106_A_G', 'mut_106_A_T', 'mut_107_T_A', 'mut_107_T_C', 'mut_107_T_G', 'mut_108_T_A', 'mut_108_T_C', 'mut_108_T_G', 'mut_109_C_A', 'mut_109_C_G', 'mut_109_C_T', 'mut_110_A_C', 'mut_110_A_G', 'mut_110_A_T', 'mut_111_C_A', 'mut_111_C_G', 'mut_111_C_T', 'mut_112_T_A', 'mut_112_T_C', 'mut_112_T_G', 'mut_113_T_A', 'mut_113_T_C', 'mut_113_T_G', 'mut_114_T_A', 'mut_114_T_C', 'mut_114_T_G', 'mut_115_T_A', 'mut_115_T_C', 'mut_115_T_G', 'mut_116_C_A', 'mut_116_C_G', 'mut_116_C_T', 'mut_117_A_C', 'mut_117_A_G', 'mut_117_A_T', 'mut_118_G_A', 'mut_118_G_C', 'mut_118_G_T', 'mut_119_A_C', 'mut_119_A_G', 'mut_119_A_T', 'mut_120_A_C', 'mut_120_A_G', 'mut_120_A_T', 'mut_121_C_A', 'mut_121_C_G', 'mut_121_C_T', 'mut_122_A_C', 'mut_122_A_G', 'mut_122_A_T', 'mut_123_T_A', 'mut_123_T_C', 'mut_123_T_G', 'mut_124_A_C', 'mut_124_A_G', 'mut_124_A_T', 'mut_125_A_C', 'mut_125_A_G', 'mut_125_A_T', 'mut_126_T_A', 'mut_126_T_C', 'mut_126_T_G', 'mut_127_T_A', 'mut_127_T_C', 'mut_127_T_G', 'mut_128_G_A', 'mut_128_G_C', 'mut_128_G_T', 'mut_129_G_A', 'mut_129_G_C', 'mut_129_G_T', 'mut_130_G_A', 'mut_130_G_C', 'mut_130_G_T', 'mut_131_A_C', 'mut_131_A_G', 'mut_131_A_T', 'mut_132_A_C', 'mut_132_A_G', 'mut_132_A_T', 'mut_133_A_C', 'mut_133_A_G', 'mut_133_A_T', 'mut_134_C_A', 'mut_134_C_G', 'mut_134_C_T', 'mut_135_T_A', 'mut_135_T_C', 'mut_135_T_G', 'mut_136_A_C', 'mut_136_A_G', 'mut_136_A_T', 'mut_137_G_A', 'mut_137_G_C', 'mut_137_G_T', 'mut_138_T_A', 'mut_138_T_C', 'mut_138_T_G', 'mut_139_C_A', 'mut_139_C_G', 'mut_139_C_T', 'mut_140_A_C', 'mut_140_A_G', 'mut_140_A_T', 'mut_141_C_A', 'mut_141_C_G', 'mut_141_C_T', 'mut_142_T_A', 'mut_142_T_C', 'mut_142_T_G', 'mut_143_T_A', 'mut_143_T_C', 'mut_143_T_G', 'mut_144_T_A', 'mut_144_T_C', 'mut_144_T_G', 'mut_145_A_C', 'mut_145_A_G', 'mut_145_A_T', 'mut_146_C_A', 'mut_146_C_G', 'mut_146_C_T', 'mut_147_T_A', 'mut_147_T_C', 'mut_147_T_G', 'mut_148_A_C', 'mut_148_A_G', 'mut_148_A_T', 'mut_149_T_A', 'mut_149_T_C', 'mut_149_T_G', 'mut_150_T_A', 'mut_150_T_C', 'mut_150_T_G', 'mut_151_C_A', 'mut_151_C_G', 'mut_151_C_T', 'mut_152_A_C', 'mut_152_A_G', 'mut_152_A_T', 'mut_153_A_C', 'mut_153_A_G', 'mut_153_A_T', 'mut_154_A_C', 'mut_154_A_G', 'mut_154_A_T', 'mut_155_A_C', 'mut_155_A_G', 'mut_155_A_T', 'mut_156_T_A', 'mut_156_T_C', 'mut_156_T_G', 'mut_157_T_A', 'mut_157_T_C', 'mut_157_T_G', 'mut_158_T_A', 'mut_158_T_C', 'mut_158_T_G', 'mut_159_G_A', 'mut_159_G_C', 'mut_159_G_T', 'mut_160_C_A', 'mut_160_C_G', 'mut_160_C_T', 'mut_161_A_C', 'mut_161_A_G', 'mut_161_A_T', 'mut_162_A_C', 'mut_162_A_G', 'mut_162_A_T', 'mut_163_A_C', 'mut_163_A_G', 'mut_163_A_T', 'mut_164_G_A', 'mut_164_G_C', 'mut_164_G_T', 'mut_165_T_A', 'mut_165_T_C', 'mut_165_T_G', 'mut_166_A_C', 'mut_166_A_G', 'mut_166_A_T', 'mut_167_G_A', 'mut_167_G_C', 'mut_167_G_T', 'mut_168_T_A', 'mut_168_T_C', 'mut_168_T_G', 'mut_169_C_A', 'mut_169_C_G', 'mut_169_C_T'], 'sequence': ['AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'CATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'GATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'TATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'ACTATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AGTATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'ATTATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AAAATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AACATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AAGATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATCTATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATGTATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATTTATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATAAATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATACATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATAGATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATCTTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATGTTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATTTTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATAATTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATACTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATAGTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATATAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATCTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATGTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTAAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTCAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTGAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTCATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTGATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTTATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTACTCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAGTCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTATTCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAAACGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAACCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAAGCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATAGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATGGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATTGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCAGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCCGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCTGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGATGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGCTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGTTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGAGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGCGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGGGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTATATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTCTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTTTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGAATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGCATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGGATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTCTAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTGTAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTTTAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTAAAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTACAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTAGAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATCATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATGATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATTATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATACTTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAGTTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATATTTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAAATTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAACTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAAGTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATATCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATCTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATGTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTACTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTCCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTGCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTATGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTGTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTTTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCAGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCCGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCGGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTATGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTCTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTTTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGAGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGCGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGGGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTAAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTCAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTTAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGCAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGGAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGTAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGACGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAGGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGATGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAAATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAACATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAATATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGCTCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGGTCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGTTCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGAACCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGACCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGAGCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATACTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATGCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATTCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCATCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCGTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCTTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCACGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCCCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCGCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTAGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTGGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTTGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCAATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCCATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCTATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGCTACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGGTACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGTTACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGAAACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGACACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGAGACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATCCTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATGCTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATTCTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATAATTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATAGTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATATTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACATCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACCTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACGTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTACATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTCCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTGCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTAATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTGATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTTATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCCTATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCGTATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCTTATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCAAATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCACATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCAGATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATCTAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATGTAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATTTAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATAAAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATACAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATAGAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATCAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATGAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATTAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATACGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAGGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATATGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAAAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAACAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAATAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGCGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGGGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGTGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAAATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGACATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGATATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGCTTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGGTTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGTTTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGAATTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGACTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGAGTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATATTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATCTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATGTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTATGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTCTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTGTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTAGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTCGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTGGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTAAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTCAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTTAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGCGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGGGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGTGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAAAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGACAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGATAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGCGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGGGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGTGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAAAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGACAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGATAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGCGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGGGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGTGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAAAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGACAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGATAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGCGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGGGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGTGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAAAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGACAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGATAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGCGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGGGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGTGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAAAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGACAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGATAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGCGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGGGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGTGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAAAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGACAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGATAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGCACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGGACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGTACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGACCCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAGCCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGATCCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAAACAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAAGCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAATCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACAAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACGAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACTAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCCATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCGATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCTATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCACTTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAGTTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCATTTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAAATTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAACTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAAGTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATATTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATCTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATGTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTATCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTCTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTGTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTACGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTCCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTGCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTAGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTGGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTTGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCAAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCCAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCTAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGCATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGGATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGTATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGACTGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAGTGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGATTGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAAAGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAACGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAAGGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATAGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATCGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATTGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGAGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGCGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGTGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGATGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGCTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGTTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGAGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGCGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGGGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTAAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTCAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTTAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGCGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGGGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGTGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAATTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGACTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGATTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGATGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGCTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGGTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTAGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTCGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTGGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTAGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTCGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTTGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGACAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGCCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGTCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGAAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGGAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGTAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCCAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCGAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCTAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCACAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAGAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCATAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAACGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAGGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAATGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAATATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAACTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAATTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGAATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGCATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGGATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTCTTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTGTTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTTTTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTAATCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTACTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTAGTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATACACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATCCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATGCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTAACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTGACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTTACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCCCTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCGCTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCTCTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCAATTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCAGTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCATTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACATTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACCTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACGTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTATTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTCTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTGTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTATCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTCTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTGTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTACAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTCCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTGCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTAAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTGAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTTAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCCGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCGGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCTGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAAAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCACAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCATAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGCACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGGACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGTACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGACCATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAGCATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGATCATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAAAATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAAGATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAATATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACCTAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACGTAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACTTAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACAAAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACACAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACAGAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATCATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATGATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATTATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATACTTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAGTTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATATTTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAAATGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAACTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAAGTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATAGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATCGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATGGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTAGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTCGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTTGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGAGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGCGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGTGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGAAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGCAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGTAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGCAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGGAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGTAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGACACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAGACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGATACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAACCTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAGCTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAATCTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAAATAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAAGTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAATTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACAAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACCAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACGAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTCGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTGGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTTGTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAATCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTACTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTATTCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGACACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGCCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGGCACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTAACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTGACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTTACTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCCCTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCGCTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCTCTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCAATTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCAGTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCATTTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACATTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACCTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACGTTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTATACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTCTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTGTACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTAACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTCACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTGACTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTCCTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTGCTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTTCTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTAATATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTAGTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTATTATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACAATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACCATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACGATTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTCTTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTGTTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTTTTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTAATCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTACTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTAGTCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATACAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATCCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATGCAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTAAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTGAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTTAAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCCAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCGAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCTAAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCACAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAGAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCATAATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAACATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAGATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAATATTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAACTTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAGTTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAATTTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAAATTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAACTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAAGTTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATATGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATCTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATGTGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTAGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTCGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTGGCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTACAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTCCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTTCAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGAAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGGAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGTAAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCCAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCGAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCTAAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCACAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAGAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCATAGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAACGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAGGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAATGTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAATAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAACTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAATTAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGAAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGCAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGGAGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTCGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTGGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTTGTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAATC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTACTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTATTC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGAC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGCC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGGC', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTA', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTG', 'AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTT']}\n</pre> In\u00a0[7]: Copied! <pre># Predict on saturation mutagenesis sequences\npreds = mutagenesis.evaluate(strategy=\"mean\")\n</pre> # Predict on saturation mutagenesis sequences preds = mutagenesis.evaluate(strategy=\"mean\") <pre>14:17:49 - dnallm.inference.inference - INFO - Using device: mps\n</pre> <pre>Inferring: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 511/511 [00:41&lt;00:00, 12.30it/s]\nEvaluating mutations: 510it [00:00, 234292.99it/s]\n</pre> In\u00a0[8]: Copied! <pre># Visualize mutation effects\npmut = mutagenesis.plot(preds, save_path=\"plot_mut_effects.pdf\")\n</pre> # Visualize mutation effects pmut = mutagenesis.plot(preds, save_path=\"plot_mut_effects.pdf\") <pre>Mutation effects visualization saved to plot_mut_effects_heatmap.pdf\n</pre> In\u00a0[9]: Copied! <pre># Display plot in notebook\npmut\n</pre> # Display plot in notebook pmut Out[9]:"},{"location":"example/notebooks/in_silico_mutagenesis/in_silico_mutagenesis/#in-silico-mutagenesis-prediction","title":"In-silico Mutagenesis Prediction\u00b6","text":""},{"location":"example/notebooks/inference/inference/","title":"Basic Inference","text":"In\u00a0[1]: Copied! <pre>from dnallm import load_config\nfrom dnallm import load_model_and_tokenizer, DNAInference\n</pre> from dnallm import load_config from dnallm import load_model_and_tokenizer, DNAInference In\u00a0[2]: Copied! <pre># Load configurations\nconfigs = load_config(\"./inference_config.yaml\")\n</pre> # Load configurations configs = load_config(\"./inference_config.yaml\") In\u00a0[3]: Copied! <pre># Load model and tokenizer\nmodel_name = \"zhangtaolab/plant-dnagpt-BPE-promoter\"\n# from Hugging Face\n# model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n# from ModelScope\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\n</pre> # Load model and tokenizer model_name = \"zhangtaolab/plant-dnagpt-BPE-promoter\" # from Hugging Face # model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\") # from ModelScope model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") <pre>Downloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE-promoter\nModel files are stored in /home/liuguanqing/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE-promoter\nDownloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE-promoter\nDownloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-BPE-promoter\n</pre> In\u00a0[4]: Copied! <pre># Create inference engine\ninference_engine = DNAInference(\n    model=model,\n    tokenizer=tokenizer,\n    config=configs\n)\n</pre> # Create inference engine inference_engine = DNAInference(     model=model,     tokenizer=tokenizer,     config=configs ) <pre>18:11:38 - dnallm.inference.inference - INFO - Using device: cuda\n</pre> In\u00a0[5]: Copied! <pre># Check device\nprint(model.device)\n</pre> # Check device print(model.device) <pre>cuda:0\n</pre> In\u00a0[6]: Copied! <pre># Predict sequences\nseqs = [\"GCACTTTACTTAAAGTAAAAAGAAAAAAACTGTGCGCTCTCCAACTACCGCAGCAACGTGTCGAGCACAGGAACACGTGTCACTTCAGTTCTTCCAATTGCTGGGGCCCACCACTGTTTACTTCTGTACAGGCAGGTGGCCATGCTGATGACACTCCACACTCCTCGACTTTCGTAGCAGCAAGCCACGCGTGACCGAGAAGCCTCGCG\",\n        \"TTGTCATCACATTTGATCAACTACGATTTATGTTGTACTATTCATCTGTTTTCTCCTTTTTTTTTCCCTTATTGACAGGTTGTGGAGGTTCACAACGAACAGAATACAAGAAATTTTGGTAATCATTTGAGGACTTTCATGGGGTATGAATTGTGTGCTATAATAAATTAA\"]\nresults = inference_engine.infer_seqs(seqs)\nprint(results)\n</pre> # Predict sequences seqs = [\"GCACTTTACTTAAAGTAAAAAGAAAAAAACTGTGCGCTCTCCAACTACCGCAGCAACGTGTCGAGCACAGGAACACGTGTCACTTCAGTTCTTCCAATTGCTGGGGCCCACCACTGTTTACTTCTGTACAGGCAGGTGGCCATGCTGATGACACTCCACACTCCTCGACTTTCGTAGCAGCAAGCCACGCGTGACCGAGAAGCCTCGCG\",         \"TTGTCATCACATTTGATCAACTACGATTTATGTTGTACTATTCATCTGTTTTCTCCTTTTTTTTTCCCTTATTGACAGGTTGTGGAGGTTCACAACGAACAGAATACAAGAAATTTTGGTAATCATTTGAGGACTTTCATGGGGTATGAATTGTGTGCTATAATAAATTAA\"] results = inference_engine.infer_seqs(seqs) print(results) <pre>Encoding inputs:   0%|          | 0/2 [00:00&lt;?, ? examples/s]</pre> <pre>Inferring: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.10it/s]\n</pre> <pre>{0: {'sequence': 'GCACTTTACTTAAAGTAAAAAGAAAAAAACTGTGCGCTCTCCAACTACCGCAGCAACGTGTCGAGCACAGGAACACGTGTCACTTCAGTTCTTCCAATTGCTGGGGCCCACCACTGTTTACTTCTGTACAGGCAGGTGGCCATGCTGATGACACTCCACACTCCTCGACTTTCGTAGCAGCAAGCCACGCGTGACCGAGAAGCCTCGCG', 'label': 'positive', 'scores': {'negative': 0.027381975203752518, 'positive': 0.972618043422699}}, 1: {'sequence': 'TTGTCATCACATTTGATCAACTACGATTTATGTTGTACTATTCATCTGTTTTCTCCTTTTTTTTTCCCTTATTGACAGGTTGTGGAGGTTCACAACGAACAGAATACAAGAAATTTTGGTAATCATTTGAGGACTTTCATGGGGTATGAATTGTGTGCTATAATAAATTAA', 'label': 'negative', 'scores': {'negative': 0.9998313188552856, 'positive': 0.00016868265811353922}}}\n</pre> In\u00a0[7]: Copied! <pre># Predict from file\nseq_file = './test.csv'\nresults, metrics = inference_engine.infer_file(seq_file, label_col='label', evaluate=True)\nprint(metrics)\n</pre> # Predict from file seq_file = './test.csv' results, metrics = inference_engine.infer_file(seq_file, label_col='label', evaluate=True) print(metrics) <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> <pre>Format labels:   0%|          | 0/500 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/500 [00:00&lt;?, ? examples/s]</pre> <pre>Inferring: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [00:04&lt;00:00,  6.46it/s]\n</pre> <pre>{'accuracy': 0.8, 'precision': 0.7708333333333334, 'recall': 0.8671875, 'f1': 0.8161764705882353, 'mcc': 0.6035366212803432, 'AUROC': 0.8783139088114755, 'AUPRC': 0.8750756751047346, 'TPR': 0.8671875, 'TNR': 0.7295081967213115, 'FPR': 0.27049180327868855, 'FNR': 0.1328125}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/inference/inference/#model-inference","title":"Model inference\u00b6","text":""},{"location":"example/notebooks/inference_for_tRNA/inference/","title":"tRNA Inference","text":"In\u00a0[1]: Copied! <pre>from dnallm import load_config\nfrom dnallm import load_model_and_tokenizer, DNAInference\n</pre> from dnallm import load_config from dnallm import load_model_and_tokenizer, DNAInference In\u00a0[2]: Copied! <pre># load config\nconfigs = load_config(\"./inference_model_config_tRNADetector.yaml\")\n\nmodel_name = \"zhangtaolab/tRNADetector\"\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'],  source=\"modelscope\")\n\npredictor = DNAInference(\n\tmodel=model,\n\ttokenizer=tokenizer,\n\tconfig=configs\n)\n</pre> # load config configs = load_config(\"./inference_model_config_tRNADetector.yaml\")  model_name = \"zhangtaolab/tRNADetector\" model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'],  source=\"modelscope\")  predictor = DNAInference( \tmodel=model, \ttokenizer=tokenizer, \tconfig=configs ) <pre>Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/tRNADetector\nModel files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/tRNADetector\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/tRNADetector\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/tRNADetector\n</pre> <pre>Importing `MambaCache` from `transformers.cache_utils` is deprecated and will be removed in a future version. Please import it from `transformers` or `transformers.models.mamba.cache_mamba` instead.\nThe fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the mamba.py backend. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n</pre> <pre>15:04:25 - dnallm.inference.inference - INFO - Using device: cpu\n</pre> In\u00a0[3]: Copied! <pre>seq = ['AAGAAAGCTCAAATAGTATACGAAGAACTCGAAGCTAAGCAACTGTGAAGAGAAATTAAGTAGCTACAATTAGGTTATAAATAATTTGATTTCTACTCTAACTGTGACGTGGGGATGTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGTACGGGGATCGATACCCCGCATCTCCATTTTTTTATTTTTTTTTAGAATTCTACTTTTTCTAAAATTGACCCTTTAATTTTGTATTTATATTTCTTTTATAATGTATATGCATTCTGCATTTTATTTTTCCTTTACATTTTTTCTTATATAATGTAAGTTATGCATTCTGCATTTTCTTTTGTCTTTTTTTTTTCTTATAAGTGGTTGG', 'AAAACCCCAACTAGCTAGCATCGATCGAGCTAGCATGCATCGATCGATCGATCGATCGATCGATCGATCGAACACCCCGCGCGTAGCTACGGCTCAGAGCATCGATGCGCAGTCGAGCCGGGGGGGACATCGATCGATCGATCGATCGAGTCGACGATCGATCGAGCATATAATCGAGTCGACTGATCGATCGAGCGTACGATCGATCGATCGATGCATCCCCGATCGATCGATCGATCTTATAACACACACACACACACACGGAAAA']\nresults = predictor.infer_file(seq,  evaluate=False)\n</pre> seq = ['AAGAAAGCTCAAATAGTATACGAAGAACTCGAAGCTAAGCAACTGTGAAGAGAAATTAAGTAGCTACAATTAGGTTATAAATAATTTGATTTCTACTCTAACTGTGACGTGGGGATGTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGTACGGGGATCGATACCCCGCATCTCCATTTTTTTATTTTTTTTTAGAATTCTACTTTTTCTAAAATTGACCCTTTAATTTTGTATTTATATTTCTTTTATAATGTATATGCATTCTGCATTTTATTTTTCCTTTACATTTTTTCTTATATAATGTAAGTTATGCATTCTGCATTTTCTTTTGTCTTTTTTTTTTCTTATAAGTGGTTGG', 'AAAACCCCAACTAGCTAGCATCGATCGAGCTAGCATGCATCGATCGATCGATCGATCGATCGATCGATCGAACACCCCGCGCGTAGCTACGGCTCAGAGCATCGATGCGCAGTCGAGCCGGGGGGGACATCGATCGATCGATCGATCGAGTCGACGATCGATCGAGCATATAATCGAGTCGACTGATCGATCGAGCGTACGATCGATCGATCGATGCATCCCCGATCGATCGATCGATCTTATAACACACACACACACACACGGAAAA'] results = predictor.infer_file(seq,  evaluate=False) <pre>Encoding inputs:   0%|          | 0/2 [00:00&lt;?, ? examples/s]</pre> <pre>Inferring: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:09&lt;00:00,  9.68s/it]\n</pre> In\u00a0[4]: Copied! <pre>for i in results:\n\tsequence = results[i]['sequence']\n\tlabel = results[i]['label']\n\tscore = results[i]['scores'][label]\n\tprint(f'input sequence:{sequence}\\n',\n\t   \tf'predict label:{label}\\n', \n\t\tf'predict score:{score}\\n',\n\t\tf'*'*20)\n</pre> for i in results: \tsequence = results[i]['sequence'] \tlabel = results[i]['label'] \tscore = results[i]['scores'][label] \tprint(f'input sequence:{sequence}\\n', \t   \tf'predict label:{label}\\n',  \t\tf'predict score:{score}\\n', \t\tf'*'*20) <pre>input sequence:AAGAAAGCTCAAATAGTATACGAAGAACTCGAAGCTAAGCAACTGTGAAGAGAAATTAAGTAGCTACAATTAGGTTATAAATAATTTGATTTCTACTCTAACTGTGACGTGGGGATGTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGTACGGGGATCGATACCCCGCATCTCCATTTTTTTATTTTTTTTTAGAATTCTACTTTTTCTAAAATTGACCCTTTAATTTTGTATTTATATTTCTTTTATAATGTATATGCATTCTGCATTTTATTTTTCCTTTACATTTTTTCTTATATAATGTAAGTTATGCATTCTGCATTTTCTTTTGTCTTTTTTTTTTCTTATAAGTGGTTGG\n predict label:tRNA\n predict score:0.9999682903289795\n ********************\ninput sequence:AAAACCCCAACTAGCTAGCATCGATCGAGCTAGCATGCATCGATCGATCGATCGATCGATCGATCGATCGAACACCCCGCGCGTAGCTACGGCTCAGAGCATCGATGCGCAGTCGAGCCGGGGGGGACATCGATCGATCGATCGATCGAGTCGACGATCGATCGAGCATATAATCGAGTCGACTGATCGATCGAGCGTACGATCGATCGATCGATGCATCCCCGATCGATCGATCGATCTTATAACACACACACACACACACGGAAAA\n predict label:Partial tRNA\n predict score:1.0\n ********************\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># load config\nconfigs = load_config(\"./inference_model_config_tRNAPointer.yaml\")\n\nmodel_name = \"zhangtaolab/tRNAPointer\"\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'],  source=\"modelscope\")\n\npredictor = DNAInference(\n\tmodel=model,\n\ttokenizer=tokenizer,\n\tconfig=configs\n)\n</pre> # load config configs = load_config(\"./inference_model_config_tRNAPointer.yaml\")  model_name = \"zhangtaolab/tRNAPointer\" model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'],  source=\"modelscope\")  predictor = DNAInference( \tmodel=model, \ttokenizer=tokenizer, \tconfig=configs ) <pre>Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/tRNAPointer\nModel files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/tRNAPointer\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/tRNAPointer\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/tRNAPointer\nDownloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/tRNAPointer\n15:04:44 - dnallm.inference.inference - INFO - Using device: cpu\n</pre> In\u00a0[\u00a0]: Copied! <pre>seq = ['AAGAAAGCTCAAATAGTATACGAAGAACTCGAAGCTAAGCAACTGTGAAGAGAAATTAAGTAGCTACAATTAGGTTATAAATAATTTGATTTCTACTCTAACTGTGACGTGGGGATGTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGTACGGGGATCGATACCCCGCATCTCCATTTTTTTATTTTTTTTTAGAATTCTACTTTTTCTAAAATTGACCCTTTAATTTTGTATTTATATTTCTTTTATAATGTATATGCATTCTGCATTTTATTTTTCCTTTACATTTTTTCTTATATAATGTAAGTTATGCATTCTGCATTTTCTTTTGTCTTTTTTTTTTCTTATAAGTGGTTGG', 'AAAACCCCAACTAGCTAGCATCGATCGAGCTAGCATGCATCGATCGATCGATCGATCGATCGATCGATCGAACACCCCGCGCGTAGCTACGGCTCAGAGCATCGATGCGCAGTCGAGCCGGGGGGGACATCGATCGATCGATCGATCGAGTCGACGATCGATCGAGCATATAATCGAGTCGACTGATCGATCGAGCGTACGATCGATCGATCGATGCATCCCCGATCGATCGATCGATCTTATAACACACACACACACACACGGAAAA']\n\nseq_token = []\nfor _ in seq:\n\tseq_token.append([base for base in _])\n\nresults = predictor.infer_file(seq_token,  evaluate=False)\n</pre> seq = ['AAGAAAGCTCAAATAGTATACGAAGAACTCGAAGCTAAGCAACTGTGAAGAGAAATTAAGTAGCTACAATTAGGTTATAAATAATTTGATTTCTACTCTAACTGTGACGTGGGGATGTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGTACGGGGATCGATACCCCGCATCTCCATTTTTTTATTTTTTTTTAGAATTCTACTTTTTCTAAAATTGACCCTTTAATTTTGTATTTATATTTCTTTTATAATGTATATGCATTCTGCATTTTATTTTTCCTTTACATTTTTTCTTATATAATGTAAGTTATGCATTCTGCATTTTCTTTTGTCTTTTTTTTTTCTTATAAGTGGTTGG', 'AAAACCCCAACTAGCTAGCATCGATCGAGCTAGCATGCATCGATCGATCGATCGATCGATCGATCGATCGAACACCCCGCGCGTAGCTACGGCTCAGAGCATCGATGCGCAGTCGAGCCGGGGGGGACATCGATCGATCGATCGATCGAGTCGACGATCGATCGAGCATATAATCGAGTCGACTGATCGATCGAGCGTACGATCGATCGATCGATGCATCCCCGATCGATCGATCGATCTTATAACACACACACACACACACGGAAAA']  seq_token = [] for _ in seq: \tseq_token.append([base for base in _])  results = predictor.infer_file(seq_token,  evaluate=False)   <pre>Encoding inputs:   0%|          | 0/2 [00:00&lt;?, ? examples/s]</pre> <pre>Inferring: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:08&lt;00:00,  8.74s/it]\n</pre> In\u00a0[7]: Copied! <pre>for i in results:\n\tsequence = ''.join(results[i]['sequence'])\n\tlabel = results[i]['label']\n\ttry:\n\t\tstart = label.index(\"B-tRNA\")\n\t\tend = len(label) - 1 - label[::-1].index(\"I-tRNA\")\n\t\ttRNA_sequence = sequence[start:end+1]\n\t\tprint(f'input sequence:{sequence}\\n',\n\t   \t      f'tRNA start index in sequence:{start}\\n',\n\t\t      f'tRNA end index in sequence:{end}\\n',\n\t\t      f'tRNA sequence:{tRNA_sequence}\\n',\n\t\t\t  f'*'*20)\n\texcept:\n\t\tprint(f'input sequence:{sequence}\\n',\n\t   \t      'No tRNA found\\n',\n\t\t\t  f'*'*20)\n</pre> for i in results: \tsequence = ''.join(results[i]['sequence']) \tlabel = results[i]['label'] \ttry: \t\tstart = label.index(\"B-tRNA\") \t\tend = len(label) - 1 - label[::-1].index(\"I-tRNA\") \t\ttRNA_sequence = sequence[start:end+1] \t\tprint(f'input sequence:{sequence}\\n', \t   \t      f'tRNA start index in sequence:{start}\\n', \t\t      f'tRNA end index in sequence:{end}\\n', \t\t      f'tRNA sequence:{tRNA_sequence}\\n', \t\t\t  f'*'*20) \texcept: \t\tprint(f'input sequence:{sequence}\\n', \t   \t      'No tRNA found\\n', \t\t\t  f'*'*20) <pre>input sequence:AAGAAAGCTCAAATAGTATACGAAGAACTCGAAGCTAAGCAACTGTGAAGAGAAATTAAGTAGCTACAATTAGGTTATAAATAATTTGATTTCTACTCTAACTGTGACGTGGGGATGTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGTACGGGGATCGATACCCCGCATCTCCATTTTTTTATTTTTTTTTAGAATTCTACTTTTTCTAAAATTGACCCTTTAATTTTGTATTTATATTTCTTTTATAATGTATATGCATTCTGCATTTTATTTTTCCTTTACATTTTTTCTTATATAATGTAAGTTATGCATTCTGCATTTTCTTTTGTCTTTTTTTTTTCTTATAAGTGGTTGG\n tRNA start index in sequence:111\n tRNA end index in sequence:183\n tRNA sequence:GGGATGTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGTACGGGGATCGATACCCCGCATCTCCAT\n ********************\ninput sequence:AAAACCCCAACTAGCTAGCATCGATCGAGCTAGCATGCATCGATCGATCGATCGATCGATCGATCGATCGAACACCCCGCGCGTAGCTACGGCTCAGAGCATCGATGCGCAGTCGAGCCGGGGGGGACATCGATCGATCGATCGATCGAGTCGACGATCGATCGAGCATATAATCGAGTCGACTGATCGATCGAGCGTACGATCGATCGATCGATGCATCCCCGATCGATCGATCGATCTTATAACACACACACACACACACGGAAAA\n No tRNA found\n ********************\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/inference_for_tRNA/inference/#trnadetector","title":"tRNADetector\u00b6","text":""},{"location":"example/notebooks/inference_for_tRNA/inference/#trnapointer","title":"tRNAPointer\u00b6","text":""},{"location":"example/notebooks/interpretation/interpretation/","title":"Interpretation","text":"In\u00a0[1]: Copied! <pre>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm import DNAInterpret, Mutagenesis\n</pre> from dnallm import load_config, load_model_and_tokenizer from dnallm import DNAInterpret, Mutagenesis In\u00a0[2]: Copied! <pre># Load configurations\nconfigs = load_config(\"./inference_config.yaml\")\n</pre> # Load configurations configs = load_config(\"./inference_config.yaml\") In\u00a0[3]: Copied! <pre># Load model and tokenizer\nmodel_name = \"zhangtaolab/plant-dnabert-BPE-promoter_strength_leaf\"\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\n</pre> # Load model and tokenizer model_name = \"zhangtaolab/plant-dnabert-BPE-promoter_strength_leaf\" model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") <pre>Downloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/zhangtaolab/plant-dnabert-BPE-promoter_strength_leaf\n00:41:18 - dnallm.models.model - INFO - Model files are stored in /home/liuguanqing/.cache/modelscope/hub/models/zhangtaolab/plant-dnabert-BPE-promoter_strength_leaf\n</pre> In\u00a0[4]: Copied! <pre>interpreter = DNAInterpret(model, tokenizer, config=configs)\n</pre> interpreter = DNAInterpret(model, tokenizer, config=configs) In\u00a0[5]: Copied! <pre>sequence = (\n    \"AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGA\"\n    \"ACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTAT\"\n    \"TCAAAATTTGCAAAGTAGTC\"\n)\nsequences = [\n    \"AACACTCTATTTCGGGTATTGTCTCTGTGTTCCTTTAGCGGCGGCTTTACTTTAGATTCTTCTAGGGTTTCTAGA\"\n    \"TTGTATACCCTAGATAAGCATCCTATAAAGTAAACACAAGTACTTGCAGAGACTTTAGATTAGAGGGCTAGCGAC\"\n    \"TGCAGAAGAAGAGTAACACG\",\n    \"TAAAGGAACATATTCCCGTCATAAAGAAAAGTTGACTATATTTAGCCCATGCAAAAAGAAAATAGATAAATTTAG\"\n    \"AAATCTATATGCATATATTCCTTCTCAAGGGTTATAAAAAGAGAGCACATCCATGTGAGGAATGAGGCAACACAT\"\n    \"ATTGAGAGTAATAAAGAGTA\",\n    \"TTCACCAGCTAGGCCATAGTGCCGGCCCTTGCACAATGTTGTATCTGATCACCTAGCTAGTGTGAAGTGTTTGGA\"\n    \"GGAACTCTAGGTGTTATCCAGCAATGTTTCATAGTTTGTGAAACTGTAAAAGGTTTTGGTAAGACGATGATCAGA\"\n    \"TTTGGTGTTATCATGAGTTC\",\n]\n</pre> sequence = (     \"AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGA\"     \"ACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTAT\"     \"TCAAAATTTGCAAAGTAGTC\" ) sequences = [     \"AACACTCTATTTCGGGTATTGTCTCTGTGTTCCTTTAGCGGCGGCTTTACTTTAGATTCTTCTAGGGTTTCTAGA\"     \"TTGTATACCCTAGATAAGCATCCTATAAAGTAAACACAAGTACTTGCAGAGACTTTAGATTAGAGGGCTAGCGAC\"     \"TGCAGAAGAAGAGTAACACG\",     \"TAAAGGAACATATTCCCGTCATAAAGAAAAGTTGACTATATTTAGCCCATGCAAAAAGAAAATAGATAAATTTAG\"     \"AAATCTATATGCATATATTCCTTCTCAAGGGTTATAAAAAGAGAGCACATCCATGTGAGGAATGAGGCAACACAT\"     \"ATTGAGAGTAATAAAGAGTA\",     \"TTCACCAGCTAGGCCATAGTGCCGGCCCTTGCACAATGTTGTATCTGATCACCTAGCTAGTGTGAAGTGTTTGGA\"     \"GGAACTCTAGGTGTTATCCAGCAATGTTTCATAGTTTGTGAAACTGTAAAAGGTTTTGGTAAGACGATGATCAGA\"     \"TTTGGTGTTATCATGAGTTC\", ] In\u00a0[11]: Copied! <pre>tokens, lig_scores = interpreter.interpret(sequence, method=\"deeplift\", target=0)\n</pre> tokens, lig_scores = interpreter.interpret(sequence, method=\"deeplift\", target=0) In\u00a0[12]: Copied! <pre>interpreter.plot_attributions(plot_type=\"token\")\n</pre> interpreter.plot_attributions(plot_type=\"token\") Out[12]: In\u00a0[13]: Copied! <pre>interpreter.plot_attributions(plot_type=\"line\")\n</pre> interpreter.plot_attributions(plot_type=\"line\") Out[13]: In\u00a0[9]: Copied! <pre>attributions = interpreter.batch_interpret(sequences, method=\"deeplift\", targets=[0] * len(sequences))\n</pre> attributions = interpreter.batch_interpret(sequences, method=\"deeplift\", targets=[0] * len(sequences)) In\u00a0[10]: Copied! <pre>interpreter.plot_attributions()\n</pre> interpreter.plot_attributions() <pre>Warning: Multiple attributions found, falling back to 'multi' plot.\n</pre> Out[10]: In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># Initialize in-silico mutagenesis analyzer\nmutagenesis = Mutagenesis(config=configs, model=model, tokenizer=tokenizer)\n</pre> # Initialize in-silico mutagenesis analyzer mutagenesis = Mutagenesis(config=configs, model=model, tokenizer=tokenizer) In\u00a0[\u00a0]: Copied! <pre>mutagenesis.mutate_sequence(sequence, replace_mut=True)\n</pre> mutagenesis.mutate_sequence(sequence, replace_mut=True) In\u00a0[\u00a0]: Copied! <pre>preds = mutagenesis.evaluate()\n</pre> preds = mutagenesis.evaluate() In\u00a0[\u00a0]: Copied! <pre>preds\n</pre> preds In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nbase_scores = mutagenesis.process_ism_data(preds)\nhotspots = mutagenesis.find_hotspots(preds, window_size=10, percentile_threshold=90.0)\n_, hyp_scores, _ = mutagenesis.prepare_tfmodisco_inputs([preds])\nhyp_scores = hyp_scores[0]\n\nhotspot_motifs = {}\nfor start, end in hotspots:\n    hotspot_id = f\"hotspot_{start}-{end}\"\n    # Extract the slice corresponding to the hotspot from hyp_scores\n    motif_matrix = hyp_scores[start:end, :]\n    motif_df = pd.DataFrame(motif_matrix, columns=['A', 'C', 'G', 'T'])\n    hotspot_motifs[hotspot_id] = motif_df\n</pre> import pandas as pd base_scores = mutagenesis.process_ism_data(preds) hotspots = mutagenesis.find_hotspots(preds, window_size=10, percentile_threshold=90.0) _, hyp_scores, _ = mutagenesis.prepare_tfmodisco_inputs([preds]) hyp_scores = hyp_scores[0]  hotspot_motifs = {} for start, end in hotspots:     hotspot_id = f\"hotspot_{start}-{end}\"     # Extract the slice corresponding to the hotspot from hyp_scores     motif_matrix = hyp_scores[start:end, :]     motif_df = pd.DataFrame(motif_matrix, columns=['A', 'C', 'G', 'T'])     hotspot_motifs[hotspot_id] = motif_df In\u00a0[\u00a0]: Copied! <pre>def plot_motif_logo( motif_df: pd.DataFrame, logo_type: str = 'bits', title: str = \"Discovered Motif\" ):\n    \"\"\" Plots a sequence logo from a motif matrix using Logomaker. \"\"\"\n    import logomaker\n    import matplotlib.pyplot as plt\n    print(f\"Generating '{logo_type}' logo plot for: {title}\")\n\n    if logo_type == 'bits':\n        logo_df = logomaker.transform_matrix(motif_df, from_type='probability', to_type='information')\n        y_label = 'Bits'\n    elif logo_type == 'weights':\n        logo_df = motif_df\n        y_label = 'Contribution Score'\n    else: raise ValueError(\"logo_type must be 'bits' or 'weights'\")\n    logo = logomaker.Logo(logo_df, font_name='Arial Rounded MT Bold')\n    logo.style_spines(visible=False)\n    logo.style_spines(spines=['left', 'bottom'], visible=True)\n    logo.ax.set_ylabel(y_label)\n    logo.ax.set_title(title)\n    plt.show()\n</pre> def plot_motif_logo( motif_df: pd.DataFrame, logo_type: str = 'bits', title: str = \"Discovered Motif\" ):     \"\"\" Plots a sequence logo from a motif matrix using Logomaker. \"\"\"     import logomaker     import matplotlib.pyplot as plt     print(f\"Generating '{logo_type}' logo plot for: {title}\")      if logo_type == 'bits':         logo_df = logomaker.transform_matrix(motif_df, from_type='probability', to_type='information')         y_label = 'Bits'     elif logo_type == 'weights':         logo_df = motif_df         y_label = 'Contribution Score'     else: raise ValueError(\"logo_type must be 'bits' or 'weights'\")     logo = logomaker.Logo(logo_df, font_name='Arial Rounded MT Bold')     logo.style_spines(visible=False)     logo.style_spines(spines=['left', 'bottom'], visible=True)     logo.ax.set_ylabel(y_label)     logo.ax.set_title(title)     plt.show() In\u00a0[\u00a0]: Copied! <pre>regions = list(hotspot_motifs.keys())\nplot_motif_logo(hotspot_motifs[regions[0]], logo_type='weights', title=\"Hotspot Motif\")\n</pre> regions = list(hotspot_motifs.keys()) plot_motif_logo(hotspot_motifs[regions[0]], logo_type='weights', title=\"Hotspot Motif\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>pmut = mutagenesis.plot(preds)\npmut\n</pre> pmut = mutagenesis.plot(preds) pmut In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/lora_finetune_inference/lora_finetune/","title":"LoRA Fine-tuning","text":"In\u00a0[\u00a0]: Copied! <pre>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n</pre> from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer In\u00a0[\u00a0]: Copied! <pre># Load the config file\nconfigs = load_config(\"./finetune_config.yaml\")\n</pre> # Load the config file configs = load_config(\"./finetune_config.yaml\") In\u00a0[\u00a0]: Copied! <pre># Load the model and tokenizer\nmodel_name = \"kuleshov-group/PlantCAD2-Small-l24-d0768\"\n# from ModelScope\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n</pre> # Load the model and tokenizer model_name = \"kuleshov-group/PlantCAD2-Small-l24-d0768\" # from ModelScope model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\") In\u00a0[\u00a0]: Copied! <pre># Load the datasets\ndata_name = \"zhangtaolab/plant-multi-species-core-promoters\"\n# from ModelScope\ndatasets = DNADataset.from_modelscope(data_name, seq_col=\"sequence\", label_col=\"label\", tokenizer=tokenizer, max_length=512)\n# downsampling datasets\nsampled_datasets = datasets.sampling(0.05, overwrite=True)\n# Encode the datasets\nsampled_datasets.encode_sequences(remove_unused_columns=True)\n</pre> # Load the datasets data_name = \"zhangtaolab/plant-multi-species-core-promoters\" # from ModelScope datasets = DNADataset.from_modelscope(data_name, seq_col=\"sequence\", label_col=\"label\", tokenizer=tokenizer, max_length=512) # downsampling datasets sampled_datasets = datasets.sampling(0.05, overwrite=True) # Encode the datasets sampled_datasets.encode_sequences(remove_unused_columns=True) In\u00a0[\u00a0]: Copied! <pre># Initialize the trainer with lora\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=sampled_datasets,\n    use_lora=True    # load lora config from the configs\n)\n</pre> # Initialize the trainer with lora trainer = DNATrainer(     model=model,     config=configs,     datasets=sampled_datasets,     use_lora=True    # load lora config from the configs ) In\u00a0[\u00a0]: Copied! <pre># Start training\nmetrics = trainer.train()\nprint(metrics)\n</pre> # Start training metrics = trainer.train() print(metrics) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"example/notebooks/lora_finetune_inference/lora_inference/","title":"LoRA Inference","text":"In\u00a0[1]: Copied! <pre>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm import DNAInference\n</pre> from dnallm import load_config, load_model_and_tokenizer from dnallm import DNAInference In\u00a0[2]: Copied! <pre># Load the config file\nconfigs = load_config(\"inference_config.yaml\")\n</pre> # Load the config file configs = load_config(\"inference_config.yaml\") In\u00a0[\u00a0]: Copied! <pre># Load the model and tokenizer\nmodel_name = \"kuleshov-group/PlantCAD2-Small-l24-d0768\"\n# from Hugging Face\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\")\n# from ModelScope\n# model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")\n</pre> # Load the model and tokenizer model_name = \"kuleshov-group/PlantCAD2-Small-l24-d0768\" # from Hugging Face model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"huggingface\") # from ModelScope # model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\") <pre>Downloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/PlantCAD2-Small-l24-d0768\n16:51:20 - dnallm.models.model - INFO - Model files are stored in /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/PlantCAD2-Small-l24-d0768\nDownloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/PlantCAD2-Small-l24-d0768\nDownloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/PlantCAD2-Small-l24-d0768\n</pre> <pre>Some weights of the model checkpoint at /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/PlantCAD2-Small-l24-d0768 were not used when initializing CaduceusForSequenceClassification: ['lm_head.complement_map', 'lm_head.lm_head.weight']\n- This IS expected if you are initializing CaduceusForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CaduceusForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of CaduceusForSequenceClassification were not initialized from the model checkpoint at /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/PlantCAD2-Small-l24-d0768 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Create inference engine\nlora_adapter_path = \"plantcad/cross_species_acr_train_on_arabidopsis_plantcad2_small\"\ninference_engine = DNAInference(\n    model=model,\n    tokenizer=tokenizer,\n    config=configs,\n    lora_adapter=lora_adapter_path\n)\n</pre> # Create inference engine lora_adapter_path = \"plantcad/cross_species_acr_train_on_arabidopsis_plantcad2_small\" inference_engine = DNAInference(     model=model,     tokenizer=tokenizer,     config=configs,     lora_adapter=lora_adapter_path ) <pre>Downloading Model from https://www.modelscope.cn to directory: /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/cross_species_acr_train_on_arabidopsis_plantcad2_small\n</pre> <pre>2025-09-19 16:51:24,567 - modelscope - INFO - Got 8 files, start to download ...\n</pre> <pre>Processing 8 items:   0%|          | 0.00/8.00 [00:00&lt;?, ?it/s]</pre> <pre>Downloading [adapter_config.json]:   0%|          | 0.00/781 [00:00&lt;?, ?B/s]</pre> <pre>Downloading [optimizer.pt]:   0%|          | 0.00/11.8M [00:00&lt;?, ?B/s]</pre> <pre>Downloading [rng_state.pth]:   0%|          | 0.00/13.9k [00:00&lt;?, ?B/s]</pre> <pre>Downloading [adapter_model.safetensors]:   0%|          | 0.00/9.26M [00:00&lt;?, ?B/s]</pre> <pre>Downloading [README.md]:   0%|          | 0.00/5.02k [00:00&lt;?, ?B/s]</pre> <pre>Downloading [trainer_state.json]:   0%|          | 0.00/11.4k [00:00&lt;?, ?B/s]</pre> <pre>Downloading [scheduler.pt]:   0%|          | 0.00/1.04k [00:00&lt;?, ?B/s]</pre> <pre>Downloading [training_args.bin]:   0%|          | 0.00/5.43k [00:00&lt;?, ?B/s]</pre> <pre>2025-09-19 16:51:29,935 - modelscope - INFO - Download model 'lgq12697/cross_species_acr_train_on_arabidopsis_plantcad2_small' successfully.\n</pre> <pre>16:51:29 - dnallm.models.model - INFO - Model files are stored in /home/liuguanqing/.cache/modelscope/hub/models/lgq12697/cross_species_acr_train_on_arabidopsis_plantcad2_small\n16:51:30 - dnallm.models.model - INFO - Loaded LoRA adapter from lgq12697/cross_species_acr_train_on_arabidopsis_plantcad2_small\n16:51:30 - dnallm.models.model - INFO - Using device: cuda\n</pre> In\u00a0[5]: Copied! <pre>seqs = [(\n    \"AAAAATTTAAATATCGTCTGTAGATATTTTATGGGATGCTTTGAGAATGGGCTTCGTTTTAATGGGCCTC\"\n    \"CTCTGCAATCATTGTCCAGAGTCGAGAAACCACCTCTTCTTCTCTTGTTCTTTCTCCAAATCGATTTGGT\"\n    \"CCCAACTCTCTTCAAGCAAAGGAGAGATATGAAAATGAAAGCTCTTACGGCGAACAAGTTTTTCCGATTG\"\n    \"AAGAAGAGAAGAATCTAGAAGATGAAGACAACACTAGTGCACCAAACAGTTTTGCGCGTCTTGAGAGGAA\"\n    \"ACAAAAAACTATTCAGAGTTCAGAGAGAGTCAACCCCCAAACGAGACTTAAACGATGAGCCCACTATAAT\"\n    \"TTTATAATTTATGGGCCATCAGGCCCAAATGATCAGTAGTAGTTATTATTTGACTTTTGACATGGTGGAT\"\n    \"TTGGTTTAACCACCAAACCGAACGAGTAAAACACTATTGGATTGGGTGATGATATCCCGGTTTTATTTGG\"\n    \"TTAAAATCACAAAATCCTGATTTTGGTTCGCGGCTTGATTCTGCCGCTCTCTCGTCTTTAACCTAACTAA\"\n    \"AGACGTAGAATGATTCTGGTTATTGAATTAGTTTGATACA\"\n)]\nresults = inference_engine.infer_seqs(seqs)\n</pre> seqs = [(     \"AAAAATTTAAATATCGTCTGTAGATATTTTATGGGATGCTTTGAGAATGGGCTTCGTTTTAATGGGCCTC\"     \"CTCTGCAATCATTGTCCAGAGTCGAGAAACCACCTCTTCTTCTCTTGTTCTTTCTCCAAATCGATTTGGT\"     \"CCCAACTCTCTTCAAGCAAAGGAGAGATATGAAAATGAAAGCTCTTACGGCGAACAAGTTTTTCCGATTG\"     \"AAGAAGAGAAGAATCTAGAAGATGAAGACAACACTAGTGCACCAAACAGTTTTGCGCGTCTTGAGAGGAA\"     \"ACAAAAAACTATTCAGAGTTCAGAGAGAGTCAACCCCCAAACGAGACTTAAACGATGAGCCCACTATAAT\"     \"TTTATAATTTATGGGCCATCAGGCCCAAATGATCAGTAGTAGTTATTATTTGACTTTTGACATGGTGGAT\"     \"TTGGTTTAACCACCAAACCGAACGAGTAAAACACTATTGGATTGGGTGATGATATCCCGGTTTTATTTGG\"     \"TTAAAATCACAAAATCCTGATTTTGGTTCGCGGCTTGATTCTGCCGCTCTCTCGTCTTTAACCTAACTAA\"     \"AGACGTAGAATGATTCTGGTTATTGAATTAGTTTGATACA\" )] results = inference_engine.infer_seqs(seqs) <pre>Encoding inputs:   0%|          | 0/1 [00:00&lt;?, ? examples/s]</pre> <pre>Inferring: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:06&lt;00:00,  6.34s/it]\n</pre> In\u00a0[6]: Copied! <pre>print(results)\n</pre> print(results) <pre>{0: {'sequence': 'AAAAATTTAAATATCGTCTGTAGATATTTTATGGGATGCTTTGAGAATGGGCTTCGTTTTAATGGGCCTCCTCTGCAATCATTGTCCAGAGTCGAGAAACCACCTCTTCTTCTCTTGTTCTTTCTCCAAATCGATTTGGTCCCAACTCTCTTCAAGCAAAGGAGAGATATGAAAATGAAAGCTCTTACGGCGAACAAGTTTTTCCGATTGAAGAAGAGAAGAATCTAGAAGATGAAGACAACACTAGTGCACCAAACAGTTTTGCGCGTCTTGAGAGGAAACAAAAAACTATTCAGAGTTCAGAGAGAGTCAACCCCCAAACGAGACTTAAACGATGAGCCCACTATAATTTTATAATTTATGGGCCATCAGGCCCAAATGATCAGTAGTAGTTATTATTTGACTTTTGACATGGTGGATTTGGTTTAACCACCAAACCGAACGAGTAAAACACTATTGGATTGGGTGATGATATCCCGGTTTTATTTGGTTAAAATCACAAAATCCTGATTTTGGTTCGCGGCTTGATTCTGCCGCTCTCTCGTCTTTAACCTAACTAAAGACGTAGAATGATTCTGGTTATTGAATTAGTTTGATACA', 'label': 'positive', 'scores': {'negative': 0.37261253595352173, 'positive': 0.6273874640464783}}}\n</pre> In\u00a0[10]: Copied! <pre>infer_file = \"./test.csv\"\nresults, metrics = inference_engine.infer_file(\n    infer_file, seq_col=\"sequence\", label_col=\"label\", evaluate=True\n)\n</pre> infer_file = \"./test.csv\" results, metrics = inference_engine.infer_file(     infer_file, seq_col=\"sequence\", label_col=\"label\", evaluate=True ) <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> <pre>Format labels:   0%|          | 0/250 [00:00&lt;?, ? examples/s]</pre> <pre>Encoding inputs:   0%|          | 0/250 [00:00&lt;?, ? examples/s]</pre> <pre>Inferring: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:14&lt;00:00,  1.13it/s]\n</pre> In\u00a0[11]: Copied! <pre>for i, res in results.items():\n    print(res['label'], res['scores'], sep=\"\\n\")\n    break\n\nprint(metrics)\n</pre> for i, res in results.items():     print(res['label'], res['scores'], sep=\"\\n\")     break  print(metrics) <pre>negative\n{'negative': 0.9992390871047974, 'positive': 0.0007609084714204073}\n{'accuracy': 0.776, 'precision': 1.0, 'recall': 0.06666666666666667, 'f1': 0.125, 'mcc': 0.2269152152350059, 'AUROC': 0.8207017543859649, 'AUPRC': 0.6004600585479547, 'TPR': 0.06666666666666667, 'TNR': 1.0, 'FPR': 0.0, 'FNR': 0.9333333333333333}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>This comprehensive FAQ addresses common issues and questions you might encounter while using DNALLM.</p>"},{"location":"faq/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation Issues</li> <li>Training and Fine-tuning Issues</li> <li>Model Loading and Inference Issues</li> <li>Model-Specific Issues</li> <li>Performance and Memory Issues</li> <li>Task-Specific Issues</li> <li>General Usage Questions</li> <li>Troubleshooting Guides</li> </ul>"},{"location":"faq/#installation-issues","title":"Installation Issues","text":""},{"location":"faq/#q-mamba-ssm-or-flash-attn-installation-fails","title":"Q: <code>mamba-ssm</code> or <code>flash-attn</code> Installation Fails","text":"<p>Problem: These packages require specific versions of the CUDA toolkit and a C++ compiler, and compilation often fails.</p> <p>Solution: - Ensure you have a compatible NVIDIA GPU and the correct CUDA toolkit version installed on your system. - Install the necessary build tools: <code>conda install -c conda-forge gxx clang</code>. - Try installing pre-compiled wheels if available for your system. Check the official repositories for <code>mamba-ssm</code> and <code>flash-attention</code> for installation instructions. - For Mamba, use the provided installation script: <code>sh scripts/install_mamba.sh</code>.</p>"},{"location":"faq/#q-uv-pip-install-fails-due-to-network-issues","title":"Q: <code>uv pip install</code> Fails Due to Network Issues","text":"<p>Problem: Your network may be blocking access to PyPI or GitHub.</p> <p>Solution: Configure <code>uv</code> or <code>pip</code> to use a proxy or a mirror. For example, you can set environment variables: <pre><code>export HTTP_PROXY=\"http://your.proxy.server:port\"\nexport HTTPS_PROXY=\"http://your.proxy.server:port\"\n</code></pre></p>"},{"location":"faq/#q-importerror-not-installed-for-specific-models","title":"Q: <code>ImportError: ... not installed</code> for Specific Models","text":"<p>Error Messages: - <code>ImportError: EVO-1 package is required...</code> - <code>ImportError: No module named 'mamba_ssm'</code> - <code>ImportError: No module named 'gpn'</code> - <code>ImportError: No module named 'ai2_olmo'</code></p> <p>Solution: You must install the required dependencies for the specific model you are trying to use.</p> <p>Example for Mamba: <pre><code>uv pip install -e '.[mamba]' --no-cache-dir --no-build-isolation\n</code></pre></p> <p>Example for Evo-1: <pre><code>uv pip install evo-model\n</code></pre></p>"},{"location":"faq/#q-flash_attn-installation-fails","title":"Q: <code>flash_attn</code> Installation Fails","text":"<p>Error Message: <code>HTTP Error 404: Not Found</code> during <code>pip install</code> or compilation errors.</p> <p>Cause: FlashAttention is highly specific to your Python, PyTorch, and CUDA versions. A pre-compiled wheel might not be available for your exact environment.</p> <p>Solution: 1. Check Compatibility: Visit the FlashAttention GitHub releases and find a wheel that matches your environment. 2. Install Manually: Download the <code>.whl</code> file and install it directly:    <pre><code>uv pip install /path/to/flash_attn-2.5.8+cu122torch2.3-cp312-cp312-linux_x86_64.whl\n</code></pre> 3. Compile from Source: If no wheel is available, you may need to compile it from source, which requires having the CUDA toolkit and a C++ compiler installed. 4. Run without FlashAttention: Most models can run without FlashAttention by using a slower, \"eager\" attention mechanism. Performance will be reduced, but the model will still work.</p>"},{"location":"faq/#training-and-fine-tuning-issues","title":"Training and Fine-tuning Issues","text":""},{"location":"faq/#q-cuda-out-of-memory-error","title":"Q: <code>CUDA out of memory</code> Error","text":"<p>Problem: Your model, data, and gradients are too large to fit in your GPU's VRAM.</p> <p>Solution: This is the most common training error. Try these steps in order: 1. Enable Gradient Accumulation: In your config file, set <code>training_args.gradient_accumulation_steps</code> to a value like 4 or 8. This is the most effective solution. 2. Reduce Batch Size: Lower <code>training_args.per_device_train_batch_size</code> to 4, 2, or even 1. 3. Enable Mixed Precision: Set <code>training_args.fp16: true</code>. This halves the memory required for the model and activations. 4. Use an 8-bit Optimizer: Set <code>training_args.optim: \"adamw_8bit\"</code>. This requires the <code>bitsandbytes</code> library. 5. Enable Gradient Checkpointing: Set <code>training_args.gradient_checkpointing: true</code>. This saves a lot of memory at the cost of slower training.</p>"},{"location":"faq/#q-loss-is-nan-or-explodes","title":"Q: Loss is <code>NaN</code> or Explodes","text":"<p>Problem: The training process is unstable. This can be caused by a learning rate that is too high, or numerical instability with FP16.</p> <p>Solution: - Lower the Learning Rate: Decrease <code>training_args.learning_rate</code> by a factor of 10 (e.g., from <code>5e-5</code> to <code>5e-6</code>). - Use a Learning Rate Scheduler: Ensure <code>lr_scheduler_type</code> is set to <code>linear</code> or <code>cosine</code>. - Use BF16 instead of FP16: If you have an Ampere-based GPU (A100, RTX 30xx/40xx) or newer, use <code>bf16: true</code> instead of <code>fp16: true</code>. Bfloat16 is more numerically stable.</p>"},{"location":"faq/#model-loading-and-inference-issues","title":"Model Loading and Inference Issues","text":""},{"location":"faq/#q-trust_remote_codetrue-is-required","title":"Q: <code>trust_remote_code=True</code> is Required","text":"<p>Problem: You are trying to load a model with a custom architecture (e.g., Hyena, Caduceus, Evo) that is not yet part of the main <code>transformers</code> library.</p> <p>Solution: You must pass <code>trust_remote_code=True</code> when loading the model. This allows <code>transformers</code> to download and run the model's defining Python code from the Hugging Face Hub.</p> <pre><code>model, tokenizer = load_model_and_tokenizer(\n    \"togethercomputer/evo-1-131k-base\",\n    trust_remote_code=True\n)\n</code></pre>"},{"location":"faq/#q-valueerror-model-not-found-locally","title":"Q: <code>ValueError: Model ... not found locally.</code>","text":"<p>Cause: You specified <code>source: \"local\"</code> but the path provided in <code>model_name</code> is incorrect or does not point to a valid model directory.</p> <p>Solution: - Double-check that the path in your configuration or code is correct. - Ensure the directory contains the necessary model files (e.g., <code>pytorch_model.bin</code>, <code>config.json</code>).</p>"},{"location":"faq/#q-valueerror-failed-to-load-model","title":"Q: <code>ValueError: Failed to load model: ...</code>","text":"<p>This is a general error that can have several causes.</p> <p>Common Causes &amp; Solutions: 1. Incorrect <code>task_type</code>: You are trying to load a model for a task it wasn't designed for without a proper configuration.    - Fix: Ensure your <code>task</code> configuration in the YAML file is correct. For classification/regression, <code>num_labels</code> must be specified.</p> <ol> <li>Corrupted Model Cache: The downloaded model files may be incomplete or corrupted.</li> <li> <p>Fix: Clear the cache and let DNALLM re-download the model.    <pre><code>from dnallm.models.model import clear_model_cache\n\n# For models from Hugging Face\nclear_model_cache(source=\"huggingface\")\n\n# For models from ModelScope\nclear_model_cache(source=\"modelscope\")\n</code></pre></p> </li> <li> <p>Network Issues: The model download failed due to an unstable connection.</p> </li> <li>Fix: Use a mirror by setting <code>use_mirror=True</code>.    <pre><code>model, tokenizer = load_model_and_tokenizer(\n    \"zhihan1996/DNABERT-2-117M\",\n    task_config=configs['task'],\n    source=\"huggingface\",\n    use_mirror=True # This uses hf-mirror.com\n)\n</code></pre></li> </ol>"},{"location":"faq/#q-tokenizer-mismatch-or-poor-performance","title":"Q: Tokenizer Mismatch or Poor Performance","text":"<p>Problem: You are using a model pre-trained on natural language (like the original LLaMA) directly on DNA sequences. The tokenizer doesn't understand DNA k-mers, leading to poor results.</p> <p>Solution: Always use a model that has been specifically pre-trained or fine-tuned on DNA. These models, like DNABERT or GENERator, come with a tokenizer designed for DNA. Check the model card on Hugging Face to confirm it's intended for genomic data.</p>"},{"location":"faq/#model-specific-issues","title":"Model-Specific Issues","text":""},{"location":"faq/#q-evo-model-installation-and-usage","title":"Q: EVO Model Installation and Usage","text":"<p>Problem: <code>ImportError: EVO-1 package is required...</code> or <code>EVO2 package is required...</code></p> <p>Solution: You have not installed the required package. Follow the installation steps:</p> <p>EVO-1 Installation: <pre><code>uv pip install evo-model\n</code></pre></p> <p>EVO-2 Installation: <pre><code># 1. Install the Transformer Engine from NVIDIA\nuv pip install \"transformer-engine[pytorch]==2.3.0\" --no-build-isolation --no-cache-dir\n\n# 2. Install the EVO-2 package\nuv pip install evo2\n\n# 3. (Optional but Recommended) Install Flash Attention for performance\nuv pip install \"flash_attn&lt;=2.7.4.post1\" --no-build-isolation --no-cache-dir\n</code></pre></p>"},{"location":"faq/#q-cuda-out-of-memory-with-evo-2","title":"Q: CUDA Out-of-Memory with EVO-2","text":"<p>Cause: EVO-2 models, especially the larger ones, are very memory-intensive.</p> <p>Solution: 1. Ensure you are using a GPU with sufficient VRAM (e.g., A100, H100). 2. Reduce the <code>batch_size</code> in your configuration to 1 if necessary. 3. If you are on a Hopper-series GPU (H100/H200), ensure FP8 is enabled, as DNALLM's EVO-2 handler attempts to use it automatically for efficiency.</p>"},{"location":"faq/#performance-and-memory-issues","title":"Performance and Memory Issues","text":""},{"location":"faq/#q-cuda-out-of-memory-during-inference","title":"Q: <code>CUDA Out-of-Memory</code> During Inference","text":"<p>Cause: The model, data, and intermediate activations require more GPU VRAM than is available.</p> <p>Solutions: - Primary: Reduce <code>batch_size</code> in your <code>inference</code> or <code>training</code> configuration. This is the most effective way to lower memory usage. - Secondary: Reduce <code>max_length</code>. The memory requirement for transformers scales quadratically with sequence length. - Use Half-Precision: Set <code>use_fp16: true</code> or <code>use_bf16: true</code>. This can nearly halve the model's memory footprint. - Disable Interpretability Features: For large-scale runs, ensure <code>output_hidden_states</code> and <code>output_attentions</code> are <code>False</code>.</p>"},{"location":"faq/#task-specific-issues","title":"Task-Specific Issues","text":""},{"location":"faq/#q-model-outputs-unexpected-scores-or-flat-predictions","title":"Q: Model outputs unexpected scores or flat predictions","text":"<p>Cause: There is a mismatch between the model's architecture and the task it's being used for.</p> <p>Solutions: - Check Model Type vs. Task:   - For classification/regression, fine-tuned models are generally required. Using a base MLM/CLM model without fine-tuning will likely produce random or uniform predictions on a classification task.   - For zero-shot mutation analysis, you should use a base MLM or CLM model with the appropriate <code>task_type</code> (<code>mask</code> or <code>generation</code>) to get meaningful likelihood scores. - Verify Tokenizer: Ensure the tokenizer is appropriate for the model. - Check <code>max_length</code>: If your sequences are being truncated too much, the model may not have enough information to make accurate predictions.</p>"},{"location":"faq/#q-indexerror-target-out-of-bounds-during-trainingevaluation","title":"Q: <code>IndexError: Target out of bounds</code> during training/evaluation","text":"<p>Cause: The labels in your dataset do not match the <code>num_labels</code> specified in your task configuration. For example, your data has labels <code>[0, 1, 2]</code> but you set <code>num_labels: 2</code>.</p> <p>Solution: - Verify <code>num_labels</code>: Ensure <code>num_labels</code> in your YAML configuration correctly reflects the number of unique classes in your dataset. - Check Label Encoding: Make sure your labels are encoded as integers starting from 0 (i.e., <code>0, 1, 2, ...</code>). If your labels are strings or start from 1, they must be preprocessed correctly.</p>"},{"location":"faq/#general-usage-questions","title":"General Usage Questions","text":""},{"location":"faq/#q-how-do-i-choose-the-right-model-for-my-task","title":"Q: How do I choose the right model for my task?","text":"<p>Answer:  - For Classification Tasks: Choose BERT-based models (DNABERT, Plant DNABERT) - For Generation Tasks: Use CausalLM models (Plant DNAGPT, GenomeOcean) - For Large-scale Analysis: Consider Nucleotide Transformer or EVO models - For Plant-specific Tasks: Prefer Plant-prefixed models</p> <p>See the Model Selection Guide for detailed guidance.</p>"},{"location":"faq/#q-what-are-the-system-requirements-for-dnallm","title":"Q: What are the system requirements for DNALLM?","text":"<p>Answer: - Python: 3.10 or higher (Python 3.12 recommended) - GPU: NVIDIA GPU with at least 8GB VRAM recommended for optimal performance - Memory: 16GB RAM minimum, 32GB+ recommended for large models - Storage: At least 10GB free space for model downloads and cache</p>"},{"location":"faq/#q-how-can-i-improve-inference-speed","title":"Q: How can I improve inference speed?","text":"<p>Answer: - Use smaller models for faster inference - Enable mixed precision (FP16/BF16) - Reduce sequence length when possible - Use batch processing for multiple sequences - Consider model quantization for deployment</p> <p>See the Performance Optimization Guide for detailed tips.</p>"},{"location":"faq/#q-where-can-i-find-example-configurations","title":"Q: Where can I find example configurations?","text":"<p>Answer: Example configurations are available in the <code>example/</code> directory of the DNALLM repository. You can also use the interactive configuration generator:</p> <pre><code>dnallm model-config-generator --output my_config.yaml\n</code></pre>"},{"location":"faq/#troubleshooting-guides","title":"Troubleshooting Guides","text":"<p>For specific troubleshooting guides by topic:</p> <ul> <li>Installation Troubleshooting - Common installation issues and solutions</li> <li>Models Troubleshooting - Common model-related issues and solutions</li> <li>Benchmark Troubleshooting - Common issues with model benchmarking</li> <li>Fine-tuning Troubleshooting - Common issues with model fine-tuning</li> <li>Data Processing Troubleshooting - Common issues with data preparation and processing</li> <li>CLI Troubleshooting - Common issues with command-line interface</li> <li>Inference Troubleshooting - Common issues with model inference</li> <li>MCP Troubleshooting - Common issues with Model Context Protocol server</li> </ul>"},{"location":"faq/#still-need-help","title":"Still Need Help?","text":"<p>If you can't find the answer to your question in this FAQ:</p> <ol> <li>Check the Documentation: Browse the User Guide for detailed tutorials and guides</li> <li>Search Issues: Look through existing GitHub Issues</li> <li>Create an Issue: If your problem isn't documented, create a new issue with:</li> <li>A clear description of the problem</li> <li>Steps to reproduce the issue</li> <li>Your system information (OS, Python version, CUDA version)</li> <li>Relevant error messages and logs</li> <li>Join Discussions: Participate in community discussions on GitHub</li> </ol> <p>This FAQ is regularly updated. If you find a solution that's not documented here, please consider contributing to help other users.</p>"},{"location":"faq/benchmark_troubleshooting/","title":"Benchmark Troubleshooting","text":"<p>TBD</p>"},{"location":"faq/cli_troubleshooting/","title":"CLI Troubleshooting","text":"<p>TBD</p>"},{"location":"faq/data_processing_troubleshooting/","title":"Data Processing Troubleshooting","text":"<p>TBD</p>"},{"location":"faq/finetuning_troubleshooting/","title":"Fine-tuning Troubleshooting","text":"<p>TBD</p>"},{"location":"faq/inference_troubleshooting/","title":"Inference Troubleshooting","text":"<p>TBD</p>"},{"location":"faq/install_troubleshooting/","title":"Installation Troubleshooting","text":"<p>TBD</p>"},{"location":"faq/mcp_troubleshooting/","title":"MCP Troubleshooting","text":"<p>TBD</p>"},{"location":"faq/models_troubleshooting/","title":"Models Troubleshooting","text":"<p>TBD</p>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>DNALLM is a comprehensive, open-source toolkit designed for fine-tuning and inference with DNA Language Models. This guide will help you install DNALLM and its dependencies.</p>"},{"location":"getting_started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher (Python 3.12 recommended)</li> <li>Git</li> <li>CUDA-compatible GPU (optional, for GPU acceleration)</li> <li>Environment Manager: Choose one of the following:</li> <li>Python venv (built-in)</li> <li>Conda/Miniconda (recommended for scientific computing)</li> </ul>"},{"location":"getting_started/installation/#quick-installation-with-uv-recommended","title":"Quick Installation with uv (Recommended)","text":"<p>DNALLM uses uv for dependency management and packaging.</p> <p>What is uv is a fast Python package manager that is 10-100x faster than traditional tools like pip.</p>"},{"location":"getting_started/installation/#method-1-using-venv-uv","title":"Method 1: Using venv + uv","text":"<pre><code># Clone repository\ngit clone https://github.com/zhangtaolab/DNALLM.git\ncd DNALLM\n\n# Create virtual environment\npython -m venv .venv\n\n# Activate virtual environment\nsource .venv/bin/activate  # Linux/MacOS\n# or\n.venv\\Scripts\\activate     # Windows\n\n# Upgrade pip (recommended)\npip install --upgrade pip\n\n# Install uv in virtual environment\npip install uv\n\n# Install DNALLM with base dependencies\nuv pip install -e '.[base]'\n\n# Verify installation\npython -c \"import dnallm; print('DNALLM installed successfully!')\"\n</code></pre>"},{"location":"getting_started/installation/#method-2-using-conda-uv","title":"Method 2: Using conda + uv","text":"<pre><code># Clone repository\ngit clone https://github.com/zhangtaolab/DNALLM.git\ncd DNALLM\n\n# Create conda environment\nconda create -n dnallm python=3.12 -y\n\n# Activate conda environment\nconda activate dnallm\n\n# Install uv in conda environment\nconda install uv -c conda-forge\n\n# Install DNALLM with base dependencies\nuv pip install -e '.[base]'\n\n# Verify installation\npython -c \"import dnallm; print('DNALLM installed successfully!')\"\n</code></pre>"},{"location":"getting_started/installation/#gpu-support","title":"GPU Support","text":"<p>For GPU acceleration, install the appropriate CUDA version:</p> <pre><code># For venv users: activate virtual environment\nsource .venv/bin/activate  # Linux/MacOS\n# or\n.venv\\Scripts\\activate     # Windows\n\n# For conda users: activate conda environment\n# conda activate dnallm\n\n# CUDA 12.4 (recommended for recent GPUs)\nuv pip install -e '.[cuda124]'\n\n# Other supported versions: cpu, cuda121, cuda126, cuda128\nuv pip install -e '.[cuda121]'\n</code></pre>"},{"location":"getting_started/installation/#dependency-groups","title":"Dependency Groups","text":"<p>DNALLM provides multiple dependency groups for different use cases:</p>"},{"location":"getting_started/installation/#core-dependency-groups","title":"Core Dependency Groups","text":"Dependency Group Purpose When to Use base Development tools + ML libraries Recommended for most users dev Complete development environment For contributors test Testing environment only For running tests notebook Jupyter and Marimo support For interactive notebooks docs Documentation building For building docs mcp MCP server support For MCP deployment <p>Note: Core ML libraries (torch, transformers, datasets, peft, accelerate) are installed automatically as main dependencies. The groups above add additional functionality.</p>"},{"location":"getting_started/installation/#hardware-specific-groups","title":"Hardware-Specific Groups","text":"Dependency Group PyTorch Version GPU Type When to Use cpu 2.4.0-2.7 CPU only Development without GPU cuda121 2.2.0-2.7 NVIDIA (older) Volta/Turing/Ampere early cuda124 2.4.0-2.7 NVIDIA (recommended) Most modern GPUs cuda126 2.6.0-2.7 NVIDIA (latest) Ada/Hopper with Flash Attention cuda128 2.7.0+ NVIDIA (cutting-edge) Latest hardware rocm 2.5.0-2.7 AMD GPUs AMD GPU users mamba 2.4.0-2.7 NVIDIA + Mamba For Mamba architecture models"},{"location":"getting_started/installation/#installation-scenarios","title":"Installation Scenarios","text":""},{"location":"getting_started/installation/#scenario-1-cpu-only-development","title":"Scenario 1: CPU-only Development","text":"<p>For development and testing without GPU acceleration:</p> <pre><code># Create environment\nconda create -n dnallm-cpu python=3.12 -y\nconda activate dnallm-cpu\n\n# Install base dependencies and CPU version\nuv pip install -e '.[base,cpu]'\n\n# Verify installation\npython -c \"import dnallm; print('DNALLM installed successfully!')\"\n</code></pre>"},{"location":"getting_started/installation/#scenario-2-using-nvidia-gpu-for-training-and-inference","title":"Scenario 2: Using NVIDIA GPU for Training and Inference","text":"<p>For GPU-accelerated training and inference:</p> <pre><code># Determine CUDA version\nnvidia-smi\n\n# Create environment (using CUDA 12.4 as example)\nconda create -n dnallm-gpu python=3.12 -y\nconda activate dnallm-gpu\n\n# Install base dependencies and CUDA 12.4 support\nuv pip install -e '.[base,cuda124]'\n\n# Verify installation\npython -c \"import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')\"\n</code></pre>"},{"location":"getting_started/installation/#scenario-3-using-mamba-model-architecture","title":"Scenario 3: Using Mamba Model Architecture","text":"<p>For models with Mamba architecture (Plant DNAMamba, Caduceus, Jamba-DNA):</p> <pre><code># Create environment\nconda create -n dnallm-mamba python=3.12 -y\nconda activate dnallm-mamba\n\n# Install base dependencies\nuv pip install -e '.[base]'\n\n# Install Mamba support (requires GPU)\nuv pip install -e '.[cuda124,mamba]' --no-cache-dir --no-build-isolation\n\n# Verify installation\npython -c \"from mambapy import Mamba; print('Mamba installed successfully!')\"\n</code></pre>"},{"location":"getting_started/installation/#scenario-4-complete-development-environment","title":"Scenario 4: Complete Development Environment","text":"<p>For contributors and developers:</p> <pre><code># Create environment\nconda create -n dnallm-dev python=3.12 -y\nconda activate dnallm-dev\n\n# Install complete development dependencies\nuv pip install -e '.[dev,notebook,docs,mcp,cuda124]'\n\n# Verify installation\npython -c \"\nimport dnallm\nimport torch\nprint('DNALLM:', dnallm.__version__)\nprint('PyTorch:', torch.__version__)\nprint('CUDA:', torch.version.cuda if torch.cuda.is_available() else 'CPU')\n\"\n</code></pre>"},{"location":"getting_started/installation/#scenario-5-running-mcp-server-only","title":"Scenario 5: Running MCP Server Only","text":"<p>For MCP server deployment:</p> <pre><code># Create environment\nconda create -n dnallm-mcp python=3.12 -y\nconda activate dnallm-mcp\n\n# Install MCP-related dependencies\nuv pip install -e '.[base,mcp,cuda124]'\n\n# Verify installation\npython -c \"from dnallm.mcp import server; print('MCP server dependencies installed!')\"\n</code></pre>"},{"location":"getting_started/installation/#verification","title":"Verification","text":""},{"location":"getting_started/installation/#basic-verification","title":"Basic Verification","text":"<pre><code># Verify DNALLM import\npython -c \"import dnallm; print(f'DNALLM version: {dnallm.__version__}')\"\n\n# Verify core modules\npython -c \"\nfrom dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.datahandling import DNADataset\nfrom dnallm.finetune import DNATrainer\nfrom dnallm.inference import DNAInference\nprint('All core modules imported successfully!')\n\"\n</code></pre>"},{"location":"getting_started/installation/#hardware-verification","title":"Hardware Verification","text":"<pre><code># Verify PyTorch and CUDA\npython -c \"\nimport torch\nprint(f'PyTorch version: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'CUDA version: {torch.version.cuda}')\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')\n\"\n\n# Verify Mamba (if installed)\npython -c \"\ntry:\n    from mambapy import Mamba\n    print('Mamba: Available')\nexcept ImportError:\n    print('Mamba: Not installed')\n\"\n</code></pre>"},{"location":"getting_started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting_started/installation/#cuda-version-mismatch","title":"CUDA Version Mismatch","text":"<p>Issue: Installed PyTorch CUDA version doesn't match system CUDA version</p> <p>Solution: <pre><code># 1. Check system CUDA version\nnvidia-smi\nnvcc --version\n\n# 2. Uninstall installed torch\nuv pip uninstall torch torchvision torchaudio\n\n# 3. Reinstall matching version\nuv pip install -e '.[cuda121]'  # Choose based on actual situation\n</code></pre></p>"},{"location":"getting_started/installation/#mamba-installation-failure","title":"Mamba Installation Failure","text":"<p>Issue: mamba-ssm or causal_conv1d installation fails</p> <p>Solution: <pre><code># 1. Install compilation dependencies\nconda install -c conda-forge gxx clang ninja\n\n# 2. Clear cache and reinstall\nrm -rf .venv/lib/python*/site-packages/mamba_ssm*\nrm -rf .venv/lib/python*/site-packages/causal_conv1d*\nuv pip install -e '.[mamba]' --no-cache-dir --no-build-isolation\n\n# 3. Or use installation script\nsh scripts/install_mamba.sh\n</code></pre></p>"},{"location":"getting_started/installation/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>Issue: Dependency conflicts during installation</p> <p>Solution: <pre><code># 1. Create new environment\nconda create -n dnallm-new python=3.12 -y\nconda activate dnallm-new\n\n# 2. Use uv to resolve dependencies\nuv pip install -e '.[base]' --resolution=lowest\n</code></pre></p>"},{"location":"getting_started/installation/#native-mamba-support","title":"Native Mamba Support","text":"<p>Native Mamba architecture runs significantly faster than transformer-compatible Mamba architecture, but native Mamba depends on Nvidia GPUs.</p> <p>If you need native Mamba architecture support, after installing DNALLM dependencies, use the following command:</p> <pre><code># For venv users: activate virtual environment\nsource .venv/bin/activate  # Linux/MacOS\n\n# For conda users: activate conda environment\n# conda activate dnallm\n\n# Install Mamba support\nuv pip install -e '.[mamba]' --no-cache-dir --no-build-isolation\n\n# If encounter network or compile issue, using the special install script for mamba (optional)\nsh scripts/install_mamba.sh  # select github proxy\n</code></pre> <p>Please ensure your machine can connect to GitHub, otherwise Mamba dependencies may fail to download.</p>"},{"location":"getting_started/installation/#additional-model-dependencies","title":"Additional Model Dependencies","text":""},{"location":"getting_started/installation/#specialized-model-dependencies","title":"Specialized Model Dependencies","text":"<p>Some models use their own developed model architectures that haven't been integrated into HuggingFace's transformers library yet. Therefore, fine-tuning and inference for these models require pre-installing the corresponding model dependency libraries:</p>"},{"location":"getting_started/installation/#evo2","title":"EVO2","text":"<p><code>EVO2</code> model fine-tuning and inference depends on its own software package or third-party Python library1/library2:</p> <pre><code># evo2 requires python version &gt;=3.11\n# Install transformer torch engine\nuv pip install \"transformer-engine[pytorch]==2.3.0\" --no-build-isolation --no-cache-dir\n# Install evo2\nuv pip install evo2\n# (Optional) Install flash attention 2\nuv pip install \"flash_attn&lt;=2.7.4.post1\" --no-build-isolation --no-cache-dir\n## Note that build transformer-engine and flash-attn package will cost much time.\n\n# add cudnn path to environment\nexport LD_LIBRARY_PATH=[path_to_DNALLM]/.venv/lib64/python3.11/site-packages/nvidia/cudnn/lib:${LD_LIBRARY_PATH}\n</code></pre>"},{"location":"getting_started/installation/#evo-1","title":"EVO-1","text":"<pre><code># Install evo-1 model\nuv pip install evo-model\n# (Optional) Install flash attention\nuv pip install \"flash_attn&lt;=2.7.4.post1\" --no-build-isolation --no-cache-dir\n</code></pre>"},{"location":"getting_started/installation/#gpn","title":"GPN","text":"<p>Project address: https://github.com/songlab-cal/gpn <pre><code>uv pip install git+https://github.com/songlab-cal/gpn.git\n</code></pre></p>"},{"location":"getting_started/installation/#megadna","title":"megaDNA","text":"<p>Note that megaDNA weights stored at the Hugging Face can be accessed after requesting permission from the author.</p> <p>Project address: https://github.com/lingxusb/megaDNA</p> <pre><code>git clone https://github.com/lingxusb/megaDNA\ncd megaDNA\nuv pip install .\n</code></pre>"},{"location":"getting_started/installation/#lucaone","title":"LucaOne","text":"<p>Project address: https://github.com/LucaOne/LucaOneTasks <pre><code>uv pip install lucagplm\n</code></pre></p>"},{"location":"getting_started/installation/#omni-dna","title":"Omni-DNA","text":"<p>Project address: https://huggingface.co/zehui127/Omni-DNA-20M <pre><code>uv pip install ai2-olmo\n</code></pre></p>"},{"location":"getting_started/installation/#enformer","title":"Enformer","text":"<p>Project address: https://github.com/lucidrains/enformer-pytorch <pre><code>uv pip install enformer-pytorch\n</code></pre></p>"},{"location":"getting_started/installation/#borzoi","title":"Borzoi","text":"<p>Project address: https://github.com/johahi/borzoi-pytorch <pre><code>uv pip install borzoi-pytorch\n</code></pre></p> <p>Some models require support from other dependencies. We will continue to add dependencies requirement for different models.</p>"},{"location":"getting_started/installation/#flash-attention-support","title":"Flash Attention Support","text":"<p>Some models support Flash Attention acceleration. If you need to install this dependency, you can refer to the project GitHub for installation. Note that <code>flash-attn</code> versions are tied to different Python versions, PyTorch versions, and CUDA versions. Please first check if there are matching version installation packages in GitHub Releases, otherwise you may encounter <code>HTTP Error 404: Not Found</code> errors.</p> <pre><code>uv pip install flash-attn --no-build-isolation --no-cache-dir\n</code></pre>"},{"location":"getting_started/installation/#compilation-dependencies","title":"Compilation Dependencies","text":"<p>If compilation is required during installation and compilation errors occur, please first install the dependencies that may be needed. We recommend using <code>conda</code> to install dependencies.</p> <pre><code>conda install -c conda-forge gxx clang\n</code></pre>"},{"location":"getting_started/installation/#verify-installation","title":"Verify Installation","text":"<p>Check if installation was successful:</p> <pre><code># Test basic functionality\npython -c \"import dnallm; print('DNALLM installed successfully!')\"\n\n# Run comprehensive tests\nsh tests/test_all.sh\n</code></pre>"},{"location":"getting_started/quick_start/","title":"Quick Start","text":"<p>This guide will help you get started with DNALLM quickly. DNALLM is a comprehensive, open-source toolkit designed for fine-tuning and inference with DNA Language Models.</p>"},{"location":"getting_started/quick_start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher (Python 3.12 recommended)</li> <li>Git</li> <li>CUDA-compatible GPU (optional, for GPU acceleration)</li> <li>Environment Manager: Choose one of the following:</li> <li>Python venv (built-in)</li> <li>Conda/Miniconda (recommended for scientific computing)</li> </ul>"},{"location":"getting_started/quick_start/#installation","title":"Installation","text":""},{"location":"getting_started/quick_start/#quick-installation-with-uv-recommended","title":"Quick Installation with uv (Recommended)","text":"<p>DNALLM uses uv for dependency management and packaging.</p> <p>What is uv is a fast Python package manager that is 10-100x faster than traditional tools like pip.</p>"},{"location":"getting_started/quick_start/#method-1-using-venv-uv","title":"Method 1: Using venv + uv","text":"<pre><code># Clone repository\ngit clone https://github.com/zhangtaolab/DNALLM.git\ncd DNALLM\n\n# Create virtual environment\npython -m venv .venv\n\n# Activate virtual environment\nsource .venv/bin/activate  # Linux/MacOS\n# or\n.venv\\Scripts\\activate     # Windows\n\n# Upgrade pip (recommended)\npip install --upgrade pip\n\n# Install uv in virtual environment\npip install uv\n\n# Install DNALLM with base dependencies\nuv pip install -e '.[base]'\n\n# Verify installation\npython -c \"import dnallm; print('DNALLM installed successfully!')\"\n</code></pre>"},{"location":"getting_started/quick_start/#method-2-using-conda-uv","title":"Method 2: Using conda + uv","text":"<pre><code># Clone repository\ngit clone https://github.com/zhangtaolab/DNALLM.git\ncd DNALLM\n\n# Create conda environment\nconda create -n dnallm python=3.12 -y\n\n# Activate conda environment\nconda activate dnallm\n\n# Install uv in conda environment\nconda install uv -c conda-forge\n\n# Install DNALLM with base dependencies\nuv pip install -e '.[base]'\n\n# Verify installation\npython -c \"import dnallm; print('DNALLM installed successfully!')\"\n</code></pre>"},{"location":"getting_started/quick_start/#gpu-support","title":"GPU Support","text":"<p>For GPU acceleration, install the appropriate CUDA version:</p> <pre><code># For venv users: activate virtual environment\nsource .venv/bin/activate  # Linux/MacOS\n# or\n.venv\\Scripts\\activate     # Windows\n\n# For conda users: activate conda environment\n# conda activate dnallm\n\n# CUDA 12.4 (recommended for recent GPUs)\nuv pip install -e '.[cuda124]'\n\n# Other supported versions: cpu, cuda121, cuda126, cuda128\nuv pip install -e '.[cuda121]'\n</code></pre>"},{"location":"getting_started/quick_start/#native-mamba-support","title":"Native Mamba Support","text":"<p>Native Mamba architecture runs significantly faster than transformer-compatible Mamba architecture, but native Mamba depends on Nvidia GPUs.</p> <p>If you need native Mamba architecture support, after installing DNALLM dependencies, use the following command:</p> <pre><code># For venv users: activate virtual environment\nsource .venv/bin/activate  # Linux/MacOS\n# or\n.venv\\Scripts\\activate     # Windows\n\n# For conda users: activate conda environment\n# conda activate dnallm\n\n# Install Mamba support\nuv pip install -e '.[mamba]' --no-cache-dir --no-build-isolation\n</code></pre> <p>Please ensure your machine can connect to GitHub, otherwise Mamba dependencies may fail to download.</p>"},{"location":"getting_started/quick_start/#basic-usage","title":"Basic Usage","text":""},{"location":"getting_started/quick_start/#1-basic-model-loading-and-inference","title":"1. Basic Model Loading and Inference","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.inference import DNAInference\n\n# Load configuration\nconfigs = load_config(\"./example/notebooks/inference/inference_config.yaml\")\n\n# Load model and tokenizer\nmodel_name = \"zhangtaolab/plant-dnagpt-BPE-promoter\"\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name, \n    task_config=configs[\"task\"], \n    source=\"huggingface\"\n)\n\n# Initialize inference engine\ninference_engine = DNAInference(config=configs, model=model, tokenizer=tokenizer)\n\n# Make inference\nsequence = \"TCACATCCGGGTGAAACCTCGAGTTCCTATAACCTGCCGACAGGTGGCGGGTCTTATAAAACTGATCACTACAATTCCCAATGGAAAAAAAAAAAAAAAAACCCTTATTTGACTCTCATTATAGATCAACGATGGATCTAGCTCTTCTTTTGTAATTACCTGACTTTTGACCTGACGAACCAAGTTATCGGTTGGGGCCCTGTCAAACGACAGGTCGCTTAGAGGGCATATGTGAGAAAAAGGGTCCTGTTTTTTATCCACGGAGAAAGAAAGCAAGAAGAGGAGAGGTTTTAAAAAAAA\"\ninference_result = inference_engine.infer(sequence)\nprint(f\"Inference result: {inference_result}\")\n</code></pre>"},{"location":"getting_started/quick_start/#2-in-silico-mutagenesis-analysis","title":"2. In-silico Mutagenesis Analysis","text":"<pre><code>from dnallm import load_config\nfrom dnallm.inference import Mutagenesis\n\n# Load configuration\nconfigs = load_config(\"./example/notebooks/in_silico_mutagenesis/inference_config.yaml\")\n\n# Load model and tokenizer\nmodel_name = \"zhangtaolab/plant-dnagpt-BPE-promoter_strength_protoplast\"\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name,\n    task_config=configs[\"task\"],\n    source=\"huggingface\"\n)\n\n# Initialize mutagenesis analyzer\nmutagenesis = Mutagenesis(config=configs, model=model, tokenizer=tokenizer)\n\n# Generate saturation mutations\nsequence = \"AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC\"\nmutagenesis.mutate_sequence(sequence, replace_mut=True)\n\n# Evaluate mutation effects\npredictions = mutagenesis.evaluate(strategy=\"mean\")\n\n# Visualize results\nplot = mutagenesis.plot(predictions, save_path=\"mutation_effects.pdf\")\n</code></pre>"},{"location":"getting_started/quick_start/#3-model-fine-tuning","title":"3. Model Fine-tuning","text":"<pre><code>from dnallm import load_config\nfrom dnallm.datahandling import DNADataset\nfrom dnallm.finetune import DNATrainer\n\n# Load configuration\nconfigs = load_config(\"./example/notebooks/finetune_binary/finetune_config.yaml\")\n\n# Load model and tokenizer\nmodel_name = \"zhangtaolab/plant-dnabert-BPE\"\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name,\n    task_config=configs[\"task\"],\n    source=\"huggingface\"\n)\n\n# Prepare dataset\ndataset = DNADataset.load_local_data(\n    file_paths=\"./tests/test_data/binary_classification/train.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    tokenizer=tokenizer,\n)\n\n# Encode the sequences in the dataset\ndataset.encode_sequences()\n\n# Initialize trainer\ntrainer = DNATrainer(\n    config=configs,\n    model=model,\n    datasets=dataset\n)\n\n# Start training\ntrainer.train()\n</code></pre>"},{"location":"getting_started/quick_start/#4-models-benchmark","title":"4. Models Benchmark","text":"<pre><code>from dnallm import load_config\nfrom dnallm.inference import Benchmark\n\n# Load configuration\nconfigs = load_config(\"./example/notebooks/benchmark/benchmark_config.yaml\")\n\n# Initialize benchmark\nbenchmark = Benchmark(config=configs)\n\n# Run benchmark\nresults = benchmark.run()\n\n# Display results\nfor dataset_name, dataset_results in results.items():\n    print(f\"\\n{dataset_name}:\")\n    for model_name, metrics in dataset_results.items():\n        print(f\"  {model_name}:\")\n        for metric, value in metrics.items():\n            if metric not in [\"curve\", \"scatter\"]:\n                print(f\"    {metric}: {value:.4f}\")\n\n# Plot metrics\n# pbar: bar chart for all the scores, pline: ROC curve\npbar, pline = benchmark.plot(results, save_path=\"plot.pdf\")\n</code></pre>"},{"location":"getting_started/quick_start/#examples-and-tutorials","title":"Examples and Tutorials","text":""},{"location":"getting_started/quick_start/#interactive-demos-marimo","title":"Interactive Demos (Marimo)","text":"<pre><code># Launch Jupyter Lab\nuv run jupyter lab\n\n# Launch Marimo\nuv run marimo run xxx.py\n\n# Fine-tuning demo\nuv run marimo run example/marimo/finetune/finetune_demo.py\n\n# Inference demo\nuv run marimo run example/marimo/inference/inference_demo.py\n\n# Benchmark demo\nuv run marimo run example/marimo/benchmark/benchmark_demo.py\n</code></pre>"},{"location":"getting_started/quick_start/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<pre><code># Launch Jupyter Lab\nuv run jupyter lab\n\n# Available notebooks:\n# - example/notebooks/finetune_binary/ - Binary classification fine-tuning\n# - example/notebooks/finetune_multi_labels/ - Multi-label classification\n# - example/notebooks/finetune_NER_task/ - Named Entity Recognition\n# - example/notebooks/inference/ - Model inference\n# - example/notebooks/in_silico_mutagenesis/ - Mutation analysis\n# - example/notebooks/embedding_attention.ipynb - Embedding and attention analysis\n# - example/notebooks/generation_evo_models/ - EVO model inference\n# - example/notebooks/lora_finetune_inference/ - LoRA fine-tuning\n</code></pre>"},{"location":"getting_started/quick_start/#command-line-interface","title":"Command Line Interface","text":"<p>DNALLM provides convenient CLI tools:</p> <pre><code># Training\ndnallm-train --config path/to/config.yaml\n\n# Inference\ndnallm-inference --config path/to/config.yaml --input path/to/sequences.txt\n\n# Model configuration generator\ndnallm-model-config-generator\n\n# MCP server\ndnallm-mcp-server --config path/to/config.yaml\n</code></pre>"},{"location":"getting_started/quick_start/#supported-task-types","title":"Supported Task Types","text":"<p>DNALLM supports the following task types:</p> <ul> <li>EMBEDDING: Extract embeddings, attention maps, and token probabilities for downstream analysis</li> <li>MASK: Masked language modeling task for pre-training</li> <li>GENERATION: Text generation task for causal language models</li> <li>BINARY: Binary classification task with two possible labels</li> <li>MULTICLASS: Multi-class classification task that specifies which class the input belongs to (more than two)</li> <li>MULTILABEL: Multi-label classification task with multiple binary labels per sample</li> <li>REGRESSION: Regression task which returns a continuous score</li> <li>NER: Token classification task which is usually for Named Entity Recognition</li> </ul>"},{"location":"getting_started/quick_start/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API documentation for detailed function references</li> <li>Check out user guides for specific use cases</li> <li>Visit the FAQ for common questions</li> <li>Join the community discussions on GitHub</li> </ul>"},{"location":"getting_started/quick_start/#need-more-details","title":"Need More Details?","text":"<ul> <li>See Installation Guide for complete dependency information, including:</li> <li>All available dependency groups (base, dev, test, notebook, docs, mcp)</li> <li>Hardware-specific groups (cpu, cuda121, cuda124, cuda126, cuda128, rocm, mamba)</li> <li>Installation scenarios for different use cases</li> <li>Troubleshooting common issues</li> </ul>"},{"location":"getting_started/quick_start/#need-help","title":"Need Help?","text":"<ul> <li>Documentation: Browse the complete documentation</li> <li>Issues: Report bugs or request features on GitHub</li> <li>Examples: Check the example notebooks for working code</li> <li>Configuration: Refer to the configuration examples in the docs</li> </ul>"},{"location":"resources/datahandling/","title":"Datahandling","text":"<p>The datasets module is primarily used for reading online or local datasets, generating datasets, or processing already loaded datasets. The module defines a <code>DNADataset</code> class.</p>"},{"location":"resources/datahandling/#overview","title":"Overview","text":"<p>The <code>DNADataset</code> class provides comprehensive functionality for DNA sequence data management and processing, including:</p> <ul> <li> <p>Data Loading: Read data from online sources like <code>HuggingFace</code> or <code>ModelScope</code>, or from local files supporting multiple formats (.csv, .tsv, .json, .arrow, .parquet, .txt, dict, fasta, etc.)</p> </li> <li> <p>Data Validation: Filter sequences based on length, GC content, and valid base composition requirements</p> </li> <li> <p>Data Cleaning: Remove entries with missing sequence or label information</p> </li> <li> <p>Data Augmentation:</p> </li> <li>Random reverse complement generation</li> <li>Add reverse complement sequences (doubles the original dataset size)</li> <li> <p>Generate random synthetic data with configurable sequence length, count, GC content distribution, N-base inclusion, and padding requirements</p> </li> <li> <p>Data Shuffling: Randomize dataset order</p> </li> <li> <p>Data Splitting: Divide datasets into train/validation/test sets</p> </li> <li> <p>Sequence Tokenization: Convert DNA sequences to model-compatible tokens</p> </li> </ul>"},{"location":"resources/datahandling/#examples","title":"Examples","text":""},{"location":"resources/datahandling/#basic-setup","title":"Basic Setup","text":"<pre><code>from dnallm import DNADataset\nfrom transformers import AutoTokenizer\n</code></pre>"},{"location":"resources/datahandling/#data-loading","title":"Data Loading","text":"<pre><code># Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"zhangtaolab/plant-dnabert-BPE\")\n\n# Load data\n# 1. Load local data (specify sequence and label column headers)\n# 1.1 Single file\ndna_ds = DNADataset.load_local_data(\n    \"/path_to_your_datasets/data.csv\", \n    seq_col=\"sequence\", \n    label_col=\"labels\",\n    tokenizer=tokenizer, \n    max_length=512\n)\n\n# 1.2 Multiple files (e.g., pre-split datasets)\ndna_ds = DNADataset.load_local_data(\n    {\n        \"train\": \"train.csv\", \n        \"test\": \"test.csv\", \n        \"validation\": \"validation.csv\"\n    },\n    seq_col=\"sequence\", \n    label_col=\"labels\",\n    tokenizer=tokenizer, \n    max_length=512\n)\n\n# 2. Load online data\n# 2.1 From HuggingFace\ndna_ds = DNADataset.from_huggingface(\n    \"zhangtaolab/plant-multi-species-open-chromatin\", \n    seq_col=\"sequence\", \n    label_col=\"label\", \n    tokenizer=tokenizer, \n    max_length=512\n)\n\n# 2.2 From ModelScope\ndna_ds = DNADataset.from_modelscope(\n    \"zhangtaolab/plant-multi-species-open-chromatin\", \n    seq_col=\"sequence\", \n    label_col=\"label\", \n    tokenizer=tokenizer, \n    max_length=512\n)\n\n# 2.3 From preset dataset\nshow_preset_dataset()  # Show available preset dataset\ndna_ds = load_preset_dataset(\n    dataset_name='plant-genomic-benchmark',\n    task='promoter_strength.leaf'\n)\n</code></pre>"},{"location":"resources/datahandling/#data-processing-and-augmentation","title":"Data Processing and Augmentation","text":"<pre><code># Common functionality demonstration\n\n# 1. Data validation (filter sequences by length, GC content, valid base composition)\ndna_ds.validate_sequences(minl=200, maxl=1000, valid_chars=\"ACGT\")\n\n# 2. Data cleaning (remove entries with missing sequence or label information)\ndna_ds.process_missing_data()\n\n# 3. Data splitting\ndna_ds.split_data(test_size=0.2, val_size=0.1)\n\n# 4. Data shuffling\ndna_ds.shuffle(seed=42)\n\n# 5. Data augmentation\n# 5.1 Random reverse complement\ndna_ds.raw_reverse_complement(ratio=0.5)  # Apply reverse complement to 50% of sequences\n\n# 5.2 Add reverse complement sequences (doubles the original dataset size)\ndna_ds.augment_reverse_complement()\n\n# 5.3 Generate random synthetic data\ndna_ds.random_generate(\n    minl=200,           # Minimum sequence length\n    maxl=2000,          # Maximum sequence length\n    samples=3000,       # Number of sequences to generate\n    gc=(0.1, 0.9),     # GC content range\n    N_ratio=0.0,        # Ratio of N bases to include\n    padding_size=1,     # Ensure sequences are multiples of this value\n    append=True,         # Append to existing dataset\n    label_func=None     # Custom function for generating labels\n)\n\n# Note: label_func is a custom function for generating data labels\n# If append=True is not specified, generated data will replace the original dataset\n# (useful for random dataset initialization)\n\n# 6. Data downsampling\nnew_ds = dna_ds.sampling(ratio=0.1)\n\n# 7. Data inspection\ndna_ds.show(head=20)          # Display first N formatted data entries\ntmp_ds = dna_ds.head(head=5)  # Extract first N data entries\n\n# 8. Sequence tokenization (requires DNADataset.tokenizer to be defined)\ndna_ds.encode_sequences()\n\n# 9. Data statistics\ndna_ds.statistics()\n\n# 10. Data statistics visualization\ndna_ds.plot_statistics()\n</code></pre>"},{"location":"resources/datahandling/#api-reference","title":"API Reference","text":"<p>For detailed function documentation and parameter descriptions, please refer to the API Reference.</p>"},{"location":"resources/datahandling/#key-features-summary","title":"Key Features Summary","text":"Feature Description Method Data Loading Load from HuggingFace, ModelScope, or local files <code>load_local_data()</code>, <code>from_huggingface()</code>, <code>from_modelscope()</code> Validation Filter by length, GC content, base composition <code>validate_sequences()</code> Cleaning Remove missing data entries <code>process_missing_data()</code> Augmentation Generate synthetic sequences and variants <code>raw_reverse_complement()</code>, <code>augment_reverse_complement()</code>, <code>random_generate()</code> Processing Split, shuffle, and sample datasets <code>split_data()</code>, <code>shuffle()</code>, <code>sampling()</code> Tokenization Convert sequences to model tokens <code>encode_sequences()</code>"},{"location":"resources/datahandling/#tips","title":"Tips","text":"<ul> <li>Always validate your data before processing to ensure quality</li> <li>Use data augmentation to increase dataset size and improve model robustness</li> <li>Consider GC content distribution when generating synthetic sequences</li> <li>Set appropriate sequence length limits based on your model's requirements</li> </ul>"},{"location":"resources/model_selection/","title":"Model Selection Guide","text":"<p>Choosing the right DNA language model is crucial for the success of your analysis. DNALLM supports a wide array of models, each with unique architectures, training data, and strengths. This guide will help you understand the different model types, browse the available options, and select the best model for your task.</p> <p>Related Documents: - Model Zoo - Installation Guide</p>"},{"location":"resources/model_selection/#1-overview-of-dna-llm-categories","title":"1. Overview of DNA LLM Categories","text":"<p>DNA language models primarily fall into two categories based on their training objective:</p>"},{"location":"resources/model_selection/#causal-language-models-clm","title":"Causal Language Models (CLM)","text":"<ul> <li>How they work: CLMs are trained to predict the next nucleotide (or token) in a sequence given all the preceding ones. They process sequences in a single direction (forward).</li> <li>Architectures: GPT, Llama, Mamba, HyenaDNA.</li> <li>Best for:<ul> <li>Sequence Generation: Designing new DNA sequences (e.g., promoters, enhancers).</li> <li>Sequence Scoring: Calculating the overall likelihood or \"fitness\" of a sequence. This is useful for zero-shot mutation effect prediction.</li> <li>Tasks where understanding the full context from start to finish is important.</li> </ul> </li> </ul>"},{"location":"resources/model_selection/#masked-language-models-mlm","title":"Masked Language Models (MLM)","text":"<ul> <li>How they work: MLMs are trained to predict a masked (hidden) nucleotide or token within a sequence by looking at both the preceding and following context (bi-directional).</li> <li>Architectures: BERT, RoBERTa, ESM, Caduceus.</li> <li>Best for:<ul> <li>Sequence Classification: Predicting functional labels for a whole sequence (e.g., \"is this a promoter?\"). Their bi-directional nature allows them to capture a holistic representation.</li> <li>Token Classification: Predicting a label for each nucleotide/token in a sequence (e.g., identifying transcription factor binding sites).</li> <li>Feature Extraction: Generating high-quality embeddings that capture rich contextual information.</li> </ul> </li> </ul>"},{"location":"resources/model_selection/#2-introduction-to-base-model-architectures","title":"2. Introduction to Base Model Architectures","text":"<p>DNALLM integrates models built on several foundational architectures:</p> <ul> <li>BERT-based (e.g., DNABERT, Plant DNABERT): The classic bi-directional transformer. Excellent for classification and generating sequence embeddings.</li> <li>GPT-based (e.g., Plant DNAGPT): A popular auto-regressive (causal) model. Strong at generation and scoring tasks.</li> <li>HyenaDNA / StripedHyena (e.g., HyenaDNA, Evo): A newer, convolution-based architecture designed to handle extremely long sequences more efficiently than standard transformers. Ideal for modeling entire genes or genomic regions.</li> <li>Mamba (e.g., Plant DNAMamba): A state-space model (SSM) that offers a balance between the performance of transformers and the efficiency of convolutional models, particularly for long sequences.</li> <li>Caduceus: A specialized bi-directional architecture that uses an S4 model to handle long-range dependencies in DNA, making it very effective for regulatory genomics.</li> </ul>"},{"location":"resources/model_selection/#3-supported-models-and-their-properties","title":"3. Supported Models and Their Properties","text":"<p>DNALLM provides access to a vast collection of pre-trained and fine-tuned models. You can find a complete, up-to-date list in our Model Zoo.</p> <p>Here is a summary of key model families available in DNALLM:</p> Model Family Category Examples Key Feature DNABERT Series MLM DNABERT, DNABERT-2, Plant DNABERT, DNABERT-S Widely used bi-directional models, great for classification. Caduceus Series MLM Caduceus-Ph, Caduceus-PS, PlantCaduceus Specialized for long-range, single-nucleotide resolution. Nucleotide Transformer MLM nucleotide-transformer-2.5b-multi-species Large, powerful models trained on multi-species data. Other MLMs MLM AgroNT, GENA-LM, GPN, GROVER, MutBERT, ProkBERT A variety of specialized models for specific domains. EVO Series CLM EVO-1, EVO-2 State-of-the-art generative models for very long sequences. Plant CLMs CLM Plant DNAGemma, Plant DNAGPT, Plant DNAMamba A suite of causal models pre-trained on plant genomes. HyenaDNA CLM HyenaDNA Efficient convolution-based model for long sequences. Other CLMs CLM GENERator, GenomeOcean, Jamba-DNA, Mistral-DNA Other generative models with diverse architectures."},{"location":"resources/model_selection/#4-general-model-loading-and-usage","title":"4. General Model Loading and Usage","text":"<p>Loading any supported model in DNALLM is straightforward using the <code>load_model_and_tokenizer</code> function. The library handles downloading the model from its source (Hugging Face or ModelScope) and configuring it for the specified task.</p>"},{"location":"resources/model_selection/#basic-loading","title":"Basic Loading","text":"<p>You need to provide the model's name/ID, a <code>task_config</code>, and the <code>source</code>.</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer\n\n# 1. Load a configuration file that defines the task\n# For a fine-tuned classification model:\nconfigs = load_config(\"path/to/your/finetune_config.yaml\")\n# The config specifies task_type, num_labels, etc.\n\n# 2. Load the model and tokenizer\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name=\"zhangtaolab/plant-dnabert-BPE-promoter\",\n    task_config=configs['task'],\n    source=\"modelscope\" # or \"huggingface\"\n)\n\nprint(f\"Model class: {model.__class__.__name__}\")\n# &gt;&gt; Model class: BertForSequenceClassification\n</code></pre>"},{"location":"resources/model_selection/#loading-a-base-model-for-a-new-task","title":"Loading a Base Model for a New Task","text":"<p>If you want to fine-tune a base pre-trained model, you must configure the <code>task</code> section of your YAML file to match the new task. <code>load_model_and_tokenizer</code> will then add the appropriate classification/regression head to the model.</p> <p>Example <code>finetune_config.yaml</code> for binary classification:</p> <pre><code>task:\n  task_type: binary\n  num_labels: 2\n  label_names: [\"Not Promoter\", \"Promoter\"]\n</code></pre> <p>Loading the base model with the new head:</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer\n\nconfigs = load_config(\"finetune_config.yaml\")\n\n# Load a base MLM model and add a classification head\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name=\"zhihan1996/DNABERT-2-117M\",\n    task_config=configs['task'],\n    source=\"huggingface\"\n)\n\n# The model is now ready for fine-tuning on a binary classification task.\nprint(f\"Model class: {model.__class__.__name__}\")\n# &gt;&gt; Model class: BertForSequenceClassification\n</code></pre>"},{"location":"resources/model_selection/#5-model-selection-recommendations","title":"5. Model Selection Recommendations","text":"<p>Here are some general guidelines for choosing a model. Always consider benchmarking several candidates on your specific data.</p>"},{"location":"resources/model_selection/#for-sequence-classification-eg-promoter-enhancer-prediction","title":"For Sequence Classification (e.g., promoter, enhancer prediction)","text":"<ul> <li>Good Start: <code>zhangtaolab/plant-dnabert-BPE</code> (for plants), <code>zhihan1996/DNABERT-2-117M</code> (for general genomics).</li> <li>High Performance: <code>InstaDeepAI/nucleotide-transformer-2.5b-multi-species</code>. Larger models often yield better results but require more compute.</li> <li>Long Sequences (&gt; 4kb): <code>kuleshov-group/PlantCAD2-Small-l24-d0768</code>. The Caduceus architecture is designed for this.</li> </ul>"},{"location":"resources/model_selection/#for-sequence-generation-or-zero-shot-scoring","title":"For Sequence Generation or Zero-Shot Scoring","text":"<ul> <li>Good Start: <code>zhangtaolab/plant-dnagpt-BPE</code> (for plants), <code>LongSafari/hyenadna-small-32k-seqlen-hf</code>.</li> <li>High Performance / Long Context: <code>arcinstitute/evo-1-131k-base</code> or <code>lgq12697/evo2_1b_base</code>. These are state-of-the-art but require significant resources.</li> </ul>"},{"location":"resources/model_selection/#for-token-classification-eg-finding-binding-sites","title":"For Token Classification (e.g., finding binding sites)","text":"<ul> <li>Good Start: Any BERT-based model like <code>zhihan1996/DNABERT-2-117M</code>.</li> <li>High Resolution: Models with single-base tokenizers can provide nucleotide-level predictions.</li> </ul>"},{"location":"resources/model_selection/#for-a-specific-domain","title":"For a Specific Domain","text":"<ul> <li>Plants: Start with models from the <code>zhangtaolab</code> or <code>kuleshov-group/PlantCAD</code> series.</li> <li>Human: <code>InstaDeepAI/nucleotide-transformer-500m-human-ref</code> is a strong choice.</li> <li>Microbiome: <code>neuralbioinfo/prokbert-mini</code> is trained on prokaryotic genomes.</li> </ul> <p>Next, if you encounter any issues with model loading or usage, please refer to our Model Troubleshooting Guide.</p>"},{"location":"resources/model_zoo/","title":"Model Zoo","text":"<p>DNALLM includes almost all publicly available DNA Large Language Models and some DNA-based deep learning models. We have adapted these models to work seamlessly with the DNALLM package for fine-tuning and inference.</p>"},{"location":"resources/model_zoo/#model-collection","title":"Model Collection","text":"<p>The following table shows all currently supported models and their fine-tuning/inference capabilities:</p> Model Name Model Type Architecture Fine-tuning Support Author Model Size Count Source Plant DNABERT MaskedLM BERT \u2705 zhangtaolab 100M 1 Molecular Plant Plant DNAGPT CausalLM GPT2 \u2705 zhangtaolab 100M 1 Molecular Plant Plant Nucleotide Transformer MaskedLM ESM \u2705 zhangtaolab 100M 1 Molecular Plant Plant DNAGemma CausalLM Gemma \u2705 zhangtaolab 150M 1 Molecular Plant Plant DNAMamba CausalLM Mamba \u2705 zhangtaolab 100M 1 Molecular Plant Plant DNAModernBert MaskedLM ModernBert \u2705 zhangtaolab 100M 1 Molecular Plant Nucleotide Transformer MaskedLM ESM \u2705 InstaDeepAI 50M / 100M / 250M / 500M / 2.5B 8 Nature Methods AgroNT MaskedLM ESM \u2705 InstaDeepAI 1B 1 Current Biology Caduceus-Ph MaskedLM Caduceus \u2705 Kuleshov-Group 0.5M / 2M / 8M 3 arXiv Caduceus-Ps MaskedLM Caduceus \u2705 Kuleshov-Group 0.5M / 2M / 8M 3 arXiv PlantCaduceus MaskedLM Caduceus \u2705 Kuleshov-Group 20M / 40M / 112M / 225M 4 PNAS PlantCAD2 MaskedLM Caduceus \u2705 Kuleshov-Group 88M / 311M / 694M 3 bioRxiv DNABERT MaskedLM BERT \u2705 Zhihan1996 100M 4 Bioinformatics DNABERT-2 MaskedLM BERT \u2705 Zhihan1996 117M 1 arXiv DNABERT-S MaskedLM BERT \u2705 Zhihan1996 117M 1 arXiv EVO-1 CausalLM StripedHyena \u274c togethercomputer 6.5B 2 Science EVO-2 CausalLM StripedHyena2 \u274c arcinstitute 1B / 1.5B / 7B / 40B 4 bioRxiv GENA-LM MaskedLM BERT \u2705 AIRI-Institute 150M / 500M 7 Nucleic Acids Research GENA-LM-BigBird MaskedLM BigBird \u2705 AIRI-Institute 150M 3 Nucleic Acids Research GENERator CausalLM Llama \u2705 GenerTeam 1.2B / 3B 2 arXiv GENERanno CausalLM Generanno \u2705 GenerTeam 0.5B 2 bioRxiv GenomeOcean CausalLM Mistral \u2705 DOEJGI 100M / 500M / 4B 3 bioRxiv GPN MaskedLM ConvNet \u2705 songlab 60M 1 PNAS GROVER MaskedLM BERT \u2705 PoetschLab 100M 1 Nature Machine Intelligence HyenaDNA CausalLM HyenaDNA \u2705 LongSafari 0.5M / 0.7M / 2M / 4M / 15M / 30M / 55M 7 arXiv LucaOne MaskedLM LucaGPLM \u2b55 LucaGroup 5.6M / 17.6M / 36M 3 Nature Machine Intelligence JanusDNA MaskedLM JanusDNA \u2b55 Qihao-Duan unknown 6 arXiv Jamba-DNA CausalLM Jamba \u2705 RaphaelMourad 114M 1 GitHub Mistral-DNA CausalLM Mistral \u2705 RaphaelMourad 1M / 17M / 138M / 417M / 422M 10 [GitHub](https://github.com/raphaelmourad/ ModernBert-DNA MaskedLM ModernBert \u2705 RaphaelMourad 37M 3 GitHub megaDNA CausalLM MEGADNA \u2b55 lingxusb 78M / 145M / 277M 3 arXiv MutBERT MaskedLM RoPEBert \u2705 JadenLong 86M 3 bioRxiv OmniNA CausalLM Llama \u2705 XLS 66M / 220M 2 bioRxiv Omni-DNA CausalLM OLMoModel \u2705 zehui127 20M / 60M / 116M / 300M / 700M / 1B 6 arXiv plant-genomic-jamba CausalLM StripedMamba \u2705 suzuki-2001 50M 1 GitHub ProkBERT MaskedLM MegatronBert \u2705 neuralbioinfo 21M / 25M / 27M 3 Frontiers in Microbiology"},{"location":"resources/model_zoo/#model-categories","title":"Model Categories","text":""},{"location":"resources/model_zoo/#by-architecture-type","title":"By Architecture Type","text":""},{"location":"resources/model_zoo/#masked-language-models-mlm","title":"Masked Language Models (MLM)","text":"<ul> <li>BERT-based: DNABERT, DNABERT-2, DNABERT-S, Plant DNABERT, GENA-LM, GROVER, MutBERT, ProkBERT, MutBERT, Plant DNAModernBert</li> <li>ESM-based: Nucleotide Transformer, AgroNT, Plant Nucleotide Transformer</li> <li>Caduceus-based: Caduceus-Ph, Caduceus-Ps, PlantCaduceus\u3001PlantCAD2</li> <li>Other: GENA-LM-BigBird, GPN, JanusDNA, LucaOne</li> </ul>"},{"location":"resources/model_zoo/#causal-language-models-clm","title":"Causal Language Models (CLM)","text":"<ul> <li>Llama-based: GENERator, OmniNA</li> <li>Mistral-based: GenomeOcean, Mistral-DNA</li> <li>Hyena-based: HyenaDNA, EVO-1, EVO-2</li> <li>Other: Jamba-DNA, plant-genomic-jamba, Plant DNAGPT, Plant DNAGemma, Plant DNAMamba, Omni-DNA, megaDNA</li> </ul>"},{"location":"resources/model_zoo/#by-model-size","title":"By Model Size","text":"Size Category Model Count Examples Small (&lt;100M) 15 Caduceus-Ph, HyenaDNA variants, ModernBert-DNA Medium (100M-1B) 18 DNABERT series, Plant models, GENA-LM Large (1B-10B) 8 Nucleotide Transformer, EVO-1, GENERator Extra Large (&gt;10B) 3 EVO-2 (40B)"},{"location":"resources/model_zoo/#by-source-platform","title":"By Source Platform","text":"Platform Model Count Examples Hugging Face Hub 25+ Most models with direct integration ModelScope 10+ Alternative source for some models GitHub 8 Community-contributed models Academic Journals 15+ Peer-reviewed publications"},{"location":"resources/model_zoo/#usage-guidelines","title":"Usage Guidelines","text":""},{"location":"resources/model_zoo/#fine-tuning-support","title":"Fine-tuning Support","text":"<ul> <li>\u2705 Native Supported: 35 models with full fine-tuning capabilities based on its own model implementation</li> <li>\u2b55 Custom Supported: 3 models (LucaOne, megaDNA, JanusDNA) with fine-tuning capabilities based on custom implementation for sequence classification</li> <li>\u274c Not Supported: 2 models (EVO-1, EVO-2) - inference only</li> </ul>"},{"location":"resources/model_zoo/#model-selection-tips","title":"Model Selection Tips","text":"<ol> <li>For Classification Tasks: Choose BERT-based models (DNABERT, Plant DNABERT)</li> <li>For Generation Tasks: Use CausalLM models (Plant DNAGPT, GenomeOcean)</li> <li>For Large-scale Analysis: Consider Nucleotide Transformer or EVO models</li> <li>For Plant-specific Tasks: Prefer Plant-prefixed models</li> </ol>"},{"location":"resources/model_zoo/#plant-models","title":"Plant Models","text":"<p>The following models are specifically designed for plant genomics:</p> <ul> <li>Plant DNABERT: BERT-based model for plant DNA sequence analysis</li> <li>Plant DNAGPT: GPT-based model for plant DNA sequence generation</li> <li>Plant Nucleotide Transformer: ESM-based model for plant genomics</li> <li>Plant DNAGemma: Gemma-based model for plant DNA analysis</li> <li>Plant DNAMamba: Mamba-based model for efficient plant sequence processing</li> <li>Plant DNAModernBert: ModernBert-based model for plant genomics</li> <li>PlantCaduceus: Caduceus-based model for plant sequence analysis</li> </ul>"},{"location":"resources/model_zoo/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Small Models (&lt;100M): Fast inference, suitable for real-time applications</li> <li>Medium Models (100M-1B): Good balance of performance and speed</li> <li>Large Models (&gt;1B): Best performance but slower inference</li> </ul>"},{"location":"resources/model_zoo/#getting-started","title":"Getting Started","text":"<p>To use any of these models with DNALLM:</p> <pre><code>from dnallm import load_model_and_tokenizer\n\n# Load a supported model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    source=\"huggingface\"\n)\n\n# For fine-tuning\nfrom dnallm.finetune import DNATrainer\ntrainer = DNATrainer(model=model, tokenizer=tokenizer)\n</code></pre>"},{"location":"resources/model_zoo/#contributing-new-models","title":"Contributing New Models","text":"<p>To add support for new DNA language models:</p> <ol> <li>Ensure the model is publicly available</li> <li>Test compatibility with DNALLM's architecture</li> <li>Submit a pull request with integration code</li> <li>Include proper documentation and examples</li> </ol> <p>For detailed integration instructions, see the Development Guide.</p>"},{"location":"resources/troubleshooting_models/","title":"Troubleshooting Models","text":"<p>This guide addresses common issues you might encounter when loading and using DNA language models with DNALLM.</p> <p>Related Documents: - Model Selection Guide - Installation Guide - Performance Optimization</p>"},{"location":"resources/troubleshooting_models/#1-installation-and-dependency-issues","title":"1. Installation and Dependency Issues","text":""},{"location":"resources/troubleshooting_models/#problem-importerror-not-installed","title":"Problem: <code>ImportError: ... not installed</code>","text":"<p>Some models require special packages that are not part of the base DNALLM installation.</p> <p>Error Messages: - <code>ImportError: EVO-1 package is required...</code> - <code>ImportError: No module named 'mamba_ssm'</code> - <code>ImportError: No module named 'gpn'</code> - <code>ImportError: No module named 'ai2_olmo'</code></p> <p>Solution: You must install the required dependencies for the specific model you are trying to use. Refer to the Installation Guide for detailed commands.</p> <p>Example for Mamba: <pre><code># Activate your virtual environment first\nuv pip install -e '.[mamba]' --no-cache-dir --no-build-isolation\n</code></pre></p> <p>Example for Evo-1: <pre><code>uv pip install evo-model\n</code></pre></p>"},{"location":"resources/troubleshooting_models/#problem-flash_attn-installation-fails","title":"Problem: <code>flash_attn</code> installation fails","text":"<p>Error Message: <code>HTTP Error 404: Not Found</code> during <code>pip install</code> or compilation errors.</p> <p>Cause: FlashAttention is highly specific to your Python, PyTorch, and CUDA versions. A pre-compiled wheel might not be available for your exact environment.</p> <p>Solution: 1.  Check Compatibility: Visit the FlashAttention GitHub releases and find a wheel that matches your environment. 2.  Install Manually: Download the <code>.whl</code> file and install it directly:     <pre><code>uv pip install /path/to/flash_attn-2.5.8+cu122torch2.3-cp312-cp312-linux_x86_64.whl\n</code></pre> 3.  Compile from Source: If no wheel is available, you may need to compile it from source, which requires having the CUDA toolkit and a C++ compiler installed. This can be complex. 4.  Run without FlashAttention: Most models can run without FlashAttention by using a slower, \"eager\" attention mechanism. Performance will be reduced, but the model will still work. DNALLM attempts to fall back to this mode automatically.</p>"},{"location":"resources/troubleshooting_models/#2-model-loading-issues","title":"2. Model Loading Issues","text":""},{"location":"resources/troubleshooting_models/#problem-valueerror-model-not-found-locally","title":"Problem: <code>ValueError: Model ... not found locally.</code>","text":"<p>Cause: You specified <code>source: \"local\"</code> but the path provided in <code>model_name</code> is incorrect or does not point to a valid model directory.</p> <p>Solution: - Double-check that the path in your configuration or code is correct. - Ensure the directory contains the necessary model files (e.g., <code>pytorch_model.bin</code>, <code>config.json</code>).</p>"},{"location":"resources/troubleshooting_models/#problem-valueerror-failed-to-load-model","title":"Problem: <code>ValueError: Failed to load model: ...</code>","text":"<p>This is a general error that can have several causes.</p> <p>Common Causes &amp; Solutions: 1.  Incorrect <code>task_type</code>: You are trying to load a model for a task it wasn't designed for without a proper configuration. For example, loading a base MLM model with a <code>regression</code> task config but <code>num_labels</code> is missing.     - Fix: Ensure your <code>task</code> configuration in the YAML file is correct. For classification/regression, <code>num_labels</code> must be specified.</p> <pre><code>```yaml\n# Correct config for fine-tuning a base model for regression\ntask:\n  task_type: regression\n  num_labels: 1 # This is required!\n```\n</code></pre> <ol> <li> <p>Corrupted Model Cache: The downloaded model files may be incomplete or corrupted.</p> <ul> <li>Fix: Clear the cache and let DNALLM re-download the model. <pre><code>from dnallm.models.model import clear_model_cache\n\n# For models from Hugging Face\nclear_model_cache(source=\"huggingface\")\n\n# For models from ModelScope\nclear_model_cache(source=\"modelscope\")\n</code></pre></li> </ul> </li> <li> <p>Network Issues: The model download failed due to an unstable connection.</p> <ul> <li>Fix: The <code>load_model_and_tokenizer</code> function has a built-in retry mechanism. If it consistently fails, check your network connection. For users in regions with restricted access to Hugging Face, consider using a mirror by setting <code>use_mirror=True</code>. <pre><code>model, tokenizer = load_model_and_tokenizer(\n    \"zhihan1996/DNABERT-2-117M\",\n    task_config=configs['task'],\n    source=\"huggingface\",\n    use_mirror=True # This uses hf-mirror.com\n)\n</code></pre></li> </ul> </li> </ol>"},{"location":"resources/troubleshooting_models/#3-performance-and-memory-issues","title":"3. Performance and Memory Issues","text":""},{"location":"resources/troubleshooting_models/#problem-cuda-out-of-memory","title":"Problem: <code>CUDA Out-of-Memory</code>","text":"<p>Cause: The model, data, and intermediate activations require more GPU VRAM than is available.</p> <p>Solutions: - Primary: Reduce <code>batch_size</code> in your <code>inference</code> or <code>training</code> configuration. This is the most effective way to lower memory usage. - Secondary: Reduce <code>max_length</code>. The memory requirement for transformers scales quadratically with sequence length. - Use Half-Precision: Set <code>use_fp16: true</code> or <code>use_bf16: true</code>. This can nearly halve the model's memory footprint. - Disable Interpretability Features: For large-scale runs, ensure <code>output_hidden_states</code> and <code>output_attentions</code> are <code>False</code>.</p> <p>For a detailed guide, see the Performance Optimization tutorial.</p>"},{"location":"resources/troubleshooting_models/#4-task-specific-issues","title":"4. Task-Specific Issues","text":""},{"location":"resources/troubleshooting_models/#problem-model-outputs-unexpected-scores-or-flat-predictions","title":"Problem: Model outputs unexpected scores or flat predictions.","text":"<p>Cause: There is a mismatch between the model's architecture and the task it's being used for.</p> <p>Solutions: - Check Model Type vs. Task:     - For classification/regression, fine-tuned models are generally required. Using a base MLM/CLM model without fine-tuning will likely produce random or uniform predictions on a classification task.     - For zero-shot mutation analysis, you should use a base MLM or CLM model with the appropriate <code>task_type</code> (<code>mask</code> or <code>generation</code>) to get meaningful likelihood scores. - Verify Tokenizer: Ensure the tokenizer is appropriate for the model. DNALLM handles this automatically when using <code>load_model_and_tokenizer</code>, but if you are loading components manually, a mismatch can cause poor performance. - Check <code>max_length</code>: If your sequences are being truncated too much, the model may not have enough information to make accurate predictions.</p>"},{"location":"resources/troubleshooting_models/#problem-indexerror-target-out-of-bounds-during-trainingevaluation","title":"Problem: <code>IndexError: Target out of bounds</code> during training/evaluation.","text":"<p>Cause: The labels in your dataset do not match the <code>num_labels</code> specified in your task configuration. For example, your data has labels <code>[0, 1, 2]</code> but you set <code>num_labels: 2</code>.</p> <p>Solution: - Verify <code>num_labels</code>: Ensure <code>num_labels</code> in your YAML configuration correctly reflects the number of unique classes in your dataset. - Check Label Encoding: Make sure your labels are encoded as integers starting from 0 (i.e., <code>0, 1, 2, ...</code>). If your labels are strings or start from 1, they must be preprocessed correctly. The <code>DNADataset</code> class typically handles this if the <code>label_names</code> are provided.</p>"},{"location":"resources/models/bert_models/","title":"Using BERT Models in DNALLM","text":"<p>BERT (Bidirectional Encoder Representations from Transformers) models are a class of Transformer-based language models renowned for their ability to understand deep bidirectional context. In genomics, models like DNABERT and its variants have been specifically pre-trained on DNA sequences, making them highly effective for a wide range of downstream tasks.</p> <p>DNALLM Examples: <code>DNABERT</code>, <code>DNABERT-2</code>, <code>Plant DNABERT</code>, <code>ProkBERT</code></p>"},{"location":"resources/models/bert_models/#1-architecture-overview","title":"1. Architecture Overview","text":"<p>BERT-based models utilize a Transformer encoder architecture.</p> <ul> <li>Bidirectional Context: Unlike traditional left-to-right or right-to-left models, BERT processes the entire input sequence at once. This allows each token's representation to be fused with information from both its left and right contexts, which is crucial for understanding genomic syntax.</li> <li>Masked Language Modeling (MLM): BERT is pre-trained by randomly masking some of the tokens in the input sequence and then predicting the original identity of the masked tokens. This objective teaches the model to learn a rich internal representation of the language (in this case, the \"language\" of DNA).</li> <li>K-mer Tokenization: Many DNA-specific BERT models use k-mer tokenization, where the DNA sequence is broken down into overlapping substrings of length <code>k</code>. This helps the model capture local sequence patterns.</li> </ul> <p>In DNALLM, BERT models serve as powerful feature extractors for tasks like classification, regression, and token-level prediction.</p>"},{"location":"resources/models/bert_models/#2-environment-and-installation","title":"2. Environment and Installation","text":"<p>BERT models are part of the core <code>transformers</code> library and do not require special dependencies beyond a standard DNALLM installation.</p>"},{"location":"resources/models/bert_models/#installation","title":"Installation","text":"<p>A standard DNALLM installation is sufficient.</p> <pre><code># Install DNALLM with core dependencies\npip install dnallm\n</code></pre>"},{"location":"resources/models/bert_models/#3-model-loading-and-configuration","title":"3. Model Loading and Configuration","text":"<p>You can load a DNA-specific BERT model using the <code>AutoModel</code> classes from <code>transformers</code> or the DNALLM utility functions.</p>"},{"location":"resources/models/bert_models/#loading-a-model","title":"Loading a Model","text":"<p>Here\u2019s how to load a DNABERT model for a sequence classification task.</p> <pre><code>from dnallm.utils.load import load_model_and_tokenizer\n\n# Use a specific DNABERT model\nmodel_name = \"zhihan1996/DNABERT-2-117M\"\n\n# Load model and tokenizer for a classification task\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name_or_path=model_name\n)\n\nprint(\"Model:\", type(model))\nprint(\"Tokenizer:\", type(tokenizer))\n</code></pre>"},{"location":"resources/models/bert_models/#4-inference-example","title":"4. Inference Example","text":"<p>Let's use the loaded DNABERT-2 to get embeddings for a DNA sequence.</p> <pre><code>import torch\nfrom dnallm.utils.load import load_model_and_tokenizer\n\n# 1. Load the pre-trained model and tokenizer\nmodel_name = \"zhihan1996/DNABERT-2-117M\"\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name_or_path=model_name\n)\nmodel.eval()\n\n# 2. Prepare and tokenize the DNA sequence\ndna_sequence = \"GATTACAGATTACAGATTACAGATTACAGATTACAGATTACA\"\ninputs = tokenizer(dna_sequence, return_tensors=\"pt\")\n\n# 3. Perform inference\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# The last hidden state contains the contextual embeddings for each token\nembeddings = outputs.last_hidden_state\nprint(\"Shape of embeddings:\", embeddings.shape)\n</code></pre>"},{"location":"resources/models/caduceus_models/","title":"Using Caduceus Models in DNALLM","text":"<p>Caduceus is a family of bi-directional and equivariant models designed specifically for long-range DNA sequence modeling. It introduces architectural innovations to handle the unique symmetries of DNA, such as reverse-complement equivariance, making it particularly powerful for genomics.</p> <p>DNALLM Examples: <code>Caduceus-Ph</code>, <code>Caduceus-PS</code>, <code>PlantCaduceus</code>, <code>PlantCAD2</code></p>"},{"location":"resources/models/caduceus_models/#1-architecture-overview","title":"1. Architecture Overview","text":"<p>Caduceus models are built on a custom architecture that modifies the standard Transformer to better suit DNA.</p> <ul> <li>Reverse-Complement Equivariance: The model is designed to produce equivalent representations for a DNA sequence and its reverse complement. This is a natural inductive bias for DNA, as functionality is often preserved in both strands.</li> <li>Bi-directional Long-Range Modeling: It processes sequences bi-directionally and is optimized to handle very long DNA contexts, which is essential for capturing distal regulatory elements.</li> <li>Masked Language Modeling: Like BERT, Caduceus is pre-trained using a masked language modeling objective, where it learns to predict masked nucleotides within a long sequence.</li> </ul> <p>These features make Caduceus highly effective for tasks requiring an understanding of long-range dependencies in genomes.</p>"},{"location":"resources/models/caduceus_models/#2-environment-and-installation","title":"2. Environment and Installation","text":"<p>Caduceus models are supported by the standard <code>transformers</code> library and do not require any special dependencies beyond the core DNALLM installation.</p>"},{"location":"resources/models/caduceus_models/#installation","title":"Installation","text":"<p>A standard DNALLM installation is sufficient.</p> <pre><code># Install DNALLM with core dependencies\npip install dnallm\n</code></pre>"},{"location":"resources/models/caduceus_models/#3-model-loading-and-configuration","title":"3. Model Loading and Configuration","text":"<p>You can load a Caduceus model using the <code>AutoModel</code> classes from <code>transformers</code> or the DNALLM utility functions.</p>"},{"location":"resources/models/caduceus_models/#loading-a-model","title":"Loading a Model","text":"<p>Here\u2019s how to load a Caduceus model for a masked language modeling task.</p> <pre><code>from dnallm.utils.load import load_model_and_tokenizer\n\n# Use a specific Caduceus model\nmodel_name = \"kuleshov-group/caduceus-ph_seqlen-131k_d_model-256_n_layer-16\"\n\n# Load model and tokenizer\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name_or_path=model_name\n)\n\nprint(\"Model:\", type(model))\nprint(\"Tokenizer:\", type(tokenizer))\n</code></pre>"},{"location":"resources/models/caduceus_models/#4-inference-example","title":"4. Inference Example","text":"<p>Let's use a Caduceus model to get embeddings for a DNA sequence.</p> <pre><code>import torch\nfrom dnallm.utils.load import load_model_and_tokenizer\n\n# 1. Load the pre-trained model and tokenizer\nmodel_name = \"kuleshov-group/PlantCaduceus_l20\"\nmodel, tokenizer = load_model_and_tokenizer(model_name)\nmodel.eval()\n\n# 2. Prepare and tokenize the DNA sequence\ndna_sequence = \"GATTACAGATTACAGATTACAGATTACAGATTACAGATTACA\"\ninputs = tokenizer(dna_sequence, return_tensors=\"pt\")\n\n# 3. Perform inference\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nembeddings = outputs.last_hidden_state\nprint(\"Shape of embeddings:\", embeddings.shape)\n</code></pre>"},{"location":"resources/models/esm_models/","title":"Using ESM Models in DNALLM","text":"<p>ESM (Evolutionary Scale Modeling) models are a family of protein language models adapted for genomics. While originally trained on protein sequences, their Transformer-encoder architecture is highly effective for DNA, and they have been successfully fine-tuned for various nucleotide tasks. Models like the Nucleotide Transformer and AgroNT are prominent examples of this approach.</p> <p>DNALLM Examples: <code>Nucleotide Transformer</code>, <code>AgroNT</code> (adapted for DNA)</p>"},{"location":"resources/models/esm_models/#1-architecture-overview","title":"1. Architecture Overview","text":"<p>ESM models are based on the Transformer encoder architecture, similar to BERT.</p> <ul> <li>Bidirectional Context: Like BERT, ESM models process the entire sequence at once, capturing rich contextual information.</li> <li>Pre-trained on Biology: ESM models were pre-trained on massive datasets of protein sequences, learning fundamental biological patterns that can be transferred to DNA.</li> <li>Focus on Embeddings: They are particularly renowned for producing high-quality embeddings that represent functional and structural properties of sequences.</li> </ul> <p>In the context of DNALLM, ESM models are treated as powerful feature extractors. Their pre-trained knowledge provides a strong starting point for fine-tuning on specific genomic tasks.</p>"},{"location":"resources/models/esm_models/#2-environment-and-installation","title":"2. Environment and Installation","text":"<p>ESM models are supported by the standard <code>transformers</code> library and do not require any special dependencies beyond the core DNALLM installation.</p>"},{"location":"resources/models/esm_models/#installation","title":"Installation","text":"<p>A standard DNALLM installation is sufficient.</p> <pre><code># Install DNALLM with core dependencies\npip install dnallm\n</code></pre>"},{"location":"resources/models/esm_models/#3-model-loading-and-configuration","title":"3. Model Loading and Configuration","text":"<p>You can load an ESM model adapted for DNA using the <code>AutoModel</code> classes from <code>transformers</code> or the DNALLM utility functions.</p>"},{"location":"resources/models/esm_models/#loading-a-model","title":"Loading a Model","text":"<p>Here\u2019s how to load an ESM model for a DNA classification task. Note that we use a version that has been fine-tuned or adapted for nucleotide data.</p> <pre><code>from transformers import AutoModelForMaskedLM, AutoTokenizer\n\n# Use a specific Nucleotide Transformer model\nmodel_name = \"InstaDeepAI/nucleotide-transformer-v2-100m-multi-species\"\n\n# Load model and tokenizer\n# The DNALLM utility handles the model type detection automatically for ESM-based models\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name_or_path=model_name,\n    model_type=\"esm\",\n    num_labels=2, # Example for binary classification\n    trust_remote_code=True\n)\n\nprint(\"Model:\", type(model))\nprint(\"Tokenizer:\", type(tokenizer))\n</code></pre> <p>Important: When using ESM for DNA, you typically replace its original amino acid tokenizer with a nucleotide tokenizer (e.g., one for k-mers). The fine-tuning process adapts the model's weights to the new vocabulary.</p>"},{"location":"resources/models/esm_models/#4-inference-example","title":"4. Inference Example","text":"<p>Let's use the loaded Nucleotide Transformer to get embeddings for a DNA sequence.</p> <pre><code>import torch\nfrom dnallm.utils.load import load_model_and_tokenizer\n\n# 1. Load the model and its specific tokenizer\nmodel_name = \"InstaDeepAI/nucleotide-transformer-v2-100m-multi-species\"\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name_or_path=model_name\n)\nmodel.eval()\n\n# 2. Prepare and tokenize the DNA sequence\ndna_sequence = \"GATTACAGATTACAGATTACAGATTACAGATTACAGATTACA\"\ninputs = tokenizer(dna_sequence, return_tensors=\"pt\")\n\n# 3. Perform inference\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# The last hidden state contains the contextual embeddings for each token\nembeddings = outputs.last_hidden_state\nprint(\"Shape of embeddings:\", embeddings.shape)\n</code></pre>"},{"location":"resources/models/esm_models/#5-common-issues-and-solutions","title":"5. Common Issues and Solutions","text":"<ol> <li> <p>Tokenizer Mismatch:</p> <ul> <li>Issue: The default ESM tokenizer is for amino acids, not nucleotides. Using it directly with DNA will fail.</li> <li>Solution: You must use a nucleotide-based tokenizer. When fine-tuning, you need to resize the model's token embeddings to match the new vocabulary size of the DNA tokenizer. The DNALLM fine-tuning script handles this automatically.</li> </ul> </li> <li> <p>Poor Performance on DNA Tasks Out-of-the-Box:</p> <ul> <li>Issue: An ESM model pre-trained only on proteins will not perform well on DNA tasks without fine-tuning.</li> <li>Solution: Fine-tuning is essential. The model must learn to apply its learned representations to the new domain of genomics. Use the DNALLM <code>finetune</code> CLI for this purpose.</li> </ul> </li> </ol> <p>Next: Compare with other encoder models like BERT-based Models.</p>"},{"location":"resources/models/evo_models/","title":"Guide to EVO Models (EVO-1 &amp; EVO-2)","text":"<p>This guide covers the installation and usage of the EVO family of models, which are state-of-the-art generative models for DNA sequences. DNALLM provides seamless integration for these highly specialized models.</p> <p>Related Documents: - Installation Guide - Model Selection Guide</p>"},{"location":"resources/models/evo_models/#1-introduction-to-evo-models","title":"1. Introduction to EVO Models","text":"<p>The EVO models, developed by Arc Institute and collaborators, are based on the StripedHyena architecture, a hybrid of convolutions and attention mechanisms. They are designed to handle extremely long sequence contexts (up to 1 million tokens for EVO-2) and are pre-trained on a massive corpus of genomic data.</p>"},{"location":"resources/models/evo_models/#evo-1","title":"EVO-1","text":"<ul> <li>Architecture: Based on StripedHyena.</li> <li>Key Feature: Can handle contexts up to 131k tokens.</li> <li>Primary Use: Sequence scoring and generation for long genomic regions.</li> </ul>"},{"location":"resources/models/evo_models/#evo-2","title":"EVO-2","text":"<ul> <li>Architecture: Based on StripedHyena-2, an evolution of the original architecture.</li> <li>Key Feature: Supports context lengths up to 1 million tokens and incorporates FP8 precision for efficiency on modern GPUs (NVIDIA Hopper series).</li> <li>Primary Use: State-of-the-art for ultra-long sequence modeling, generation, and scoring.</li> </ul>"},{"location":"resources/models/evo_models/#2-installation","title":"2. Installation","text":"<p>EVO models require their own specific packages.</p>"},{"location":"resources/models/evo_models/#evo-1-installation","title":"EVO-1 Installation","text":"<p>Install the <code>evo-model</code> package to use EVO-1.</p> <pre><code># Activate your virtual environment\nuv pip install evo-model\n</code></pre>"},{"location":"resources/models/evo_models/#evo-2-installation","title":"EVO-2 Installation","text":"<p>EVO-2 has more complex dependencies and requires Python &gt;= 3.11.</p> <pre><code># 1. Install the Transformer Engine from NVIDIA\nuv pip install \"transformer-engine[pytorch]==2.3.0\" --no-build-isolation --no-cache-dir\n\n# 2. Install the EVO-2 package\nuv pip install evo2\n\n# 3. (Optional but Recommended) Install Flash Attention for performance\nuv pip install \"flash_attn&lt;=2.7.4.post1\" --no-build-isolation --no-cache-dir\n</code></pre> <p>After installation, you may need to add the <code>cudnn</code> library path to your environment: <pre><code>export LD_LIBRARY_PATH=[path_to_DNALLM]/.venv/lib/python3.11/site-packages/nvidia/cudnn/lib:${LD_LIBRARY_PATH}\n</code></pre> Replace <code>[path_to_DNALLM]</code> with the absolute path to your project directory.</p>"},{"location":"resources/models/evo_models/#3-usage-and-application-scenarios","title":"3. Usage and Application Scenarios","text":"<p>Both EVO-1 and EVO-2 are causal language models (CLMs) used for generation and scoring. DNALLM's <code>Mutagenesis</code> and <code>DNAInference</code> classes have special handling for them, automatically using their optimized <code>scoring</code> methods.</p>"},{"location":"resources/models/evo_models/#example-scoring-mutations-with-evo-2","title":"Example: Scoring mutations with EVO-2","text":"<p>This example shows how to use an EVO model to score the impact of mutations on a sequence's likelihood.</p> <pre><code>from dnallm import load_config, Mutagenesis, load_model_and_tokenizer\n\n# 1. Use a config with task_type: \"generation\"\nconfigs = load_config(\"path/to/your/evo_config.yaml\")\n\n# 2. Load an EVO model\n# DNALLM will automatically detect it's an EVO model.\n# Note: The model ID might be a mirror like 'lgq12697/evo2_1b_base'\nmodel, tokenizer = load_model_and_tokenizer(\n    \"arcinstitute/evo-2-1b-8k\", # Official ID\n    task_config=configs['task'],\n    source=\"huggingface\"\n)\n\n# 3. Initialize the Mutagenesis analyzer\nmut_analyzer = Mutagenesis(model=model, tokenizer=tokenizer, config=configs)\n\nsequence = \"GATTACAGATTACAGATTACAGATTACAGATTACAGATTACAGATTACA...\"\nmut_analyzer.mutate_sequence(sequence, replace_mut=True)\n\n# 4. Evaluate mutation effects\n# The evaluate() method will automatically call the model's optimized scoring function.\n# 'mean' or 'sum' are the most effective strategies for Evo models.\npredictions = mut_analyzer.evaluate(strategy=\"mean\")\n\n# 5. Plot the results\nmut_analyzer.plot(predictions, save_path=\"./results/evo2_mut_effects.pdf\")\n</code></pre>"},{"location":"resources/models/evo_models/#application-scenarios","title":"Application Scenarios","text":"<ul> <li>Variant Effect Prediction: Score the likelihood of a sequence with and without a specific SNP to predict its functional impact.</li> <li>Enhancer/Promoter Design: Use the <code>generate()</code> method (from the underlying model) to create novel regulatory sequences.</li> <li>Long-Range Dependency Analysis: Analyze how elements separated by thousands of base pairs influence each other within a gene or regulatory region.</li> </ul>"},{"location":"resources/models/evo_models/#4-troubleshooting","title":"4. Troubleshooting","text":""},{"location":"resources/models/evo_models/#problem-importerror-evo-1-package-is-required-or-evo2-package-is-required","title":"Problem: <code>ImportError: EVO-1 package is required...</code> or <code>EVO2 package is required...</code>","text":"<ul> <li>Solution: You have not installed the required package. Follow the installation steps in Section 2 for the specific EVO model you are using.</li> </ul>"},{"location":"resources/models/evo_models/#problem-transformer-engine-or-flash_attn-fails-to-build","title":"Problem: <code>transformer-engine</code> or <code>flash_attn</code> fails to build.","text":"<ul> <li>Cause: These packages require specific versions of the CUDA toolkit, a C++ compiler, and compatible PyTorch/Python versions.</li> <li>Solution:<ol> <li>Ensure you are using a compatible environment (Python &gt;= 3.11 for EVO-2, a recent PyTorch version, and a supported CUDA version).</li> <li>Install build tools like <code>gxx</code> and <code>clang</code> (<code>conda install -c conda-forge gxx clang</code>).</li> <li>Refer to the official installation guides for Transformer Engine and FlashAttention for detailed compatibility matrices and troubleshooting.</li> </ol> </li> </ul>"},{"location":"resources/models/evo_models/#problem-cuda-out-of-memory-with-evo-2","title":"Problem: <code>CUDA Out-of-Memory</code> with EVO-2","text":"<ul> <li>Cause: EVO-2 models, especially the larger ones, are very memory-intensive.</li> <li>Solution:<ol> <li>Ensure you are using a GPU with sufficient VRAM (e.g., A100, H100).</li> <li>Reduce the <code>batch_size</code> in your configuration to 1 if necessary.</li> <li>If you are on a Hopper-series GPU (H100/H200), ensure FP8 is enabled, as DNALLM's EVO-2 handler attempts to use it automatically for efficiency.</li> </ol> </li> </ul>"},{"location":"resources/models/flash_attention_models/","title":"Guide to Using Models with Flash Attention","text":"<p>This guide explains what Flash Attention is, which models support it, and how to install and leverage it for significant performance improvements in DNALLM.</p> <p>Related Documents: - Installation Guide - Performance Optimization - Troubleshooting Models</p>"},{"location":"resources/models/flash_attention_models/#1-what-is-flash-attention","title":"1. What is Flash Attention?","text":"<p>Flash Attention is a highly optimized implementation of the attention mechanism used in Transformer models. It was developed by Dao-AILab to address the performance and memory bottlenecks of standard attention, which scale quadratically with sequence length.</p> <p>Key Benefits: - Faster: It provides significant speedups (often 1.5-3x) for both training and inference. - Memory-Efficient: It reduces the memory footprint of the attention calculation, allowing for longer sequences or larger batch sizes.</p> <p>It achieves this by using techniques like kernel fusion and tiling to minimize memory I/O between the GPU's high-bandwidth memory (HBM) and on-chip SRAM.</p>"},{"location":"resources/models/flash_attention_models/#2-supported-models","title":"2. Supported Models","text":"<p>Flash Attention is not a model architecture itself, but an implementation that can be \"plugged into\" many existing Transformer-based models. Models that benefit most include:</p> <ul> <li>EVO-1 and EVO-2: The StripedHyena architecture in these models can leverage Flash Attention for its attention components.</li> <li>Mamba-based models: While primarily SSMs, some hybrid variants can use it.</li> <li>Standard Transformers (BERT, GPT, etc.): Most modern Transformer models loaded through Hugging Face's <code>transformers</code> library can automatically use Flash Attention if it's installed and the model is configured to use <code>attn_implementation=\"flash_attention_2\"</code>.</li> </ul> <p>DNALLM automatically attempts to use the most efficient attention mechanism available. If you have Flash Attention installed, it will be prioritized for compatible models.</p>"},{"location":"resources/models/flash_attention_models/#3-installation","title":"3. Installation","text":"<p>Installing Flash Attention can be tricky because it is highly dependent on your specific hardware and software environment.</p> <p>Prerequisites: - An NVIDIA GPU (Ampere, Hopper, or Ada architecture recommended). - A compatible version of PyTorch, Python, and the CUDA toolkit.</p> <p>Installation Command:</p> <pre><code># Activate your virtual environment\nuv pip install flash-attn --no-build-isolation --no-cache-dir\n</code></pre>"},{"location":"resources/models/flash_attention_models/#troubleshooting-installation","title":"Troubleshooting Installation","text":"<p>Problem: <code>HTTP Error 404: Not Found</code> or compilation errors.</p> <ul> <li>Cause: A pre-compiled wheel is not available for your exact combination of Python, PyTorch, and CUDA versions.</li> <li>Solution:<ol> <li>Check for a compatible wheel: Visit the Flash Attention GitHub Releases page. Find a <code>.whl</code> file that matches your environment (e.g., <code>cp312</code> for Python 3.12, <code>cu122</code> for CUDA 12.2, <code>torch2.3</code>).</li> <li>Install the wheel manually:     <pre><code>uv pip install /path/to/your/downloaded/flash_attn-*.whl\n</code></pre></li> <li>Compile from source: This is the most complex option and requires a full development environment (CUDA toolkit, C++ compiler). Follow the instructions on the official Flash Attention GitHub repository.</li> </ol> </li> </ul>"},{"location":"resources/models/flash_attention_models/#4-usage","title":"4. Usage","text":"<p>Usage is largely automatic. If Flash Attention is correctly installed, DNALLM and the underlying <code>transformers</code> library will detect and use it.</p> <p>You can verify its usage by checking the model's configuration after loading it.</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer\n\nconfigs = load_config(\"config.yaml\")\nmodel, tokenizer = load_model_and_tokenizer(\n    \"arcinstitute/evo-1-131k-base\",\n    task_config=configs['task'],\n    source=\"huggingface\"\n)\n\n# Check the attention implementation in the model's config\n# For transformers-based models\nif hasattr(model, \"config\") and hasattr(model.config, \"_attn_implementation\"):\n    print(f\"Attention implementation: {model.config._attn_implementation}\")\n    # &gt;&gt; Attention implementation: flash_attention_2\n</code></pre> <p>If Flash Attention is not installed or incompatible, the framework will gracefully fall back to a slower but functional implementation like <code>\"eager\"</code> or <code>\"sdpa\"</code>.</p>"},{"location":"resources/models/hyena_models/","title":"Using HyenaDNA Models in DNALLM","text":"<p>HyenaDNA is a class of genomic foundation models designed for long-range sequence modeling at single-nucleotide resolution. It is notable for its attention-free architecture, which replaces the quadratic-cost attention mechanism of Transformers with long convolutions. This allows it to scale to extremely long sequences (up to 1 million tokens).</p> <p>DNALLM Examples: <code>HyenaDNA</code></p>"},{"location":"resources/models/hyena_models/#1-architecture-overview","title":"1. Architecture Overview","text":"<p>HyenaDNA is based on the Hyena operator, a sub-quadratic alternative to attention.</p> <ul> <li>Attention-Free: Instead of using self-attention, HyenaDNA relies on implicit long convolutions parameterized by a small neural network. This design choice significantly reduces computational complexity from O(N\u00b2) to O(N log N), where N is the sequence length.</li> <li>Causal Language Modeling: It is pre-trained as a causal (autoregressive) language model, meaning it predicts the next nucleotide in a sequence given the preceding ones.</li> <li>Single-Nucleotide Resolution: The model operates directly on single characters (A, C, G, T, N), avoiding k-mer tokenization and preserving the full resolution of the genomic sequence.</li> </ul> <p>This architecture makes HyenaDNA exceptionally well-suited for tasks involving very long-range dependencies, such as modeling entire genes or regulatory regions.</p>"},{"location":"resources/models/hyena_models/#2-environment-and-installation","title":"2. Environment and Installation","text":"<p>HyenaDNA models require specific dependencies that are not part of the standard DNALLM installation.</p>"},{"location":"resources/models/hyena_models/#installation","title":"Installation","text":"<p>You need to install <code>causal-conv1d</code> and other related packages.</p> <pre><code># Install DNALLM\npip install dnallm\n\n# Install HyenaDNA dependencies\npip install causal-conv1d&gt;=1.1.0\n</code></pre>"},{"location":"resources/models/hyena_models/#3-model-loading-and-configuration","title":"3. Model Loading and Configuration","text":"<p>You can load a HyenaDNA model using the custom <code>HyenaDNAForCausalLM</code> class or through the DNALLM utility functions.</p>"},{"location":"resources/models/hyena_models/#loading-a-model","title":"Loading a Model","text":"<p>Here\u2019s how to load a HyenaDNA model for a causal language modeling task.</p> <pre><code>from dnallm.utils.load import load_model_and_tokenizer\n\n# Use a specific HyenaDNA model\nmodel_name = \"LongSafari/hyenadna-small-32k-seqlen-hf\"\n\n# Load model and tokenizer\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name_or_path=model_name\n)\n\nprint(\"Model:\", type(model))\nprint(\"Tokenizer:\", type(tokenizer))\n</code></pre>"},{"location":"resources/models/hyena_models/#4-inference-example","title":"4. Inference Example","text":"<p>Let's use a HyenaDNA model to get embeddings for a DNA sequence.</p> <pre><code>import torch\nfrom dnallm.utils.load import load_model_and_tokenizer\n\n# 1. Load the pre-trained model and tokenizer\nmodel_name = \"LongSafari/hyenadna-tiny-1k-seqlen-hf\"\nmodel, tokenizer = load_model_and_tokenizer(model_name)\nmodel.eval()\n\n# 2. Prepare and tokenize the DNA sequence\ndna_sequence = \"GATTACAGATTACAGATTACAGATTACAGATTACAGATTACA\"\ninputs = tokenizer(dna_sequence, return_tensors=\"pt\")\n\n# 3. Perform inference\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nembeddings = outputs.hidden_states[-1] # Hyena outputs hidden states differently\nprint(\"Shape of embeddings:\", embeddings.shape)\n</code></pre>"},{"location":"resources/models/llama_models/","title":"Using Llama-based Models in DNALLM","text":"<p>Llama (Large Language Model Meta AI) is a family of powerful, open-source large language models. While originally trained on natural language text, their versatile Transformer decoder architecture has been successfully adapted for genomic sequence modeling. These models are typically used for causal (autoregressive) tasks like sequence generation.</p> <p>DNALLM Examples: <code>GENERator</code>, <code>OmniNA</code></p>"},{"location":"resources/models/llama_models/#1-architecture-overview","title":"1. Architecture Overview","text":"<p>Llama-based models are decoder-only Transformers.</p> <ul> <li>Causal (Autoregressive) Modeling: These models are trained to predict the next token in a sequence based on all previous tokens. This makes them naturally suited for sequence generation tasks.</li> <li>Unidirectional Context: Unlike BERT, which is bidirectional, Llama models only consider the left-side context when generating a representation for a token.</li> <li>Architectural Refinements: Llama includes several improvements over the original Transformer, such as pre-normalization (RMSNorm), SwiGLU activation functions, and Rotary Position Embeddings (RoPE), which contribute to its strong performance and training stability.</li> </ul> <p>In DNALLM, Llama-based models are excellent for tasks like generating novel DNA sequences with desired properties or for scoring sequences based on their learned probability distribution.</p>"},{"location":"resources/models/llama_models/#2-environment-and-installation","title":"2. Environment and Installation","text":"<p>Llama-based models are supported by the standard <code>transformers</code> library and do not require any special dependencies beyond the core DNALLM installation.</p>"},{"location":"resources/models/llama_models/#installation","title":"Installation","text":"<p>A standard DNALLM installation is sufficient.</p> <pre><code># Install DNALLM with core dependencies\npip install dnallm\n</code></pre>"},{"location":"resources/models/llama_models/#3-model-loading-and-configuration","title":"3. Model Loading and Configuration","text":"<p>You can load a Llama-based DNA model using the <code>AutoModelForCausalLM</code> class from <code>transformers</code> or the DNALLM utility functions.</p>"},{"location":"resources/models/llama_models/#loading-a-model","title":"Loading a Model","text":"<p>Here\u2019s how to load a Llama-based model for a causal language modeling task.</p> <pre><code>from dnallm.utils.load import load_model_and_tokenizer\n\n# Use a specific Llama-based DNA model\nmodel_name = \"GenerTeam/GENERator-eukaryote-1.2b-base\"\n\n# Load model and tokenizer\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name_or_path=model_name\n)\n\nprint(\"Model:\", type(model))\nprint(\"Tokenizer:\", type(tokenizer))\n</code></pre>"},{"location":"resources/models/llama_models/#4-inference-example","title":"4. Inference Example","text":"<p>Let's use a Llama-based model to get embeddings for a DNA sequence.</p> <pre><code>import torch\nfrom dnallm import load_model_and_tokenizer\n\n# 1. Load the pre-trained model and tokenizer\nmodel_name = \"XLS/OmniNA-66m\"\nmodel, tokenizer = load_model_and_tokenizer(model_name)\nmodel.eval()\n\n# 2. Prepare and tokenize the DNA sequence\ndna_sequence = \"GATTACAGATTACAGATTACAGATTACAGATTACAGATTACA\"\ninputs = tokenizer(dna_sequence, return_tensors=\"pt\")\n\n# 3. Perform inference\nwith torch.no_grad():\n    outputs = model(**inputs, output_hidden_states=True)\n\nembeddings = outputs.hidden_states[-1]\nprint(\"Shape of embeddings:\", embeddings.shape)\n</code></pre>"},{"location":"resources/models/mamba_models/","title":"Guide to Mamba and State-Space Models (SSMs)","text":"<p>This guide provides a detailed walkthrough for using models based on the Mamba architecture and other State-Space Models (SSMs) like Caduceus within the DNALLM framework. These models are highly effective for capturing long-range dependencies in DNA sequences while maintaining computational efficiency.</p> <p>Related Documents: - Installation Guide - Model Selection Guide</p>"},{"location":"resources/models/mamba_models/#1-introduction-to-mamba-and-ssms","title":"1. Introduction to Mamba and SSMs","text":"<p>Mamba is a modern sequence modeling architecture based on Structured State-Space Models (SSMs). Unlike traditional Transformers which have quadratic complexity with respect to sequence length, Mamba's complexity scales linearly. This makes it exceptionally well-suited for modeling very long DNA sequences.</p> <p>Key Advantages: - Efficiency: Linear scaling allows for faster processing and lower memory usage on long sequences compared to Transformers. - Long-Range Dependencies: The state-space mechanism is designed to effectively capture relationships between distant parts of a sequence.</p> <p>Variants in DNALLM: - Plant DNAMamba: A Mamba model pre-trained on plant genomes. - Caduceus: A bi-directional model that incorporates S4 layers (a precursor to Mamba), enabling it to model long DNA sequences with single-nucleotide resolution.</p>"},{"location":"resources/models/mamba_models/#2-installation","title":"2. Installation","text":"<p>To use Mamba-based models, you need to install specific dependencies. The native Mamba implementation requires a CUDA-enabled GPU.</p>"},{"location":"resources/models/mamba_models/#native-mamba-installation-recommended-for-nvidia-gpus","title":"Native Mamba Installation (Recommended for NVIDIA GPUs)","text":"<p>After completing the base installation, run the following command to install the necessary packages, including <code>mamba-ssm</code> and <code>causal-conv1d</code>.</p> <pre><code># Activate your virtual environment first\n# e.g., source .venv/bin/activate\n\nuv pip install -e '.[mamba]' --no-cache-dir --no-build-isolation\n</code></pre> <p>If you encounter network or compilation issues, you can use the provided helper script:</p> <pre><code>sh scripts/install_mamba.sh\n</code></pre>"},{"location":"resources/models/mamba_models/#caduceus-models","title":"Caduceus Models","text":"<p>Caduceus models are built into the DNALLM framework and do not require a separate installation beyond the base dependencies.</p>"},{"location":"resources/models/mamba_models/#3-usage-and-application-scenarios","title":"3. Usage and Application Scenarios","text":""},{"location":"resources/models/mamba_models/#using-plant-dnamamba","title":"Using Plant DNAMamba","text":"<p>Plant DNAMamba is a causal language model (CLM), making it ideal for sequence scoring and generation tasks.</p> <p>Example: Scoring a sequence with Plant DNAMamba</p> <p>This example demonstrates how to perform zero-shot mutation analysis by scoring sequence likelihood.</p> <pre><code>from dnallm import load_config, Mutagenesis, load_model_and_tokenizer\n\n# 1. Load a configuration for a generation task\nconfigs = load_config(\"path/to/your/generation_config.yaml\")\n\n# 2. Load the Plant DNAMamba model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnamamba-BPE\",\n    task_config=configs['task'],\n    source=\"modelscope\"\n)\n\n# 3. Perform in-silico mutagenesis\nmut_analyzer = Mutagenesis(model=model, tokenizer=tokenizer, config=configs)\nsequence = \"GATTACAGATTACAGATTACAGATTACAGATTACAGATTACA...\" # A long sequence\nmut_analyzer.mutate_sequence(sequence, replace_mut=True)\n\n# The evaluate() method will use the CLM scoring mechanism\npredictions = mut_analyzer.evaluate()\n\nmut_analyzer.plot(predictions, save_path=\"./results/dnamamba_mut_effects.pdf\")\n</code></pre>"},{"location":"resources/models/mamba_models/#using-caduceus-models","title":"Using Caduceus Models","text":"<p>Caduceus models are bi-directional (MLM-style) and excel at classification tasks, especially on long sequences where standard BERT models might struggle.</p> <p>Example: Fine-tuning PlantCAD2 for classification</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n\n# 1. Load a config for a classification task\nconfigs = load_config(\"path/to/your/finetune_config.yaml\")\n\n# 2. Load the PlantCAD2 model\n# Note: The model ID might be a mirror like 'lgq12697/PlantCAD2-Small-l24-d0768'\nmodel, tokenizer = load_model_and_tokenizer(\n    \"kuleshov-group/PlantCAD2-Small-l24-d0768\",\n    task_config=configs['task'],\n    source=\"huggingface\"\n)\n\n# 3. Load your dataset and initialize the trainer\n# ... (code for loading DNADataset)\n\ntrainer = DNATrainer(model=model, config=configs, datasets=my_datasets)\ntrainer.train()\n</code></pre>"},{"location":"resources/models/mamba_models/#4-troubleshooting","title":"4. Troubleshooting","text":""},{"location":"resources/models/mamba_models/#problem-importerror-no-module-named-mamba_ssm-or-causal_conv1d","title":"Problem: <code>ImportError: No module named 'mamba_ssm'</code> or <code>causal_conv1d</code>","text":"<ul> <li>Solution: You have not installed the Mamba-specific dependencies. Please run <code>uv pip install -e '.[mamba]'</code> as described in the installation section.</li> </ul>"},{"location":"resources/models/mamba_models/#problem-compilation-errors-during-mamba-installation","title":"Problem: Compilation errors during Mamba installation.","text":"<ul> <li>Cause: The native Mamba packages require a C++ compiler and the CUDA toolkit to be properly installed and configured on your system.</li> <li>Solution:<ol> <li>Ensure you have <code>gxx</code> and <code>clang</code> installed. On conda environments, you can run <code>conda install -c conda-forge gxx clang</code>.</li> <li>Verify that your NVIDIA driver version and CUDA toolkit version are compatible with the PyTorch and Mamba versions being installed.</li> <li>If issues persist, try using the <code>sh scripts/install_mamba.sh</code> script, which can help resolve some common path and environment issues.</li> </ol> </li> </ul>"},{"location":"resources/models/special_models/","title":"Guide to Other Special Models","text":"<p>DNALLM supports several other specialized models that have unique architectures or dependencies. This guide covers how to install and use them. DNALLM supports several specialized models that have unique architectures or require extra dependencies beyond the base installation. This guide covers how to install and use them.</p> <p>Related Documents: - Installation Guide - Model Selection Guide - Guide to Mamba and State-Space Models (SSMs) - Guide to EVO Models (EVO-1 &amp; EVO-2)</p>"},{"location":"resources/models/special_models/#1-gpn-genome-wide-pathogen-derived-network","title":"1. GPN (Genome-wide Pathogen-derived Network)","text":"<p>[!NOTE] Dependency: <code>gpn</code></p>"},{"location":"resources/models/special_models/#introduction","title":"Introduction","text":"<p>GPN is a convolutional neural network designed specifically for predicting the effects of genomic variants. Unlike transformer-based models, it uses a different architectural approach that can be effective for certain variant effect prediction tasks.</p>"},{"location":"resources/models/special_models/#installation","title":"Installation","text":"<p>GPN requires installing its package directly from the original GitHub repository.</p> <pre><code># Activate your virtual environment\nuv pip install git+https://github.com/songlab-cal/gpn.git\n</code></pre>"},{"location":"resources/models/special_models/#usage-and-application-scenarios","title":"Usage and Application Scenarios","text":"<p>GPN is a Masked Language Model (MLM) and is best used for zero-shot scoring tasks, such as in silico mutagenesis.</p> <p>Example: Using GPN for mutation analysis</p> <pre><code>from dnallm import load_config, Mutagenesis, load_model_and_tokenizer\n\n# 1. Use a config with task_type: \"mask\"\nconfigs = load_config(\"path/to/your/mask_config.yaml\")\n\n# 2. Load the GPN model\n# Note: The model ID might be a mirror like 'lgq12697/gpn-brassicales'\nmodel, tokenizer = load_model_and_tokenizer(\n    \"songlab/gpn-brassicales\",\n    task_config=configs['task'],\n    source=\"huggingface\"\n)\n\n# 3. Perform mutation analysis\nmut_analyzer = Mutagenesis(model=model, tokenizer=tokenizer, config=configs)\nsequence = \"GATTACAGATTACAGATTACA...\"\nmut_analyzer.mutate_sequence(sequence, replace_mut=True)\npredictions = mut_analyzer.evaluate() # Uses MLM scoring\nmut_analyzer.plot(predictions, save_path=\"./results/gpn_mut_effects.pdf\")\n</code></pre>"},{"location":"resources/models/special_models/#2-lucaone","title":"2. LucaOne","text":"<p>[!NOTE] Dependency: <code>lucagplm</code></p>"},{"location":"resources/models/special_models/#introduction_1","title":"Introduction","text":"<p>LucaOne is a generalized foundation model trained on a unified language of nucleic acids and proteins. Its unique training allows it to handle both DNA/RNA and protein sequences, making it versatile for tasks involving the central dogma.</p>"},{"location":"resources/models/special_models/#installation_1","title":"Installation","text":"<p>LucaOne requires the <code>lucagplm</code> package.</p> <pre><code># Activate your virtual environment\nuv pip install lucagplm\n</code></pre>"},{"location":"resources/models/special_models/#usage-and-application-scenarios_1","title":"Usage and Application Scenarios","text":"<p>LucaOne is an MLM-style model and is primarily used for feature extraction and zero-shot scoring.</p> <p>Example: Loading the LucaOne model</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer\n\nconfigs = load_config(\"path/to/your/mask_config.yaml\")\n\n# Note: The model ID might be a mirror like 'lgq12697/LucaOne-default-step36M'\nmodel, tokenizer = load_model_and_tokenizer(\n    \"LucaGroup/LucaOne-default-step36M\",\n    task_config=configs['task'],\n    source=\"huggingface\"\n)\n\nprint(\"LucaOne model loaded successfully!\")\n</code></pre>"},{"location":"resources/models/special_models/#3-omni-dna","title":"3. Omni-DNA","text":"<p>[!NOTE] Dependency: <code>ai2-olmo</code></p>"},{"location":"resources/models/special_models/#introduction_2","title":"Introduction","text":"<p>Omni-DNA is a causal language model (CLM) based on the OLMo architecture. It's designed as a unified genomic foundation model for cross-modal and multi-task learning.</p>"},{"location":"resources/models/special_models/#installation_2","title":"Installation","text":"<pre><code># Activate your virtual environment\nuv pip install ai2-olmo\n</code></pre>"},{"location":"resources/models/special_models/#usage-and-application-scenarios_2","title":"Usage and Application Scenarios","text":"<p>As a CLM, Omni-DNA is well-suited for sequence generation and zero-shot scoring tasks.</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer\n\nconfigs = load_config(\"path/to/your/generation_config.yaml\")\n\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zehui127/Omni-DNA-20M\",\n    task_config=configs['task'],\n    source=\"huggingface\"\n)\n\nprint(\"Omni-DNA model loaded successfully!\")\n</code></pre>"},{"location":"resources/models/special_models/#4-megadna","title":"4. megaDNA","text":"<p>[!NOTE] Dependency: Installed from source</p>"},{"location":"resources/models/special_models/#introduction_3","title":"Introduction","text":"<p>megaDNA is a model architecture that requires installation from its source repository.</p>"},{"location":"resources/models/special_models/#installation_3","title":"Installation","text":"<pre><code># Activate your virtual environment\ngit clone https://github.com/lingxusb/megaDNA\ncd megaDNA\nuv pip install .\n</code></pre>"},{"location":"resources/models/special_models/#5-enformer-and-borzoi","title":"5. Enformer and Borzoi","text":"<p>[!NOTE] Dependencies: <code>enformer-pytorch</code>, <code>borzoi-pytorch</code></p>"},{"location":"resources/models/special_models/#introduction_4","title":"Introduction","text":"<p>Enformer and Borzoi are popular models for predicting gene expression from DNA sequences. DNALLM supports PyTorch implementations of these models.</p>"},{"location":"resources/models/special_models/#installation_4","title":"Installation","text":"<pre><code># Activate your virtual environment\nuv pip install enformer-pytorch borzoi-pytorch\n</code></pre>"},{"location":"resources/models/special_models/#6-other-models-with-special-dependencies","title":"6. Other Models with Special Dependencies","text":"<p>For clarity, here is a summary of other model types covered in separate guides that also require special installation steps.</p>"},{"location":"resources/models/special_models/#mamba-based-models","title":"Mamba-based Models","text":"<p>[!NOTE] Dependency: <code>mamba-ssm</code>, <code>causal-conv1d</code></p> <ul> <li>Models: Plant DNAMamba, and other models using the native Mamba architecture.</li> <li>Details: These models require a CUDA-enabled GPU and specific compiled packages for optimal performance.</li> <li>Guide: See the Guide to Mamba and State-Space Models (SSMs) for full installation and usage instructions.</li> </ul>"},{"location":"resources/models/special_models/#evo-1","title":"EVO-1","text":"<p>[!NOTE] Dependency: <code>evo-model</code></p> <ul> <li>Models: <code>arcinstitute/evo-1-131k-base</code> and its variants.</li> <li>Details: EVO-1 is a long-context model based on the StripedHyena architecture.</li> <li>Guide: See the Guide to EVO Models for installation and usage.</li> </ul>"},{"location":"resources/models/special_models/#evo-2","title":"EVO-2","text":"<p>[!NOTE] Dependencies: <code>transformer-engine</code>, <code>evo2</code>, <code>flash-attn</code> (optional)</p> <ul> <li>Models: <code>arcinstitute/evo-2-1b-8k</code> and other EVO-2 variants.</li> <li>Details: EVO-2 is a state-of-the-art, ultra-long-context model requiring Python &gt;= 3.11 and several specialized packages.</li> <li>Guide: See the Guide to EVO Models for detailed installation steps.</li> </ul>"},{"location":"user_guide/best_practices/","title":"Best Practices for DNALLM","text":"<p>To get the most out of DNALLM, follow these best practices for data handling, model selection, and training.</p>"},{"location":"user_guide/best_practices/#1-data-preparation","title":"1. Data Preparation","text":"<ul> <li> <p>Start with High-Quality Data: The principle of \"garbage in, garbage out\" is especially true for deep learning. Use sequences from trusted sources like NCBI or Ensembl.</p> </li> <li> <p>Perform Quality Control: Always clean your data before training.</p> <ul> <li>Use the <code>DNADataset.validate_sequences()</code> method to filter out sequences that are too short, too long, or contain invalid characters.</li> <li>Check for and handle class imbalance in classification tasks. You can oversample the minority class or use weighted loss functions.</li> </ul> </li> <li> <p>Use Efficient Formats: For large datasets, prefer high-performance formats like Parquet or Arrow over CSV. They are significantly faster to load and process.</p> <pre><code># Save your processed DataFrame to Parquet for faster loading next time\nmy_dataframe.to_parquet(\"processed_data.parquet\")\n\n# Load it quickly later\nfrom dnallm.datahandling import DNADataset\ndna_ds = DNADataset.load_local_data(\"processed_data.parquet\")\n</code></pre> </li> <li> <p>Leverage Data Augmentation: Increase the diversity of your training data to improve model generalization.</p> <ul> <li>For most DNA tasks, adding the reverse complement is a safe and effective augmentation strategy.</li> <li>Use <code>dna_ds.augment_reverse_complement()</code> to double your dataset size.</li> </ul> </li> </ul>"},{"location":"user_guide/best_practices/#2-model-selection","title":"2. Model Selection","text":"<ul> <li> <p>Match the Model to the Task:</p> <ul> <li>Classification/Feature Extraction: Use encoder-only models like DNABERT or Nucleotide Transformer (ESM-based). They are excellent at understanding sequence context.</li> <li>Sequence Generation: Use decoder-only models like DNAGPT (LLaMA-based) or Evo (Hyena-based). They are designed to predict the next token.</li> <li>Long Sequences (&gt;5kb): For very long sequences, consider architectures designed for efficiency, such as Caduceus or HyenaDNA. Standard transformers can be too slow and memory-intensive.</li> </ul> </li> <li> <p>Start with a Pre-trained Model: Never train from scratch unless you have a massive dataset (billions of sequences). Fine-tuning a model pre-trained on a large biological corpus (like DNABERT or Evo) will yield better results much faster.</p> </li> <li> <p>Check the Tokenizer: Ensure the model's tokenizer is appropriate for your data. Most DNA models use a k-mer based tokenizer. Using a model trained on English text with its original tokenizer will not work for DNA.</p> </li> </ul>"},{"location":"user_guide/best_practices/#3-training-and-fine-tuning","title":"3. Training and Fine-tuning","text":"<ul> <li> <p>Use Mixed-Precision Training: Enable <code>fp16</code> (or <code>bf16</code> on newer GPUs) in your training configuration. This can speed up training by 2-3x and significantly reduce memory usage with minimal impact on accuracy.</p> <pre><code># In your config.yaml\ntraining_args:\n  fp16: true\n</code></pre> </li> <li> <p>Optimize Memory Usage: If you encounter <code>CUDA out of memory</code> errors:</p> <ul> <li>Gradient Accumulation: This is the most effective technique. It simulates a larger batch size without using more memory. Set <code>gradient_accumulation_steps</code> to 2, 4, 8, or higher.</li> <li>Reduce Batch Size: Lower <code>per_device_train_batch_size</code>.</li> <li>Use 8-bit Optimizers: Set <code>optim: \"adamw_8bit\"</code> in your training arguments to save VRAM used by the optimizer.</li> </ul> </li> <li> <p>Log and Monitor Training: Use logging tools like Weights &amp; Biases (<code>wandb</code>) or TensorBoard to track your training progress. This helps you spot issues like overfitting or unstable training early. Enable it in your <code>training_args</code>.</p> <pre><code>training_args:\n  report_to: \"wandb\"\n</code></pre> </li> <li> <p>Start Small: Before launching a multi-day training run on your full dataset, test your entire pipeline on a small subset (e.g., 1% of the data) for one or two epochs. This ensures there are no bugs in your code or configuration.</p> </li> </ul>"},{"location":"user_guide/common_workflows/","title":"Common Workflows in DNALLM","text":"<p>DNALLM is designed to streamline common tasks in computational genomics. This guide covers three primary workflows: fine-tuning a model, performing inference, and benchmarking multiple models.</p>"},{"location":"user_guide/common_workflows/#1-fine-tuning-a-model","title":"1. Fine-tuning a Model","text":"<p>Fine-tuning adapts a pre-trained language model to a specific downstream task, such as classifying promoter sequences.</p>"},{"location":"user_guide/common_workflows/#workflow-steps","title":"Workflow Steps","text":"<ol> <li>Prepare a Configuration File: Define the model, dataset, and training parameters in a <code>.yaml</code> file.</li> <li>Load Data: Use the <code>DNADataset</code> class to load and preprocess your training data.</li> <li>Load Model: Load a pre-trained model and tokenizer.</li> <li>Initialize Trainer: Create a <code>DNATrainer</code> instance with your configuration, model, and data.</li> <li>Start Training: Call the <code>train()</code> method.</li> </ol>"},{"location":"user_guide/common_workflows/#example","title":"Example","text":"<p>This example fine-tunes <code>plant-dnabert-BPE</code> for a binary classification task.</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.datahandling import DNADataset\nfrom dnallm.finetune import DNATrainer\n\n# 1. Load configuration from a file\nconfigs = load_config(\"./example/notebooks/finetune_binary/finetune_config.yaml\")\n\n# 2. Load model and tokenizer\nmodel_name = \"zhangtaolab/plant-dnabert-BPE\"\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name,\n    task_config=configs[\"task\"],\n    source=\"huggingface\"\n)\n\n# 3. Prepare dataset\ndataset = DNADataset.load_local_data(\n    file_paths=\"./tests/test_data/binary_classification/train.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    tokenizer=tokenizer,\n)\ndataset.encode_sequences() # Tokenize the sequences\n\n# 4. Initialize the trainer\ntrainer = DNATrainer(\n    config=configs,\n    model=model,\n    datasets=dataset\n)\n\n# 5. Start the fine-tuning process\ntrainer.train()\n</code></pre>"},{"location":"user_guide/common_workflows/#2-in-silico-mutagenesis-analysis","title":"2. In-silico Mutagenesis Analysis","text":"<p>This workflow systematically introduces mutations into a sequence and evaluates their impact on the model's prediction, which is useful for identifying important nucleotides.</p>"},{"location":"user_guide/common_workflows/#workflow-steps_1","title":"Workflow Steps","text":"<ol> <li>Load a Fine-tuned Model: Use a model that has been trained for a specific task (e.g., predicting promoter strength).</li> <li>Initialize <code>Mutagenesis</code>: Create an instance of the <code>Mutagenesis</code> analyzer.</li> <li>Generate Mutations: Use <code>mutate_sequence()</code> to create all possible single-nucleotide substitutions.</li> <li>Evaluate Effects: Run inference on all mutated sequences.</li> <li>Visualize Results: Plot the mutation effects to create a saliency map.</li> </ol>"},{"location":"user_guide/common_workflows/#example_1","title":"Example","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.inference import Mutagenesis\n\n# 1. Load configuration and a fine-tuned model\nconfigs = load_config(\"./example/notebooks/in_silico_mutagenesis/inference_config.yaml\")\nmodel_name = \"zhangtaolab/plant-dnagpt-BPE-promoter_strength_protoplast\"\nmodel, tokenizer = load_model_and_tokenizer(model_name, task_config=configs[\"task\"])\n\n# 2. Initialize the mutagenesis analyzer\nmutagenesis = Mutagenesis(config=configs, model=model, tokenizer=tokenizer)\n\n# 3. Generate and evaluate mutations for a sequence\nsequence = \"AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC\"\nmutagenesis.mutate_sequence(sequence, replace_mut=True)\npredictions = mutagenesis.evaluate(strategy=\"mean\")\n\n# 4. Plot and save the results\nplot = mutagenesis.plot(predictions, save_path=\"mutation_effects.pdf\")\nprint(\"Mutation analysis complete. Plot saved to mutation_effects.pdf\")\n</code></pre> <p>For more workflows, such as benchmarking and embedding extraction, explore the Tutorials section.</p>"},{"location":"user_guide/getting_started/","title":"Getting Started with DNALLM","text":"<p>Welcome to DNALLM! This guide will walk you through the initial setup and first steps with this powerful toolkit for DNA language models.</p>"},{"location":"user_guide/getting_started/#1-project-overview","title":"1. Project Overview","text":"<p>DNALLM is an open-source toolkit designed for large language model (LLM) applications in DNA sequence analysis and bioinformatics. It provides a comprehensive suite for:</p> <ul> <li>Model Training &amp; Fine-tuning: Supports a variety of DNA-related tasks, including classification, regression, and named entity recognition (NER).</li> <li>Inference &amp; Benchmarking: Enables efficient model inference, mutagenesis analysis, and multi-model benchmarking.</li> <li>Data Processing: Includes tools for dataset generation, cleaning, formatting, and augmentation.</li> <li>Model Management: Offers flexible loading of different DNA language models.</li> <li>Extensibility: Features a modular design for easy integration and secondary development.</li> </ul>"},{"location":"user_guide/getting_started/#2-quick-start-installation","title":"2. Quick Start: Installation","text":"<p>Getting DNALLM installed is the first step. We recommend using <code>uv</code>, a fast Python package manager, within a virtual environment.</p>"},{"location":"user_guide/getting_started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>Git</li> <li>A virtual environment manager like <code>venv</code> (built-in) or <code>conda</code>.</li> </ul>"},{"location":"user_guide/getting_started/#installation-steps","title":"Installation Steps","text":"<ol> <li> <p>Clone the Repository <pre><code>git clone https://github.com/zhangtaolab/DNALLM.git\ncd DNALLM\n</code></pre></p> </li> <li> <p>Create and Activate a Virtual Environment     We'll use Python's built-in <code>venv</code>.     <pre><code># Create the environment\npython -m venv .venv\n\n# Activate it\nsource .venv/bin/activate  # On Linux/macOS\n# .venv\\Scripts\\activate  # On Windows\n</code></pre></p> </li> <li> <p>Install <code>uv</code> and DNALLM <pre><code># Install uv, the fast package manager\npip install uv\n\n# Install DNALLM and its core dependencies\nuv pip install -e '.[base]'\n</code></pre></p> </li> <li> <p>Verify the Installation <pre><code>python -c \"import dnallm; print('DNALLM installed successfully!')\"\n</code></pre></p> </li> </ol>"},{"location":"user_guide/getting_started/#gpu-and-mamba-support-optional","title":"GPU and Mamba Support (Optional)","text":"<p>For accelerated performance, you can install support for GPU and specialized model architectures like Mamba.</p> <pre><code># For GPU support with CUDA 12.4\nuv pip install -e '.[cuda124]'\n\n# For native Mamba architecture support\nuv pip install -e '.[mamba]' --no-cache-dir --no-build-isolation\n</code></pre> <p>For more detailed instructions, please see the full Installation Guide.</p>"},{"location":"user_guide/getting_started/#3-basic-usage-first-inference","title":"3. Basic Usage: First Inference","text":"<p>Let's run a simple inference to see the toolkit in action. This example loads a pre-trained model and uses it to make a prediction on a DNA sequence.</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.inference import DNAInference\n\n# 1. Load a pre-defined configuration\nconfigs = load_config(\"./example/notebooks/inference/inference_config.yaml\")\n\n# 2. Load a pre-trained model and its tokenizer from Hugging Face\nmodel_name = \"zhangtaolab/plant-dnagpt-BPE-promoter\"\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name, \n    task_config=configs[\"task\"], \n    source=\"huggingface\"\n)\n\n# 3. Initialize the inference engine\ninference_engine = DNAInference(config=configs, model=model, tokenizer=tokenizer)\n\n# 4. Make a prediction on a sample sequence\nsequence = \"TCACATCCGGGTGAAACCTCGAGTTCCTATAACCTGCCGACAGGTGGCGGGTCTTATAAAACTGATCACTACAATTCCCAATGGAAAAA\"\ninference_result = inference_engine.infer(sequence)\n\nprint(f\"Inference result: {inference_result}\")\n</code></pre> <p>You've just completed your first task with DNALLM! Now you're ready to explore more complex workflows.</p>"},{"location":"user_guide/models/","title":"Model Guides","text":"<p>This page provides access to comprehensive guides for different DNA language model architectures and their usage with DNALLM.</p>"},{"location":"user_guide/models/#model-architecture-guides","title":"Model Architecture Guides","text":""},{"location":"user_guide/models/#core-architectures","title":"Core Architectures","text":"<ul> <li>BERT Models: DNABERT, DNABERT-2, and BERT-based models for DNA sequence analysis</li> <li>Caduceus Models: Caduceus-Ph, Caduceus-Ps, and PlantCaduceus models</li> <li>ESM Models: Nucleotide Transformer and ESM-based models</li> <li>Hyena Models: HyenaDNA and Hyena-based architectures</li> <li>Llama Models: GENERator, OmniNA, and Llama-based models</li> </ul>"},{"location":"user_guide/models/#specialized-architectures","title":"Specialized Architectures","text":"<ul> <li>EVO Models: EVO-1 and EVO-2 models for ultra-long sequence modeling</li> <li>Mamba Models: Mamba-based models for efficient sequence processing</li> <li>Flash Attention Models: Models optimized with Flash Attention</li> <li>Special Models: Other specialized model architectures</li> </ul>"},{"location":"user_guide/models/#model-resources","title":"Model Resources","text":""},{"location":"user_guide/models/#selection-and-troubleshooting","title":"Selection and Troubleshooting","text":"<ul> <li>Model Selection Guide: Choose the right model for your specific task</li> <li>Model Troubleshooting: Common issues and solutions for model usage</li> <li>Model Zoo: Complete list of supported models and their capabilities</li> </ul>"},{"location":"user_guide/models/#quick-reference","title":"Quick Reference","text":""},{"location":"user_guide/models/#by-task-type","title":"By Task Type","text":"Task Type Recommended Models Guide Classification DNABERT, Plant DNABERT BERT Models Generation Plant DNAGPT, GenomeOcean Llama Models Long Sequences EVO-1, EVO-2 EVO Models Efficient Processing DNAMamba, Mamba variants Mamba Models Plant-specific Plant DNABERT, PlantCaduceus Plant Models"},{"location":"user_guide/models/#by-model-size","title":"By Model Size","text":"Size Category Examples Use Case Small (&lt;100M) Caduceus-Ph, HyenaDNA Fast inference, real-time applications Medium (100M-1B) DNABERT, Plant models Balanced performance and speed Large (1B-10B) Nucleotide Transformer, EVO-1 High accuracy, complex tasks Extra Large (&gt;10B) EVO-2 (40B) State-of-the-art performance"},{"location":"user_guide/models/#getting-started","title":"Getting Started","text":""},{"location":"user_guide/models/#basic-model-loading","title":"Basic Model Loading","text":"<pre><code>from dnallm import load_model_and_tokenizer\n\n# Load a DNA-specific model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    source=\"huggingface\"\n)\n</code></pre>"},{"location":"user_guide/models/#model-selection-tips","title":"Model Selection Tips","text":"<ol> <li>For Classification Tasks: Choose BERT-based models (DNABERT, Plant DNABERT)</li> <li>For Generation Tasks: Use CausalLM models (Plant DNAGPT, GenomeOcean)</li> <li>For Large-scale Analysis: Consider Nucleotide Transformer or EVO models</li> <li>For Plant-specific Tasks: Prefer Plant-prefixed models</li> </ol>"},{"location":"user_guide/models/#related-resources","title":"Related Resources","text":"<ul> <li>Installation Guide: Set up your environment</li> <li>Quick Start: Get started with DNALLM</li> <li>Performance Optimization: Optimize model performance</li> <li>Fine-tuning Guide: Train models on your data</li> <li>Inference Guide: Use models for predictions</li> </ul> <p>For detailed information about specific model architectures and their usage, please refer to the individual model guides in the Resources section.</p>"},{"location":"user_guide/performance_guide/","title":"Performance Guide","text":"<p>Optimizing performance is key to working efficiently with large DNA models. This guide provides practical tips to speed up your training and inference workflows and reduce memory consumption.</p>"},{"location":"user_guide/performance_guide/#1-speeding-up-training","title":"1. Speeding Up Training","text":""},{"location":"user_guide/performance_guide/#use-mixed-precision-training-fp16bf16","title":"Use Mixed-Precision Training (FP16/BF16)","text":"<ul> <li>What it is: Using 16-bit floating-point numbers instead of 32-bit for model weights and computations.</li> <li>Why it helps: Modern GPUs have specialized Tensor Cores that process 16-bit operations much faster. It also cuts memory usage in half.</li> <li>How to use it: In your <code>config.yaml</code>, enable <code>fp16</code> or <code>bf16</code>.     <pre><code>training_args:\n  fp16: true  # For most GPUs\n  # bf16: true # For NVIDIA Ampere (A100, 30xx) or newer\n</code></pre></li> </ul>"},{"location":"user_guide/performance_guide/#use-multiple-gpus","title":"Use Multiple GPUs","text":"<ul> <li>What it is: Distributing your training job across all available GPUs on a machine.</li> <li>Why it helps: It parallelizes data processing, leading to a near-linear speedup with the number of GPUs.</li> <li>How to use it: Launch your training script with <code>torchrun</code>.     <pre><code># Example for a machine with 4 GPUs\ntorchrun --nproc_per_node=4 -m dnallm.cli.finetune --config_file your_config.yaml\n</code></pre></li> </ul>"},{"location":"user_guide/performance_guide/#enable-flash-attention","title":"Enable Flash Attention","text":"<ul> <li>What it is: A highly optimized implementation of the attention mechanism.</li> <li>Why it helps: It's faster and more memory-efficient than the standard attention, especially for longer sequences.</li> <li>How to use it: Install <code>flash-attn</code> and enable it in your model configuration.     <pre><code>pip install flash-attn\n</code></pre> <pre><code># In your config.yaml\nmodel_args:\n  attn_implementation: \"flash_attention_2\"\n</code></pre> Note: This is only supported by certain model architectures like LLaMA and Evo.</li> </ul>"},{"location":"user_guide/performance_guide/#2-reducing-memory-usage-vram","title":"2. Reducing Memory Usage (VRAM)","text":""},{"location":"user_guide/performance_guide/#use-gradient-accumulation","title":"Use Gradient Accumulation","text":"<ul> <li>What it is: Simulating a large batch size by accumulating gradients over several smaller forward/backward passes before updating the model weights.</li> <li>Why it helps: This is the most effective way to train large models on GPUs with limited VRAM. It drastically reduces memory requirements with a minimal impact on speed.</li> <li>How to use it: Set <code>gradient_accumulation_steps</code> in your training configuration.     <pre><code>training_args:\n  per_device_train_batch_size: 2 # A small size that fits in memory\n  gradient_accumulation_steps: 16 # Effective batch size = 2 * 16 = 32\n</code></pre></li> </ul>"},{"location":"user_guide/performance_guide/#use-model-quantization-qlora","title":"Use Model Quantization (QLoRA)","text":"<ul> <li>What it is: Loading the model with its weights converted to a lower-precision format, like 4-bit integers.</li> <li>Why it helps: It dramatically reduces the model's memory footprint, allowing you to fine-tune very large models on consumer-grade GPUs.</li> <li>How to use it: Use the <code>load_in_4bit</code> or <code>load_in_8bit</code> flags when loading the model. This is typically used with LoRA (Low-Rank Adaptation).     <pre><code># Example of loading a model in 4-bit for QLoRA fine-tuning\nmodel, tokenizer = load_model_and_tokenizer(\n    \"GenerTeam/GENERator-eukaryote-1.2b-base\",\n    load_in_4bit=True,\n    device_map=\"auto\"\n)\n</code></pre></li> </ul>"},{"location":"user_guide/performance_guide/#3-speeding-up-inference","title":"3. Speeding Up Inference","text":""},{"location":"user_guide/performance_guide/#use-batching","title":"Use Batching","text":"<ul> <li>What it is: Grouping multiple sequences together and processing them in a single batch.</li> <li>Why it helps: It maximizes GPU utilization and is much more efficient than processing sequences one by one.</li> </ul>"},{"location":"user_guide/performance_guide/#use-torchcompile","title":"Use <code>torch.compile</code>","text":"<ul> <li>What it is: A feature in PyTorch 2.0+ that JIT-compiles your model into optimized code.</li> <li>Why it helps: It can provide a significant speedup (up to 2x) with a single line of code, especially for inference.     <pre><code>import torch\n# model = your loaded model\ncompiled_model = torch.compile(model)\n# Use compiled_model for inference\n</code></pre></li> </ul>"},{"location":"user_guide/troubleshooting/","title":"Troubleshooting","text":"<p>This page provides quick access to troubleshooting resources and solutions for common DNALLM issues.</p>"},{"location":"user_guide/troubleshooting/#quick-links","title":"Quick Links","text":""},{"location":"user_guide/troubleshooting/#comprehensive-faq","title":"\ud83d\udd27 Comprehensive FAQ","text":"<p>For detailed solutions to common problems, see our Frequently Asked Questions (FAQ) page, which covers:</p> <ul> <li>Installation Issues: Package installation, dependency conflicts, network problems</li> <li>Training Issues: CUDA out of memory, loss instability, optimization problems</li> <li>Model Loading: Custom architectures, tokenizer mismatches, cache issues</li> <li>Performance Issues: Memory optimization, speed improvements, hardware requirements</li> <li>Task-Specific Issues: Model-task mismatches, label encoding problems</li> </ul>"},{"location":"user_guide/troubleshooting/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>Model Selection Guide: Choose the right model for your task</li> <li>Model Troubleshooting: Model-specific issues and solutions</li> <li>Performance Optimization: Speed and memory optimization guides</li> <li>Installation Guide: Complete installation instructions</li> </ul>"},{"location":"user_guide/troubleshooting/#common-quick-fixes","title":"Common Quick Fixes","text":""},{"location":"user_guide/troubleshooting/#installation-problems","title":"Installation Problems","text":"<pre><code># For Mamba models\nuv pip install -e '.[mamba]' --no-cache-dir --no-build-isolation\n\n# For EVO models\nuv pip install evo-model  # EVO-1\nuv pip install evo2       # EVO-2\n\n# For network issues\nexport HTTP_PROXY=\"http://your.proxy.server:port\"\nexport HTTPS_PROXY=\"http://your.proxy.server:port\"\n</code></pre>"},{"location":"user_guide/troubleshooting/#memory-issues","title":"Memory Issues","text":"<pre><code># In your config file\ntraining_args:\n  gradient_accumulation_steps: 4\n  per_device_train_batch_size: 2\n  fp16: true\n  gradient_checkpointing: true\n</code></pre>"},{"location":"user_guide/troubleshooting/#model-loading","title":"Model Loading","text":"<pre><code># For custom architectures\nmodel, tokenizer = load_model_and_tokenizer(\n    \"model_name\",\n    trust_remote_code=True\n)\n</code></pre>"},{"location":"user_guide/troubleshooting/#still-need-help","title":"Still Need Help?","text":"<p>If you can't find the answer to your question:</p> <ol> <li>Check the FAQ for comprehensive solutions</li> <li>Search GitHub Issues for similar problems</li> <li>Create a new issue with detailed information about your problem</li> <li>Join community discussions on GitHub</li> </ol> <p>For the most up-to-date troubleshooting information, always refer to the FAQ page.</p>"},{"location":"user_guide/benchmark/","title":"Model Benchmarking","text":"<p>This section provides comprehensive tutorials and guides for benchmarking DNA language models using DNALLM. Benchmarking allows you to compare model performance across different tasks, datasets, and evaluation metrics.</p>"},{"location":"user_guide/benchmark/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Basic Benchmarking: Get started with simple model comparisons</li> <li>Advanced Techniques: Cross-validation, custom metrics, and performance profiling</li> <li>Real-world Examples: Practical applications and use cases</li> <li>Best Practices: Optimization strategies and troubleshooting</li> </ul>"},{"location":"user_guide/benchmark/#quick-navigation","title":"Quick Navigation","text":"Topic Description Difficulty Getting Started Basic benchmarking setup and configuration Beginner Advanced Techniques Cross-validation, custom metrics, profiling Intermediate Configuration Guide Detailed configuration options and examples Intermediate Examples and Use Cases Real-world benchmarking scenarios All Levels Troubleshooting Common issues and solutions All Levels"},{"location":"user_guide/benchmark/#prerequisites","title":"Prerequisites","text":"<p>Before diving into benchmarking, ensure you have:</p> <ul> <li>\u2705 DNALLM installed and configured</li> <li>\u2705 Access to DNA language models</li> <li>\u2705 Test datasets in appropriate formats</li> <li>\u2705 Sufficient computational resources</li> </ul>"},{"location":"user_guide/benchmark/#quick-start","title":"Quick Start","text":"<pre><code>from dnallm import load_config, Benchmark\n\n# Load configuration\nconfig = load_config(\"benchmark_config.yaml\")\n\n# Initialize and run benchmark\nbenchmark = Benchmark(config=config)\nresults = benchmark.run()\n</code></pre>"},{"location":"user_guide/benchmark/#key-features","title":"Key Features","text":"<ul> <li>Multi-Model Comparison: Evaluate multiple models simultaneously</li> <li>Comprehensive Metrics: Accuracy, F1, precision, recall, ROC-AUC, and more</li> <li>Performance Profiling: Memory usage, inference time, and resource monitoring</li> <li>Flexible Output: HTML reports, CSV exports, and interactive visualizations</li> <li>Cross-Validation: Robust evaluation with k-fold validation</li> </ul>"},{"location":"user_guide/benchmark/#next-steps","title":"Next Steps","text":"<p>Choose your path:</p> <ul> <li>New to benchmarking? Start with Getting Started</li> <li>Want advanced features? Jump to Advanced Techniques</li> <li>Need configuration help? Check Configuration Guide</li> <li>Looking for examples? Explore Examples and Use Cases</li> </ul> <p>Need Help? Check our FAQ or open an issue on GitHub.</p>"},{"location":"user_guide/benchmark/advanced_techniques/","title":"Advanced Benchmarking Techniques","text":"<p>This guide covers advanced benchmarking techniques including cross-validation, custom metrics, performance profiling, and optimization strategies.</p>"},{"location":"user_guide/benchmark/advanced_techniques/#overview","title":"Overview","text":"<p>Advanced benchmarking techniques help you: - Ensure robust and reliable model evaluation - Implement custom evaluation metrics - Profile and optimize model performance - Handle complex benchmarking scenarios</p>"},{"location":"user_guide/benchmark/advanced_techniques/#cross-validation-benchmarking","title":"Cross-Validation Benchmarking","text":"<p>Cross-validation provides more robust performance estimates by testing models on multiple data splits.</p>"},{"location":"user_guide/benchmark/advanced_techniques/#k-fold-cross-validation","title":"K-Fold Cross-Validation","text":"<pre><code>from sklearn.model_selection import KFold\nimport numpy as np\nfrom dnallm import Benchmark\n\ndef run_cross_validation_benchmark(models, datasets, k_folds=5):\n    \"\"\"Run k-fold cross-validation benchmark.\"\"\"\n\n    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n    cv_results = {}\n\n    for model_name, model_info in models.items():\n        cv_results[model_name] = {}\n\n        for dataset_name, dataset in datasets.items():\n            fold_scores = []\n\n            # Split dataset into k folds\n            for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n                print(f\"Running fold {fold + 1}/{k_folds} for {model_name} on {dataset_name}\")\n\n                # Split data for this fold\n                train_data = dataset.select(train_idx)\n                val_data = dataset.select(val_idx)\n\n                # Evaluate on validation fold\n                fold_result = benchmark.evaluate_single_model(\n                    model_info[\"model\"],\n                    model_info[\"tokenizer\"],\n                    val_data,\n                    metrics=[\"accuracy\", \"f1_score\", \"precision\", \"recall\"]\n                )\n\n                fold_scores.append(fold_result)\n\n            # Aggregate fold results\n            cv_results[model_name][dataset_name] = {\n                \"mean_accuracy\": np.mean([s[\"accuracy\"] for s in fold_scores]),\n                \"std_accuracy\": np.mean([s[\"accuracy\"] for s in fold_scores]),\n                \"mean_f1\": np.mean([s[\"f1_score\"] for s in fold_scores]),\n                \"std_f1\": np.std([s[\"f1_score\"] for s in fold_scores]),\n                \"fold_results\": fold_scores\n            }\n\n    return cv_results\n\n# Usage\ncv_results = run_cross_validation_benchmark(loaded_models, datasets, k_folds=5)\n\n# Display results\nfor model_name, results in cv_results.items():\n    print(f\"\\n{model_name} Cross-Validation Results:\")\n    for dataset_name, metrics in results.items():\n        print(f\"  {dataset_name}:\")\n        print(f\"    Accuracy: {metrics['mean_accuracy']:.4f} \u00b1 {metrics['std_accuracy']:.4f}\")\n        print(f\"    F1 Score: {metrics['mean_f1']:.4f} \u00b1 {metrics['std_f1']:.4f}\")\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#stratified-k-fold-for-imbalanced-data","title":"Stratified K-Fold for Imbalanced Data","text":"<pre><code>from sklearn.model_selection import StratifiedKFold\n\ndef run_stratified_cv_benchmark(models, datasets, k_folds=5):\n    \"\"\"Run stratified k-fold cross-validation for imbalanced datasets.\"\"\"\n\n    stratified_kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n    cv_results = {}\n\n    for model_name, model_info in models.items():\n        cv_results[model_name] = {}\n\n        for dataset_name, dataset in datasets.items():\n            # Get labels for stratification\n            labels = [item[\"label\"] for item in dataset]\n\n            fold_scores = []\n            for fold, (train_idx, val_idx) in enumerate(stratified_kfold.split(dataset, labels)):\n                # ... rest of the implementation similar to above\n                pass\n\n    return cv_results\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#custom-evaluation-metrics","title":"Custom Evaluation Metrics","text":"<p>DNALLM allows you to implement custom evaluation metrics for specific use cases.</p>"},{"location":"user_guide/benchmark/advanced_techniques/#basic-custom-metric","title":"Basic Custom Metric","text":"<pre><code>from dnallm.tasks.metrics import CustomMetric\nimport numpy as np\n\nclass GCContentMetric(CustomMetric):\n    \"\"\"Custom metric to evaluate GC content prediction accuracy.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.name = \"gc_content_accuracy\"\n\n    def compute(self, predictions, targets, sequences=None):\n        \"\"\"Compute GC content prediction accuracy.\"\"\"\n        if sequences is None:\n            return {\"gc_content_accuracy\": 0.0}\n\n        gc_accuracy = []\n        for pred, target, seq in zip(predictions, targets, sequences):\n            # Calculate predicted GC content\n            pred_gc = self._calculate_gc_content(seq, pred)\n            # Calculate actual GC content\n            actual_gc = self._calculate_gc_content(seq, target)\n\n            # Compute accuracy\n            accuracy = 1.0 - abs(pred_gc - actual_gc) / max(actual_gc, 0.01)\n            gc_accuracy.append(max(0.0, accuracy))\n\n        return {\"gc_content_accuracy\": np.mean(gc_accuracy)}\n\n    def _calculate_gc_content(self, sequence, mask):\n        \"\"\"Calculate GC content based on sequence and mask.\"\"\"\n        gc_count = 0\n        total_count = 0\n\n        for i, char in enumerate(sequence):\n            if mask[i] == 1:  # If position is masked\n                if char in ['G', 'C']:\n                    gc_count += 1\n                total_count += 1\n\n        return gc_count / max(total_count, 1)\n\n# Usage in benchmark\nbenchmark = Benchmark(\n    models=loaded_models,\n    datasets=datasets,\n    metrics=[\"accuracy\", \"f1_score\", GCContentMetric()],\n    batch_size=32\n)\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#advanced-custom-metric-with-multiple-outputs","title":"Advanced Custom Metric with Multiple Outputs","text":"<pre><code>class ComprehensiveDNAMetric(CustomMetric):\n    \"\"\"Comprehensive DNA sequence evaluation metric.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.name = \"comprehensive_dna_score\"\n\n    def compute(self, predictions, targets, sequences=None, **kwargs):\n        \"\"\"Compute comprehensive DNA evaluation score.\"\"\"\n        results = {}\n\n        # Base accuracy\n        results[\"base_accuracy\"] = self._compute_accuracy(predictions, targets)\n\n        # Sequence-specific metrics\n        if sequences is not None:\n            results[\"gc_content_score\"] = self._compute_gc_content_score(predictions, targets, sequences)\n            results[\"conservation_score\"] = self._compute_conservation_score(predictions, targets, sequences)\n            results[\"motif_score\"] = self._compute_motif_score(predictions, targets, sequences)\n\n        # Overall score (weighted average)\n        weights = [0.4, 0.2, 0.2, 0.2]  # Adjust weights as needed\n        scores = [results[\"base_accuracy\"], results[\"gc_content_score\"], \n                 results[\"conservation_score\"], results[\"motif_score\"]]\n\n        results[\"overall_score\"] = np.average(scores, weights=weights)\n\n        return results\n\n    def _compute_accuracy(self, predictions, targets):\n        \"\"\"Compute basic accuracy.\"\"\"\n        return np.mean(np.array(predictions) == np.array(targets))\n\n    def _compute_gc_content_score(self, predictions, targets, sequences):\n        \"\"\"Compute GC content prediction score.\"\"\"\n        # Implementation details...\n        return 0.85\n\n    def _compute_conservation_score(self, predictions, targets, sequences):\n        \"\"\"Compute conservation prediction score.\"\"\"\n        # Implementation details...\n        return 0.78\n\n    def _compute_motif_score(self, predictions, targets, sequences):\n        \"\"\"Compute motif prediction score.\"\"\"\n        # Implementation details...\n        return 0.92\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#performance-profiling","title":"Performance Profiling","text":"<p>Performance profiling helps you understand model efficiency and identify bottlenecks.</p>"},{"location":"user_guide/benchmark/advanced_techniques/#basic-performance-profiling","title":"Basic Performance Profiling","text":"<pre><code>import time\nimport psutil\nimport torch\nfrom memory_profiler import profile\n\ndef profile_model_performance(model, tokenizer, dataset, num_samples=100):\n    \"\"\"Profile model performance including time and memory usage.\"\"\"\n\n    # Select subset for profiling\n    profile_data = dataset.select(range(min(num_samples, len(dataset))))\n\n    # Warm up (important for accurate timing)\n    print(\"Warming up model...\")\n    for _ in range(10):\n        _ = model(torch.randn(1, 512).to(model.device))\n\n    # Memory profiling\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        memory_before = torch.cuda.memory_allocated()\n        memory_reserved_before = torch.cuda.memory_reserved()\n\n    # Time profiling\n    print(\"Running performance profiling...\")\n    start_time = time.time()\n\n    predictions = []\n    batch_times = []\n\n    for i, batch in enumerate(profile_data.get_dataloader(batch_size=1)):\n        batch_start = time.time()\n\n        with torch.no_grad():\n            outputs = model(batch[\"input_ids\"].to(model.device))\n            predictions.append(outputs.logits.argmax(-1).cpu())\n\n        batch_time = time.time() - batch_start\n        batch_times.append(batch_time)\n\n        if i % 20 == 0:\n            print(f\"Processed {i+1}/{num_samples} samples...\")\n\n    total_time = time.time() - start_time\n\n    # Memory after\n    if torch.cuda.is_available():\n        memory_after = torch.cuda.memory_allocated()\n        memory_reserved_after = torch.cuda.memory_reserved()\n        memory_used = memory_after - memory_before\n        memory_reserved = memory_reserved_after - memory_reserved_before\n\n    # CPU profiling\n    cpu_percent = psutil.cpu_percent(interval=1)\n\n    # Calculate statistics\n    avg_batch_time = np.mean(batch_times)\n    std_batch_time = np.std(batch_times)\n\n    return {\n        \"total_inference_time\": total_time,\n        \"avg_batch_time\": avg_batch_time,\n        \"std_batch_time\": std_batch_time,\n        \"samples_per_second\": num_samples / total_time,\n        \"memory_used_mb\": memory_used / 1024 / 1024 if torch.cuda.is_available() else 0,\n        \"memory_reserved_mb\": memory_reserved / 1024 / 1024 if torch.cuda.is_available() else 0,\n        \"cpu_usage_percent\": cpu_percent,\n        \"throughput\": num_samples / total_time\n    }\n\n# Profile all models\nperformance_profiles = {}\nfor model_name, model_info in loaded_models.items():\n    print(f\"\\nProfiling {model_name}...\")\n    performance_profiles[model_name] = profile_model_performance(\n        model_info[\"model\"],\n        model_info[\"tokenizer\"],\n        datasets[\"promoter_strength\"],\n        num_samples=200\n    )\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#advanced-memory-profiling","title":"Advanced Memory Profiling","text":"<pre><code>import tracemalloc\nfrom contextlib import contextmanager\n\n@contextmanager\ndef memory_profiler():\n    \"\"\"Context manager for detailed memory profiling.\"\"\"\n    tracemalloc.start()\n    try:\n        yield\n    finally:\n        current, peak = tracemalloc.get_traced_memory()\n        print(f\"Current memory usage: {current / 1024 / 1024:.2f} MB\")\n        print(f\"Peak memory usage: {peak / 1024 / 1024:.2f} MB\")\n        tracemalloc.stop()\n\ndef detailed_memory_profile(model, dataset, batch_size=32):\n    \"\"\"Detailed memory profiling with tracemalloc.\"\"\"\n\n    with memory_profiler():\n        # Load data\n        dataloader = dataset.get_dataloader(batch_size=batch_size)\n\n        # Run inference\n        for batch in dataloader:\n            with torch.no_grad():\n                outputs = model(batch[\"input_ids\"].to(model.device))\n\n            # Force garbage collection\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"user_guide/benchmark/advanced_techniques/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>from torch.cuda.amp import autocast, GradScaler\n\ndef benchmark_with_mixed_precision(model, tokenizer, dataset):\n    \"\"\"Benchmark model with mixed precision for improved performance.\"\"\"\n\n    # Enable mixed precision\n    scaler = GradScaler()\n\n    start_time = time.time()\n    predictions = []\n\n    for batch in dataset.get_dataloader(batch_size=32):\n        with autocast():\n            outputs = model(batch[\"input_ids\"].to(model.device))\n            predictions.append(outputs.logits.argmax(-1).cpu())\n\n    mixed_precision_time = time.time() - start_time\n\n    # Compare with full precision\n    start_time = time.time()\n    predictions_fp32 = []\n\n    for batch in dataset.get_dataloader(batch_size=32):\n        outputs = model(batch[\"input_ids\"].to(model.device))\n        predictions_fp32.append(outputs.logits.argmax(-1).cpu())\n\n    fp32_time = time.time() - start_time\n\n    return {\n        \"mixed_precision_time\": mixed_precision_time,\n        \"fp32_time\": fp32_time,\n        \"speedup\": fp32_time / mixed_precision_time,\n        \"memory_savings\": \"~50% (estimated)\"\n    }\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#batch-size-optimization","title":"Batch Size Optimization","text":"<pre><code>def find_optimal_batch_size(model, dataset, max_batch_size=128):\n    \"\"\"Find optimal batch size for given model and hardware.\"\"\"\n\n    batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128]\n    results = {}\n\n    for batch_size in batch_sizes:\n        if batch_size &gt; max_batch_size:\n            continue\n\n        try:\n            # Test batch size\n            start_time = time.time()\n            memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n\n            # Run inference\n            dataloader = dataset.get_dataloader(batch_size=batch_size)\n            for batch in dataloader:\n                with torch.no_grad():\n                    outputs = model(batch[\"input_ids\"].to(model.device))\n                break  # Just test one batch\n\n            inference_time = time.time() - start_time\n            memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n\n            results[batch_size] = {\n                \"inference_time\": inference_time,\n                \"memory_used\": (memory_after - memory_before) / 1024 / 1024,\n                \"throughput\": batch_size / inference_time\n            }\n\n            print(f\"Batch size {batch_size}: {results[batch_size]}\")\n\n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"Batch size {batch_size} failed: OOM\")\n                break\n            else:\n                print(f\"Batch size {batch_size} failed: {e}\")\n\n    # Find optimal batch size\n    optimal_batch_size = max(results.keys(), key=lambda x: results[x][\"throughput\"])\n\n    return optimal_batch_size, results\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#advanced-benchmarking-scenarios","title":"Advanced Benchmarking Scenarios","text":""},{"location":"user_guide/benchmark/advanced_techniques/#multi-dataset-benchmarking","title":"Multi-Dataset Benchmarking","text":"<pre><code>def run_multi_dataset_benchmark(models, datasets, metrics):\n    \"\"\"Run benchmark across multiple datasets with aggregated results.\"\"\"\n\n    all_results = {}\n\n    for model_name, model_info in models.items():\n        all_results[model_name] = {\n            \"dataset_results\": {},\n            \"aggregated_metrics\": {}\n        }\n\n        # Run on each dataset\n        for dataset_name, dataset in datasets.items():\n            dataset_result = benchmark.evaluate_single_model(\n                model_info[\"model\"],\n                model_info[\"tokenizer\"],\n                dataset,\n                metrics=metrics\n            )\n\n            all_results[model_name][\"dataset_results\"][dataset_name] = dataset_result\n\n        # Aggregate across datasets\n        all_results[model_name][\"aggregated_metrics\"] = aggregate_metrics(\n            all_results[model_name][\"dataset_results\"]\n        )\n\n    return all_results\n\ndef aggregate_metrics(dataset_results):\n    \"\"\"Aggregate metrics across multiple datasets.\"\"\"\n    aggregated = {}\n\n    for metric in dataset_results[list(dataset_results.keys())[0]].keys():\n        values = [dataset_results[ds][metric] for ds in dataset_results.keys()]\n        aggregated[f\"{metric}_mean\"] = np.mean(values)\n        aggregated[f\"{metric}_std\"] = np.std(values)\n        aggregated[f\"{metric}_min\"] = np.min(values)\n        aggregated[f\"{metric}_max\"] = np.max(values)\n\n    return aggregated\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#time-series-benchmarking","title":"Time-Series Benchmarking","text":"<pre><code>def run_time_series_benchmark(model, dataset, time_column, interval_days=30):\n    \"\"\"Run benchmark on time-series data with temporal splits.\"\"\"\n\n    # Sort by time\n    sorted_data = sorted(dataset, key=lambda x: x[time_column])\n\n    # Create temporal splits\n    total_days = (sorted_data[-1][time_column] - sorted_data[0][time_column]).days\n    num_splits = total_days // interval_days\n\n    temporal_results = []\n\n    for i in range(num_splits):\n        start_idx = i * len(sorted_data) // num_splits\n        end_idx = (i + 1) * len(sorted_data) // num_splits\n\n        # Test on future data\n        test_data = sorted_data[end_idx:]\n        if len(test_data) == 0:\n            continue\n\n        # Evaluate performance\n        result = benchmark.evaluate_single_model(\n            model, tokenizer, test_data, metrics=[\"accuracy\", \"f1_score\"]\n        )\n\n        temporal_results.append({\n            \"time_period\": i,\n            \"start_date\": sorted_data[start_idx][time_column],\n            \"end_date\": sorted_data[end_idx][time_column],\n            \"test_size\": len(test_data),\n            **result\n        })\n\n    return temporal_results\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#best-practices","title":"Best Practices","text":""},{"location":"user_guide/benchmark/advanced_techniques/#1-reproducibility","title":"1. Reproducibility","text":"<pre><code># Set random seeds\nimport random\nimport numpy as np\nimport torch\n\ndef set_reproducibility(seed=42):\n    \"\"\"Set all random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Use in benchmark\nset_reproducibility(42)\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#2-resource-management","title":"2. Resource Management","text":"<pre><code>def cleanup_resources():\n    \"\"\"Clean up GPU memory and other resources.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n    import gc\n    gc.collect()\n\n# Call between model evaluations\nfor model_name, model_info in loaded_models.items():\n    # Run benchmark\n    result = benchmark.evaluate_single_model(...)\n\n    # Clean up\n    cleanup_resources()\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#3-progress-monitoring","title":"3. Progress Monitoring","text":"<pre><code>from tqdm import tqdm\nimport logging\n\ndef setup_logging():\n    \"\"\"Setup logging for benchmark progress.\"\"\"\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler('benchmark.log'),\n            logging.StreamHandler()\n        ]\n    )\n\n# Use in benchmark\nsetup_logging()\nlogger = logging.getLogger(__name__)\n\nfor model_name in tqdm(loaded_models.keys(), desc=\"Benchmarking models\"):\n    logger.info(f\"Starting benchmark for {model_name}\")\n    # ... benchmark code\n    logger.info(f\"Completed benchmark for {model_name}\")\n</code></pre>"},{"location":"user_guide/benchmark/advanced_techniques/#next-steps","title":"Next Steps","text":"<p>After mastering these advanced techniques:</p> <ol> <li>Explore Real-world Examples: See Examples and Use Cases</li> <li>Learn Configuration Options: Check Configuration Guide</li> <li>Troubleshoot Issues: Visit Troubleshooting</li> <li>Contribute: Help improve DNALLM's benchmarking capabilities</li> </ol> <p>Ready for real-world examples? Continue to Examples and Use Cases to see these techniques in action.</p>"},{"location":"user_guide/benchmark/configuration/","title":"Configuration Guide","text":"<p>This guide provides detailed information about all configuration options available for DNALLM benchmarking, including examples and best practices.</p>"},{"location":"user_guide/benchmark/configuration/#overview","title":"Overview","text":"<p>DNALLM benchmarking configuration is defined in YAML format and supports: - Model Configuration: Multiple models from different sources - Dataset Configuration: Various data formats and preprocessing options - Evaluation Settings: Metrics, batch sizes, and hardware options - Output Options: Report formats and visualization settings</p>"},{"location":"user_guide/benchmark/configuration/#configuration-structure","title":"Configuration Structure","text":""},{"location":"user_guide/benchmark/configuration/#basic-configuration-schema","title":"Basic Configuration Schema","text":"<pre><code>benchmark:\n  # Basic information\n  name: \"string\"\n  description: \"string\"\n\n  # Model definitions\n  models: []\n\n  # Dataset definitions\n  datasets: []\n\n  # Evaluation settings\n  evaluation: {}\n\n  # Output configuration\n  output: {}\n\n  # Advanced options\n  advanced: {}\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#model-configuration","title":"Model Configuration","text":""},{"location":"user_guide/benchmark/configuration/#basic-model-definition","title":"Basic Model Definition","text":"<pre><code>models:\n  - name: \"Plant DNABERT\"\n    path: \"zhangtaolab/plant-dnabert-BPE\"\n    source: \"huggingface\"\n    task_type: \"classification\"\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#advanced-model-configuration","title":"Advanced Model Configuration","text":"<pre><code>models:\n  - name: \"Plant DNABERT\"\n    path: \"zhangtaolab/plant-dnabert-BPE\"\n    source: \"huggingface\"\n    task_type: \"classification\"\n    revision: \"main\"  # Git branch/tag\n    trust_remote_code: true\n    torch_dtype: \"float16\"  # or \"float32\", \"bfloat16\"\n    device_map: \"auto\"\n    load_in_8bit: false\n    load_in_4bit: false\n\n  - name: \"Custom Model\"\n    path: \"/path/to/local/model\"\n    source: \"local\"\n    task_type: \"generation\"\n    model_class: \"CustomModelClass\"\n    tokenizer_class: \"CustomTokenizerClass\"\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#model-source-types","title":"Model Source Types","text":"Source Description Example <code>huggingface</code> Hugging Face Hub <code>\"zhangtaolab/plant-dnabert-BPE\"</code> <code>modelscope</code> ModelScope repository <code>\"zhangtaolab/plant-dnabert-BPE\"</code> <code>local</code> Local file system <code>\"/path/to/model\"</code> <code>s3</code> AWS S3 bucket <code>\"s3://bucket/model\"</code>"},{"location":"user_guide/benchmark/configuration/#task-types","title":"Task Types","text":"Task Type Description Use Case <code>classification</code> Binary/multi-class classification Promoter prediction, motif detection <code>generation</code> Sequence generation DNA synthesis, sequence design <code>masked</code> Masked language modeling Sequence completion, mutation analysis <code>embedding</code> Feature extraction Sequence representation, similarity <code>regression</code> Continuous value prediction Expression level, binding affinity"},{"location":"user_guide/benchmark/configuration/#dataset-configuration","title":"Dataset Configuration","text":""},{"location":"user_guide/benchmark/configuration/#basic-dataset-definition","title":"Basic Dataset Definition","text":"<pre><code>datasets:\n  - name: \"promoter_data\"\n    path: \"path/to/promoter_data.csv\"\n    task: \"binary_classification\"\n    text_column: \"sequence\"\n    label_column: \"label\"\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#advanced-dataset-configuration","title":"Advanced Dataset Configuration","text":"<pre><code>datasets:\n  - name: \"promoter_data\"\n    path: \"path/to/promoter_data.csv\"\n    task: \"binary_classification\"\n    text_column: \"sequence\"\n    label_column: \"label\"\n\n    # Preprocessing options\n    max_length: 512\n    truncation: true\n    padding: \"max_length\"\n\n    # Data splitting\n    test_size: 0.2\n    val_size: 0.1\n    random_state: 42\n\n    # Data filtering\n    min_length: 100\n    max_length: 1000\n    valid_chars: \"ACGT\"\n\n    # Data augmentation\n    augment: true\n    reverse_complement_ratio: 0.5\n    random_mutation_ratio: 0.1\n\n    # Custom preprocessing\n    preprocessors:\n      - \"remove_n_bases\"\n      - \"normalize_case\"\n      - \"add_padding\"\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#dataset-formats","title":"Dataset Formats","text":""},{"location":"user_guide/benchmark/configuration/#csvtsv-format","title":"CSV/TSV Format","text":"<pre><code>datasets:\n  - name: \"csv_dataset\"\n    path: \"data.csv\"\n    format: \"csv\"\n    separator: \",\"  # or \"\\t\" for TSV\n    encoding: \"utf-8\"\n    text_column: \"sequence\"\n    label_column: \"label\"\n    additional_columns: [\"metadata\", \"source\"]\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#json-format","title":"JSON Format","text":"<pre><code>datasets:\n  - name: \"json_dataset\"\n    path: \"data.json\"\n    format: \"json\"\n    text_key: \"sequence\"\n    label_key: \"label\"\n    nested_path: \"data.items\"  # For nested JSON structures\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#fasta-format","title":"FASTA Format","text":"<pre><code>datasets:\n  - name: \"fasta_dataset\"\n    path: \"sequences.fasta\"\n    format: \"fasta\"\n    label_parser: \"header\"  # Extract label from header\n    header_format: \"sequence_id|label:value\"  # Custom header format\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#arrowparquet-format","title":"Arrow/Parquet Format","text":"<pre><code>datasets:\n  - name: \"arrow_dataset\"\n    path: \"data.arrow\"\n    format: \"arrow\"\n    text_column: \"sequence\"\n    label_column: \"label\"\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#data-preprocessing-options","title":"Data Preprocessing Options","text":"<pre><code>datasets:\n  - name: \"processed_data\"\n    path: \"raw_data.csv\"\n\n    # Sequence processing\n    preprocessing:\n      remove_n_bases: true\n      normalize_case: true\n      add_padding: true\n      padding_size: 512\n\n    # Quality filtering\n    filtering:\n      min_length: 200\n      max_length: 1000\n      min_gc_content: 0.2\n      max_gc_content: 0.8\n      valid_chars: \"ACGT\"\n\n    # Data augmentation\n    augmentation:\n      reverse_complement: true\n      random_mutations: true\n      mutation_rate: 0.01\n      synthetic_samples: 1000\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#evaluation-configuration","title":"Evaluation Configuration","text":""},{"location":"user_guide/benchmark/configuration/#basic-evaluation-settings","title":"Basic Evaluation Settings","text":"<pre><code>evaluation:\n  batch_size: 32\n  max_length: 512\n  device: \"cuda\"\n  num_workers: 4\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#advanced-evaluation-options","title":"Advanced Evaluation Options","text":"<pre><code>evaluation:\n  # Batch processing\n  batch_size: 32\n  gradient_accumulation_steps: 1\n\n  # Sequence processing\n  max_length: 512\n  truncation: true\n  padding: \"max_length\"\n\n  # Hardware settings\n  device: \"cuda\"  # or \"cpu\", \"auto\"\n  num_workers: 4\n  pin_memory: true\n\n  # Performance optimization\n  use_fp16: true\n  use_bf16: false\n  mixed_precision: true\n\n  # Memory management\n  max_memory: \"16GB\"\n  memory_efficient_attention: true\n\n  # Reproducibility\n  seed: 42\n  deterministic: true\n\n  # Evaluation strategy\n  eval_strategy: \"steps\"  # or \"epoch\"\n  eval_steps: 100\n  eval_accumulation_steps: 1\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#device-configuration","title":"Device Configuration","text":"<pre><code>evaluation:\n  # Single GPU\n  device: \"cuda:0\"\n\n  # Multiple GPUs\n  device: \"cuda\"\n  parallel_strategy: \"data_parallel\"\n\n  # CPU only\n  device: \"cpu\"\n  num_threads: 8\n\n  # Auto device selection\n  device: \"auto\"\n  device_map: \"auto\"\n\n  # Mixed precision\n  use_fp16: true\n  use_bf16: false\n  mixed_precision: true\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#metrics-configuration","title":"Metrics Configuration","text":""},{"location":"user_guide/benchmark/configuration/#basic-metrics","title":"Basic Metrics","text":"<pre><code>metrics:\n  - \"accuracy\"\n  - \"f1_score\"\n  - \"precision\"\n  - \"recall\"\n  - \"roc_auc\"\n  - \"mse\"\n  - \"mae\"\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#advanced-metrics","title":"Advanced Metrics","text":"<pre><code>metrics:\n  # Classification metrics\n  - \"accuracy\"\n  - \"f1_score\"\n  - \"precision\"\n  - \"recall\"\n  - \"roc_auc\"\n  - \"pr_auc\"\n  - \"matthews_correlation\"\n\n  # Regression metrics\n  - \"mse\"\n  - \"mae\"\n  - \"rmse\"\n  - \"r2_score\"\n  - \"pearson_correlation\"\n  - \"spearman_correlation\"\n\n  # Custom metrics\n  - name: \"gc_content_accuracy\"\n    class: \"GCContentMetric\"\n    parameters:\n      threshold: 0.1\n\n  - name: \"conservation_score\"\n    class: \"ConservationMetric\"\n    parameters:\n      window_size: 10\n      similarity_threshold: 0.8\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#custom-metric-configuration","title":"Custom Metric Configuration","text":"<pre><code>metrics:\n  - name: \"custom_dna_metric\"\n    class: \"CustomDNAMetric\"\n    parameters:\n      gc_weight: 0.3\n      conservation_weight: 0.4\n      motif_weight: 0.3\n      threshold: 0.5\n    file_path: \"path/to/custom_metric.py\"\n    class_name: \"CustomDNAMetric\"\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#output-configuration","title":"Output Configuration","text":""},{"location":"user_guide/benchmark/configuration/#basic-output-settings","title":"Basic Output Settings","text":"<pre><code>output:\n  format: \"html\"\n  path: \"benchmark_results\"\n  save_predictions: true\n  generate_plots: true\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#advanced-output-options","title":"Advanced Output Options","text":"<pre><code>output:\n  # Output formats\n  formats: [\"html\", \"csv\", \"json\", \"pdf\"]\n\n  # File paths\n  path: \"benchmark_results\"\n  predictions_file: \"predictions.csv\"\n  metrics_file: \"metrics.json\"\n  plots_dir: \"plots\"\n\n  # Content options\n  save_predictions: true\n  save_embeddings: false\n  save_attention_maps: false\n  save_token_probabilities: false\n\n  # Visualization\n  generate_plots: true\n  plot_types: [\"bar\", \"line\", \"heatmap\", \"scatter\"]\n  plot_style: \"seaborn\"\n  plot_colors: [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n\n  # Report customization\n  report_title: \"DNA Model Benchmark Report\"\n  report_description: \"Comprehensive comparison of DNA language models\"\n  include_summary: true\n  include_details: true\n  include_recommendations: true\n\n  # Export options\n  export_predictions: true\n  export_metrics: true\n  export_config: true\n  export_logs: true\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#report-customization","title":"Report Customization","text":"<pre><code>output:\n  report:\n    title: \"DNA Model Benchmark Report\"\n    subtitle: \"Performance Comparison on Promoter Prediction\"\n    author: \"Your Name\"\n    date: \"auto\"\n\n    # Sections to include\n    sections:\n      - \"executive_summary\"\n      - \"model_overview\"\n      - \"dataset_description\"\n      - \"results_summary\"\n      - \"detailed_results\"\n      - \"performance_analysis\"\n      - \"recommendations\"\n      - \"appendix\"\n\n    # Custom styling\n    styling:\n      theme: \"modern\"\n      color_scheme: \"blue\"\n      font_family: \"Arial\"\n      font_size: 12\n\n    # Interactive elements\n    interactive:\n      enable_zoom: true\n      enable_hover: true\n      enable_selection: true\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"user_guide/benchmark/configuration/#cross-validation-settings","title":"Cross-Validation Settings","text":"<pre><code>advanced:\n  cross_validation:\n    enabled: true\n    method: \"k_fold\"  # or \"stratified_k_fold\", \"time_series_split\"\n    n_splits: 5\n    shuffle: true\n    random_state: 42\n\n    # Stratified options\n    stratification:\n      enabled: true\n      column: \"label\"\n      bins: 10\n\n    # Time series options\n    time_series:\n      column: \"date\"\n      test_size: 0.2\n      gap: 0\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#performance-profiling","title":"Performance Profiling","text":"<pre><code>advanced:\n  performance_profiling:\n    enabled: true\n\n    # Memory profiling\n    memory:\n      track_gpu: true\n      track_cpu: true\n      track_peak: true\n      profile_allocations: true\n\n    # Time profiling\n    timing:\n      track_inference: true\n      track_preprocessing: true\n      track_postprocessing: true\n      warmup_runs: 10\n\n    # Resource monitoring\n    resources:\n      track_cpu_usage: true\n      track_gpu_usage: true\n      track_io: true\n      sampling_interval: 0.1\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#custom-evaluation-pipeline","title":"Custom Evaluation Pipeline","text":"<pre><code>advanced:\n  custom_pipeline:\n    enabled: true\n    pipeline_file: \"path/to/custom_pipeline.py\"\n\n    # Pipeline steps\n    steps:\n      - name: \"data_preprocessing\"\n        function: \"custom_preprocess\"\n        parameters:\n          normalize: true\n          augment: false\n\n      - name: \"model_evaluation\"\n        function: \"custom_evaluate\"\n        parameters:\n          metric: \"custom_metric\"\n          threshold: 0.5\n\n      - name: \"result_aggregation\"\n        function: \"custom_aggregate\"\n        parameters:\n          method: \"weighted_average\"\n          weights: [0.4, 0.3, 0.3]\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"user_guide/benchmark/configuration/#complete-example-promoter-prediction","title":"Complete Example: Promoter Prediction","text":"<pre><code>benchmark:\n  name: \"Promoter Prediction Benchmark\"\n  description: \"Comparing DNA language models on promoter prediction tasks\"\n\n  models:\n    - name: \"Plant DNABERT\"\n      path: \"zhangtaolab/plant-dnabert-BPE\"\n      source: \"huggingface\"\n      task_type: \"classification\"\n\n    - name: \"Plant DNAGPT\"\n      path: \"zhangtaolab/plant-dnagpt-BPE\"\n      source: \"huggingface\"\n      task_type: \"generation\"\n\n    - name: \"Nucleotide Transformer\"\n      path: \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n      source: \"huggingface\"\n      task_type: \"classification\"\n\n  datasets:\n    - name: \"promoter_strength\"\n      path: \"data/promoter_strength.csv\"\n      task: \"binary_classification\"\n      text_column: \"sequence\"\n      label_column: \"label\"\n      max_length: 512\n      test_size: 0.2\n      val_size: 0.1\n\n    - name: \"open_chromatin\"\n      path: \"data/open_chromatin.csv\"\n      task: \"binary_classification\"\n      text_column: \"sequence\"\n      label_column: \"label\"\n      max_length: 512\n\n  metrics:\n    - \"accuracy\"\n    - \"f1_score\"\n    - \"precision\"\n    - \"recall\"\n    - \"roc_auc\"\n    - name: \"gc_content_accuracy\"\n      class: \"GCContentMetric\"\n\n  evaluation:\n    batch_size: 32\n    max_length: 512\n    device: \"cuda\"\n    num_workers: 4\n    use_fp16: true\n    seed: 42\n\n  output:\n    format: \"html\"\n    path: \"promoter_benchmark_results\"\n    save_predictions: true\n    generate_plots: true\n    report_title: \"Promoter Prediction Model Comparison\"\n\n  advanced:\n    cross_validation:\n      enabled: true\n      method: \"stratified_k_fold\"\n      n_splits: 5\n\n    performance_profiling:\n      enabled: true\n      memory:\n        track_gpu: true\n        track_peak: true\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#minimal-example","title":"Minimal Example","text":"<pre><code>benchmark:\n  name: \"Quick Model Test\"\n\n  models:\n    - name: \"Test Model\"\n      path: \"zhangtaolab/plant-dnabert-BPE\"\n      source: \"huggingface\"\n      task_type: \"classification\"\n\n  datasets:\n    - name: \"test_data\"\n      path: \"test.csv\"\n      task: \"binary_classification\"\n      text_column: \"sequence\"\n      label_column: \"label\"\n\n  metrics:\n    - \"accuracy\"\n    - \"f1_score\"\n\n  evaluation:\n    batch_size: 16\n    device: \"cuda\"\n\n  output:\n    format: \"csv\"\n    path: \"quick_test_results\"\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#configuration-validation","title":"Configuration Validation","text":""},{"location":"user_guide/benchmark/configuration/#schema-validation","title":"Schema Validation","text":"<p>DNALLM automatically validates your configuration:</p> <pre><code>from dnallm import validate_config\n\n# Validate configuration\ntry:\n    validate_config(\"benchmark_config.yaml\")\n    print(\"Configuration is valid!\")\nexcept ValidationError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#common-validation-errors","title":"Common Validation Errors","text":"Error Cause Solution <code>Model not found</code> Invalid model path Check model exists on specified source <code>Invalid task type</code> Unsupported task Use supported task types <code>Missing required field</code> Incomplete configuration Add missing required fields <code>Invalid metric name</code> Unknown metric Use supported metric names <code>Path not found</code> Invalid file path Check file exists and is accessible"},{"location":"user_guide/benchmark/configuration/#best-practices","title":"Best Practices","text":""},{"location":"user_guide/benchmark/configuration/#1-configuration-organization","title":"1. Configuration Organization","text":"<pre><code># Use descriptive names\nbenchmark:\n  name: \"Comprehensive DNA Model Evaluation 2024\"\n\n# Group related settings\nevaluation:\n  # Hardware settings\n  device: \"cuda\"\n  num_workers: 4\n\n  # Performance settings\n  batch_size: 32\n  use_fp16: true\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#2-environment-specific-configs","title":"2. Environment-Specific Configs","text":"<pre><code># Development config\nevaluation:\n  batch_size: 8\n  device: \"cpu\"\n\n# Production config  \nevaluation:\n  batch_size: 64\n  device: \"cuda\"\n  use_fp16: true\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#3-version-control","title":"3. Version Control","text":"<pre><code># Include version information\nbenchmark:\n  version: \"1.0.0\"\n  config_version: \"2024.1\"\n  created_by: \"Your Name\"\n  created_date: \"2024-01-15\"\n</code></pre>"},{"location":"user_guide/benchmark/configuration/#next-steps","title":"Next Steps","text":"<p>After configuring your benchmark:</p> <ol> <li>Run Your Benchmark: Follow the Getting Started guide</li> <li>Explore Advanced Features: Learn about Advanced Techniques</li> <li>See Real Examples: Check Examples and Use Cases</li> <li>Troubleshoot Issues: Visit Troubleshooting</li> </ol> <p>Need help with configuration? Check our FAQ or open an issue on GitHub.</p>"},{"location":"user_guide/benchmark/examples/","title":"Examples and Use Cases","text":"<p>This guide provides real-world examples and practical use cases for DNALLM benchmarking, demonstrating how to apply the concepts learned in previous sections.</p>"},{"location":"user_guide/benchmark/examples/#overview","title":"Overview","text":"<p>The examples in this guide cover: - Research Applications: Academic model comparison and evaluation - Production Use Cases: Model selection for deployment - Performance Analysis: Optimization and profiling scenarios - Custom Scenarios: Specialized benchmarking requirements</p>"},{"location":"user_guide/benchmark/examples/#research-applications","title":"Research Applications","text":""},{"location":"user_guide/benchmark/examples/#example-1-academic-model-comparison","title":"Example 1: Academic Model Comparison","text":"<p>Scenario: Comparing multiple DNA language models for publication in a research paper.</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom dnallm import Benchmark\n\n# Define research models\nresearch_models = [\n    {\n        \"name\": \"Plant DNABERT\",\n        \"path\": \"zhangtaolab/plant-dnabert-BPE\",\n        \"source\": \"huggingface\",\n        \"task_type\": \"classification\"\n    },\n    {\n        \"name\": \"Nucleotide Transformer\",\n        \"path\": \"InstaDeepAI/nucleotide-transformer-500m-human-ref\",\n        \"source\": \"huggingface\",\n        \"task_type\": \"classification\"\n    },\n    {\n        \"name\": \"DNABERT-2\",\n        \"path\": \"zhangtaolab/DNABERT-2\",\n        \"source\": \"huggingface\",\n        \"task_type\": \"classification\"\n    }\n]\n\n# Load research datasets\ndatasets = {\n    \"promoter_prediction\": DNADataset.load_local_data(\n        \"data/promoter_prediction.csv\",\n        seq_col=\"sequence\",\n        label_col=\"label\",\n        max_length=512\n    ),\n    \"motif_detection\": DNADataset.load_local_data(\n        \"data/motif_detection.csv\",\n        seq_col=\"sequence\",\n        label_col=\"label\",\n        max_length=512\n    )\n}\n\n# Run comprehensive benchmark with cross-validation\nbenchmark = Benchmark(\n    models=research_models,\n    datasets=datasets,\n    metrics=[\"accuracy\", \"f1_score\", \"precision\", \"recall\", \"roc_auc\"],\n    batch_size=32,\n    device=\"cuda\"\n)\n\n# Execute with cross-validation\nresults = benchmark.run_with_cross_validation(k_folds=5)\n\n# Generate publication-ready results\npublication_results = generate_publication_results(results)\ncreate_publication_plots(publication_results)\nexport_research_results(publication_results, \"research_paper_results\")\n</code></pre>"},{"location":"user_guide/benchmark/examples/#example-2-cross-species-model-evaluation","title":"Example 2: Cross-Species Model Evaluation","text":"<p>Scenario: Evaluating how well models trained on one species perform on related species.</p> <pre><code>def run_cross_species_benchmark():\n    \"\"\"Evaluate model performance across different species.\"\"\"\n\n    # Define species-specific datasets\n    species_datasets = {\n        \"Arabidopsis_thaliana\": \"data/arabidopsis_promoters.csv\",\n        \"Oryza_sativa\": \"data/rice_promoters.csv\",\n        \"Zea_mays\": \"data/maize_promoters.csv\"\n    }\n\n    # Load datasets\n    datasets = {}\n    for species, path in species_datasets.items():\n        datasets[species] = DNADataset.load_local_data(\n            path,\n            seq_col=\"sequence\",\n            label_col=\"label\",\n            max_length=512\n        )\n\n    # Run cross-species evaluation\n    cross_species_results = {}\n\n    for model_name, model_info in research_models.items():\n        cross_species_results[model_name] = {}\n\n        for source_species in species_datasets.keys():\n            for target_species in species_datasets.keys():\n                if source_species == target_species:\n                    continue\n\n                # Evaluate model trained on source species on target species\n                result = benchmark.evaluate_single_model(\n                    model_info[\"model\"],\n                    model_info[\"tokenizer\"],\n                    datasets[target_species],\n                    metrics=[\"accuracy\", \"f1_score\"]\n                )\n\n                cross_species_results[model_name][f\"{source_species}_to_{target_species}\"] = result\n\n    return cross_species_results\n\n# Run cross-species benchmark\ncross_species_results = run_cross_species_benchmark()\nanalyze_transfer_learning(cross_species_results)\n</code></pre>"},{"location":"user_guide/benchmark/examples/#production-use-cases","title":"Production Use Cases","text":""},{"location":"user_guide/benchmark/examples/#example-3-model-selection-for-production","title":"Example 3: Model Selection for Production","text":"<p>Scenario: Choosing the best model for deployment in a production environment.</p> <pre><code>def production_model_selection():\n    \"\"\"Select the best model for production deployment.\"\"\"\n\n    # Define production criteria\n    production_criteria = {\n        \"accuracy_threshold\": 0.85,\n        \"inference_time_threshold\": 0.1,  # seconds per sequence\n        \"memory_threshold\": 8,  # GB\n        \"cost_threshold\": 100  # dollars per month\n    }\n\n    # Production datasets (representative of real-world usage)\n    production_datasets = {\n        \"high_throughput\": load_production_dataset(\"data/high_throughput.csv\"),\n        \"edge_device\": load_production_dataset(\"data/edge_device.csv\"),\n        \"real_time\": load_production_dataset(\"data/real_time.csv\")\n    }\n\n    # Run production benchmark\n    production_benchmark = Benchmark(\n        models=candidate_models,\n        datasets=production_datasets,\n        metrics=[\"accuracy\", \"inference_time\", \"memory_usage\", \"throughput\"],\n        batch_size=64,  # Production batch size\n        device=\"cuda\"\n    )\n\n    # Execute with production settings\n    production_results = production_benchmark.run()\n\n    # Apply production criteria\n    qualified_models = filter_by_production_criteria(\n        production_results, \n        production_criteria\n    )\n\n    # Rank by production suitability\n    ranked_models = rank_production_models(qualified_models)\n\n    return ranked_models\n\n# Run production model selection\nproduction_rankings = production_model_selection()\n\n# Display results\nprint(\"Production Model Rankings:\")\nfor i, (model_name, score) in enumerate(production_rankings, 1):\n    print(f\"{i}. {model_name}: {score:.3f}\")\n\n# Generate production report\ngenerate_production_report(production_rankings, \"production_selection_report\")\n</code></pre>"},{"location":"user_guide/benchmark/examples/#example-4-performance-monitoring-and-alerting","title":"Example 4: Performance Monitoring and Alerting","text":"<p>Scenario: Continuous monitoring of model performance in production with automated alerting.</p> <pre><code>class ProductionModelMonitor:\n    \"\"\"Monitor model performance in production.\"\"\"\n\n    def __init__(self, model, dataset, alert_thresholds):\n        self.model = model\n        self.dataset = dataset\n        self.alert_thresholds = alert_thresholds\n        self.performance_history = []\n        self.setup_logging()\n\n    def monitor_performance(self, interval_minutes=60):\n        \"\"\"Monitor model performance continuously.\"\"\"\n        self.logger.info(\"Starting production performance monitoring...\")\n\n        while True:\n            try:\n                # Run performance evaluation\n                current_performance = self.evaluate_current_performance()\n\n                # Store in history\n                self.performance_history.append({\n                    'timestamp': datetime.now(),\n                    'performance': current_performance\n                })\n\n                # Check for performance degradation\n                if self.check_performance_degradation(current_performance):\n                    self.send_alert(current_performance)\n\n                # Log performance\n                self.logger.info(f\"Performance: {current_performance}\")\n\n                # Wait for next evaluation\n                time.sleep(interval_minutes * 60)\n\n            except Exception as e:\n                self.logger.error(f\"Monitoring error: {e}\")\n                self.send_error_alert(str(e))\n                time.sleep(interval_minutes * 60)\n\n    def evaluate_current_performance(self):\n        \"\"\"Evaluate current model performance.\"\"\"\n        benchmark = Benchmark(\n            models=[self.model],\n            datasets=[self.dataset],\n            metrics=[\"accuracy\", \"f1_score\", \"inference_time\"],\n            batch_size=32,\n            device=\"cuda\"\n        )\n\n        results = benchmark.run()\n        return results[list(results.keys())[0]]\n\n# Usage example\nalert_thresholds = {\n    \"accuracy\": 0.80,\n    \"f1_score\": 0.75,\n    \"inference_time\": 0.2\n}\n\nmonitor = ProductionModelMonitor(\n    model=selected_model,\n    dataset=production_dataset,\n    alert_thresholds=alert_thresholds\n)\n\n# Start monitoring (in production, run this as a service)\n# monitor.monitor_performance(interval_minutes=60)\n</code></pre>"},{"location":"user_guide/benchmark/examples/#performance-analysis-scenarios","title":"Performance Analysis Scenarios","text":""},{"location":"user_guide/benchmark/examples/#example-5-model-optimization-analysis","title":"Example 5: Model Optimization Analysis","text":"<p>Scenario: Analyzing model performance to identify optimization opportunities.</p> <pre><code>def analyze_model_optimization():\n    \"\"\"Analyze model performance for optimization opportunities.\"\"\"\n\n    # Test different optimization strategies\n    optimization_results = {}\n\n    # 1. Batch size optimization\n    batch_size_results = optimize_batch_size(selected_model, test_dataset)\n    optimization_results[\"batch_size\"] = batch_size_results\n\n    # 2. Mixed precision analysis\n    precision_results = analyze_mixed_precision(selected_model, test_dataset)\n    optimization_results[\"mixed_precision\"] = precision_results\n\n    # 3. Memory optimization\n    memory_results = analyze_memory_optimization(selected_model, test_dataset)\n    optimization_results[\"memory\"] = memory_results\n\n    return optimization_results\n\ndef optimize_batch_size(model, dataset):\n    \"\"\"Find optimal batch size for the model.\"\"\"\n    batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128]\n    results = {}\n\n    for batch_size in batch_sizes:\n        try:\n            # Test batch size\n            start_time = time.time()\n            memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n\n            # Run inference\n            dataloader = dataset.get_dataloader(batch_size=batch_size)\n            for batch in dataloader:\n                with torch.no_grad():\n                    outputs = model(batch[\"input_ids\"].to(model.device))\n                break  # Just test one batch\n\n            inference_time = time.time() - start_time\n            memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n\n            results[batch_size] = {\n                \"inference_time\": inference_time,\n                \"memory_used\": (memory_after - memory_before) / 1024 / 1024,\n                \"throughput\": batch_size / inference_time\n            }\n\n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"Batch size {batch_size} failed: OOM\")\n                break\n\n    return results\n\n# Run optimization analysis\noptimization_results = analyze_model_optimization()\n\n# Generate optimization report\ngenerate_optimization_report(optimization_results, \"model_optimization_report\")\n</code></pre>"},{"location":"user_guide/benchmark/examples/#custom-benchmarking-scenarios","title":"Custom Benchmarking Scenarios","text":""},{"location":"user_guide/benchmark/examples/#example-6-multi-task-model-evaluation","title":"Example 6: Multi-Task Model Evaluation","text":"<p>Scenario: Evaluating models that can perform multiple tasks simultaneously.</p> <pre><code>def multi_task_benchmark():\n    \"\"\"Benchmark models on multiple tasks simultaneously.\"\"\"\n\n    # Define multiple tasks\n    tasks = {\n        \"promoter_prediction\": {\n            \"dataset\": \"data/promoter_data.csv\",\n            \"task_type\": \"binary_classification\",\n            \"metrics\": [\"accuracy\", \"f1_score\", \"roc_auc\"]\n        },\n        \"motif_detection\": {\n            \"dataset\": \"data/motif_data.csv\",\n            \"task_type\": \"binary_classification\",\n            \"metrics\": [\"accuracy\", \"f1_score\", \"precision\"]\n        },\n        \"sequence_generation\": {\n            \"dataset\": \"data/generation_data.csv\",\n            \"task_type\": \"generation\",\n            \"metrics\": [\"perplexity\", \"bleu_score\", \"diversity\"]\n        }\n    }\n\n    # Multi-task models\n    multi_task_models = [\n        {\n            \"name\": \"Unified DNA Model\",\n            \"path\": \"path/to/unified_model\",\n            \"source\": \"local\",\n            \"task_type\": \"multi_task\"\n        }\n    ]\n\n    # Run multi-task benchmark\n    multi_task_results = {}\n\n    for model_info in multi_task_models:\n        model_name = model_info[\"name\"]\n        multi_task_results[model_name] = {}\n\n        for task_name, task_config in tasks.items():\n            # Load task-specific dataset\n            dataset = DNADataset.load_local_data(\n                task_config[\"dataset\"],\n                seq_col=\"sequence\",\n                label_col=\"label\",\n                max_length=512\n            )\n\n            # Evaluate on this task\n            task_result = benchmark.evaluate_single_model(\n                model_info[\"model\"],\n                model_info[\"tokenizer\"],\n                dataset,\n                metrics=task_config[\"metrics\"]\n            )\n\n            multi_task_results[model_name][task_name] = task_result\n\n    return multi_task_results\n\n# Run multi-task benchmark\nmulti_task_results = multi_task_benchmark()\n\n# Analyze multi-task performance\nanalyze_multi_task_performance(multi_task_results)\n</code></pre>"},{"location":"user_guide/benchmark/examples/#example-7-time-series-model-evaluation","title":"Example 7: Time-Series Model Evaluation","text":"<p>Scenario: Evaluating model performance over time with temporal data.</p> <pre><code>def time_series_benchmark():\n    \"\"\"Benchmark model performance over time.\"\"\"\n\n    # Load time-series dataset\n    time_series_data = pd.read_csv(\"data/time_series_data.csv\")\n    time_series_data['date'] = pd.to_datetime(time_series_data['date'])\n\n    # Sort by time\n    time_series_data = time_series_data.sort_values('date')\n\n    # Create temporal splits\n    total_days = (time_series_data['date'].max() - time_series_data['date'].min()).days\n    interval_days = 30\n    num_splits = total_days // interval_days\n\n    temporal_results = []\n\n    for i in range(num_splits):\n        start_idx = i * len(time_series_data) // num_splits\n        end_idx = (i + 1) * len(time_series_data) // num_splits\n\n        # Test on future data\n        test_data = time_series_data.iloc[end_idx:]\n        if len(test_data) == 0:\n            continue\n\n        # Convert to DNALLM dataset format\n        test_dataset = DNADataset.from_dataframe(\n            test_data,\n            seq_col=\"sequence\",\n            label_col=\"label\",\n            max_length=512\n        )\n\n        # Evaluate performance\n        result = benchmark.evaluate_single_model(\n            selected_model,\n            selected_tokenizer,\n            test_dataset,\n            metrics=[\"accuracy\", \"f1_score\"]\n        )\n\n        temporal_results.append({\n            \"time_period\": i,\n            \"start_date\": time_series_data.iloc[start_idx]['date'],\n            \"end_date\": time_series_data.iloc[end_idx]['date'],\n            \"test_size\": len(test_data),\n            **result\n        })\n\n    return temporal_results\n\n# Run time-series benchmark\ntemporal_results = time_series_benchmark()\n\n# Analyze temporal performance\nanalyze_temporal_performance(temporal_results)\n</code></pre>"},{"location":"user_guide/benchmark/examples/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"user_guide/benchmark/examples/#1-research-applications","title":"1. Research Applications","text":"<ul> <li>Use cross-validation for robust evaluation</li> <li>Generate publication-ready plots and tables</li> <li>Include statistical significance testing</li> <li>Document all experimental conditions</li> </ul>"},{"location":"user_guide/benchmark/examples/#2-production-use-cases","title":"2. Production Use Cases","text":"<ul> <li>Define clear performance criteria</li> <li>Test on representative production data</li> <li>Monitor performance continuously</li> <li>Implement automated alerting</li> </ul>"},{"location":"user_guide/benchmark/examples/#3-performance-analysis","title":"3. Performance Analysis","text":"<ul> <li>Profile memory and time usage</li> <li>Test optimization strategies</li> <li>Document performance baselines</li> <li>Track performance over time</li> </ul>"},{"location":"user_guide/benchmark/examples/#4-custom-scenarios","title":"4. Custom Scenarios","text":"<ul> <li>Adapt benchmarking to specific requirements</li> <li>Implement custom evaluation metrics</li> <li>Handle multi-task and time-series data</li> <li>Consider domain-specific constraints</li> </ul>"},{"location":"user_guide/benchmark/examples/#next-steps","title":"Next Steps","text":"<p>After exploring these examples:</p> <ol> <li>Adapt to Your Use Case: Modify examples for your specific requirements</li> <li>Combine Techniques: Use multiple approaches together</li> <li>Scale Up: Apply to larger datasets and model collections</li> <li>Automate: Build automated benchmarking pipelines</li> </ol> <p>Ready to implement? Start with the examples that match your use case, or combine multiple approaches for comprehensive evaluation.</p>"},{"location":"user_guide/benchmark/getting_started/","title":"Getting Started with Benchmarking","text":"<p>This guide will walk you through the basics of benchmarking DNA language models using DNALLM. You'll learn how to set up your first benchmark, configure models and datasets, and interpret results.</p>"},{"location":"user_guide/benchmark/getting_started/#overview","title":"Overview","text":"<p>Benchmarking in DNALLM allows you to: - Compare multiple DNA language models on the same tasks - Evaluate performance across different datasets - Measure accuracy, speed, and resource usage - Generate comprehensive performance reports</p>"},{"location":"user_guide/benchmark/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed and configured:</p> <pre><code># Install DNALLM\npip install dnallm\n\n# Or with uv (recommended)\nuv pip install dnallm\n</code></pre>"},{"location":"user_guide/benchmark/getting_started/#basic-setup","title":"Basic Setup","text":""},{"location":"user_guide/benchmark/getting_started/#1-import-required-modules","title":"1. Import Required Modules","text":"<pre><code>from dnallm import load_config, Benchmark\nfrom dnallm.inference import load_model_and_tokenizer\nfrom dnallm.datahandling import DNADataset\n</code></pre>"},{"location":"user_guide/benchmark/getting_started/#2-create-a-simple-configuration","title":"2. Create a Simple Configuration","text":"<p>Create a <code>benchmark_config.yaml</code> file:</p> <pre><code># benchmark_config.yaml\nbenchmark:\n  name: \"My First Benchmark\"\n  description: \"Comparing DNA models on promoter prediction\"\n\n  models:\n    - name: \"Plant DNABERT\"\n      path: \"zhangtaolab/plant-dnabert-BPE\"\n      source: \"huggingface\"\n\n    - name: \"Plant DNAGPT\"\n      path: \"zhangtaolab/plant-dnagpt-BPE\"\n      source: \"huggingface\"\n\n  datasets:\n    - name: \"promoter_data\"\n      path: \"path/to/your/data.csv\"\n      text_column: \"sequence\"\n      label_column: \"label\"\n      task_type: \"binary\"\n      num_labels: 2\n      label_names:\n        - \"negative\"\n        - \"postive\"\n      threshold: 0.5\n\n  metrics:\n    - \"accuracy\"\n    - \"f1_score\"\n    - \"precision\"\n    - \"recall\"\n\n  plot:\n    format: \"pdf\"\n\n\ninference:\n    batch_size: 16\n    max_length: 512\n    device: \"auto\"\n    num_workers: 4\n    use_fp16: False\n    output_dir: \"benchmark_results\"\n</code></pre>"},{"location":"user_guide/benchmark/getting_started/#3-run-the-benchmark","title":"3. Run the Benchmark","text":"<pre><code># Load configuration\nconfig = load_config(\"benchmark_config.yaml\")\n\n# Initialize benchmark\nbenchmark = Benchmark(config=config)\n\n# Run benchmark\nresults = benchmark.run()\n\n# Display results\nprint(\"Benchmark Results:\")\nprint(\"=\" * 50)\nfor dataset_name, dataset_results in results.items():\n    print(f\"\\n{dataset_name}:\")\n    for model_name, metrics in model_results.items():\n        print(f\"  {model_name}:\")\n        for metric, value in metrics.items():\n            print(f\"    {metric}: {value:.4f}\")\n</code></pre>"},{"location":"user_guide/benchmark/getting_started/#command-line-interface","title":"Command Line Interface","text":"<p>DNALLM also provides a convenient command-line interface:</p> <pre><code># Basic benchmark run\ndnallm-benchmark --config benchmark_config.yaml\n\n# Generate detailed report\ndnallm-benchmark --config config.yaml --output report.html\n\n# Run with custom parameters\ndnallm-benchmark --config config.yaml --batch-size 32 --device cuda\n</code></pre>"},{"location":"user_guide/benchmark/getting_started/#understanding-results","title":"Understanding Results","text":""},{"location":"user_guide/benchmark/getting_started/#basic-metrics","title":"Basic Metrics","text":"Metric Description Range Best Value Accuracy Correct predictions / Total predictions 0.0 - 1.0 1.0 F1 Score Harmonic mean of precision and recall 0.0 - 1.0 1.0 Precision True positives / (True positives + False positives) 0.0 - 1.0 1.0 Recall True positives / (True positives + False negatives) 0.0 - 1.0 1.0"},{"location":"user_guide/benchmark/getting_started/#performance-metrics","title":"Performance Metrics","text":"Metric Description Unit Inference Time Time to process one batch seconds Memory Usage GPU/RAM memory consumption MB/GB Throughput Samples processed per second samples/sec"},{"location":"user_guide/benchmark/getting_started/#example-complete-benchmark","title":"Example: Complete Benchmark","text":"<p>Here's a complete working example:</p> <pre><code>import os\nfrom dnallm import load_config, Benchmark\nfrom dnallm.datahandling import DNADataset\n\n# 1. Prepare your data\ndata_path = \"path/to/your/dna_sequences.csv\"\nif not os.path.exists(data_path):\n    print(\"Please provide a valid data path\")\n    exit()\n\n# 2. Load and prepare dataset\ndataset = DNADataset.load_local_data(\n    data_path,\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    max_length=512\n)\n\n# 3. Create configuration\nconfig = {\n    \"benchmark\": {\n        \"name\": \"DNA Model Comparison\",\n        \"models\": [\n            {\n                \"name\": \"Plant DNABERT\",\n                \"path\": \"zhangtaolab/plant-dnabert-BPE\",\n                \"source\": \"huggingface\",\n                \"task_type\": \"classification\"\n            },\n            {\n                \"name\": \"Plant DNAGPT\",\n                \"path\": \"zhangtaolab/plant-dnagpt-BPE\", \n                \"source\": \"huggingface\",\n                \"task_type\": \"generation\"\n            }\n        ],\n        \"datasets\": [dataset],\n        \"metrics\": [\"accuracy\", \"f1_score\", \"precision\", \"recall\"],\n        \"evaluation\": {\n            \"batch_size\": 16,\n            \"max_length\": 512,\n            \"device\": \"cuda\"\n        },\n        \"output\": {\n            \"format\": \"html\",\n            \"path\": \"my_benchmark_results\"\n        }\n    }\n}\n\n# 4. Run benchmark\nbenchmark = Benchmark(config=config)\nresults = benchmark.run()\n\n# 5. Generate report\nbenchmark.generate_report(\n    output_path=\"my_benchmark_results\",\n    format=\"html\",\n    include_predictions=True\n)\n\nprint(\"Benchmark completed! Check 'my_benchmark_results' folder for results.\")\n</code></pre>"},{"location":"user_guide/benchmark/getting_started/#data-format-requirements","title":"Data Format Requirements","text":"<p>Your dataset should be in one of these formats:</p>"},{"location":"user_guide/benchmark/getting_started/#csvtsv-format","title":"CSV/TSV Format","text":"<pre><code>sequence,label\nATCGATCGATCG,1\nGCTAGCTAGCTA,0\nTATATATATATA,1\n</code></pre>"},{"location":"user_guide/benchmark/getting_started/#json-format","title":"JSON Format","text":"<pre><code>[\n  {\"sequence\": \"ATCGATCGATCG\", \"label\": 1},\n  {\"sequence\": \"GCTAGCTAGCTA\", \"label\": 0}\n]\n</code></pre>"},{"location":"user_guide/benchmark/getting_started/#fasta-format","title":"FASTA Format","text":"<pre><code>&gt;sequence1|label:1\nATCGATCGATCG\n&gt;sequence2|label:0\nGCTAGCTAGCTA\n</code></pre>"},{"location":"user_guide/benchmark/getting_started/#common-tasks","title":"Common Tasks","text":""},{"location":"user_guide/benchmark/getting_started/#binary-classification","title":"Binary Classification","text":"<pre><code>task: \"binary_classification\"\nnum_labels: 2\nlabel_names: [\"Negative\", \"Positive\"]\nthreshold: 0.5\n</code></pre>"},{"location":"user_guide/benchmark/getting_started/#multi-class-classification","title":"Multi-class Classification","text":"<pre><code>task: \"multiclass\"\nnum_labels: 4\nlabel_names: [\"Class_A\", \"Class_B\", \"Class_C\", \"Class_D\"]\n</code></pre>"},{"location":"user_guide/benchmark/getting_started/#regression","title":"Regression","text":"<pre><code>task: \"regression\"\nnum_labels: 1\n</code></pre>"},{"location":"user_guide/benchmark/getting_started/#next-steps","title":"Next Steps","text":"<p>After completing this basic tutorial:</p> <ol> <li>Explore Advanced Features: Learn about cross-validation and custom metrics</li> <li>Optimize Performance: Discover performance profiling techniques</li> <li>Customize Output: Learn about advanced configuration options</li> <li>Real-world Examples: See practical use cases</li> </ol>"},{"location":"user_guide/benchmark/getting_started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide/benchmark/getting_started/#common-issues","title":"Common Issues","text":"<p>\"Model not found\" error <pre><code># Check if model exists on Hugging Face\n# Visit: https://huggingface.co/models?search=dna\n</code></pre></p> <p>Memory errors <pre><code># Reduce batch size in config\nevaluation:\n  batch_size: 8  # Reduced from 16\n</code></pre></p> <p>Slow performance <pre><code># Enable mixed precision\nevaluation:\n  use_fp16: true\n</code></pre></p>"},{"location":"user_guide/benchmark/getting_started/#additional-resources","title":"Additional Resources","text":"<ul> <li>Configuration Guide - Detailed configuration options</li> <li>Advanced Techniques - Cross-validation and custom metrics</li> <li>Examples and Use Cases - Real-world scenarios</li> <li>Troubleshooting - Common problems and solutions</li> </ul> <p>Ready for more? Continue to Advanced Techniques to learn about cross-validation, custom metrics, and performance profiling.</p>"},{"location":"user_guide/case_studies/motif_detection/","title":"Case Study: Motif Detection via In-Silico Saturation Mutagenesis","text":"<p>This case study illustrates an advanced application of DNALLM: identifying critical nucleotide positions and discovering DNA motifs by simulating saturation mutagenesis in silico.</p>"},{"location":"user_guide/case_studies/motif_detection/#1-case-background","title":"1. Case Background","text":"<p>Transcription factor binding sites (TFBS), or motifs, are short, recurring patterns in DNA that have a biological function. Discovering these motifs is key to understanding gene regulation. A powerful method for pinpointing functional elements is saturation mutagenesis, where every possible mutation is tested to see its effect on a property, such as promoter strength.</p> <p>Instead of performing costly and time-consuming lab experiments, we can use a fine-tuned DNALLM model to predict the outcome of these mutations. The workflow is as follows:</p> <ol> <li>Train a Model: Fine-tune a DNALLM model on a quantitative task, such as predicting promoter strength (a regression task).</li> <li>In-Silico Mutagenesis: For a given high-performing sequence (e.g., a strong promoter), systematically generate every possible single-nucleotide mutation.</li> <li>Predict and Score: Use the fine-tuned model to predict the strength of each mutated sequence. The change in predicted strength reveals the importance of each nucleotide position.</li> <li>Identify Important Regions: Analyze the mutation effect scores to identify regions where mutations have the largest impact.</li> <li>Discover Motifs: Submit these identified important regions to specialized motif discovery tools like MEME Suite or Homer to find conserved motifs.</li> </ol>"},{"location":"user_guide/case_studies/motif_detection/#2-code","title":"2. Code","text":"<p>This section provides the code to perform steps 2-4. It assumes you already have a fine-tuned regression model (e.g., for promoter strength).</p>"},{"location":"user_guide/case_studies/motif_detection/#setup","title":"Setup","text":"<p>Create a configuration file for the mutagenesis task.</p> <p><code>inference_config.yaml</code>: <pre><code># task configuration\ntask:\n  task_type: \"regression\"\n  num_labels: 1 # For regression, this is typically 1\n\n# inference configuration\ninference:\n  per_device_eval_batch_size: 64\n  output_dir: \"./outputs\"\n</code></pre></p>"},{"location":"user_guide/case_studies/motif_detection/#python-script","title":"Python Script","text":"<p>This script performs saturation mutagenesis on a single DNA sequence, evaluates the effect of each mutation, and visualizes the results.</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer, Mutagenesis\n\n# --- 1. Load Configuration and Model ---\n# Load settings from the YAML file\nconfigs = load_config(\"./inference_config.yaml\")\n\n# Load a model fine-tuned for a regression task (e.g., promoter strength)\n# This model should be able to predict a continuous value from a DNA sequence.\nmodel_name = \"zhangtaolab/plant-dnagpt-BPE-promoter_strength_protoplast\"\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name, \n    task_config=configs['task'], \n    source=\"modelscope\"\n)\n\n# --- 2. Perform In-Silico Mutagenesis ---\n\n# Initialize the mutagenesis tool with the loaded model and tokenizer\nmutagenesis = Mutagenesis(config=configs, model=model, tokenizer=tokenizer)\n\n# Define the wild-type (original) sequence to analyze\n# This should be a sequence for which your model gives a high score.\nwt_sequence = \"AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC\"\n\n# Generate all single-nucleotide mutations for the sequence\n# This creates a list of mutated sequences in the `mutagenesis.sequences` attribute.\nmutagenesis.mutate_sequence(wt_sequence, replace_mut=True)\n\nprint(f\"Generated {len(mutagenesis.sequences['name'])} sequences (1 wild-type + mutations).\")\n\n# --- 3. Predict and Evaluate Mutation Effects ---\n\n# Use the model to predict the output value for the wild-type and all mutated sequences\n# The `evaluate` method calculates the difference between each mutant's score and the wild-type score.\n# The `strategy` determines how to aggregate scores from the model's output logits.\nprint(\"Evaluating the effect of each mutation...\")\npreds = mutagenesis.evaluate(strategy=\"mean\")\n\n# The `preds` object now contains the raw predictions and the calculated mutation effects.\nprint(\"Evaluation complete.\")\n\n# --- 4. Visualize the Results ---\n\n# Plot the results as a heatmap, where the color indicates the effect of mutating\n# a position to a specific nucleotide.\n# This helps visually identify critical positions.\nprint(\"Generating and saving mutation effect heatmap...\")\nmut_plot = mutagenesis.plot(preds, save_path=\"mut_effects_heatmap.pdf\")\n\n# The plot can also be displayed in a notebook\n# mut_plot.show()\n\nprint(\"Heatmap saved to mut_effects_heatmap.pdf\")\n</code></pre>"},{"location":"user_guide/case_studies/motif_detection/#5-identify-important-regions-and-find-motifs","title":"5. Identify Important Regions and Find Motifs","text":"<p>After running the script, you will have a matrix of importance scores. You can programmatically identify regions with high-impact mutations and extract them. These regions are strong candidates for containing functional motifs.</p> <p>For example, you can find positions where mutations cause a significant drop in predicted promoter strength and extract the corresponding DNA sequences from the wild-type sequence.</p> <p>Once you have a set of these important sequences (as a FASTA file, for example <code>important_regions.fasta</code>), you can use external motif discovery tools.</p> <p>Using MEME Suite:</p> <p>Upload your FASTA file to the MEME Suite web server or use the command-line tool:</p> <pre><code># meme &lt;input_fasta_file&gt; [options]\nmeme important_regions.fasta -dna -o meme_out\n</code></pre> <p>Using Homer:</p> <pre><code># findMotifs.pl &lt;input_fasta_file&gt; &lt;promoter-set&gt; &lt;output_dir&gt; [options]\nfindMotifs.pl important_regions.fasta fasta homer_out/ -fasta-bg background_sequences.fasta\n</code></pre>"},{"location":"user_guide/case_studies/motif_detection/#3-expected-results","title":"3. Expected Results","text":"<ol> <li>Mutation Effect Data: The <code>preds</code> object from <code>mutagenesis.evaluate()</code> contains a detailed breakdown of the predicted score for the wild-type and each mutant.</li> <li>Heatmap Visualization: The <code>mutagenesis.plot()</code> function generates a PDF file (<code>mut_effects_heatmap.pdf</code>). This heatmap shows the original sequence on the x-axis and the four nucleotides (A, C, G, T) on the y-axis. The color of each cell indicates the predicted change in score if the original base is mutated to that nucleotide. Darker colors (e.g., blue or red, depending on the color scheme) highlight mutations that significantly decrease or increase the model's output, indicating critical positions.</li> <li>Motif Logo: After using a tool like MEME, the expected output is a set of discovered motifs, often visualized as sequence logos. A sequence logo provides a graphical representation of the conservation of nucleotides at each position in the motif.</li> </ol>"},{"location":"user_guide/case_studies/motif_detection/#4-tuning-strategies","title":"4. Tuning Strategies","text":"<ul> <li>Model Choice: The quality of motif detection is highly dependent on the accuracy of the underlying regression model. A well-trained model that accurately predicts the quantitative trait (e.g., promoter strength) is essential.</li> <li>Selection of Sequences: Instead of analyzing just one sequence, run the mutagenesis analysis on multiple, diverse, high-activity sequences. Finding conserved patterns of important nucleotides across these sequences will yield more robust motifs.</li> <li>Thresholding: When extracting important regions to send to a motif finder, the threshold you use to decide whether a mutation's effect is \"significant\" is a key parameter. You may need to experiment with this threshold to reduce noise and focus on the most promising regions.</li> </ul>"},{"location":"user_guide/case_studies/motif_detection/#5-troubleshooting","title":"5. Troubleshooting","text":"<ul> <li>Noisy Heatmap: If the mutation effect heatmap looks random with no clear hotspots, it could indicate that the underlying model is not sensitive enough or that the chosen sequence does not contain strong functional elements. Try using a better model or a different set of sequences.</li> <li>No Motifs Found: If MEME or Homer do not return any significant motifs, consider the following:<ul> <li>The extracted \"important regions\" may be too long, too short, or too noisy. Try adjusting the threshold for significance.</li> <li>The background set of sequences used for statistical comparison might be inappropriate. Ensure your background set has a similar nucleotide composition.</li> <li>The motif may not be well-represented by a simple position weight matrix (PWM), or it might be a structural motif not easily found by these tools.</li> </ul> </li> <li><code>CUDA out of memory</code>: The prediction step can be memory-intensive. Reduce the <code>per_device_eval_batch_size</code> in your configuration file.</li> </ul>"},{"location":"user_guide/case_studies/multi_task_learning/","title":"Case Study: Multi-Task Learning for DNA Sequences","text":"<p>This case study demonstrates how to train a single DNALLM model to perform multiple classification tasks simultaneously. This approach, known as multi-task learning, can improve model performance and efficiency.</p>"},{"location":"user_guide/case_studies/multi_task_learning/#1-case-background","title":"1. Case Background","text":"<p>In genomics, a single DNA sequence can often be associated with multiple properties or functions. For example, a regulatory element might be active in different tissues (leaf, root) or under different conditions (stress, normal). Instead of training separate models for each property, we can train one model to predict all of them at once.</p> <p>Multi-task learning has several benefits: -   Improved Generalization: By learning related tasks together, the model can leverage shared representations, which can lead to better performance on all tasks, especially when data for some tasks is scarce. -   Efficiency: Training and deploying a single model is more computationally and operationally efficient than managing multiple models.</p> <p>In this example, we will fine-tune a model to predict multiple binary labels for a given DNA sequence. This is framed as a multi-label classification problem.</p>"},{"location":"user_guide/case_studies/multi_task_learning/#2-code","title":"2. Code","text":"<p>This section provides a script for fine-tuning a model on a multi-label dataset.</p>"},{"location":"user_guide/case_studies/multi_task_learning/#setup","title":"Setup","text":"<p>First, prepare your dataset and configuration file.</p> <p>Dataset: The dataset should be a local file (e.g., CSV or TSV) where one column contains the DNA sequences and another column contains the labels. For multi-label tasks, the labels for a single sequence should be consolidated into a single string, separated by a specific character (e.g., a comma).</p> <p>Example <code>multi_label_data.tsv</code>:</p> <pre><code>sequence    labels\nCACGGTCA... label1,label3\nAGTCGCTA... label2\nGCGATATA... label1,label2,label4\n</code></pre> <p><code>multi_labels_config.yaml</code>: In the configuration, you must define the <code>task_type</code> as <code>sequence_classification</code> and provide a <code>label_map</code> that includes all possible labels across all tasks.</p> <pre><code># task configuration\ntask:\n  task_type: \"sequence_classification\"\n  problem_type: \"multi_label_classification\" # Specify multi-label problem\n  num_labels: 4 # Total number of unique labels\n  label_map:\n    0: \"label1\"\n    1: \"label2\"\n    2: \"label3\"\n    3: \"label4\"\n\n# training configuration\ntraining:\n  output_dir: \"./outputs_multi_task\"\n  num_train_epochs: 5\n  per_device_train_batch_size: 16\n  per_device_eval_batch_size: 16\n  warmup_steps: 100\n  weight_decay: 0.01\n  logging_dir: \"./logs_multi_task\"\n  logging_steps: 20\n  evaluation_strategy: \"epoch\"\n  save_strategy: \"epoch\"\n  load_best_model_at_end: True\n  metric_for_best_model: \"f1_macro\"\n  greater_is_better: True\n</code></pre>"},{"location":"user_guide/case_studies/multi_task_learning/#python-script","title":"Python Script","text":"finetune_multi_label.py<pre><code>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n\n# --- 1. Load Configuration ---\nconfigs = load_config(\"./multi_labels_config.yaml\")\n\n# --- 2. Load Model and Tokenizer ---\n# We use Plant-DNAGPT as an example. Any sequence classification model can be used.\nmodel_name = \"zhangtaolab/plant-dnagpt-BPE\"\n\n# The `problem_type` in the config tells the model to handle multi-label outputs.\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name, \n    task_config=configs['task'], \n    source=\"modelscope\"\n)\n\n# --- 3. Load and Prepare Dataset ---\n# Load a local dataset. \n# `multi_label_sep` tells the loader how to split the labels in the 'labels' column.\ndatasets = DNADataset.load_local_data(\n    \"./maize_test.tsv\", \n    seq_col=\"sequence\", \n    label_col=\"labels\", \n    multi_label_sep=\",\", \n    tokenizer=tokenizer, \n    max_length=256,\n    config=configs\n)\n\n# Tokenize the sequences\ndatasets.encode_sequences()\n\n# Split the data into training, validation, and test sets\ndatasets.split_data(train_size=0.8, test_size=0.1, validation_size=0.1)\n\n# --- 4. Fine-tune the Model ---\n# Initialize the trainer\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=datasets\n)\n\n# Start the training process\nprint(\"Starting multi-task model fine-tuning...\")\ntrainer.train()\nprint(\"Training finished.\")\n\n# --- 5. Evaluate the Model ---\n# Run inference on the test set\n# For multi-label tasks, metrics are typically computed for each label and averaged.\nprint(\"Evaluating model on the test set...\")\ntest_metrics = trainer.infer()\nprint(\"Test Metrics:\", test_metrics)\n</code></pre>"},{"location":"user_guide/case_studies/multi_task_learning/#3-expected-results","title":"3. Expected Results","text":"<p>During training, the model's loss will be a sum of the losses from all individual labels (typically Binary Cross-Entropy with Logits). </p> <p>After evaluation, the <code>test_metrics</code> dictionary will contain metrics that are averaged across all labels, such as: -   <code>test_f1_macro</code>: The F1 score calculated independently for each label and then averaged. It treats all labels equally. -   <code>test_f1_micro</code>: The F1 score calculated globally by counting the total true positives, false negatives, and false positives across all labels. -   <code>test_accuracy</code>: The subset accuracy, which is a strict metric that considers a prediction correct only if all labels for a given sequence are correctly predicted.</p> <p>Additionally, the output might include per-label metrics, allowing you to see how well the model performs on each individual task.</p>"},{"location":"user_guide/case_studies/multi_task_learning/#4-tuning-strategies","title":"4. Tuning Strategies","text":"<ul> <li>Balancing Tasks: In multi-task learning, some tasks can dominate others. If your model performs well on common labels but poorly on rare ones, you might need to re-balance your dataset, for instance by over-sampling sequences with rare labels.</li> <li>Model Architecture: While a standard classification head works well, more complex architectures could be explored. For example, using separate prediction heads for different groups of related tasks might improve performance if the tasks are not closely related.</li> <li>Hyperparameter Tuning: Standard hyperparameters like <code>learning_rate</code>, <code>weight_decay</code>, and <code>num_train_epochs</code> are just as important here. It may require more careful tuning to find a set of parameters that works well for all tasks simultaneously.</li> </ul>"},{"location":"user_guide/case_studies/multi_task_learning/#5-troubleshooting","title":"5. Troubleshooting","text":"<ul> <li>Negative Transfer: Sometimes, training on multiple tasks can hurt performance on a specific task compared to training a model for that task alone. This is known as negative transfer. If this occurs:<ul> <li>Verify that the tasks are actually related. Forcing a model to learn unrelated tasks can be detrimental.</li> <li>Consider grouping tasks. Instead of one model for all tasks, you could train one model for a subset of highly related tasks.</li> <li>Adjusting the learning rate or using a more sophisticated optimization scheme can sometimes help.</li> </ul> </li> <li>Incorrect Label Preparation: Ensure your <code>label_map</code> in the config file is complete and that the <code>multi_label_sep</code> character in <code>load_local_data</code> matches what is used in your data file. Any mismatch will lead to incorrect label parsing and poor model performance.</li> <li><code>CUDA out of memory</code>: As with other tasks, this can be resolved by reducing <code>per_device_train_batch_size</code> in the configuration file.</li> </ul>"},{"location":"user_guide/case_studies/promoter_prediction/","title":"Case Study: Promoter Prediction with DNALLM","text":"<p>This case study demonstrates how to use DNALLM to train a model for promoter prediction, a fundamental task in genomics. We will frame this as a binary classification problem: distinguishing promoter sequences from non-promoter sequences.</p>"},{"location":"user_guide/case_studies/promoter_prediction/#1-case-background","title":"1. Case Background","text":"<p>A promoter is a region of DNA that initiates the transcription of a particular gene. Identifying promoters is crucial for understanding gene regulation and function. In this example, we will fine-tune a pre-trained DNA foundation model to classify sequences as either \"promoter\" (positive class) or \"non-promoter\" (negative class).</p>"},{"location":"user_guide/case_studies/promoter_prediction/#2-code","title":"2. Code","text":"<p>This section provides a complete Python script to perform fine-tuning and inference for promoter prediction. The workflow consists of: 1.  Loading a configuration file. 2.  Loading a pre-trained model and tokenizer (e.g., Plant-DNABERT). 3.  Loading and processing a dataset of DNA sequences with binary labels. 4.  Initializing and running the <code>DNATrainer</code> to fine-tune the model. 5.  Running inference on the test set to evaluate performance.</p>"},{"location":"user_guide/case_studies/promoter_prediction/#setup","title":"Setup","text":"<p>First, ensure you have a YAML configuration file (<code>finetune_config.yaml</code>) and your dataset.</p> <p><code>finetune_config.yaml</code>: <pre><code># task configuration\ntask:\n  task_type: \"sequence_classification\"\n  num_labels: 2\n  label_map:\n    0: \"non-promoter\"\n    1: \"promoter\"\n\n# training configuration\ntraining:\n  output_dir: \"./outputs\"\n  num_train_epochs: 3\n  per_device_train_batch_size: 32\n  per_device_eval_batch_size: 32\n  warmup_steps: 500\n  weight_decay: 0.01\n  logging_dir: \"./logs\"\n  logging_steps: 10\n  evaluation_strategy: \"steps\"\n  save_steps: 100\n  eval_steps: 100\n  load_best_model_at_end: True\n  metric_for_best_model: \"f1\"\n  greater_is_better: True\n</code></pre></p> <p>Dataset: The model expects a dataset (e.g., from Hugging Face, ModelScope, or a local CSV/TSV file) with at least two columns: one for the DNA sequences and one for the corresponding labels (0 or 1).</p>"},{"location":"user_guide/case_studies/promoter_prediction/#python-script","title":"Python Script","text":"finetune_binary.py<pre><code>import os\nfrom dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n\n# --- 1. Load Configuration ---\n# Load settings from the YAML file\nconfigs = load_config(\"./finetune_config.yaml\")\n\n# --- 2. Load Model and Tokenizer ---\n# Specify the pre-trained model to use. You can choose from ModelScope or Hugging Face.\n# For this example, we use a plant-specific BERT model.\nmodel_name = \"zhangtaolab/plant-dnabert-BPE\" \n\n# Load the model for sequence classification and its corresponding tokenizer\n# The `task_config` provides the model with the number of labels.\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name, \n    task_config=configs['task'], \n    source=\"modelscope\" # or \"huggingface\"\n)\n\n# --- 3. Load and Prepare Dataset ---\n# Load a dataset from ModelScope Hub. \n# Replace with your dataset if needed.\ndata_name = \"zhangtaolab/plant-multi-species-core-promoters\"\n\n# Create a DNADataset object\n# `seq_col` and `label_col` specify the column names for sequences and labels.\ndatasets = DNADataset.from_modelscope(\n    data_name, \n    seq_col=\"sequence\", \n    label_col=\"label\", \n    tokenizer=tokenizer, \n    max_length=512\n)\n\n# Tokenize the sequences\ndatasets.encode_sequences()\n\n# For demonstration purposes, we'll use a small sample of the data.\n# Remove the next line to train on the full dataset.\nsampled_datasets = datasets.sampling(0.05, overwrite=True)\n\n# --- 4. Fine-tune the Model ---\n# Initialize the trainer with the model, configs, and datasets\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=sampled_datasets\n)\n\n# Start the training process\nprint(\"Starting model fine-tuning...\")\ntrain_metrics = trainer.train()\nprint(\"Training finished.\")\nprint(\"Training Metrics:\", train_metrics)\n\n# --- 5. Evaluate the Model ---\n# Run inference on the test set to get performance metrics\nprint(\"Evaluating model on the test set...\")\ntest_metrics = trainer.infer()\nprint(\"Evaluation finished.\")\nprint(\"Test Metrics:\", test_metrics)\n</code></pre>"},{"location":"user_guide/case_studies/promoter_prediction/#3-expected-results","title":"3. Expected Results","text":"<p>After running the script, the training process will output logs showing the loss at each step. Upon completion, the <code>train_metrics</code> and <code>test_metrics</code> dictionaries will be printed.</p> <p>The <code>test_metrics</code> dictionary will contain key performance indicators for binary classification, such as: -   <code>test_accuracy</code>: The proportion of correctly classified sequences. -   <code>test_precision</code>: The ability of the model to avoid false positives. -   <code>test_recall</code>: The ability of the model to find all true positive sequences. -   <code>test_f1</code>: The harmonic mean of precision and recall, providing a single score to balance them. -   <code>test_AUROC</code>: The Area Under the Receiver Operating Characteristic Curve, which measures the model's ability to distinguish between classes. -   <code>test_AUPRC</code>: The Area Under the Precision-Recall Curve, which is especially useful for imbalanced datasets.</p> <p>Example output: <pre><code>{\n    'test_loss': 0.547,\n    'test_accuracy': 0.740,\n    'test_precision': 0.721,\n    'test_recall': 0.808,\n    'test_f1': 0.762,\n    'test_mcc': 0.482,\n    'test_AUROC': 0.821,\n    'test_AUPRC': 0.810,\n    ...\n}\n</code></pre> The fine-tuned model and training checkpoints will be saved in the directory specified by <code>output_dir</code> in the config file.</p>"},{"location":"user_guide/case_studies/promoter_prediction/#4-tuning-strategies","title":"4. Tuning Strategies","text":"<p>To improve model performance, consider the following strategies:</p> <ul> <li>Learning Rate: The default learning rate is <code>5e-5</code>. If the model is not converging, you can try adjusting it in the <code>training</code> section of the config file (e.g., <code>learning_rate: 3e-5</code>). A good starting point for fine-tuning is often between <code>1e-5</code> and <code>5e-5</code>.</li> <li>Batch Size: <code>per_device_train_batch_size</code> can be increased if you have more GPU memory. Larger batch sizes can lead to more stable training.</li> <li>Epochs: The <code>num_train_epochs</code> determines how many times the model sees the entire training dataset. If the model is underfitting, increase the number of epochs. If it is overfitting (validation loss increases), consider reducing it or using early stopping.</li> <li>Model Choice: DNALLM supports various models. A larger or more domain-specific model (e.g., one pre-trained on plant genomes) might yield better results for this task.</li> <li>Max Sequence Length: Adjust <code>max_length</code> during dataset loading based on the typical length of your promoter sequences. Longer sequences require more memory.</li> </ul>"},{"location":"user_guide/case_studies/promoter_prediction/#5-troubleshooting","title":"5. Troubleshooting","text":"<ul> <li><code>CUDA out of memory</code>: This is a common issue. The primary solution is to decrease <code>per_device_train_batch_size</code> and <code>per_device_eval_batch_size</code> in your <code>finetune_config.yaml</code>. You can also reduce <code>max_length</code> if your sequences are very long.</li> <li>Slow Training: Training large models on large datasets takes time. To speed up development, use the <code>.sampling()</code> method on your dataset to test your pipeline on a smaller subset first. For actual training, using a GPU is highly recommended.</li> <li>Low Performance (Low F1/Accuracy):<ul> <li>Ensure your data is clean and correctly labeled.</li> <li>Try tuning the hyperparameters mentioned above (learning rate, epochs, etc.).</li> <li>Consider if the pre-trained model is a good fit for your specific type of data (e.g., human, plant, bacteria). You may need a different base model.</li> <li>Check for severe class imbalance in your dataset. If one class is much rarer than the other, metrics like <code>AUPRC</code> are more informative than <code>accuracy</code>. You may need to use techniques like over-sampling or using class weights.</li> </ul> </li> </ul>"},{"location":"user_guide/case_studies/sequence_generation/","title":"Case Study: DNA Sequence Generation with Evo and Mamba Models","text":"<p>This case study demonstrates how to use DNALLM for DNA sequence generation, a powerful technique for designing novel sequences with desired properties. We will showcase this capability using advanced generative models like Evo and Mamba.</p>"},{"location":"user_guide/case_studies/sequence_generation/#1-case-background","title":"1. Case Background","text":"<p>Generative models can learn the underlying patterns of biological sequences from large datasets. Once trained, they can generate new sequences that resemble the training data but are entirely novel. This has applications in synthetic biology, such as designing new promoters, enhancers, or other functional elements.</p> <p>DNALLM integrates state-of-the-art generative architectures, including: -   Evo: A family of models (like Evo-1 and Evo-2) based on the StripedHyena architecture, designed for long-context sequence modeling. -   Mamba: A State Space Model (SSM) architecture that offers efficient, high-quality sequence modeling without the quadratic complexity of traditional Transformers.</p> <p>In this example, we will load a pre-trained Evo model and use it to generate new DNA sequences from a starting prompt. The same workflow can be applied to Mamba-based models.</p>"},{"location":"user_guide/case_studies/sequence_generation/#2-code","title":"2. Code","text":"<p>This section provides a complete script for loading a generative model and performing inference.</p>"},{"location":"user_guide/case_studies/sequence_generation/#setup","title":"Setup","text":"<p>First, ensure you have the necessary dependencies installed for the model you wish to use. For Evo models, this may include <code>evo-model</code> and <code>flash-attn</code>.</p> <p><code>inference_evo_config.yaml</code>: Create a configuration file that specifies the generation parameters.</p> <pre><code># task configuration\ntask:\n  task_type: \"sequence_generation\"\n\n# inference configuration\ninference:\n  per_device_eval_batch_size: 1\n  output_dir: \"./outputs_generation\"\n  generation_max_length: 400\n  temperature: 1.0\n  top_k: 50\n</code></pre>"},{"location":"user_guide/case_studies/sequence_generation/#python-script","title":"Python Script","text":"<p>This script loads a pre-trained Evo-2 model and uses it for both sequence generation and scoring.</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer, DNAInference\n\n# --- 1. Load Configuration and Model ---\n# Load settings from the YAML file\nconfigs = load_config(\"./inference_evo_config.yaml\")\n\n# Specify the pre-trained generative model. Here, we use Evo-2.\n# You can replace this with other generative models like Mamba.\nmodel_name = \"lgq12697/evo2_1b_base\"\n\n# Load the model and its corresponding tokenizer\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name,\n    task_config=configs['task'],\n    source=\"modelscope\"\n)\n\n# --- 2. Initialize Inference Engine ---\ninference_engine = DNAInference(\n    model=model,\n    tokenizer=tokenizer,\n    config=configs\n)\n\n# --- 3. Generate Sequences ---\n# Provide one or more prompts to start the generation process.\n# A special token like \"@\" can be used to start generation from scratch.\nprompts = [\"@\", \"ATG\"]\nprint(f\"Generating sequences from prompts: {prompts}\")\ngenerated_output = inference_engine.generate(prompts)\n\nprint(\"\\n--- Generated Sequences ---\")\nfor seq in generated_output:\n    print(f\"Prompt: {seq['Prompt']}\")\n    print(f\"Generated Sequence: {seq['Output']}\")\n    print(f\"Score: {seq['Score']}\\n\")\n\n# --- 4. Score Existing Sequences ---\n# The same engine can be used to score the likelihood of existing sequences.\nsequences_to_score = [\"ATCCGCATG\", \"ATGCGCATG\"]\nprint(f\"Scoring sequences: {sequences_to_score}\")\nscores = inference_engine.scoring(sequences_to_score)\n\nprint(\"\\n--- Scored Sequences ---\")\nfor res in scores:\n    print(f\"Input Sequence: {res['Input']}\")\n    print(f\"Score: {res['Score']}\\n\")\n</code></pre>"},{"location":"user_guide/case_studies/sequence_generation/#3-expected-results","title":"3. Expected Results","text":"<p>The script will produce two sets of outputs:</p> <ol> <li>Generated Sequences: For each prompt, the model will generate a new DNA sequence. The output will include the original prompt, the generated sequence, and a score (typically the average log-likelihood) indicating how probable the sequence is according to the model.</li> <li>Scored Sequences: For each input sequence provided to the <code>scoring</code> method, the model will return its score, which reflects how well the sequence fits the patterns learned by the model.</li> </ol>"},{"location":"user_guide/case_studies/sequence_generation/#4-tuning-strategies","title":"4. Tuning Strategies","text":"<ul> <li><code>generation_max_length</code>: Controls the length of the generated sequences. Adjust this based on your application's needs.</li> <li><code>temperature</code>: This parameter controls the randomness of the generation. A higher temperature (e.g., 1.0) produces more diverse and creative outputs, while a lower temperature (e.g., 0.7) makes the output more deterministic and focused.</li> <li><code>top_k</code>: This parameter limits the sampling pool to the <code>k</code> most likely next tokens. It can prevent the model from picking highly improbable tokens, leading to more coherent sequences.</li> <li>Prompt Engineering: The starting prompt can significantly influence the generated sequence. Experiment with different prompts, including biologically meaningful ones, to guide the generation process.</li> </ul>"},{"location":"user_guide/case_studies/sequence_generation/#5-troubleshooting","title":"5. Troubleshooting","text":"<ul> <li>Dependency Issues: Generative models like Evo and Mamba may have specific dependencies (e.g., <code>flash-attn</code>, <code>causal-conv1d</code>). Ensure you have installed all required packages for the chosen model. The installation process may involve compilation, which can take time.</li> <li><code>CUDA out of memory</code>: Sequence generation can be memory-intensive, especially with long sequences. If you encounter this error, try reducing <code>per_device_eval_batch_size</code> or <code>generation_max_length</code>.</li> <li>Low-Quality Generations: If the generated sequences are repetitive or nonsensical, try adjusting the <code>temperature</code> and <code>top_k</code> parameters. A very high temperature can lead to randomness, while a very low temperature can cause repetition.</li> <li>Slow Inference: Generation is an auto-regressive process and can be slow. For large-scale generation, ensure you are using a GPU. The model's size and the sequence length will be the primary factors affecting speed.</li> </ul>"},{"location":"user_guide/cli/","title":"CLI Tools","text":"<p>DNALLM provides a comprehensive set of command-line interface tools for various DNA language model tasks.</p>"},{"location":"user_guide/cli/#available-commands","title":"Available Commands","text":""},{"location":"user_guide/cli/#core-commands","title":"Core Commands","text":"<ul> <li><code>dnallm train</code> - Fine-tune DNA language models</li> <li><code>dnallm inference</code> - Run inference with trained models</li> <li><code>dnallm benchmark</code> - Compare multiple models</li> <li><code>dnallm mutagenesis</code> - Perform in silico mutagenesis</li> </ul>"},{"location":"user_guide/cli/#server-commands","title":"Server Commands","text":"<ul> <li><code>dnallm mcp-server</code> - Start MCP (Model Context Protocol) server</li> <li><code>dnallm-mcp-server</code> - Standalone MCP server script</li> </ul>"},{"location":"user_guide/cli/#configuration-tools","title":"Configuration Tools","text":"<ul> <li><code>dnallm model-config-generator</code> - Interactive configuration file generator</li> </ul>"},{"location":"user_guide/cli/#quick-start","title":"Quick Start","text":"<pre><code># Generate a configuration file\ndnallm model-config-generator --output finetune_config.yaml\n\n# Train a model\ndnallm train --config finetune_config.yaml\n\n# Run inference\ndnallm inference --config inference_config.yaml --model-path ./models/trained_model\n\n# Benchmark models\ndnallm benchmark --config benchmark_config.yaml\n\n# Start MCP server\ndnallm mcp-server --config mcp_server_config.yaml\n</code></pre>"},{"location":"user_guide/cli/#configuration","title":"Configuration","text":"<p>All DNALLM commands use YAML configuration files that define:</p> <ul> <li>Task parameters (classification, regression, generation, etc.)</li> <li>Model settings and hyperparameters</li> <li>Data preprocessing options</li> <li>Evaluation metrics and output formats</li> </ul>"},{"location":"user_guide/cli/#getting-help","title":"Getting Help","text":"<pre><code># Show help for a specific command\ndnallm train --help\n\n# Show general help\ndnallm --help\n</code></pre>"},{"location":"user_guide/cli/#next-steps","title":"Next Steps","text":"<ul> <li>Usage Guide - Complete CLI usage instructions and examples</li> <li>Configuration Generator - Learn how to create configuration files</li> <li>MCP Server - Learn about the Model Context Protocol server</li> <li>Fine-tuning Tutorials - Learn to train models</li> <li>Benchmark Tutorials - Compare model performance</li> <li>CLI Troubleshooting - Common CLI issues and solutions</li> </ul>"},{"location":"user_guide/cli/config_generator/","title":"Configuration Generator","text":"<p>The DNALLM Configuration Generator is an interactive CLI tool that helps you create configuration files for various DNALLM tasks without manually writing YAML files.</p>"},{"location":"user_guide/cli/config_generator/#features","title":"Features","text":"<ul> <li>Interactive Configuration: Step-by-step prompts guide you through configuration options</li> <li>Three Configuration Types: Support for fine-tuning, inference, and benchmark configurations</li> <li>Smart Defaults: Sensible default values for common use cases</li> <li>Validation: Built-in validation to ensure configuration correctness</li> <li>Flexible Output: Save configurations to custom file paths</li> </ul>"},{"location":"user_guide/cli/config_generator/#usage","title":"Usage","text":""},{"location":"user_guide/cli/config_generator/#basic-usage","title":"Basic Usage","text":"<pre><code># Generate configuration interactively\ndnallm config-generator\n\n# Generate specific configuration type\ndnallm config-generator --type finetune\ndnallm config-generator --type inference\ndnallm config-generator --type benchmark\n\n# Specify output file\ndnallm config-generator --output my_config.yaml\n</code></pre>"},{"location":"user_guide/cli/config_generator/#command-line-options","title":"Command Line Options","text":"<ul> <li><code>--type, -t</code>: Specify configuration type (finetune, inference, benchmark)</li> <li><code>--output, -o</code>: Specify output file path (default: auto-generated based on type)</li> </ul>"},{"location":"user_guide/cli/config_generator/#configuration-types","title":"Configuration Types","text":""},{"location":"user_guide/cli/config_generator/#1-fine-tuning-configuration","title":"1. Fine-tuning Configuration","text":"<p>Generates configuration for training/fine-tuning DNA language models.</p> <p>Includes: - Task configuration (task type, labels, threshold) - Training parameters (epochs, batch size, learning rate) - Optimization settings (weight decay, warmup ratio) - Logging and evaluation settings - Inference settings for evaluation</p> <p>Example Output:</p> <pre><code>task:\n  task_type: binary\n  num_labels: 2\n  threshold: 0.5\nfinetune:\n  output_dir: ./outputs\n  num_train_epochs: 3\n  per_device_train_batch_size: 8\n  learning_rate: 2e-5\n  weight_decay: 0.01\n  warmup_ratio: 0.1\n  logging_steps: 100\n  eval_steps: 100\n  save_steps: 500\n  seed: 42\ninference:\n  batch_size: 16\n  max_length: 512\n  device: auto\n  num_workers: 4\n  output_dir: ./results\n</code></pre>"},{"location":"user_guide/cli/config_generator/#2-inference-configuration","title":"2. Inference Configuration","text":"<p>Generates configuration for running inference with trained models.</p> <p>Includes: - Task configuration - Inference parameters (batch size, sequence length) - Hardware settings (device, workers) - Output configuration</p> <p>Example Output:</p> <pre><code>task:\n  task_type: binary\n  num_labels: 2\n  threshold: 0.5\ninference:\n  batch_size: 16\n  max_length: 512\n  device: auto\n  num_workers: 4\n  use_fp16: false\n  output_dir: ./results\n</code></pre>"},{"location":"user_guide/cli/config_generator/#3-benchmark-configuration","title":"3. Benchmark Configuration","text":"<p>Generates configuration for benchmarking multiple models.</p> <p>Includes: - Benchmark metadata (name, description) - Model configurations (multiple models with sources) - Dataset configurations (multiple datasets with formats) - Evaluation metrics - Performance settings - Output and reporting options</p> <p>Example Output:</p> <pre><code>benchmark:\n  name: DNA Model Benchmark\n  description: Comparing DNA language models\nmodels:\n  - name: Plant DNABERT\n    path: zhangtaolab/plant-dnabert-BPE-promoter\n    source: huggingface\n    task_type: classification\n  - name: Plant DNAGPT\n    path: zhangtaolab/plant-dnagpt-BPE-promoter\n    source: huggingface\n    task_type: generation\ndatasets:\n  - name: promoter_data\n    path: data/promoters.csv\n    format: csv\n    task: binary_classification\n    text_column: sequence\n    label_column: label\nmetrics:\n  - accuracy\n  - f1_score\n  - precision\n  - recall\nevaluation:\n  batch_size: 32\n  max_length: 512\n  device: auto\n  num_workers: 4\n  seed: 42\noutput:\n  format: html\n  path: benchmark_results\n  save_predictions: true\n  generate_plots: true\n</code></pre>"},{"location":"user_guide/cli/config_generator/#interactive-prompts","title":"Interactive Prompts","text":"<p>The tool will guide you through each configuration section with helpful prompts:</p>"},{"location":"user_guide/cli/config_generator/#task-configuration","title":"Task Configuration","text":"<ul> <li>Task Type: Choose from supported task types</li> <li>Number of Labels: For classification tasks</li> <li>Threshold: For binary/multilabel classification</li> <li>Label Names: Optional human-readable labels</li> </ul>"},{"location":"user_guide/cli/config_generator/#training-configuration","title":"Training Configuration","text":"<ul> <li>Basic Settings: Output directory, epochs, batch sizes</li> <li>Learning Parameters: Learning rate, weight decay, warmup</li> <li>Advanced Options: Gradient accumulation, scheduler, precision</li> <li>Logging: Steps for logging, evaluation, and saving</li> </ul>"},{"location":"user_guide/cli/config_generator/#model-configuration-benchmark","title":"Model Configuration (Benchmark)","text":"<ul> <li>Model Details: Name, path, source</li> <li>Source Types: Hugging Face, ModelScope, local files</li> <li>Task Types: Classification, generation, embedding, etc.</li> <li>Advanced Settings: Revision, data types, trust settings</li> </ul>"},{"location":"user_guide/cli/config_generator/#dataset-configuration-benchmark","title":"Dataset Configuration (Benchmark)","text":"<ul> <li>Dataset Info: Name, file path, format</li> <li>Format Support: CSV, TSV, JSON, FASTA, Arrow, Parquet</li> <li>Task Types: Binary/multiclass classification, regression</li> <li>Preprocessing: Sequence length, truncation, padding</li> <li>Data Splitting: Test/validation ratios, random seed</li> </ul>"},{"location":"user_guide/cli/config_generator/#evaluation-configuration","title":"Evaluation Configuration","text":"<ul> <li>Performance: Batch size, sequence length, workers</li> <li>Hardware: Device selection (CPU, GPU, auto)</li> <li>Optimization: Mixed precision, memory efficiency</li> <li>Reproducibility: Random seed, deterministic mode</li> </ul>"},{"location":"user_guide/cli/config_generator/#output-configuration","title":"Output Configuration","text":"<ul> <li>Formats: HTML, CSV, JSON, PDF reports</li> <li>Content: Predictions, embeddings, attention maps</li> <li>Visualization: Plots, charts, interactive elements</li> <li>Customization: Report titles, sections, recommendations</li> </ul>"},{"location":"user_guide/cli/config_generator/#examples","title":"Examples","text":""},{"location":"user_guide/cli/config_generator/#quick-fine-tuning-setup","title":"Quick Fine-tuning Setup","text":"<pre><code># Generate fine-tuning config with defaults\ndnallm config-generator --type finetune --output my_training.yaml\n\n# Customize specific parameters\ndnallm config-generator --type finetune\n# Follow prompts to set custom values\n</code></pre>"},{"location":"user_guide/cli/config_generator/#benchmark-multiple-models","title":"Benchmark Multiple Models","text":"<pre><code># Generate benchmark config\ndnallm config-generator --type benchmark --output model_comparison.yaml\n\n# Add multiple models and datasets interactively\n# Configure evaluation metrics and output format\n</code></pre>"},{"location":"user_guide/cli/config_generator/#inference-configuration","title":"Inference Configuration","text":"<pre><code># Generate inference config for inference\ndnallm config-generator --type inference --output inference_config.yaml\n\n# Set batch size, device, and output options\n</code></pre>"},{"location":"user_guide/cli/config_generator/#integration-with-dnallm","title":"Integration with DNALLM","text":"<p>Generated configurations can be used directly with DNALLM commands:</p> <pre><code># Use generated config for training\ndnallm train --config finetune_config.yaml\n\n# Use generated config for inference\ndnallm inference --config inference_config.yaml\n\n# Use generated config for benchmarking\ndnallm benchmark --config benchmark_config.yaml\n</code></pre>"},{"location":"user_guide/cli/config_generator/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li>Start with Defaults: Use default values for initial setup, then customize as needed</li> <li>Validate Paths: Ensure all file paths in the configuration exist</li> <li>Hardware Considerations: Choose appropriate batch sizes and devices for your hardware</li> <li>Task Alignment: Ensure model task types match your dataset and evaluation goals</li> <li>Save Templates: Keep generated configs as templates for similar future tasks</li> </ol>"},{"location":"user_guide/cli/config_generator/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide/cli/config_generator/#common-issues","title":"Common Issues","text":"<ul> <li>Invalid Task Type: Ensure task type matches your model and data</li> <li>Path Errors: Verify all file paths exist and are accessible</li> <li>Memory Issues: Reduce batch sizes for large models or limited memory</li> <li>Device Errors: Check GPU availability and CUDA installation</li> </ul>"},{"location":"user_guide/cli/config_generator/#getting-help","title":"Getting Help","text":"<ul> <li>Review the generated configuration file for any obvious errors</li> <li>Check DNALLM documentation for parameter descriptions</li> <li>Use smaller datasets for testing configurations</li> <li>Verify model compatibility with your chosen task type</li> </ul>"},{"location":"user_guide/cli/config_generator/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user_guide/cli/config_generator/#custom-metrics","title":"Custom Metrics","text":"<p>Add custom evaluation metrics in benchmark configurations:</p> <pre><code>metrics:\n  - name: custom_dna_metric\n    class: CustomDNAMetric\n    parameters:\n      threshold: 0.5\n</code></pre>"},{"location":"user_guide/cli/config_generator/#model-variants","title":"Model Variants","text":"<p>Configure multiple variants of the same model:</p> <pre><code>models:\n  - name: plant-dnamamba-6mer-open_chromatin\n    path: zhangtaolab/plant-dnamamba-6mer-open_chromatin\n    source: huggingface\n    task_type: classification\n  - name: plant-dnabert-BPE-open_chromatin\n    path: zhangtaolab/plant-dnabert-BPE-open_chromatin\n    source: huggingface\n    task_type: classification\n</code></pre>"},{"location":"user_guide/cli/config_generator/#data-augmentation","title":"Data Augmentation","text":"<p>Enable data augmentation for training:</p> <pre><code>dataset:\n  preprocessing:\n    augment: true\n    reverse_complement_ratio: 0.5\n    random_mutation_ratio: 0.1\n</code></pre> <p>The Configuration Generator makes it easy to create comprehensive, validated configurations for all your DNALLM tasks!</p>"},{"location":"user_guide/cli/mcp_server/","title":"MCP Server","text":"<p>The DNALLM MCP (Model Context Protocol) Server provides a standardized interface for DNA sequence prediction and analysis through the Model Context Protocol. This enables seamless integration with MCP-compatible clients and tools.</p>"},{"location":"user_guide/cli/mcp_server/#overview","title":"Overview","text":"<p>The MCP Server allows you to:</p> <ul> <li>Serve DNA Models: Host multiple DNA language models simultaneously</li> <li>Real-time Prediction: Provide fast DNA sequence predictions via MCP protocol</li> <li>Multiple Transport Protocols: Support stdio, SSE, and HTTP transport methods</li> <li>Model Management: Dynamically load and manage different DNA models</li> <li>Health Monitoring: Built-in health checks and status monitoring</li> </ul>"},{"location":"user_guide/cli/mcp_server/#quick-start","title":"Quick Start","text":""},{"location":"user_guide/cli/mcp_server/#basic-usage","title":"Basic Usage","text":"<pre><code># Start MCP server with default configuration\ndnallm mcp-server\n\n# Start with custom configuration\ndnallm mcp-server --config path/to/config.yaml\n\n# Start with custom port and host\ndnallm mcp-server --host 0.0.0.0 --port 9000\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#standalone-usage","title":"Standalone Usage","text":"<pre><code># Use the standalone MCP server script\ndnallm-mcp-server --config path/to/config.yaml\n\n# With custom transport protocol\ndnallm-mcp-server --transport sse --port 8000\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#command-reference","title":"Command Reference","text":""},{"location":"user_guide/cli/mcp_server/#dnallm-mcp-server","title":"<code>dnallm mcp-server</code>","text":"<p>Start the DNALLM MCP server with specified configuration.</p>"},{"location":"user_guide/cli/mcp_server/#options","title":"Options","text":"Option Short Type Default Description <code>--config</code> <code>-c</code> PATH <code>dnallm/mcp/configs/mcp_server_config.yaml</code> Path to MCP server configuration file <code>--host</code> TEXT <code>0.0.0.0</code> Host to bind the server to <code>--port</code> <code>-p</code> INTEGER <code>8000</code> Port to bind the server to <code>--log-level</code> CHOICE <code>INFO</code> Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL) <code>--transport</code> CHOICE <code>stdio</code> Transport protocol (stdio, sse, streamable-http)"},{"location":"user_guide/cli/mcp_server/#examples","title":"Examples","text":"<pre><code># Basic server startup\ndnallm mcp-server\n\n# Custom configuration and port\ndnallm mcp-server --config custom_config.yaml --port 9000\n\n# SSE transport with debug logging\ndnallm mcp-server --transport sse --log-level DEBUG\n\n# Bind to specific host\ndnallm mcp-server --host 127.0.0.1 --port 8000\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#configuration","title":"Configuration","text":""},{"location":"user_guide/cli/mcp_server/#server-configuration-file","title":"Server Configuration File","text":"<p>The MCP server uses YAML configuration files to define server settings and model configurations.</p>"},{"location":"user_guide/cli/mcp_server/#basic-configuration-structure","title":"Basic Configuration Structure","text":"<pre><code># MCP Server Configuration\nmcp:\n  name: \"DNALLM MCP Server\"\n  version: \"1.0.0\"\n  description: \"MCP server for DNA sequence prediction\"\n\n# Server settings\nserver:\n  host: \"0.0.0.0\"\n  port: 8000\n  transport: \"stdio\"  # stdio, sse, streamable-http\n  log_level: \"INFO\"\n\n# Model configurations\nmodels:\n  - name: \"promoter_model\"\n    path: \"zhangtaolab/plant-dnabert-BPE-promoter\"\n    source: \"modelscope\"\n    task_type: \"binary_classification\"\n    enabled: true\n  - name: \"conservation_model\"\n    path: \"zhangtaolab/plant-dnabert-BPE-conservation\"\n    source: \"modelscope\"\n    task_type: \"binary_classification\"\n    enabled: true\n\n# Logging configuration\nlogging:\n  level: \"INFO\"\n  file: \"./logs/mcp_server.log\"\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#model-configuration","title":"Model Configuration","text":"<p>Each model in the configuration includes:</p> <ul> <li>name: Unique identifier for the model</li> <li>path: Model path (Hugging Face model ID or local path)</li> <li>source: Model source (<code>huggingface</code>, <code>modelscope</code>, or <code>local</code>)</li> <li>task_type: Type of task the model performs</li> <li>enabled: Whether the model should be loaded</li> </ul>"},{"location":"user_guide/cli/mcp_server/#transport-protocols","title":"Transport Protocols","text":""},{"location":"user_guide/cli/mcp_server/#1-stdio-default","title":"1. stdio (Default)","text":"<ul> <li>Use Case: Direct integration with MCP clients</li> <li>Protocol: Standard input/output communication</li> <li>Best For: Claude Desktop, other MCP clients</li> </ul>"},{"location":"user_guide/cli/mcp_server/#2-sse-server-sent-events","title":"2. SSE (Server-Sent Events)","text":"<ul> <li>Use Case: Web-based applications</li> <li>Protocol: HTTP with Server-Sent Events</li> <li>Best For: Web dashboards, real-time applications</li> </ul>"},{"location":"user_guide/cli/mcp_server/#3-streamable-http","title":"3. streamable-http","text":"<ul> <li>Use Case: HTTP-based integrations</li> <li>Protocol: Standard HTTP with streaming support</li> <li>Best For: REST API integrations</li> <li>Client Access Point: <code>http://localhost:8000/mcp</code></li> </ul>"},{"location":"user_guide/cli/mcp_server/#client-access-points","title":"Client Access Points","text":""},{"location":"user_guide/cli/mcp_server/#default-configuration","title":"Default Configuration","text":"<ul> <li>Host: <code>0.0.0.0</code> (listens on all interfaces)</li> <li>Port: <code>8000</code></li> <li>Base URL: <code>http://localhost:8000</code></li> </ul>"},{"location":"user_guide/cli/mcp_server/#transport-specific-endpoints","title":"Transport-Specific Endpoints","text":""},{"location":"user_guide/cli/mcp_server/#stdio-transport","title":"STDIO Transport","text":"<ul> <li>Access: Direct process communication</li> <li>Usage: MCP clients like Claude Desktop</li> <li>Configuration: No URL needed, uses process communication</li> </ul>"},{"location":"user_guide/cli/mcp_server/#sse-transport","title":"SSE Transport","text":"<ul> <li>SSE Connection: <code>http://localhost:8000/sse</code></li> <li>MCP Messages: <code>http://localhost:8000/mcp/messages/</code></li> <li>Usage: Real-time web applications</li> </ul>"},{"location":"user_guide/cli/mcp_server/#streamable-http-transport","title":"Streamable HTTP Transport","text":"<ul> <li>Main Endpoint: <code>http://localhost:8000/mcp</code></li> <li>Available Endpoints:</li> <li><code>http://localhost:8000/mcp</code> - Main MCP protocol endpoint</li> <li><code>http://localhost:8000/mcp/tools</code> - Tool listing endpoint</li> <li><code>http://localhost:8000/mcp/messages</code> - MCP message handling endpoint</li> </ul>"},{"location":"user_guide/cli/mcp_server/#testing-client-access","title":"Testing Client Access","text":""},{"location":"user_guide/cli/mcp_server/#test-streamable-http-connection","title":"Test Streamable HTTP Connection","text":"<pre><code># Test basic connectivity\ncurl http://localhost:8000/mcp\n\n# List available tools\ncurl -X POST \"http://localhost:8000/mcp/tools\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/list\", \"params\": {}}'\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#test-sse-connection","title":"Test SSE Connection","text":"<pre><code># Test SSE connection\ncurl -N -H \"Accept: text/event-stream\" http://localhost:8000/sse\n\n# Test MCP messages (requires valid session_id)\ncurl -X POST \"http://localhost:8000/mcp/messages/?session_id=YOUR_SESSION_ID\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/list\", \"params\": {}}'\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#example-dna-prediction-request","title":"Example DNA Prediction Request","text":"<pre><code># Single sequence prediction\ncurl -X POST \"http://localhost:8000/mcp/messages\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n      \"name\": \"dna_sequence_predict\",\n      \"arguments\": {\n        \"sequence\": \"ATCGATCGATCGATCG\",\n        \"model_name\": \"promoter_model\"\n      }\n    }\n  }'\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#custom-configuration","title":"Custom Configuration","text":"<p>You can customize the access points by modifying the server configuration:</p> <pre><code># Custom server configuration\nserver:\n  host: \"127.0.0.1\"  # Bind to localhost only\n  port: 9000         # Use custom port\n\n# Custom mount path for SSE\nsse:\n  mount_path: \"/api/mcp\"  # Custom mount path\n</code></pre> <p>With custom configuration, the endpoints would be: - Streamable HTTP: <code>http://127.0.0.1:9000/mcp</code> - SSE: <code>http://127.0.0.1:9000/sse</code> - MCP Messages: <code>http://127.0.0.1:9000/api/mcp/messages/</code></p>"},{"location":"user_guide/cli/mcp_server/#api-reference","title":"API Reference","text":""},{"location":"user_guide/cli/mcp_server/#available-tools","title":"Available Tools","text":"<p>The MCP server provides the following tools:</p>"},{"location":"user_guide/cli/mcp_server/#1-health-check","title":"1. Health Check","text":"<ul> <li>Tool: <code>health_check</code></li> <li>Description: Check server and model status</li> <li>Returns: Server health information and loaded models</li> </ul>"},{"location":"user_guide/cli/mcp_server/#2-dna-sequence-prediction","title":"2. DNA Sequence Prediction","text":"<ul> <li>Tool: <code>predict_dna_sequence</code></li> <li>Description: Predict properties of DNA sequences</li> <li>Parameters:</li> <li><code>sequence</code>: DNA sequence to analyze</li> <li><code>model_name</code>: Specific model to use (optional)</li> <li><code>task_type</code>: Type of prediction task (optional)</li> </ul>"},{"location":"user_guide/cli/mcp_server/#3-model-information","title":"3. Model Information","text":"<ul> <li>Tool: <code>get_model_info</code></li> <li>Description: Get information about available models</li> <li>Returns: List of loaded models and their capabilities</li> </ul>"},{"location":"user_guide/cli/mcp_server/#example-api-usage","title":"Example API Usage","text":"<pre><code># Health check\n{\n  \"tool\": \"health_check\",\n  \"arguments\": {}\n}\n\n# DNA sequence prediction\n{\n  \"tool\": \"predict_dna_sequence\",\n  \"arguments\": {\n    \"sequence\": \"ATCGATCGATCG\",\n    \"model_name\": \"promoter_model\"\n  }\n}\n\n# Get model information\n{\n  \"tool\": \"get_model_info\",\n  \"arguments\": {}\n}\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#integration-examples","title":"Integration Examples","text":""},{"location":"user_guide/cli/mcp_server/#claude-desktop-integration","title":"Claude Desktop Integration","text":"<p>Add to your Claude Desktop configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"dnallm\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"/path/to/DNALLM/dnallm/mcp/start_server.py\",\n        \"--config\",\n        \"/path/to/DNALLM/dnallm/mcp/configs/mcp_server_config.yaml\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#web-application-integration","title":"Web Application Integration","text":"<pre><code>// Connect to SSE endpoint\nconst eventSource = new EventSource('http://localhost:8000/sse');\n\neventSource.onmessage = function(event) {\n  const data = JSON.parse(event.data);\n  console.log('Prediction result:', data);\n};\n\n// Send prediction request\nfetch('http://localhost:8000/predict', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    sequence: 'ATCGATCGATCG',\n    model_name: 'promoter_model'\n  })\n});\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide/cli/mcp_server/#common-issues","title":"Common Issues","text":""},{"location":"user_guide/cli/mcp_server/#1-port-already-in-use","title":"1. Port Already in Use","text":"<pre><code># Check what's using the port\nlsof -i :8000\n\n# Use a different port\ndnallm mcp-server --port 9000\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#2-model-loading-failures","title":"2. Model Loading Failures","text":"<ul> <li>Check model paths and sources</li> <li>Ensure internet connection for remote models</li> <li>Verify model compatibility with task type</li> </ul>"},{"location":"user_guide/cli/mcp_server/#3-configuration-errors","title":"3. Configuration Errors","text":"<ul> <li>Validate YAML syntax</li> <li>Check file paths are correct</li> <li>Ensure all required fields are present</li> </ul>"},{"location":"user_guide/cli/mcp_server/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable debug logging\ndnallm mcp-server --log-level DEBUG\n\n# Check server logs\ntail -f logs/mcp_server.log\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#health-monitoring","title":"Health Monitoring","text":"<pre><code># Check server health\ncurl -X POST http://localhost:8000/health\n\n# Get model status\ncurl -X POST http://localhost:8000/models\n</code></pre>"},{"location":"user_guide/cli/mcp_server/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user_guide/cli/mcp_server/#model-loading","title":"Model Loading","text":"<ul> <li>Load only required models</li> <li>Use GPU acceleration when available</li> <li>Consider model quantization for faster inference</li> </ul>"},{"location":"user_guide/cli/mcp_server/#server-configuration","title":"Server Configuration","text":"<ul> <li>Adjust batch sizes based on available memory</li> <li>Use appropriate transport protocol for your use case</li> <li>Monitor resource usage and adjust accordingly</li> </ul>"},{"location":"user_guide/cli/mcp_server/#caching","title":"Caching","text":"<ul> <li>Enable model caching for frequently used models</li> <li>Use appropriate cache sizes based on available memory</li> </ul>"},{"location":"user_guide/cli/mcp_server/#security-considerations","title":"Security Considerations","text":""},{"location":"user_guide/cli/mcp_server/#network-security","title":"Network Security","text":"<ul> <li>Use appropriate host binding (avoid 0.0.0.0 in production)</li> <li>Implement authentication if needed</li> <li>Use HTTPS for production deployments</li> </ul>"},{"location":"user_guide/cli/mcp_server/#model-security","title":"Model Security","text":"<ul> <li>Validate input sequences</li> <li>Implement rate limiting</li> <li>Monitor for unusual usage patterns</li> </ul>"},{"location":"user_guide/cli/mcp_server/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Generator - Learn how to create configuration files</li> <li>Fine-tuning Tutorials - Train your own models</li> <li>API Reference - Detailed API documentation</li> <li>FAQ - Common questions and solutions</li> <li>CLI Troubleshooting - Common CLI issues and solutions</li> </ul>"},{"location":"user_guide/cli/usage/","title":"CLI Usage Guide","text":""},{"location":"user_guide/cli/usage/#overview","title":"Overview","text":"<p>DNALLM provides two ways to use the command-line interface: 1. After package installation: Using the <code>dnallm-*</code> commands 2. Development environment: Running directly from the project root</p>"},{"location":"user_guide/cli/usage/#usage-after-installation","title":"Usage After Installation","text":"<p>After installing the DNALLM package, you can use the following commands:</p> <pre><code># Training\ndnallm-train --config config.yaml\ndnallm-train --model model_name --data data_path --output output_dir\n\n# Run inference\ndnallm-inference --config config.yaml\ndnallm-inference --model model_name --input input_file\n\n# Generate configuration files\ndnallm-model-config-generator --output config.yaml\ndnallm-model-config-generator --preview\n\n# MCP server\ndnallm-mcp-server --config config.yaml\n</code></pre>"},{"location":"user_guide/cli/usage/#development-environment-usage","title":"Development Environment Usage","text":"<p>In the project root directory, you can use the following methods:</p>"},{"location":"user_guide/cli/usage/#1-using-the-launcher-script","title":"1. Using the Launcher Script","text":"<pre><code># Main CLI\npython run_cli.py --help\n\n# Training\npython run_cli.py train --config config.yaml\n\n# Inference\npython run_cli.py inference --config config.yaml\n\n# Generate configuration\npython run_cli.py model-config-generator --output config.yaml\n</code></pre>"},{"location":"user_guide/cli/usage/#2-running-cli-modules-directly","title":"2. Running CLI Modules Directly","text":"<pre><code># Main CLI\npython cli/cli.py --help\n\n# Training\npython cli/train.py config.yaml model_path data_path\n\n# Inference\npython cli/inference.py config.yaml model_path\n\n# Configuration generator\npython cli/model_config_generator.py --output config.yaml\n</code></pre>"},{"location":"user_guide/cli/usage/#3-using-package-modules","title":"3. Using Package Modules","text":"<pre><code># Package CLI\npython -m dnallm.cli.cli --help\n\n# Package training\npython -m dnallm.cli.train config.yaml model_path data_path\n\n# Package inference\npython -m dnallm.cli.inference config.yaml model_path\n\n# Package configuration generator\npython -m dnallm.cli.model_config_generator --output config.yaml\n</code></pre>"},{"location":"user_guide/cli/usage/#command-reference","title":"Command Reference","text":""},{"location":"user_guide/cli/usage/#dnallm-train","title":"<code>dnallm-train</code>","text":"<p>Train a DNA language model with specified configuration.</p> <p>Options: - <code>--config, -c</code>: Path to training configuration file - <code>--model, -m</code>: Model name or path - <code>--data, -d</code>: Path to training data - <code>--output, -o</code>: Output directory for training results</p> <p>Examples: <pre><code># Using configuration file\ndnallm-train --config finetune_config.yaml\n\n# Using command line arguments\ndnallm-train --model zhangtaolab/plant-dnagpt-BPE --data ./data --output ./outputs\n</code></pre></p>"},{"location":"user_guide/cli/usage/#dnallm-inference","title":"<code>dnallm-inference</code>","text":"<p>Run inference with a trained DNA language model.</p> <p>Options: - <code>--config, -c</code>: Path to inference configuration file - <code>--model, -m</code>: Model name or path - <code>--input, -i</code>: Path to input data file - <code>--output, -o</code>: Output file path</p> <p>Examples: <pre><code># Using configuration file\ndnallm-inference --config inference_config.yaml\n\n# Using command line arguments\ndnallm-inference --model ./models/trained_model --input ./test_data.csv\n</code></pre></p>"},{"location":"user_guide/cli/usage/#dnallm-model-config-generator","title":"<code>dnallm-model-config-generator</code>","text":"<p>Generate configuration files for DNALLM tasks.</p> <p>Options: - <code>--output, -o</code>: Output file path for configuration - <code>--preview</code>: Preview configuration without saving - <code>--template</code>: Template type (training, inference, benchmark)</p> <p>Examples: <pre><code># Generate training configuration\ndnallm-model-config-generator --output training_config.yaml\n\n# Preview configuration\ndnallm-model-config-generator --preview\n</code></pre></p>"},{"location":"user_guide/cli/usage/#dnallm-mcp-server","title":"<code>dnallm-mcp-server</code>","text":"<p>Start MCP (Model Context Protocol) server.</p> <p>Options: - <code>--config, -c</code>: Path to configuration file - <code>--port, -p</code>: Server port (default: 8000) - <code>--host, -h</code>: Server host (default: localhost)</p> <p>Examples: <pre><code># Start server with configuration\ndnallm-mcp-server --config mcp_config.yaml\n\n# Start server on specific port\ndnallm-mcp-server --port 9000\n</code></pre></p>"},{"location":"user_guide/cli/usage/#configuration-examples","title":"Configuration Examples","text":""},{"location":"user_guide/cli/usage/#training-configuration-configyaml","title":"Training Configuration (config.yaml)","text":"<pre><code>model:\n  name_or_path: \"zhangtaolab/plant-dnagpt-BPE\"\n  source: \"huggingface\"\n\ntask:\n  task_type: \"binary_classification\"\n  num_labels: 2\n  label_names: [\"negative\", \"positive\"]\n\ndata:\n  train_file: \"path/to/train.csv\"\n  eval_file: \"path/to/eval.csv\"\n  text_column: \"sequence\"\n  label_column: \"label\"\n\ntraining:\n  num_train_epochs: 3\n  per_device_train_batch_size: 8\n  learning_rate: 5e-5\n  save_steps: 1000\n  eval_steps: 1000\n</code></pre>"},{"location":"user_guide/cli/usage/#inference-configuration-configyaml","title":"Inference Configuration (config.yaml)","text":"<pre><code>model:\n  name_or_path: \"path/to/trained/model\"\n  source: \"local\"\n\ntask:\n  task_type: \"binary_classification\"\n  num_labels: 2\n\ndata:\n  input_file: \"path/to/input/data\"\n  output_file: \"predictions.csv\"\n</code></pre>"},{"location":"user_guide/cli/usage/#benchmark-configuration-configyaml","title":"Benchmark Configuration (config.yaml)","text":"<pre><code>benchmark:\n  name: \"DNA Model Benchmark\"\n  description: \"Comparing DNA language models on various tasks\"\n\nmodels:\n  - name: \"Model 1\"\n    source: \"huggingface\"\n    path: \"zhangtaolab/plant-dnagpt-BPE\"\n    task_type: \"binary_classification\"\n  - name: \"Model 2\"\n    source: \"modelscope\"\n    path: \"zhangtaolab/plant-dnabert\"\n    task_type: \"binary_classification\"\n\ndatasets:\n  - name: \"Test Dataset\"\n    path: \"path/to/dataset.csv\"\n    format: \"csv\"\n    task: \"binary_classification\"\n\nevaluation:\n  metrics: [\"accuracy\", \"precision\", \"recall\", \"f1\", \"mcc\"]\n  save_predictions: true\n  output_dir: \"./benchmark_results\"\n</code></pre>"},{"location":"user_guide/cli/usage/#project-structure","title":"Project Structure","text":"<pre><code>DNALLM/\n\u251c\u2500\u2500 cli/                    # Root directory CLI entry points\n\u2502   \u251c\u2500\u2500 cli.py            # Main CLI\n\u2502   \u251c\u2500\u2500 train.py          # Training CLI\n\u2502   \u251c\u2500\u2500 inference.py      # Inference CLI\n\u2502   \u2514\u2500\u2500 model_config_generator.py # Configuration generator\n\u251c\u2500\u2500 ui/                    # UI applications\n\u2502   \u251c\u2500\u2500 run_config_app.py # Configuration generator launcher\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 dnallm/               # Core package\n\u2502   \u251c\u2500\u2500 cli/             # Package CLI modules\n\u2502   \u2502   \u251c\u2500\u2500 cli.py       # Package CLI implementation\n\u2502   \u2502   \u251c\u2500\u2500 train.py     # Package training module\n\u2502   \u2502   \u251c\u2500\u2500 inference.py # Package inference module\n\u2502   \u2502   \u2514\u2500\u2500 model_config_generator.py # Package config generator\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 run_cli.py           # Root directory CLI launcher\n\u2514\u2500\u2500 pyproject.toml       # Package configuration\n</code></pre>"},{"location":"user_guide/cli/usage/#important-notes","title":"Important Notes","text":"<ol> <li>Development Environment: Ensure you're running commands from the project root directory</li> <li>Dependencies: Make sure all dependencies are properly installed</li> <li>Path Configuration: Use absolute paths or paths relative to the project root</li> <li>Python Version: Requires Python 3.10 or higher</li> </ol>"},{"location":"user_guide/cli/usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide/cli/usage/#import-errors","title":"Import Errors","text":"<ul> <li>Ensure you're running from the project root directory</li> <li>Check Python path settings</li> <li>Verify the package is properly installed</li> </ul>"},{"location":"user_guide/cli/usage/#configuration-errors","title":"Configuration Errors","text":"<ul> <li>Check configuration file format</li> <li>Verify file paths are correct</li> <li>Ensure all required configuration parameters are present</li> </ul>"},{"location":"user_guide/cli/usage/#permission-errors","title":"Permission Errors","text":"<ul> <li>Check file and directory permissions</li> <li>Ensure you have write permissions for output directories</li> </ul>"},{"location":"user_guide/cli/usage/#getting-help","title":"Getting Help","text":"<pre><code># Show help for a specific command\ndnallm-train --help\n\n# Show help for configuration generator\ndnallm-model-config-generator --help\n\n# Show help for MCP server\ndnallm-mcp-server --help\n</code></pre>"},{"location":"user_guide/cli/usage/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Generator - Learn how to create configuration files</li> <li>MCP Server - Learn about the Model Context Protocol server</li> <li>Fine-tuning Tutorials - Learn to train models</li> <li>Benchmark Tutorials - Compare model performance</li> <li>Inference Tutorials - Run model inference</li> <li>CLI Troubleshooting - Common CLI issues and solutions</li> </ul>"},{"location":"user_guide/data_processing/data_augmentation/","title":"Data Augmentation for DNA Sequences","text":"<p>Data augmentation is a powerful technique to increase the diversity of your training data without collecting new samples. By applying realistic transformations to your existing sequences, you can help the model generalize better and prevent overfitting.</p>"},{"location":"user_guide/data_processing/data_augmentation/#1-why-augment-dna-sequences","title":"1. Why Augment DNA Sequences?","text":"<p>In biology, certain transformations result in a sequence that is functionally equivalent or very similar to the original.</p> <ul> <li>Biological Equivalence: The reverse complement of a DNA strand carries the same genetic information.</li> <li>Robustness to Noise: Small mutations or sequencing errors should not drastically change a model's prediction for robust tasks.</li> <li>Increased Data Size: Augmentation artificially expands your dataset, which is especially useful when you have limited labeled data.</li> </ul>"},{"location":"user_guide/data_processing/data_augmentation/#2-common-augmentation-methods","title":"2. Common Augmentation Methods","text":"<p>Here are some common methods for augmenting DNA sequences, which can be implemented with simple Python functions.</p>"},{"location":"user_guide/data_processing/data_augmentation/#reverse-complement","title":"Reverse Complement","text":"<p>This is the most common and biologically sound augmentation method. The model should learn that a sequence and its reverse complement are often functionally identical.</p>"},{"location":"user_guide/data_processing/data_augmentation/#how-to-operate","title":"How to Operate","text":"<p>The <code>dnallm.datahandling.data</code> module provides an efficient <code>reverse_complement</code> function.</p> <pre><code>from dnallm.datahandling.data import reverse_complement\n\n# Example\noriginal_seq = \"ATGC\"\naugmented_seq = reverse_complement(original_seq)\nprint(f\"Original:   {original_seq}\")\nprint(f\"Augmented:  {augmented_seq}\") # Output: GCAT\n</code></pre> <p>When training, you can randomly choose to replace a sequence with its reverse complement in each training batch.</p>"},{"location":"user_guide/data_processing/data_augmentation/#random-mutations","title":"Random Mutations","text":"<p>Introducing random point mutations (substitutions, insertions, or deletions) can make the model more robust to natural variations and sequencing errors.</p>"},{"location":"user_guide/data_processing/data_augmentation/#how-to-operate_1","title":"How to Operate","text":"<p>The <code>dnallm.datahandling.data</code> module includes a <code>random_mutation</code> function for this purpose.</p> <pre><code>from dnallm.datahandling.data import random_mutation\n\n# Example\noriginal_seq = \"GATTACAGATTACA\"\n# The function returns the mutated sequence and the number of mutations\naugmented_seq, num_mutations = random_mutation(original_seq, num_mutations=2)\n\nprint(f\"Original:   {original_seq}\")\nprint(f\"Augmented:  {augmented_seq}\") # e.g., GATCACAGATTACA\nprint(f\"Mutations:  {num_mutations}\")\n</code></pre> <p>Caution: Use a low <code>mutation_rate</code>. High rates can destroy the biological signal in the sequence, turning it into noise.</p>"},{"location":"user_guide/data_processing/data_augmentation/#next-steps","title":"Next Steps","text":"<ul> <li>Data Preparation - Learn about data collection and organization</li> <li>Format Conversion - Convert between different data formats</li> <li>Quality Control - Ensure data quality and consistency</li> <li>Data Processing Troubleshooting - Common data processing issues and solutions</li> </ul>"},{"location":"user_guide/data_processing/data_preparation/","title":"Data Preparation for DNALLM","text":"<p>The quality and structure of your training data are critical for the success of your DNA language model. This guide covers the types of data you can use, where to find it, and how to organize it for use with DNALLM.</p>"},{"location":"user_guide/data_processing/data_preparation/#1-types-of-training-data","title":"1. Types of Training Data","text":"<p>DNALLM can be trained on a wide variety of genomic data, depending on your task.</p> <ul> <li> <p>Raw DNA Sequences (FASTA): This is the most common data type, used for pre-training and many fine-tuning tasks. It consists of long strings of nucleotides (A, C, G, T, N).</p> <ul> <li>Example: Whole genomes, chromosomes, genes, or promoter regions.</li> </ul> </li> <li> <p>Genomic Regions with Labels (BED/CSV/JSON): For classification tasks, you need sequences associated with specific labels.</p> <ul> <li>Example: A list of promoter sequences labeled as 'active' or 'inactive'.</li> <li>Example: A set of enhancer regions labeled by their target tissue.</li> </ul> </li> <li> <p>Sequence-to-Sequence Data: For tasks that map one sequence to another.</p> <ul> <li>Example: Mapping a noisy sequence read to a corrected reference sequence.</li> </ul> </li> <li> <p>Paired Data (Sequence and Value): For regression tasks where you predict a continuous value from a sequence.</p> <ul> <li>Example: Predicting the expression level (a number) from a promoter sequence.</li> </ul> </li> </ul>"},{"location":"user_guide/data_processing/data_preparation/#2-data-collection-and-sourcing","title":"2. Data Collection and Sourcing","text":"<p>High-quality genomic data can be obtained from several public databases.</p> <ul> <li> <p>NCBI (National Center for Biotechnology Information): A primary source for raw genomes, gene annotations, and experimental data (SRA).</p> <ul> <li>Link: https://www.ncbi.nlm.nih.gov/</li> <li>Tools: Use <code>NCBI Datasets</code> or <code>Entrez Direct</code> command-line tools to download data in bulk.</li> </ul> </li> <li> <p>Ensembl/GENCODE: Provides comprehensive gene annotations and reference genomes for many species.</p> <ul> <li>Link: https://www.ensembl.org/</li> </ul> </li> <li> <p>UCSC Genome Browser: An excellent resource for downloading genomic regions (in BED format) and associated annotations.</p> <ul> <li>Link: https://genome.ucsc.edu/</li> </ul> </li> <li> <p>Hugging Face Hub: A growing number of pre-processed genomic datasets are available on the Hub, ready for use with <code>transformers</code>.</p> <ul> <li>Link: https://huggingface.co/datasets?sort=trending&amp;search=dna</li> </ul> </li> </ul>"},{"location":"user_guide/data_processing/data_preparation/#example-downloading-a-human-promoter-dataset","title":"Example: Downloading a Human Promoter Dataset","text":"<p>You can use the UCSC Table Browser to get a list of human promoter regions.</p> <ol> <li>Go to the UCSC Table Browser.</li> <li>Select the assembly (e.g., <code>hg38</code>).</li> <li>Choose the group <code>Genes and Gene Predictions</code> and the track <code>GENCODE V45</code>.</li> <li>Set the output format to <code>BED</code> and define the promoter region (e.g., 1000 bases upstream and 100 bases downstream of the transcription start site).</li> <li>Download the file. You can then use tools like <code>bedtools getfasta</code> to extract the corresponding DNA sequences.</li> </ol>"},{"location":"user_guide/data_processing/data_preparation/#3-data-organization","title":"3. Data Organization","text":"<p>For use with DNALLM's fine-tuning scripts, it's best to organize your data into a simple, clean format. A CSV or JSONL file is often the most convenient.</p> <p>For Classification: A CSV file with <code>sequence</code> and <code>label</code> columns is standard.</p> <pre><code>sequence,label\n\"GATTACAGATTACA...\",0\n\"CGCGCGCGCGCGCG...\",1\n\"AAATTTCCGGGAAA...\",0\n</code></pre> <p>For Pre-training: A text file where each line is a complete DNA sequence.</p> <pre><code>GATTACAGATTACAGATTACAGATTACAGATTACAGATTACA...\nCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCG...\n</code></pre> <p>See the Format Conversion guide for more details on how to structure your files.</p>"},{"location":"user_guide/data_processing/data_preparation/#next-steps","title":"Next Steps","text":"<ul> <li>Data Augmentation - Learn about data augmentation techniques</li> <li>Format Conversion - Convert between different data formats</li> <li>Quality Control - Ensure data quality and consistency</li> <li>Data Processing Troubleshooting - Common data processing issues and solutions</li> </ul>"},{"location":"user_guide/data_processing/format_conversion/","title":"Supported Data Formats and Conversion","text":"<p>The <code>DNADataset</code> class in DNALLM is highly flexible and can load data from a wide variety of formats. This guide covers the most common formats and provides examples for loading and converting them.</p>"},{"location":"user_guide/data_processing/format_conversion/#1-supported-data-formats","title":"1. Supported Data Formats","text":"<p>The <code>DNADataset.load_local_data()</code> method can handle:</p> <ul> <li>Tabular Files: <code>csv</code>, <code>tsv</code></li> <li>Structured Files: <code>json</code>, <code>jsonl</code></li> <li>High-Performance Formats: <code>arrow</code>, <code>parquet</code></li> <li>Raw Sequence Files: <code>fasta</code>, <code>txt</code></li> <li>In-Memory Objects: Python <code>dict</code> or <code>list</code> of dictionaries.</li> </ul> <p>For security and compatibility reasons, loading directly from <code>pickle</code> files is not supported, but you can easily convert them.</p>"},{"location":"user_guide/data_processing/format_conversion/#2-loading-standard-formats","title":"2. Loading Standard Formats","text":"<p>For most file-based formats, you can use the <code>DNADataset.load_local_data()</code> class method. The key is to specify the column names for your sequences and labels if they differ from the defaults (<code>sequence</code> and <code>label</code>).</p>"},{"location":"user_guide/data_processing/format_conversion/#csv-tsv","title":"CSV / TSV","text":"<pre><code>from dnallm.datahandling.data import DNADataset\n\n# Assuming 'my_data.csv' has columns 'dna_string' and 'target'\ndna_ds = DNADataset.load_local_data(\n    \"my_data.csv\",\n    seq_col=\"dna_string\",\n    label_col=\"target\"\n)\nprint(dna_ds)\n</code></pre> <p>JSONL Format: Create a file named <code>train.jsonl</code>. Each line is a JSON object.</p> <pre><code>// file: my_dataset/train.jsonl\n{\"sequence\": \"GATTACAGATTACAGATTACAGATTACA\", \"label\": 1}\n{\"sequence\": \"CGCGCGCGCGCGCGCGCGCGCGCGCGCG\", \"label\": 0}\n{\"sequence\": \"AAATTTCCGGGAAATTTCCGGGAAATTT\", \"label\": 1}\n</code></pre>"},{"location":"user_guide/data_processing/format_conversion/#for-pre-training","title":"For Pre-training","text":"<p>A simple text file where each line is a sequence is sufficient.</p> <pre><code># file: my_corpus.txt\nGATTACAGATTACAGATTACAGATTACAGATTACAGATTACAGATTACAGATTACAGATTACA...\nCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGC...\n</code></pre>"},{"location":"user_guide/data_processing/format_conversion/#3-conversion-example-fasta-to-csv","title":"3. Conversion Example: FASTA to CSV","text":"<p>Often, you will have your sequences in a FASTA file and your labels in a separate file. The <code>dnallm.datahandling.data</code> module provides a <code>fasta_to_df</code> utility to easily parse FASTA files into a pandas DataFrame, which you can then merge with your labels.</p> <p>Let's assume you have <code>sequences.fa</code> and <code>labels.csv</code> (with a <code>name</code> column matching the FASTA headers and a <code>label</code> column).</p> <pre><code>import pandas as pd\nfrom dnallm.datahandling.data import fasta_to_df\n\n# Example usage\nfasta_path = \"sequences.fa\"\nlabels_path = \"labels.csv\"\noutput_path = \"train_dataset.csv\"\n\n# 1. Load sequences from FASTA into a DataFrame\n# The 'name' column will contain the sequence headers from the FASTA file.\nseq_df = fasta_to_df(fasta_path) # Columns: 'name', 'sequence'\n\n# 2. Load labels and merge with sequences based on the name\nlabel_df = pd.read_csv(labels_path)\nmerged_df = pd.merge(seq_df, label_df, on=\"name\")\n\n# 3. Save the final dataset to a CSV file\nmerged_df[[\"sequence\", \"label\"]].to_csv(output_path, index=False)\nprint(\"Conversion complete!\")\n</code></pre>"},{"location":"user_guide/data_processing/format_conversion/#4-loading-other-formats-arrow-parquet-pickle","title":"4. Loading Other Formats (Arrow, Parquet, Pickle)","text":"<p>The DNALLM <code>DNADataset</code> class can directly load data from several high-performance formats like Apache Arrow and Parquet. This is often more efficient than using CSV, especially for large datasets.</p>"},{"location":"user_guide/data_processing/format_conversion/#loading-arrow-or-parquet-files","title":"Loading Arrow or Parquet Files","text":"<p>If your data is already in Arrow or Parquet format with <code>sequence</code> and <code>label</code> columns, you can load it directly.</p> <pre><code>from dnallm.datahandling.data import DNADataset\n\n# Load from a Parquet file\ndna_ds_from_parquet = DNADataset.load_local_data(\"my_dataset.parquet\")\n\n# Load from an Arrow file\ndna_ds_from_arrow = DNADataset.load_local_data(\"my_dataset.arrow\")\n\nprint(dna_ds_from_parquet)\n</code></pre>"},{"location":"user_guide/data_processing/format_conversion/#converting-from-pickle-to-a-supported-format","title":"Converting from Pickle to a Supported Format","text":"<p>While <code>DNADataset</code> doesn't load Pickle files directly for security and compatibility reasons, you can easily convert them using <code>pandas</code>.</p> <p>Let's say you have a <code>data.pkl</code> file containing a list of dictionaries or a pandas DataFrame.</p> <p><pre><code>import pandas as pd\n\n# 1. Load the data from the Pickle file\ndata = pd.read_pickle(\"my_dataset.pkl\")\n\n# 2. Convert to a pandas DataFrame if it's not already\ndf = pd.DataFrame(data)\n\n# 3. Save to a supported format like CSV or Parquet\ndf[[\"sequence\", \"label\"]].to_csv(\"converted_dataset.csv\", index=False)\n# Or for better performance:\n# df[[\"sequence\", \"label\"]].to_parquet(\"converted_dataset.parquet\")\n\n---\n\n## Next Steps\n\n- [Data Preparation](data_preparation.md) - Learn about data collection and organization\n- [Data Augmentation](data_augmentation.md) - Learn about data augmentation techniques\n- [Quality Control](quality_control.md) - Ensure data quality and consistency\n- [Data Processing Troubleshooting](../../faq/data_processing_troubleshooting.md) - Common data processing issues and solutions\n</code></pre> ```</p>"},{"location":"user_guide/data_processing/quality_control/","title":"Data Quality Control","text":"<p>\"Garbage in, garbage out.\" This saying is especially true for training deep learning models. Ensuring your DNA data is clean, consistent, and free of errors is a crucial step before training. This guide outlines common quality control checks.</p>"},{"location":"user_guide/data_processing/quality_control/#1-common-data-issues-and-solutions","title":"1. Common Data Issues and Solutions","text":""},{"location":"user_guide/data_processing/quality_control/#invalid-characters","title":"Invalid Characters","text":"<p>Problem: Your DNA sequences might contain characters other than <code>A</code>, <code>C</code>, <code>G</code>, <code>T</code>, and <code>N</code> (for unknown nucleotides). These can come from parsing errors or malformed source files. Most tokenizers will fail or produce incorrect tokens if they encounter unexpected characters like <code>U</code>, <code>R</code>, <code>Y</code>, or punctuation.</p> <p>Solution: Sanitize your sequences to ensure they only contain valid characters.</p> <p>The <code>dnallm.datahandling.data.clean_sequence</code> function is designed for this purpose.</p> <pre><code>from dnallm.datahandling.data import clean_sequence\n\n# Apply this function to every sequence in your dataset before saving.\n# For a DNADataset object:\n# dna_ds.validate_sequences(valid_chars=\"ACGTN\")\n</code></pre>"},{"location":"user_guide/data_processing/quality_control/#inconsistent-sequence-lengths","title":"Inconsistent Sequence Lengths","text":"<p>Problem: For some biological tasks, all sequences are expected to be the same length (e.g., classifying 150bp promoter regions). Drastic variations in length might indicate data collection errors.</p> <p>Solution: Analyze the distribution of sequence lengths in your dataset.</p> <p><pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"your_dataset.csv\")\nlengths = df[\"sequence\"].str.len()\n\nprint(lengths.describe())\n\n# Plot a histogram to visualize the distribution\nlengths.hist(bins=50)\nplt.title(\"Distribution of Sequence Lengths\")\nplt.xlabel(\"Length\")\nplt.ylabel(\"Frequency\")\nplt.show()\n</code></pre> Based on the distribution, you can decide whether to filter out sequences that are too short or too long, or to pad/truncate them during tokenization.</p>"},{"location":"user_guide/data_processing/quality_control/#label-imbalance","title":"Label Imbalance","text":"<p>Problem: In classification tasks, having a severe imbalance between classes (e.g., 99% negative examples and 1% positive examples) can cause the model to simply predict the majority class every time.</p> <p>Solution: -   Check Class Distribution: Use <code>df['label'].value_counts()</code> to see the number of samples per class. -   Resampling:     -   Oversampling: Randomly duplicate samples from the minority class.     -   Undersampling: Randomly remove samples from the majority class. -   Weighted Loss: During training, you can assign a higher weight to the minority class in the loss function. The DNALLM <code>finetune</code> command can handle this if class weights are provided.</p>"},{"location":"user_guide/data_processing/quality_control/#next-steps","title":"Next Steps","text":"<ul> <li>Data Preparation - Learn about data collection and organization</li> <li>Data Augmentation - Learn about data augmentation techniques</li> <li>Format Conversion - Convert between different data formats</li> <li>Data Processing Troubleshooting - Common data processing issues and solutions</li> </ul>"},{"location":"user_guide/fine_tuning/","title":"Fine-tuning DNA Language Models","text":"<p>This section provides comprehensive tutorials and guides for fine-tuning DNA language models using DNALLM. Fine-tuning allows you to adapt pre-trained models to your specific DNA analysis tasks and datasets.</p>"},{"location":"user_guide/fine_tuning/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Basic Fine-tuning: Get started with simple model adaptation</li> <li>Advanced Techniques: Custom loss functions, data augmentation, and optimization</li> <li>Task-Specific Guides: Classification, generation, and specialized tasks</li> <li>Best Practices: Hyperparameter tuning, monitoring, and deployment</li> </ul>"},{"location":"user_guide/fine_tuning/#quick-navigation","title":"Quick Navigation","text":"Topic Description Difficulty Getting Started Basic fine-tuning setup and configuration Beginner Task-Specific Guides Fine-tuning for different task types Intermediate Advanced Techniques Custom training, optimization, and monitoring Advanced Configuration Guide Detailed configuration options and examples Intermediate Examples and Use Cases Real-world fine-tuning scenarios All Levels Troubleshooting Common issues and solutions All Levels"},{"location":"user_guide/fine_tuning/#prerequisites","title":"Prerequisites","text":"<p>Before diving into fine-tuning, ensure you have:</p> <ul> <li>\u2705 DNALLM installed and configured</li> <li>\u2705 Access to pre-trained DNA language models</li> <li>\u2705 Training datasets in appropriate formats</li> <li>\u2705 Sufficient computational resources (GPU recommended)</li> <li>\u2705 Understanding of your target task and data</li> </ul>"},{"location":"user_guide/fine_tuning/#quick-start","title":"Quick Start","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n\n# Load configuration\nconfig = load_config(\"finetune_config.yaml\")\n\n# Load pre-trained model and tokenizer\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load and prepare dataset\ndataset = DNADataset.load_local_data(\n    \"path/to/your/data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Initialize trainer and start fine-tuning\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    config=config\n)\n\ntrainer.train()\n</code></pre>"},{"location":"user_guide/fine_tuning/#supported-task-types","title":"Supported Task Types","text":"Task Type Description Use Cases Classification Binary, multi-class, and multi-label classification Promoter prediction, motif detection, functional annotation Generation Sequence generation and completion DNA synthesis, sequence design, mutation analysis Masked Language Modeling Sequence completion and prediction Sequence analysis, mutation prediction Token Classification Named entity recognition and tagging Gene identification, regulatory element detection Regression Continuous value prediction Expression level prediction, binding affinity"},{"location":"user_guide/fine_tuning/#key-features","title":"Key Features","text":"<ul> <li>Flexible Architecture: Support for various model architectures (BERT, GPT, Transformer variants)</li> <li>Task-Specific Heads: Automatic head selection based on task type</li> <li>Data Processing: Built-in DNA sequence preprocessing and augmentation</li> <li>Training Optimization: Mixed precision, gradient accumulation, and scheduling</li> <li>Monitoring: TensorBoard integration and comprehensive logging</li> <li>Checkpointing: Automatic model saving and resumption</li> </ul>"},{"location":"user_guide/fine_tuning/#model-sources","title":"Model Sources","text":"<ul> <li>Hugging Face Hub: Access to thousands of pre-trained models</li> <li>ModelScope: Alternative model repository with specialized models</li> <li>Local Models: Use your own pre-trained models</li> <li>Custom Architectures: Implement and fine-tune custom model designs</li> </ul>"},{"location":"user_guide/fine_tuning/#next-steps","title":"Next Steps","text":"<p>Choose your path:</p> <ul> <li>New to fine-tuning? Start with Getting Started</li> <li>Want task-specific guidance? Check Task-Specific Guides</li> <li>Need advanced features? Explore Advanced Techniques</li> <li>Looking for examples? See Examples and Use Cases</li> </ul> <p>Need Help? Check our FAQ or open an issue on GitHub.</p>"},{"location":"user_guide/fine_tuning/advanced_techniques/","title":"Advanced Fine-tuning Techniques","text":"<p>This guide covers advanced fine-tuning techniques including custom training strategies, optimization methods, monitoring, and deployment considerations.</p>"},{"location":"user_guide/fine_tuning/advanced_techniques/#overview","title":"Overview","text":"<p>Advanced fine-tuning techniques help you: - Implement custom training loops and loss functions - Optimize training performance and memory usage - Monitor and debug training progress effectively - Deploy fine-tuned models in production environments</p>"},{"location":"user_guide/fine_tuning/advanced_techniques/#custom-training-strategies","title":"Custom Training Strategies","text":""},{"location":"user_guide/fine_tuning/advanced_techniques/#custom-loss-functions","title":"Custom Loss Functions","text":"<p>DNALLM allows you to implement custom loss functions for specific use cases.</p>"},{"location":"user_guide/fine_tuning/advanced_techniques/#weighted-loss-for-imbalanced-data","title":"Weighted Loss for Imbalanced Data","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass WeightedCrossEntropyLoss(nn.Module):\n    \"\"\"Weighted cross-entropy loss for imbalanced datasets.\"\"\"\n\n    def __init__(self, class_weights=None, label_smoothing=0.0):\n        super().__init__()\n        self.class_weights = class_weights\n        self.label_smoothing = label_smoothing\n\n    def forward(self, logits, targets):\n        if self.class_weights is not None:\n            loss = F.cross_entropy(\n                logits, \n                targets, \n                weight=self.class_weights,\n                label_smoothing=self.label_smoothing\n            )\n        else:\n            loss = F.cross_entropy(logits, targets, label_smoothing=self.label_smoothing)\n\n        return loss\n\n# Usage in trainer\nclass CustomDNATrainer(DNATrainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # Calculate class weights\n        labels = [item['label'] for item in self.train_dataset]\n        class_counts = torch.bincount(torch.tensor(labels))\n        class_weights = 1.0 / class_counts\n        class_weights = class_weights / class_weights.sum()\n\n        # Set custom loss\n        self.criterion = WeightedCrossEntropyLoss(class_weights=class_weights)\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        outputs = model(**inputs)\n        logits = outputs.logits\n        labels = inputs[\"labels\"]\n\n        loss = self.criterion(logits, labels)\n\n        if return_outputs:\n            return loss, outputs\n        return loss\n</code></pre>"},{"location":"user_guide/fine_tuning/advanced_techniques/#focal-loss-for-hard-examples","title":"Focal Loss for Hard Examples","text":"<pre><code>class FocalLoss(nn.Module):\n    \"\"\"Focal loss for handling hard examples.\"\"\"\n\n    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, logits, targets):\n        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\n# Usage\ntrainer = CustomDNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\ntrainer.criterion = FocalLoss(alpha=1, gamma=2)\n</code></pre>"},{"location":"user_guide/fine_tuning/advanced_techniques/#custom-training-loops","title":"Custom Training Loops","text":"<p>Implement custom training loops for advanced control over the training process.</p>"},{"location":"user_guide/fine_tuning/advanced_techniques/#custom-training-with-gradient-accumulation","title":"Custom Training with Gradient Accumulation","text":"<pre><code>class CustomTrainer:\n    \"\"\"Custom trainer with advanced features.\"\"\"\n\n    def __init__(self, model, tokenizer, train_dataset, config):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.train_dataset = train_dataset\n        self.config = config\n\n        # Setup optimizer and scheduler\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=config.finetune.learning_rate,\n            weight_decay=config.finetune.weight_decay\n        )\n\n        self.scheduler = self._get_scheduler()\n        self.scaler = torch.cuda.amp.GradScaler() if config.finetune.bf16 else None\n\n    def _get_scheduler(self):\n        \"\"\"Get learning rate scheduler.\"\"\"\n        num_training_steps = len(self.train_dataset) // self.config.finetune.per_device_train_batch_size\n        num_warmup_steps = int(num_training_steps * self.config.finetune.warmup_ratio)\n\n        return torch.optim.lr_scheduler.get_scheduler(\n            name=self.config.finetune.lr_scheduler_type,\n            optimizer=self.optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n\n    def train_epoch(self, epoch):\n        \"\"\"Train for one epoch.\"\"\"\n        self.model.train()\n        total_loss = 0\n\n        # Create data loader\n        dataloader = torch.utils.data.DataLoader(\n            self.train_dataset,\n            batch_size=self.config.finetune.per_device_train_batch_size,\n            shuffle=True,\n            collate_fn=self._collate_fn\n        )\n\n        for step, batch in enumerate(dataloader):\n            # Move to device\n            batch = {k: v.to(self.model.device) for k, v in batch.items()}\n\n            # Forward pass with mixed precision\n            if self.scaler:\n                with torch.cuda.amp.autocast():\n                    outputs = self.model(**batch)\n                    loss = outputs.loss\n            else:\n                outputs = self.model(**batch)\n                loss = outputs.loss\n\n            # Scale loss for gradient accumulation\n            loss = loss / self.config.finetune.gradient_accumulation_steps\n\n            # Backward pass\n            if self.scaler:\n                self.scaler.scale(loss).backward()\n            else:\n                loss.backward()\n\n            # Gradient accumulation\n            if (step + 1) % self.config.finetune.gradient_accumulation_steps == 0:\n                if self.scaler:\n                    self.scaler.unscale_(self.optimizer)\n                    torch.nn.utils.clip_grad_norm_(\n                        self.model.parameters(), \n                        self.config.finetune.max_grad_norm\n                    )\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n                else:\n                    torch.nn.utils.clip_grad_norm_(\n                        self.model.parameters(), \n                        self.config.finetune.max_grad_norm\n                    )\n                    self.optimizer.step()\n\n                self.scheduler.step()\n                self.optimizer.zero_grad()\n\n            total_loss += loss.item()\n\n            # Logging\n            if step % self.config.finetune.logging_steps == 0:\n                print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item():.4f}\")\n\n        return total_loss / len(dataloader)\n\n    def train(self, num_epochs):\n        \"\"\"Main training loop.\"\"\"\n        for epoch in range(num_epochs):\n            print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n            loss = self.train_epoch(epoch)\n            print(f\"Epoch {epoch + 1} completed. Average loss: {loss:.4f}\")\n\n# Usage\ncustom_trainer = CustomTrainer(model, tokenizer, dataset.train_data, config)\ncustom_trainer.train(num_epochs=config.finetune.num_train_epochs)\n</code></pre>"},{"location":"user_guide/fine_tuning/advanced_techniques/#advanced-optimization-techniques","title":"Advanced Optimization Techniques","text":""},{"location":"user_guide/fine_tuning/advanced_techniques/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>Implement advanced learning rate scheduling strategies.</p>"},{"location":"user_guide/fine_tuning/advanced_techniques/#cosine-annealing-with-warm-restarts","title":"Cosine Annealing with Warm Restarts","text":"<pre><code>class CosineAnnealingWarmRestarts:\n    \"\"\"Cosine annealing with warm restarts.\"\"\"\n\n    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0):\n        self.optimizer = optimizer\n        self.T_0 = T_0\n        self.T_mult = T_mult\n        self.eta_min = eta_min\n        self.T_cur = 0\n        self.base_lr = optimizer.param_groups[0]['lr']\n\n    def step(self):\n        if self.T_cur &gt;= self.T_0:\n            self.T_0 *= self.T_mult\n            self.T_cur = 0\n\n        lr = self.eta_min + (self.base_lr - self.eta_min) * \\\n             (1 + math.cos(math.pi * self.T_cur / self.T_0)) / 2\n\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n\n        self.T_cur += 1\n\n# Usage\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1000, T_mult=2)\n</code></pre>"},{"location":"user_guide/fine_tuning/advanced_techniques/#one-cycle-policy","title":"One Cycle Policy","text":"<pre><code>class OneCycleScheduler:\n    \"\"\"One cycle learning rate scheduler.\"\"\"\n\n    def __init__(self, optimizer, max_lr, total_steps, pct_start=0.3):\n        self.optimizer = optimizer\n        self.max_lr = max_lr\n        self.total_steps = total_steps\n        self.pct_start = pct_start\n        self.step_count = 0\n\n        # Calculate step counts\n        self.warmup_steps = int(total_steps * pct_start)\n        self.decay_steps = total_steps - self.warmup_steps\n\n    def step(self):\n        if self.step_count &lt; self.warmup_steps:\n            # Warmup phase\n            lr = self.max_lr * (self.step_count / self.warmup_steps)\n        else:\n            # Decay phase\n            decay_step = self.step_count - self.warmup_steps\n            lr = self.max_lr * (1 - decay_step / self.decay_steps)\n\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n\n        self.step_count += 1\n\n# Usage\nscheduler = OneCycleScheduler(optimizer, max_lr=1e-3, total_steps=10000)\n</code></pre>"},{"location":"user_guide/fine_tuning/advanced_techniques/#memory-optimization-techniques","title":"Memory Optimization Techniques","text":""},{"location":"user_guide/fine_tuning/advanced_techniques/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<p>Implement gradient checkpointing to reduce memory usage.</p> <pre><code># Enable gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# Or in configuration\nfinetune:\n  gradient_checkpointing: true\n</code></pre>"},{"location":"user_guide/fine_tuning/advanced_techniques/#memory-efficient-attention","title":"Memory Efficient Attention","text":"<p>Use memory-efficient attention mechanisms.</p> <pre><code># Enable memory efficient attention\nfrom transformers import AutoConfig\n\nconfig = AutoConfig.from_pretrained(\"zhangtaolab/plant-dnabert-BPE\")\nconfig.use_memory_efficient_attention = True\n\nmodel = AutoModel.from_pretrained(\"zhangtaolab/plant-dnabert-BPE\", config=config)\n</code></pre>"},{"location":"user_guide/fine_tuning/advanced_techniques/#dynamic-batching","title":"Dynamic Batching","text":"<p>Implement dynamic batching for variable-length sequences.</p> <pre><code>class DynamicBatchSampler:\n    \"\"\"Dynamic batch sampler for variable-length sequences.\"\"\"\n\n    def __init__(self, dataset, max_tokens_per_batch=4096, max_batch_size=32):\n        self.dataset = dataset\n        self.max_tokens_per_batch = max_tokens_per_batch\n        self.max_batch_size = max_batch_size\n\n        # Sort by length for efficient batching\n        self.lengths = [len(item['sequence']) for item in dataset]\n        self.indices = sorted(range(len(dataset)), key=lambda i: self.lengths[i])\n\n    def __iter__(self):\n        batch = []\n        current_tokens = 0\n\n        for idx in self.indices:\n            sequence_length = self.lengths[idx]\n\n            # Check if adding this sample would exceed limits\n            if (len(batch) &gt;= self.max_batch_size or \n                current_tokens + sequence_length &gt; self.max_tokens_per_batch):\n                if batch:\n                    yield batch\n                    batch = []\n                    current_tokens = 0\n\n            batch.append(idx)\n            current_tokens += sequence_length\n\n        if batch:\n            yield batch\n\n# Usage\nsampler = DynamicBatchSampler(dataset.train_data, max_tokens_per_batch=4096)\ndataloader = torch.utils.data.DataLoader(\n    dataset.train_data,\n    batch_sampler=sampler,\n    collate_fn=custom_collate_fn\n)\n</code></pre>"},{"location":"user_guide/fine_tuning/advanced_techniques/#advanced-monitoring-and-debugging","title":"Advanced Monitoring and Debugging","text":""},{"location":"user_guide/fine_tuning/advanced_techniques/#custom-callbacks","title":"Custom Callbacks","text":"<p>Implement custom callbacks for advanced monitoring.</p> <pre><code>class CustomCallback:\n    \"\"\"Custom callback for advanced monitoring.\"\"\"\n\n    def __init__(self, model, tokenizer, eval_dataset):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.eval_dataset = eval_dataset\n        self.best_metric = float('inf')\n        self.patience_counter = 0\n\n    def on_step_end(self, step, logs=None):\n        \"\"\"Called at the end of each step.\"\"\"\n        if step % 100 == 0:\n            # Log learning rate\n            lr = self.model.optimizer.param_groups[0]['lr']\n            print(f\"Step {step}, Learning Rate: {lr:.2e}\")\n\n            # Log gradient norm\n            total_norm = 0\n            for p in self.model.parameters():\n                if p.grad is not None:\n                    param_norm = p.grad.data.norm(2)\n                    total_norm += param_norm.item() ** 2\n            total_norm = total_norm ** (1. / 2)\n            print(f\"Step {step}, Gradient Norm: {total_norm:.4f}\")\n\n    def on_epoch_end(self, epoch, logs=None):\n        \"\"\"Called at the end of each epoch.\"\"\"\n        # Evaluate on validation set\n        self.model.eval()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for batch in self.eval_dataset:\n                outputs = self.model(**batch)\n                loss = outputs.loss\n                total_loss += loss.item()\n\n                # Calculate accuracy\n                predictions = outputs.logits.argmax(-1)\n                correct += (predictions == batch['labels']).sum().item()\n                total += batch['labels'].size(0)\n\n        avg_loss = total_loss / len(self.eval_dataset)\n        accuracy = correct / total\n\n        print(f\"Epoch {epoch}, Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n        # Early stopping logic\n        if avg_loss &lt; self.best_metric:\n            self.best_metric = avg_loss\n            self.patience_counter = 0\n            # Save best model\n            self.model.save_pretrained(f\"best_model_epoch_{epoch}\")\n        else:\n            self.patience_counter += 1\n            if self.patience_counter &gt;= 3:\n                print(\"Early stopping triggered!\")\n                return True\n\n        return False\n\n# Usage\ncallback = CustomCallback(model, tokenizer, dataset.val_data)\n</code></pre>"},{"location":"user_guide/fine_tuning/advanced_techniques/#advanced-logging","title":"Advanced Logging","text":"<p>Implement comprehensive logging for debugging.</p> <pre><code>import logging\nimport json\nfrom datetime import datetime\n\nclass AdvancedLogger:\n    \"\"\"Advanced logging for fine-tuning experiments.\"\"\"\n\n    def __init__(self, log_dir, experiment_name):\n        self.log_dir = log_dir\n        self.experiment_name = experiment_name\n        self.setup_logging()\n\n        # Track metrics\n        self.metrics = {\n            'train_loss': [],\n            'val_loss': [],\n            'learning_rate': [],\n            'gradient_norm': [],\n            'memory_usage': []\n        }\n\n    def setup_logging(self):\n        \"\"\"Setup logging configuration.\"\"\"\n        log_file = f\"{self.log_dir}/{self.experiment_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.FileHandler(log_file),\n                logging.StreamHandler()\n            ]\n        )\n\n        self.logger = logging.getLogger(__name__)\n\n    def log_metrics(self, step, metrics):\n        \"\"\"Log training metrics.\"\"\"\n        for key, value in metrics.items():\n            if key in self.metrics:\n                self.metrics[key].append(value)\n\n        # Log to file\n        self.logger.info(f\"Step {step}: {json.dumps(metrics, indent=2)}\")\n\n        # Save metrics to JSON\n        with open(f\"{self.log_dir}/metrics.json\", 'w') as f:\n            json.dump(self.metrics, f, indent=2)\n\n    def log_model_info(self, model):\n        \"\"\"Log model architecture information.\"\"\"\n        total_params = sum(p.numel() for p in model.parameters())\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n        self.logger.info(f\"Total parameters: {total_params:,}\")\n        self.logger.info(f\"Trainable parameters: {trainable_params:,}\")\n        self.logger.info(f\"Model size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n\n# Usage\nlogger = AdvancedLogger(\"./logs\", \"promoter_classification\")\nlogger.log_model_info(model)\n</code></pre>"},{"location":"user_guide/fine_tuning/advanced_techniques/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":""},{"location":"user_guide/fine_tuning/advanced_techniques/#grid-search","title":"Grid Search","text":"<p>Implement grid search for hyperparameter optimization.</p> <pre><code>def grid_search_hyperparameters():\n    \"\"\"Grid search for hyperparameter optimization.\"\"\"\n\n    # Define hyperparameter grid\n    hyperparams = {\n        'learning_rate': [1e-5, 2e-5, 5e-5, 1e-4],\n        'batch_size': [8, 16, 32],\n        'weight_decay': [0.01, 0.05, 0.1],\n        'warmup_ratio': [0.1, 0.2, 0.3]\n    }\n\n    best_config = None\n    best_score = float('inf')\n\n    # Generate all combinations\n    from itertools import product\n    keys = hyperparams.keys()\n    values = hyperparams.values()\n\n    for combination in product(*values):\n        config_dict = dict(zip(keys, combination))\n\n        # Update configuration\n        config.finetune.learning_rate = config_dict['learning_rate']\n        config.finetune.per_device_train_batch_size = config_dict['batch_size']\n        config.finetune.weight_decay = config_dict['weight_decay']\n        config.finetune.warmup_ratio = config_dict['warmup_ratio']\n\n        # Train and evaluate\n        score = train_and_evaluate(config)\n\n        print(f\"Config: {config_dict}, Score: {score}\")\n\n        if score &lt; best_score:\n            best_score = score\n            best_config = config_dict.copy()\n\n    print(f\"Best config: {best_config}\")\n    print(f\"Best score: {best_score}\")\n\n    return best_config\n\ndef train_and_evaluate(config):\n    \"\"\"Train model and return validation score.\"\"\"\n    # Implementation of training and evaluation\n    # Return validation loss or other metric\n    pass\n</code></pre>"},{"location":"user_guide/fine_tuning/advanced_techniques/#bayesian-optimization","title":"Bayesian Optimization","text":"<p>Use Bayesian optimization for more efficient hyperparameter search.</p> <pre><code>from skopt import gp_minimize\nfrom skopt.space import Real, Integer\n\ndef objective(params):\n    \"\"\"Objective function for Bayesian optimization.\"\"\"\n    lr, batch_size, weight_decay, warmup_ratio = params\n\n    # Update configuration\n    config.finetune.learning_rate = lr\n    config.finetune.per_device_train_batch_size = int(batch_size)\n    config.finetune.weight_decay = weight_decay\n    config.finetune.warmup_ratio = warmup_ratio\n\n    # Train and evaluate\n    score = train_and_evaluate(config)\n    return score\n\ndef bayesian_optimization():\n    \"\"\"Bayesian optimization for hyperparameters.\"\"\"\n\n    # Define search space\n    space = [\n        Real(1e-5, 1e-4, name='learning_rate', prior='log-uniform'),\n        Integer(4, 64, name='batch_size'),\n        Real(0.001, 0.1, name='weight_decay'),\n        Real(0.05, 0.5, name='warmup_ratio')\n    ]\n\n    # Run optimization\n    result = gp_minimize(\n        objective,\n        space,\n        n_calls=20,\n        random_state=42,\n        n_initial_points=5\n    )\n\n    print(f\"Best parameters: {result.x}\")\n    print(f\"Best score: {result.fun}\")\n\n    return result.x\n</code></pre>"},{"location":"user_guide/fine_tuning/advanced_techniques/#next-steps","title":"Next Steps","text":"<p>After mastering these advanced techniques:</p> <ol> <li>Explore Real-world Examples: See Examples and Use Cases</li> <li>Configuration Options: Check detailed configuration options</li> <li>Troubleshooting: Visit common issues and solutions</li> <li>Deployment: Learn about model deployment and serving</li> </ol> <p>Ready for real-world examples? Continue to Examples and Use Cases to see these advanced techniques in action.</p>"},{"location":"user_guide/fine_tuning/configuration/","title":"Configuration Guide","text":"<p>This guide provides detailed information about all configuration options available for DNALLM fine-tuning, including examples and best practices.</p>"},{"location":"user_guide/fine_tuning/configuration/#overview","title":"Overview","text":"<p>DNALLM fine-tuning configuration is defined in YAML format and supports: - Task Configuration: Task type, labels, and thresholds - Training Configuration: Learning rates, batch sizes, and optimization - Model Configuration: Architecture, tokenizer, and source settings - Advanced Options: Custom training, monitoring, and deployment</p>"},{"location":"user_guide/fine_tuning/configuration/#configuration-structure","title":"Configuration Structure","text":""},{"location":"user_guide/fine_tuning/configuration/#basic-configuration-schema","title":"Basic Configuration Schema","text":"<pre><code># finetune_config.yaml\ntask:\n  # Task-specific settings\n  task_type: \"string\"\n  num_labels: integer\n  label_names: []\n  threshold: float\n\nfinetune:\n  # Training parameters\n  output_dir: \"string\"\n  num_train_epochs: integer\n  per_device_train_batch_size: integer\n  learning_rate: float\n\n  # Optimization settings\n  weight_decay: float\n  warmup_ratio: float\n  gradient_accumulation_steps: integer\n\n  # Monitoring and saving\n  logging_strategy: \"string\"\n  eval_strategy: \"string\"\n  save_strategy: \"string\"\n\n  # Advanced training options\n  bf16: boolean\n  fp16: boolean\n  load_best_model_at_end: boolean\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#task-configuration","title":"Task Configuration","text":""},{"location":"user_guide/fine_tuning/configuration/#basic-task-settings","title":"Basic Task Settings","text":"<pre><code>task:\n  task_type: \"binary\"           # Required: task type\n  num_labels: 2                 # Required: number of output classes\n  label_names: [\"neg\", \"pos\"]   # Optional: human-readable labels\n  threshold: 0.5                # Optional: classification threshold\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#task-types-and-requirements","title":"Task Types and Requirements","text":"Task Type Required Fields Optional Fields Description <code>binary</code> <code>num_labels: 2</code> <code>label_names</code>, <code>threshold</code> Binary classification <code>multiclass</code> <code>num_labels: &gt;2</code> <code>label_names</code> Multi-class classification <code>multilabel</code> <code>num_labels: &gt;1</code> <code>label_names</code>, <code>threshold</code> Multi-label classification <code>regression</code> <code>num_labels: 1</code> None Continuous value prediction <code>generation</code> None None Sequence generation <code>mask</code> None None Masked language modeling <code>token</code> <code>num_labels: &gt;1</code> <code>label_names</code> Token classification"},{"location":"user_guide/fine_tuning/configuration/#task-configuration-examples","title":"Task Configuration Examples","text":""},{"location":"user_guide/fine_tuning/configuration/#binary-classification","title":"Binary Classification","text":"<pre><code>task:\n  task_type: \"binary\"\n  num_labels: 2\n  label_names: [\"negative\", \"positive\"]\n  threshold: 0.5\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#multi-class-classification","title":"Multi-class Classification","text":"<pre><code>task:\n  task_type: \"multiclass\"\n  num_labels: 4\n  label_names: [\"enzyme\", \"receptor\", \"structural\", \"regulatory\"]\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#multi-label-classification","title":"Multi-label Classification","text":"<pre><code>task:\n  task_type: \"multilabel\"\n  num_labels: 5\n  label_names: [\"promoter\", \"enhancer\", \"silencer\", \"insulator\", \"locus_control\"]\n  threshold: 0.5\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#regression","title":"Regression","text":"<pre><code>task:\n  task_type: \"regression\"\n  num_labels: 1\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#generation","title":"Generation","text":"<pre><code>task:\n  task_type: \"generation\"\n  # No additional fields needed\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#training-configuration","title":"Training Configuration","text":""},{"location":"user_guide/fine_tuning/configuration/#basic-training-settings","title":"Basic Training Settings","text":"<pre><code>finetune:\n  # Output and logging\n  output_dir: \"./outputs\"\n  report_to: \"tensorboard\"\n\n  # Training duration\n  num_train_epochs: 3\n  max_steps: -1  # -1 means use epochs\n\n  # Batch sizes\n  per_device_train_batch_size: 8\n  per_device_eval_batch_size: 16\n  gradient_accumulation_steps: 1\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#optimization-settings","title":"Optimization Settings","text":"<pre><code>finetune:\n  # Learning rate and scheduling\n  learning_rate: 2e-5\n  lr_scheduler_type: \"linear\"  # linear, cosine, cosine_with_restarts, polynomial\n  warmup_ratio: 0.1\n  warmup_steps: 0  # Alternative to warmup_ratio\n\n  # Optimizer settings\n  weight_decay: 0.01\n  adam_beta1: 0.9\n  adam_beta2: 0.999\n  adam_epsilon: 1e-8\n\n  # Gradient handling\n  max_grad_norm: 1.0\n  gradient_accumulation_steps: 1\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":""},{"location":"user_guide/fine_tuning/configuration/#linear-scheduler","title":"Linear Scheduler","text":"<pre><code>finetune:\n  lr_scheduler_type: \"linear\"\n  warmup_ratio: 0.1\n  # Learning rate decreases linearly from warmup to 0\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#cosine-scheduler","title":"Cosine Scheduler","text":"<pre><code>finetune:\n  lr_scheduler_type: \"cosine\"\n  warmup_ratio: 0.1\n  # Learning rate follows cosine curve\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#cosine-with-restarts","title":"Cosine with Restarts","text":"<pre><code>finetune:\n  lr_scheduler_type: \"cosine_with_restarts\"\n  warmup_ratio: 0.1\n  num_train_epochs: 6\n  # Learning rate restarts every 2 epochs\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#polynomial-scheduler","title":"Polynomial Scheduler","text":"<pre><code>finetune:\n  lr_scheduler_type: \"polynomial\"\n  warmup_ratio: 0.1\n  power: 1.0  # Polynomial power\n  # Learning rate decreases polynomially\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#monitoring-and-evaluation","title":"Monitoring and Evaluation","text":"<pre><code>finetune:\n  # Logging\n  logging_strategy: \"steps\"  # steps, epoch, no\n  logging_steps: 100\n  logging_first_step: true\n\n  # Evaluation\n  eval_strategy: \"steps\"  # steps, epoch, no\n  eval_steps: 100\n  eval_delay: 0\n\n  # Saving\n  save_strategy: \"steps\"  # steps, epoch, no\n  save_steps: 500\n  save_total_limit: 3\n  save_safetensors: true\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#model-selection-and-checkpointing","title":"Model Selection and Checkpointing","text":"<pre><code>finetune:\n  # Model selection\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_loss\"  # or \"eval_accuracy\", \"eval_f1\"\n  greater_is_better: false  # false for loss, true for accuracy/f1\n\n  # Checkpointing\n  save_total_limit: 3\n  save_safetensors: true\n  resume_from_checkpoint: null  # Path to resume from\n\n  # Early stopping\n  early_stopping_patience: 3\n  early_stopping_threshold: 0.001\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#advanced-training-options","title":"Advanced Training Options","text":""},{"location":"user_guide/fine_tuning/configuration/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>finetune:\n  # Mixed precision options\n  fp16: false\n  bf16: false\n\n  # FP16 specific settings\n  fp16_full_eval: false\n  fp16_eval: false\n\n  # BF16 specific settings\n  bf16_full_eval: false\n  bf16_eval: false\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#memory-optimization","title":"Memory Optimization","text":"<pre><code>finetune:\n  # Memory optimization\n  dataloader_pin_memory: true\n  dataloader_num_workers: 4\n\n  # Gradient checkpointing\n  gradient_checkpointing: false\n\n  # Memory efficient attention\n  memory_efficient_attention: false\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#reproducibility","title":"Reproducibility","text":"<pre><code>finetune:\n  # Reproducibility\n  seed: 42\n  deterministic: true\n\n  # Data loading\n  dataloader_drop_last: false\n  remove_unused_columns: true\n\n  # Training\n  group_by_length: false\n  length_column_name: \"length\"\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#model-configuration","title":"Model Configuration","text":""},{"location":"user_guide/fine_tuning/configuration/#model-loading","title":"Model Loading","text":"<pre><code>model:\n  # Model source\n  source: \"huggingface\"  # huggingface, modelscope, local\n\n  # Model path\n  path: \"zhangtaolab/plant-dnabert-BPE\"\n\n  # Model options\n  revision: \"main\"\n  trust_remote_code: true\n  torch_dtype: \"float32\"  # float32, float16, bfloat16\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#tokenizer-configuration","title":"Tokenizer Configuration","text":"<pre><code>tokenizer:\n  # Tokenizer options\n  use_fast: true\n  model_max_length: 512\n\n  # Special tokens\n  pad_token: \"[PAD]\"\n  unk_token: \"[UNK]\"\n  mask_token: \"[MASK]\"\n  sep_token: \"[SEP]\"\n  cls_token: \"[CLS]\"\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#data-configuration","title":"Data Configuration","text":""},{"location":"user_guide/fine_tuning/configuration/#dataset-settings","title":"Dataset Settings","text":"<pre><code>dataset:\n  # Data loading\n  max_length: 512\n  truncation: true\n  padding: \"max_length\"\n\n  # Data splitting\n  test_size: 0.2\n  val_size: 0.1\n  random_state: 42\n\n  # Data augmentation\n  augment: true\n  reverse_complement_ratio: 0.5\n  random_mutation_ratio: 0.1\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#data-preprocessing","title":"Data Preprocessing","text":"<pre><code>dataset:\n  preprocessing:\n    # Sequence processing\n    remove_n_bases: true\n    normalize_case: true\n    add_padding: true\n    padding_size: 512\n\n    # Quality filtering\n    min_length: 100\n    max_length: 1000\n    valid_chars: \"ACGT\"\n\n    # Data augmentation\n    reverse_complement: true\n    random_mutations: true\n    mutation_rate: 0.01\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#complete-configuration-examples","title":"Complete Configuration Examples","text":""},{"location":"user_guide/fine_tuning/configuration/#binary-classification-example","title":"Binary Classification Example","text":"<pre><code>task:\n  task_type: \"binary\"\n  num_labels: 2\n  label_names: [\"negative\", \"positive\"]\n  threshold: 0.5\n\nfinetune:\n  output_dir: \"./promoter_classification\"\n  num_train_epochs: 5\n  per_device_train_batch_size: 16\n  per_device_eval_batch_size: 32\n  gradient_accumulation_steps: 1\n\n  # Optimization\n  learning_rate: 2e-5\n  weight_decay: 0.01\n  warmup_ratio: 0.1\n  lr_scheduler_type: \"linear\"\n\n  # Monitoring\n  logging_strategy: \"steps\"\n  logging_steps: 100\n  eval_strategy: \"steps\"\n  eval_steps: 100\n  save_strategy: \"steps\"\n  save_steps: 500\n  save_total_limit: 3\n\n  # Model selection\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_f1\"\n  greater_is_better: true\n\n  # Mixed precision\n  bf16: true\n\n  # Reproducibility\n  seed: 42\n  deterministic: true\n\n  # Reporting\n  report_to: \"tensorboard\"\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#multi-class-classification-example","title":"Multi-class Classification Example","text":"<pre><code>task:\n  task_type: \"multiclass\"\n  num_labels: 4\n  label_names: [\"enzyme\", \"receptor\", \"structural\", \"regulatory\"]\n\nfinetune:\n  output_dir: \"./functional_annotation\"\n  num_train_epochs: 8\n  per_device_train_batch_size: 16\n  per_device_eval_batch_size: 32\n  gradient_accumulation_steps: 2\n\n  # Higher learning rate for multi-class\n  learning_rate: 3e-5\n  weight_decay: 0.02\n  warmup_ratio: 0.15\n  lr_scheduler_type: \"cosine\"\n\n  # Monitoring\n  logging_strategy: \"steps\"\n  logging_steps: 200\n  eval_strategy: \"steps\"\n  eval_steps: 200\n  save_strategy: \"steps\"\n  save_steps: 1000\n  save_total_limit: 5\n\n  # Model selection\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_accuracy\"\n  greater_is_better: true\n\n  # Mixed precision\n  fp16: true\n\n  # Reproducibility\n  seed: 42\n  deterministic: true\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#generation-task-example","title":"Generation Task Example","text":"<pre><code>task:\n  task_type: \"generation\"\n\nfinetune:\n  output_dir: \"./sequence_generation\"\n  num_train_epochs: 15\n  per_device_train_batch_size: 8\n  per_device_eval_batch_size: 16\n  gradient_accumulation_steps: 2\n\n  # Higher learning rate for generation\n  learning_rate: 5e-5\n  weight_decay: 0.01\n  warmup_ratio: 0.2\n  lr_scheduler_type: \"cosine_with_restarts\"\n\n  # Monitoring\n  logging_strategy: \"steps\"\n  logging_steps: 500\n  eval_strategy: \"steps\"\n  eval_steps: 500\n  save_strategy: \"steps\"\n  save_steps: 2000\n  save_total_limit: 3\n\n  # Model selection\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_loss\"\n  greater_is_better: false\n\n  # Generation settings\n  generation_max_length: 512\n  generation_num_beams: 4\n  generation_early_stopping: true\n\n  # Mixed precision\n  bf16: true\n\n  # Reproducibility\n  seed: 42\n  deterministic: true\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#regression-task-example","title":"Regression Task Example","text":"<pre><code>task:\n  task_type: \"regression\"\n  num_labels: 1\n\nfinetune:\n  output_dir: \"./expression_prediction\"\n  num_train_epochs: 10\n  per_device_train_batch_size: 16\n  per_device_eval_batch_size: 32\n  gradient_accumulation_steps: 1\n\n  # Higher learning rate for regression\n  learning_rate: 1e-4\n  weight_decay: 0.05\n  warmup_ratio: 0.1\n  lr_scheduler_type: \"polynomial\"\n\n  # Monitoring\n  logging_strategy: \"steps\"\n  logging_steps: 100\n  eval_strategy: \"steps\"\n  eval_steps: 100\n  save_strategy: \"steps\"\n  save_steps: 500\n  save_total_limit: 3\n\n  # Model selection\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_rmse\"\n  greater_is_better: false\n\n  # Mixed precision\n  fp16: true\n\n  # Reproducibility\n  seed: 42\n  deterministic: true\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"user_guide/fine_tuning/configuration/#development-configuration","title":"Development Configuration","text":"<pre><code>finetune:\n  # Development settings\n  num_train_epochs: 1\n  per_device_train_batch_size: 4\n  logging_steps: 10\n  eval_steps: 50\n  save_steps: 100\n\n  # Quick testing\n  max_steps: 100\n  eval_delay: 0\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#production-configuration","title":"Production Configuration","text":"<pre><code>finetune:\n  # Production settings\n  num_train_epochs: 10\n  per_device_train_batch_size: 32\n  gradient_accumulation_steps: 2\n\n  # Robust training\n  early_stopping_patience: 5\n  save_total_limit: 10\n\n  # Monitoring\n  logging_steps: 500\n  eval_steps: 500\n  save_steps: 2000\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#gpu-memory-optimization","title":"GPU Memory Optimization","text":"<pre><code>finetune:\n  # Memory optimization\n  per_device_train_batch_size: 8\n  gradient_accumulation_steps: 4\n  gradient_checkpointing: true\n  memory_efficient_attention: true\n\n  # Mixed precision\n  bf16: true\n\n  # Data loading\n  dataloader_num_workers: 2\n  dataloader_pin_memory: false\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#configuration-validation","title":"Configuration Validation","text":""},{"location":"user_guide/fine_tuning/configuration/#schema-validation","title":"Schema Validation","text":"<p>DNALLM automatically validates your configuration:</p> <pre><code>from dnallm import validate_config\n\n# Validate configuration\ntry:\n    validate_config(\"finetune_config.yaml\")\n    print(\"Configuration is valid!\")\nexcept ValidationError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#common-validation-errors","title":"Common Validation Errors","text":"Error Cause Solution <code>Invalid task type</code> Unsupported task type Use supported task types <code>Missing required field</code> Incomplete configuration Add missing required fields <code>Invalid learning rate</code> Learning rate too high/low Use reasonable values (1e-6 to 1e-3) <code>Invalid batch size</code> Batch size too large Reduce batch size or use gradient accumulation"},{"location":"user_guide/fine_tuning/configuration/#best-practices","title":"Best Practices","text":""},{"location":"user_guide/fine_tuning/configuration/#1-configuration-organization","title":"1. Configuration Organization","text":"<pre><code># Use descriptive names\nfinetune:\n  output_dir: \"./promoter_classification_2024\"\n\n# Group related settings\nfinetune:\n  # Training duration\n  num_train_epochs: 5\n  max_steps: -1\n\n  # Batch processing\n  per_device_train_batch_size: 16\n  gradient_accumulation_steps: 1\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#2-environment-specific-configs","title":"2. Environment-Specific Configs","text":"<pre><code># Development config\nfinetune:\n  num_train_epochs: 1\n  per_device_train_batch_size: 4\n\n# Production config\nfinetune:\n  num_train_epochs: 10\n  per_device_train_batch_size: 32\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#3-version-control","title":"3. Version Control","text":"<pre><code># Include version information\nconfig_version: \"1.0.0\"\ncreated_by: \"Your Name\"\ncreated_date: \"2024-01-15\"\nexperiment_name: \"promoter_classification_v1\"\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#4-hyperparameter-tuning","title":"4. Hyperparameter Tuning","text":"<pre><code># Use consistent naming for experiments\nfinetune:\n  output_dir: \"./experiments/lr_{learning_rate}_bs_{per_device_train_batch_size}\"\n\n# Document hyperparameter choices\nnotes: \"Testing different learning rates for promoter classification\"\nhyperparameters:\n  learning_rate: \"2e-5\"\n  batch_size: \"16\"\n  scheduler: \"linear\"\n</code></pre>"},{"location":"user_guide/fine_tuning/configuration/#next-steps","title":"Next Steps","text":"<p>After configuring your fine-tuning:</p> <ol> <li>Run Your Training: Follow the Getting Started guide</li> <li>Explore Task-Specific Guides: Check Task-Specific Guides</li> <li>Advanced Techniques: Learn about Advanced Techniques</li> <li>Real-world Examples: See Examples and Use Cases</li> </ol> <p>Need help with configuration? Check our FAQ or open an issue on GitHub.</p>"},{"location":"user_guide/fine_tuning/getting_started/","title":"Getting Started with Fine-tuning","text":"<p>This guide will walk you through the basics of fine-tuning DNA language models using DNALLM. You'll learn how to set up your first fine-tuning experiment, configure models and datasets, and monitor training progress.</p>"},{"location":"user_guide/fine_tuning/getting_started/#overview","title":"Overview","text":"<p>Fine-tuning in DNALLM allows you to: - Adapt pre-trained DNA language models to your specific tasks - Leverage transfer learning for better performance on small datasets - Customize models for domain-specific DNA analysis - Achieve state-of-the-art results with minimal data</p>"},{"location":"user_guide/fine_tuning/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed and configured:</p> <pre><code># Install DNALLM\npip install dnallm\n\n# Or with uv (recommended)\nuv pip install dnallm\n\n# Install additional dependencies for fine-tuning\npip install torch transformers datasets accelerate\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#basic-setup","title":"Basic Setup","text":""},{"location":"user_guide/fine_tuning/getting_started/#1-import-required-modules","title":"1. Import Required Modules","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\nfrom transformers import TrainingArguments\nimport torch\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#2-create-a-simple-configuration","title":"2. Create a Simple Configuration","text":"<p>Create a <code>finetune_config.yaml</code> file:</p> <pre><code># finetune_config.yaml\ntask:\n  task_type: \"binary\"  # binary, multiclass, multilabel, regression, generation, mask, token\n  num_labels: 2\n  label_names: [\"negative\", \"positive\"]\n  threshold: 0.5\n\nfinetune:\n  output_dir: \"./outputs\"\n  num_train_epochs: 3\n  per_device_train_batch_size: 8\n  per_device_eval_batch_size: 16\n  gradient_accumulation_steps: 1\n  learning_rate: 2e-5\n  weight_decay: 0.01\n  warmup_ratio: 0.1\n  logging_strategy: \"steps\"\n  logging_steps: 100\n  eval_strategy: \"steps\"\n  eval_steps: 100\n  save_strategy: \"steps\"\n  save_steps: 500\n  save_total_limit: 3\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_loss\"\n  report_to: \"tensorboard\"\n  seed: 42\n  bf16: false\n  fp16: false\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#3-load-your-data","title":"3. Load Your Data","text":"<pre><code># Load your dataset\ndataset = DNADataset.load_local_data(\n    \"path/to/your/data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    max_length=512\n)\n\n# Split data into train/validation sets\nif not dataset.is_split:\n    dataset.split_data(test_size=0.2, val_size=0.1)\n\nprint(f\"Training samples: {len(dataset.train_data)}\")\nprint(f\"Validation samples: {len(dataset.val_data)}\")\nprint(f\"Test samples: {len(dataset.test_data)}\")\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#4-load-pre-trained-model","title":"4. Load Pre-trained Model","text":"<pre><code># Load configuration\nconfig = load_config(\"finetune_config.yaml\")\n\n# Load pre-trained model and tokenizer\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(f\"Model loaded on device: {device}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#5-initialize-trainer-and-start-training","title":"5. Initialize Trainer and Start Training","text":"<pre><code># Initialize trainer\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\n# Start training\nprint(\"Starting fine-tuning...\")\ntrainer.train()\n\n# Save the final model\ntrainer.save_model(\"./final_model\")\ntokenizer.save_pretrained(\"./final_model\")\nprint(\"Training completed! Model saved to ./final_model\")\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#command-line-interface","title":"Command Line Interface","text":"<p>DNALLM also provides a convenient command-line interface:</p> <pre><code># Basic fine-tuning run\ndnallm-finetune --config finetune_config.yaml --model zhangtaolab/plant-dnabert-BPE --dataset path/to/data.csv\n\n# Fine-tune with custom parameters\ndnallm-finetune --config config.yaml --epochs 5 --batch-size 16 --learning-rate 1e-4\n\n# Resume from checkpoint\ndnallm-finetune --config config.yaml --resume-from-checkpoint ./checkpoint-1000\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#understanding-the-configuration","title":"Understanding the Configuration","text":""},{"location":"user_guide/fine_tuning/getting_started/#task-configuration","title":"Task Configuration","text":"<p>The <code>task</code> section defines what type of task you're fine-tuning for:</p> <pre><code>task:\n  task_type: \"binary\"           # Task type (see table below)\n  num_labels: 2                 # Number of output classes\n  label_names: [\"neg\", \"pos\"]   # Human-readable label names\n  threshold: 0.5                # Classification threshold\n</code></pre> Task Type Description Output <code>binary</code> Binary classification Single probability (0-1) <code>multiclass</code> Multi-class classification Probability distribution <code>multilabel</code> Multi-label classification Multiple binary outputs <code>regression</code> Continuous value prediction Single real number <code>generation</code> Sequence generation Generated text <code>mask</code> Masked language modeling Predicted tokens <code>token</code> Token classification Labels per token"},{"location":"user_guide/fine_tuning/getting_started/#training-configuration","title":"Training Configuration","text":"<p>The <code>finetune</code> section controls training parameters:</p> <pre><code>finetune:\n  # Basic training settings\n  num_train_epochs: 3                    # Total training epochs\n  per_device_train_batch_size: 8         # Batch size per device\n  per_device_eval_batch_size: 16         # Evaluation batch size\n\n  # Optimization\n  learning_rate: 2e-5                    # Learning rate\n  weight_decay: 0.01                     # Weight decay\n  warmup_ratio: 0.1                      # Warmup proportion\n\n  # Training strategy\n  gradient_accumulation_steps: 1         # Gradient accumulation\n  max_grad_norm: 1.0                    # Gradient clipping\n\n  # Monitoring and saving\n  logging_strategy: \"steps\"              # When to log\n  logging_steps: 100                     # Log every N steps\n  eval_strategy: \"steps\"                 # When to evaluate\n  eval_steps: 100                        # Evaluate every N steps\n  save_strategy: \"steps\"                 # When to save\n  save_steps: 500                        # Save every N steps\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#data-format-requirements","title":"Data Format Requirements","text":"<p>Your dataset should be in one of these formats:</p>"},{"location":"user_guide/fine_tuning/getting_started/#csvtsv-format","title":"CSV/TSV Format","text":"<pre><code>sequence,label\nATCGATCGATCG,1\nGCTAGCTAGCTA,0\nTATATATATATA,1\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#json-format","title":"JSON Format","text":"<pre><code>[\n  {\"sequence\": \"ATCGATCGATCG\", \"label\": 1},\n  {\"sequence\": \"GCTAGCTAGCTA\", \"label\": 0}\n]\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#fasta-format","title":"FASTA Format","text":"<pre><code>&gt;sequence1|label:1\nATCGATCGATCG\n&gt;sequence2|label:0\nGCTAGCTAGCTA\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#example-complete-fine-tuning-workflow","title":"Example: Complete Fine-tuning Workflow","text":"<p>Here's a complete working example:</p> <pre><code>import os\nfrom dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n\ndef run_finetuning():\n    # 1. Check data availability\n    data_path = \"path/to/your/dna_sequences.csv\"\n    if not os.path.exists(data_path):\n        print(\"Please provide a valid data path\")\n        return\n\n    # 2. Load configuration\n    config = load_config(\"finetune_config.yaml\")\n\n    # 3. Load and prepare dataset\n    dataset = DNADataset.load_local_data(\n        data_path,\n        seq_col=\"sequence\",\n        label_col=\"label\",\n        max_length=512\n    )\n\n    # Split data\n    if not dataset.is_split:\n        dataset.split_data(test_size=0.2, val_size=0.1)\n\n    print(f\"Dataset loaded: {len(dataset.train_data)} train, {len(dataset.val_data)} val\")\n\n    # 4. Load pre-trained model\n    model, tokenizer = load_model_and_tokenizer(\n        \"zhangtaolab/plant-dnabert-BPE\",\n        task_config=config['task'],\n        source=\"huggingface\"\n    )\n\n    # 5. Initialize trainer\n    trainer = DNATrainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=dataset.train_data,\n        eval_dataset=dataset.val_data,\n        config=config\n    )\n\n    # 6. Start training\n    print(\"Starting fine-tuning...\")\n    trainer.train()\n\n    # 7. Evaluate on test set\n    test_results = trainer.evaluate(dataset.test_data)\n    print(f\"Test results: {test_results}\")\n\n    # 8. Save model\n    output_dir = \"./finetuned_model\"\n    trainer.save_model(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n    print(f\"Fine-tuning completed! Model saved to {output_dir}\")\n    return output_dir\n\n# Run the complete workflow\nif __name__ == \"__main__\":\n    model_path = run_finetuning()\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#monitoring-training-progress","title":"Monitoring Training Progress","text":""},{"location":"user_guide/fine_tuning/getting_started/#tensorboard-integration","title":"TensorBoard Integration","text":"<p>DNALLM automatically logs training metrics to TensorBoard:</p> <pre><code># Start TensorBoard\ntensorboard --logdir ./outputs\n\n# Open in browser: http://localhost:6006\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<ul> <li>Training Loss: Should decrease over time</li> <li>Validation Loss: Should decrease but not overfit</li> <li>Learning Rate: Should follow the scheduled curve</li> <li>Gradient Norm: Should be stable (around 1.0)</li> <li>Memory Usage: Monitor GPU memory consumption</li> </ul>"},{"location":"user_guide/fine_tuning/getting_started/#early-stopping","title":"Early Stopping","text":"<p>Configure early stopping to prevent overfitting:</p> <pre><code>finetune:\n  # ... other settings ...\n  early_stopping_patience: 3\n  early_stopping_threshold: 0.001\n  metric_for_best_model: \"eval_loss\"\n  greater_is_better: false\n</code></pre>"},{"location":"user_guide/fine_tuning/getting_started/#common-hyperparameters","title":"Common Hyperparameters","text":""},{"location":"user_guide/fine_tuning/getting_started/#learning-rate","title":"Learning Rate","text":"<ul> <li>Conservative: 1e-5 to 5e-5 (good for most cases)</li> <li>Aggressive: 5e-5 to 1e-4 (when you have more data)</li> <li>Very Small: 1e-6 to 1e-5 (when fine-tuning on very similar data)</li> </ul>"},{"location":"user_guide/fine_tuning/getting_started/#batch-size","title":"Batch Size","text":"<ul> <li>Small: 4-8 (when memory is limited)</li> <li>Medium: 8-16 (good balance)</li> <li>Large: 16-32 (when you have sufficient memory)</li> </ul>"},{"location":"user_guide/fine_tuning/getting_started/#training-epochs","title":"Training Epochs","text":"<ul> <li>Short: 1-3 epochs (when data is similar to pre-training)</li> <li>Medium: 3-10 epochs (typical fine-tuning)</li> <li>Long: 10+ epochs (when data is very different)</li> </ul>"},{"location":"user_guide/fine_tuning/getting_started/#next-steps","title":"Next Steps","text":"<p>After completing this basic tutorial:</p> <ol> <li>Explore Task-Specific Guides: Learn about different task types</li> <li>Advanced Techniques: Discover custom training strategies</li> <li>Configuration Options: Check detailed configuration options</li> <li>Real-world Examples: See practical use cases</li> </ol>"},{"location":"user_guide/fine_tuning/getting_started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide/fine_tuning/getting_started/#common-issues","title":"Common Issues","text":"<p>\"CUDA out of memory\" error <pre><code># Reduce batch size\nfinetune:\n  per_device_train_batch_size: 4  # Reduced from 8\n  gradient_accumulation_steps: 2   # Compensate for smaller batch\n</code></pre></p> <p>Training loss not decreasing <pre><code># Adjust learning rate\nfinetune:\n  learning_rate: 5e-5  # Increased from 2e-5\n  warmup_ratio: 0.2    # Increased warmup\n</code></pre></p> <p>Overfitting (validation loss increases) <pre><code># Add regularization\nfinetune:\n  weight_decay: 0.1    # Increased from 0.01\n  dropout: 0.2         # Add dropout\n</code></pre></p>"},{"location":"user_guide/fine_tuning/getting_started/#additional-resources","title":"Additional Resources","text":"<ul> <li>Task-Specific Guides - Fine-tuning for different tasks</li> <li>Advanced Techniques - Custom training and optimization</li> <li>Configuration Guide - Detailed configuration options</li> <li>Examples and Use Cases - Real-world scenarios</li> <li>Troubleshooting - Common problems and solutions</li> </ul> <p>Ready for more? Continue to Task-Specific Guides to learn about fine-tuning for different types of DNA analysis tasks.</p>"},{"location":"user_guide/fine_tuning/task_guides/","title":"Task-Specific Fine-tuning Guides","text":"<p>This guide provides detailed instructions for fine-tuning DNA language models on different types of tasks. Each task type has specific requirements, configurations, and best practices.</p>"},{"location":"user_guide/fine_tuning/task_guides/#overview","title":"Overview","text":"<p>DNALLM supports various task types, each requiring different model architectures, loss functions, and evaluation metrics:</p> <ul> <li>Classification Tasks: Binary, multi-class, and multi-label classification</li> <li>Generation Tasks: Sequence generation and completion</li> <li>Masked Language Modeling: Sequence prediction and analysis</li> <li>Token Classification: Named entity recognition and tagging</li> <li>Regression Tasks: Continuous value prediction</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#binary-classification","title":"Binary Classification","text":""},{"location":"user_guide/fine_tuning/task_guides/#use-cases","title":"Use Cases","text":"<ul> <li>Promoter prediction (promoter vs. non-promoter)</li> <li>Motif detection (contains motif vs. doesn't contain)</li> <li>Functional annotation (functional vs. non-functional)</li> <li>Disease association (disease-related vs. normal)</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#configuration","title":"Configuration","text":"<pre><code>task:\n  task_type: \"binary\"\n  num_labels: 2\n  label_names: [\"negative\", \"positive\"]\n  threshold: 0.5  # Classification threshold\n\nfinetune:\n  learning_rate: 2e-5\n  num_train_epochs: 5\n  per_device_train_batch_size: 16\n  metric_for_best_model: \"eval_f1\"  # or \"eval_accuracy\"\n  greater_is_better: true\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#data-format","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,1\nGCTAGCTAGCTA,0\nTATATATATATA,1\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#example-implementation","title":"Example Implementation","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n\n# Load configuration\nconfig = load_config(\"binary_classification_config.yaml\")\n\n# Load pre-trained model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load dataset\ndataset = DNADataset.load_local_data(\n    \"promoter_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Split data\ndataset.split_data(test_size=0.2, val_size=0.1)\n\n# Initialize trainer\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\n# Train\ntrainer.train()\n\n# Evaluate\ntest_results = trainer.evaluate(dataset.test_data)\nprint(f\"Test F1: {test_results['eval_f1']:.4f}\")\nprint(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#best-practices","title":"Best Practices","text":"<ul> <li>Data Balance: Ensure balanced positive/negative samples</li> <li>Threshold Tuning: Adjust classification threshold based on your needs</li> <li>Evaluation Metrics: Use F1-score for imbalanced datasets</li> <li>Data Augmentation: Apply reverse complement and random mutations</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#multi-class-classification","title":"Multi-class Classification","text":""},{"location":"user_guide/fine_tuning/task_guides/#use-cases_1","title":"Use Cases","text":"<ul> <li>Functional category classification (enzyme, receptor, structural, etc.)</li> <li>Tissue-specific expression classification</li> <li>Evolutionary conservation level classification</li> <li>Regulatory element type classification</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#configuration_1","title":"Configuration","text":"<pre><code>task:\n  task_type: \"multiclass\"\n  num_labels: 4\n  label_names: [\"enzyme\", \"receptor\", \"structural\", \"regulatory\"]\n  # No threshold needed for multi-class\n\nfinetune:\n  learning_rate: 3e-5  # Slightly higher for multi-class\n  num_train_epochs: 8\n  per_device_train_batch_size: 16\n  metric_for_best_model: \"eval_accuracy\"\n  greater_is_better: true\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#data-format_1","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,0\nGCTAGCTAGCTA,1\nTATATATATATA,2\nCGCGCGCGCGCG,3\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#example-implementation_1","title":"Example Implementation","text":"<pre><code># Load multi-class model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load multi-class dataset\ndataset = DNADataset.load_local_data(\n    \"functional_annotation.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Train and evaluate\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\ntrainer.train()\n\n# Multi-class evaluation\ntest_results = trainer.evaluate(dataset.test_data)\nprint(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")\nprint(f\"Test Macro F1: {test_results['eval_f1_macro']:.4f}\")\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#best-practices_1","title":"Best Practices","text":"<ul> <li>Label Encoding: Use integer labels (0, 1, 2, 3) instead of strings</li> <li>Class Balance: Monitor class distribution and use weighted loss if needed</li> <li>Evaluation: Focus on macro-averaged metrics for imbalanced classes</li> <li>Data Augmentation: Apply class-specific augmentation strategies</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#multi-label-classification","title":"Multi-label Classification","text":""},{"location":"user_guide/fine_tuning/task_guides/#use-cases_2","title":"Use Cases","text":"<ul> <li>Multiple functional annotations per sequence</li> <li>Multiple binding site predictions</li> <li>Multiple regulatory element types</li> <li>Multiple disease associations</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#configuration_2","title":"Configuration","text":"<pre><code>task:\n  task_type: \"multilabel\"\n  num_labels: 5\n  label_names: [\"promoter\", \"enhancer\", \"silencer\", \"insulator\", \"locus_control\"]\n  threshold: 0.5  # Per-label threshold\n\nfinetune:\n  learning_rate: 2e-5\n  num_train_epochs: 6\n  per_device_train_batch_size: 16\n  metric_for_best_model: \"eval_f1_micro\"\n  greater_is_better: true\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#data-format_2","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,\"1,0,1,0,0\"\nGCTAGCTAGCTA,\"0,1,0,1,0\"\nTATATATATATA,\"1,1,0,0,1\"\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#example-implementation_2","title":"Example Implementation","text":"<pre><code># Load multi-label model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load multi-label dataset\ndataset = DNADataset.load_local_data(\n    \"multi_label_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    tokenizer=tokenizer,\n    max_length=512,\n    label_separator=\",\"  # Specify label separator\n)\n\n# Train\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\ntrainer.train()\n\n# Multi-label evaluation\ntest_results = trainer.evaluate(dataset.test_data)\nprint(f\"Test Micro F1: {test_results['eval_f1_micro']:.4f}\")\nprint(f\"Test Macro F1: {test_results['eval_f1_macro']:.4f}\")\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#best-practices_2","title":"Best Practices","text":"<ul> <li>Label Separator: Specify the separator used in your label column</li> <li>Threshold Tuning: Optimize per-label thresholds for your use case</li> <li>Loss Function: Use binary cross-entropy with sigmoid activation</li> <li>Evaluation: Focus on micro-averaged metrics for overall performance</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#regression-tasks","title":"Regression Tasks","text":""},{"location":"user_guide/fine_tuning/task_guides/#use-cases_3","title":"Use Cases","text":"<ul> <li>Expression level prediction</li> <li>Binding affinity prediction</li> <li>Conservation score prediction</li> <li>Functional activity prediction</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#configuration_3","title":"Configuration","text":"<pre><code>task:\n  task_type: \"regression\"\n  num_labels: 1  # Single continuous output\n  # No label_names or threshold needed\n\nfinetune:\n  learning_rate: 1e-4  # Higher learning rate for regression\n  num_train_epochs: 10\n  per_device_train_batch_size: 16\n  metric_for_best_model: \"eval_rmse\"\n  greater_is_better: false  # Lower is better for RMSE\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#data-format_3","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,0.85\nGCTAGCTAGCTA,0.23\nTATATATATATA,0.67\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#example-implementation_3","title":"Example Implementation","text":"<pre><code># Load regression model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load regression dataset\ndataset = DNADataset.load_local_data(\n    \"expression_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"expression_level\",\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Train\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\ntrainer.train()\n\n# Regression evaluation\ntest_results = trainer.evaluate(dataset.test_data)\nprint(f\"Test RMSE: {test_results['eval_rmse']:.4f}\")\nprint(f\"Test MAE: {test_results['eval_mae']:.4f}\")\nprint(f\"Test R\u00b2: {test_results['eval_r2']:.4f}\")\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#best-practices_3","title":"Best Practices","text":"<ul> <li>Data Normalization: Normalize your target values (0-1 or z-score)</li> <li>Loss Function: Use MSE or MAE depending on your needs</li> <li>Evaluation: Monitor RMSE, MAE, and R\u00b2 metrics</li> <li>Outlier Handling: Consider robust loss functions for noisy data</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#generation-tasks","title":"Generation Tasks","text":""},{"location":"user_guide/fine_tuning/task_guides/#use-cases_4","title":"Use Cases","text":"<ul> <li>DNA sequence generation</li> <li>Sequence completion</li> <li>Mutant sequence generation</li> <li>Synthetic promoter design</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#configuration_4","title":"Configuration","text":"<pre><code>task:\n  task_type: \"generation\"\n  # No num_labels, label_names, or threshold needed\n\nfinetune:\n  learning_rate: 5e-5  # Higher learning rate for generation\n  num_train_epochs: 15\n  per_device_train_batch_size: 8  # Smaller batch size\n  metric_for_best_model: \"eval_loss\"\n  greater_is_better: false\n  generation_max_length: 512\n  generation_num_beams: 4\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#data-format_4","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,ATCGATCGATCG\nGCTAGCTAGCTA,GCTAGCTAGCTA\nTATATATATATA,TATATATATATA\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#example-implementation_4","title":"Example Implementation","text":"<pre><code># Load generation model (GPT-style)\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnagpt-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load generation dataset\ndataset = DNADataset.load_local_data(\n    \"generation_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"target_sequence\",\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Train\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\ntrainer.train()\n\n# Test generation\ntest_sequences = [\"ATCG\", \"GCTA\", \"TATA\"]\nfor seq in test_sequences:\n    inputs = tokenizer(seq, return_tensors=\"pt\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        max_length=512,\n        num_beams=4,\n        early_stopping=True\n    )\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Input: {seq} -&gt; Generated: {generated}\")\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#best-practices_4","title":"Best Practices","text":"<ul> <li>Model Architecture: Use GPT-style models for generation tasks</li> <li>Sequence Length: Ensure consistent input/output lengths</li> <li>Beam Search: Use beam search for better generation quality</li> <li>Evaluation: Monitor perplexity and generation quality metrics</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#masked-language-modeling","title":"Masked Language Modeling","text":""},{"location":"user_guide/fine_tuning/task_guides/#use-cases_5","title":"Use Cases","text":"<ul> <li>Sequence completion</li> <li>Mutation prediction</li> <li>Missing data imputation</li> <li>Sequence analysis</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#configuration_5","title":"Configuration","text":"<pre><code>task:\n  task_type: \"mask\"\n  # No num_labels, label_names, or threshold needed\n\nfinetune:\n  learning_rate: 3e-5\n  num_train_epochs: 8\n  per_device_train_batch_size: 16\n  metric_for_best_model: \"eval_loss\"\n  greater_is_better: false\n  mlm_probability: 0.15  # Probability of masking tokens\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#data-format_5","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,ATCGATCGATCG\nGCTAGCTAGCTA,GCTAGCTAGCTA\nTATATATATATA,TATATATATATA\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#example-implementation_5","title":"Example Implementation","text":"<pre><code># Load MLM model (BERT-style)\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load MLM dataset\ndataset = DNADataset.load_local_data(\n    \"mlm_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"sequence\",  # Same as input for MLM\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Train\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\ntrainer.train()\n\n# Test MLM\ntest_sequence = \"ATCG[MASK]ATCG\"\ninputs = tokenizer(test_sequence, return_tensors=\"pt\")\noutputs = model(**inputs)\npredictions = outputs.logits.argmax(-1)\npredicted_token = tokenizer.decode([predictions[0][4]])  # Position of [MASK]\nprint(f\"Input: {test_sequence} -&gt; Predicted: {predicted_token}\")\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#best-practices_5","title":"Best Practices","text":"<ul> <li>Masking Strategy: Use appropriate masking probability (15% is standard)</li> <li>Model Architecture: Use BERT-style models for MLM tasks</li> <li>Evaluation: Monitor perplexity and accuracy on masked tokens</li> <li>Data Preparation: Ensure sequences are properly tokenized</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#token-classification","title":"Token Classification","text":""},{"location":"user_guide/fine_tuning/task_guides/#use-cases_6","title":"Use Cases","text":"<ul> <li>Named entity recognition (gene identification)</li> <li>Regulatory element tagging</li> <li>Motif boundary detection</li> <li>Functional region annotation</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#configuration_6","title":"Configuration","text":"<pre><code>task:\n  task_type: \"token\"\n  num_labels: 4  # Number of entity types + O (outside)\n  label_names: [\"O\", \"GENE\", \"PROMOTER\", \"ENHANCER\"]\n  # No threshold needed\n\nfinetune:\n  learning_rate: 2e-5\n  num_train_epochs: 6\n  per_device_train_batch_size: 16\n  metric_for_best_model: \"eval_f1\"\n  greater_is_better: true\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#data-format_6","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,\"O O O O O O O O O O O O\"\nGCTAGCTAGCTA,\"O GENE GENE GENE O O O O O O O O\"\nTATATATATATA,\"O O O O O O O O O O O O\"\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#example-implementation_6","title":"Example Implementation","text":"<pre><code># Load token classification model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load token classification dataset\ndataset = DNADataset.load_local_data(\n    \"ner_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"labels\",\n    tokenizer=tokenizer,\n    max_length=512,\n    label_separator=\" \"  # Space-separated labels\n)\n\n# Train\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\ntrainer.train()\n\n# Test token classification\ntest_sequence = \"ATCGATCGATCG\"\ninputs = tokenizer(test_sequence, return_tensors=\"pt\")\noutputs = model(**inputs)\npredictions = outputs.logits.argmax(-1)\nlabels = [config['task']['label_names'][p] for p in predictions[0]]\nprint(f\"Sequence: {test_sequence}\")\nprint(f\"Labels: {labels}\")\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#best-practices_6","title":"Best Practices","text":"<ul> <li>Label Encoding: Use BIO or BIOES tagging schemes for better performance</li> <li>Sequence Length: Keep sequences manageable for token-level annotation</li> <li>Evaluation: Use sequence-level F1 score and entity-level metrics</li> <li>Data Quality: Ensure high-quality annotations for training</li> </ul>"},{"location":"user_guide/fine_tuning/task_guides/#task-specific-data-augmentation","title":"Task-Specific Data Augmentation","text":""},{"location":"user_guide/fine_tuning/task_guides/#classification-tasks","title":"Classification Tasks","text":"<pre><code># Apply reverse complement augmentation\naugmented_data = []\nfor item in dataset.train_data:\n    # Original sequence\n    augmented_data.append(item)\n\n    # Reverse complement\n    rc_sequence = reverse_complement(item['sequence'])\n    augmented_data.append({\n        'sequence': rc_sequence,\n        'label': item['label']\n    })\n\n# Apply random mutations\nfor item in dataset.train_data:\n    if random.random() &lt; 0.1:  # 10% mutation rate\n        mutated_sequence = apply_random_mutations(item['sequence'])\n        augmented_data.append({\n            'sequence': mutated_sequence,\n            'label': item['label']\n        })\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#generation-tasks_1","title":"Generation Tasks","text":"<pre><code># Apply sequence truncation for generation\naugmented_data = []\nfor item in dataset.train_data:\n    # Full sequence\n    augmented_data.append(item)\n\n    # Truncated sequences for training\n    for length in [256, 384]:\n        if len(item['sequence']) &gt; length:\n            truncated = item['sequence'][:length]\n            augmented_data.append({\n                'sequence': truncated,\n                'label': truncated\n            })\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#evaluation-strategies","title":"Evaluation Strategies","text":""},{"location":"user_guide/fine_tuning/task_guides/#classification-metrics","title":"Classification Metrics","text":"<pre><code># Binary classification\nfrom sklearn.metrics import classification_report, roc_auc_score\n\npredictions = trainer.infer(dataset.test_data)\ny_true = [item['label'] for item in dataset.test_data]\ny_pred = predictions.predictions.argmax(-1)\n\nprint(classification_report(y_true, y_pred))\nprint(f\"ROC AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#generation-metrics","title":"Generation Metrics","text":"<pre><code># Generation quality metrics\nfrom nltk.translate.bleu_score import sentence_bleu\n\ngenerated_sequences = []\nfor item in dataset.test_data:\n    inputs = tokenizer(item['sequence'], return_tensors=\"pt\")\n    outputs = model.generate(inputs[\"input_ids\"], max_length=512)\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    generated_sequences.append(generated)\n\n# Calculate BLEU score\nbleu_scores = []\nfor pred, ref in zip(generated_sequences, [item['label'] for item in dataset.test_data]):\n    score = sentence_bleu([ref.split()], pred.split())\n    bleu_scores.append(score)\n\nprint(f\"Average BLEU: {np.mean(bleu_scores):.4f}\")\n</code></pre>"},{"location":"user_guide/fine_tuning/task_guides/#next-steps","title":"Next Steps","text":"<p>After mastering task-specific fine-tuning:</p> <ol> <li>Explore Advanced Techniques: Learn about custom training strategies</li> <li>Configuration Options: Check detailed configuration options</li> <li>Real-world Examples: See practical use cases</li> <li>Troubleshooting: Visit common issues and solutions</li> </ol> <p>Ready for advanced techniques? Continue to Advanced Techniques to learn about custom training strategies, optimization, and monitoring.</p>"},{"location":"user_guide/inference/advanced_inference/","title":"Advanced Inference with DNALLM","text":"<p>This tutorial is designed for users who are already familiar with Basic Inference and want to explore the more powerful and flexible features of the <code>DNAInference</code> engine. We will cover advanced topics such as batch processing, extracting internal model states, custom inference workflows, and advanced result handling.</p>"},{"location":"user_guide/inference/advanced_inference/#1-advanced-features-overview","title":"1. Advanced Features Overview","text":"<p>The <code>DNAInference</code> engine is more than just a tool for getting predictions. It provides a suite of advanced capabilities for in-depth model analysis and integration into complex pipelines:</p> <ul> <li>Direct Batch Inference: Process large datasets efficiently using <code>batch_infer()</code> for fine-grained control.</li> <li>Model Introspection: Extract hidden states and attention weights to understand how the model makes its decisions.</li> <li>Custom Workflows: Build tailored inference pipelines by directly using <code>DNADataset</code> and <code>DataLoader</code>.</li> <li>Sequence Generation &amp; Scoring: Use generative models like Evo for tasks beyond classification, such as creating new DNA sequences or scoring their likelihood.</li> <li>Advanced Configuration: Fine-tune every aspect of the inference process for specific hardware and data characteristics.</li> </ul>"},{"location":"user_guide/inference/advanced_inference/#2-deep-dive-into-batch-inference","title":"2. Deep Dive into Batch Inference","text":"<p>While <code>infer()</code> is a convenient wrapper, <code>batch_infer()</code> is the workhorse method that gives you direct access to the model's raw outputs. This is useful when you need to implement custom post-processing or integrate with other ML frameworks.</p> <p>The <code>batch_infer()</code> method returns three items: 1.  <code>all_logits</code>: A raw tensor of model outputs before any activation function (like Softmax or Sigmoid) is applied. 2.  <code>predictions</code>: A formatted dictionary of predictions (this is <code>None</code> if <code>do_pred=False</code>). 3.  <code>embeddings</code>: A dictionary containing hidden states and/or attention weights if requested.</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer, DNAInference\n\nconfigs = load_config(\"inference_config.yaml\")\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnagpt-BPE-promoter\", \n    task_config=configs['task'], \n    source=\"modelscope\"\n)\n\ninference_engine = DNAInference(\n    model=model,\n    tokenizer=tokenizer,\n    config=configs\n)\n\n# 1. Generate a dataset and dataloader\nsequences = [\"GATTACA...\", \"CGCGCGC...\"]\n_, dataloader = inference_engine.generate_dataset(\n    sequences, \n    batch_size=configs['inference'].batch_size\n)\n\n# 2. Run batch inference, requesting hidden states and attentions\nall_logits, _, embeddings = inference_engine.batch_infer(\n    dataloader,\n    do_pred=False,  # We'll process logits ourselves\n    output_hidden_states=True,\n    output_attentions=True\n)\n\n# 3. Now you have the raw materials\nprint(\"Logits shape:\", all_logits.shape)\n# &gt;&gt; Logits shape: torch.Size([2, 2])\n\nprint(\"Hidden states available:\", \"hidden_states\" in embeddings)\n# &gt;&gt; Hidden states available: True\n\nprint(\"Attention weights available:\", \"attentions\" in embeddings)\n# &gt;&gt; Attention weights available: True\n</code></pre>"},{"location":"user_guide/inference/advanced_inference/#when-to-use-batch_infer","title":"When to use <code>batch_infer()</code>:","text":"<ul> <li>When you need raw logits for custom analysis (e.g., temperature scaling, ensemble methods).</li> <li>When you only need embeddings and don't want the overhead of formatting predictions.</li> <li>When integrating into a larger pipeline that has its own post-processing logic.</li> </ul>"},{"location":"user_guide/inference/advanced_inference/#3-customizing-the-inference-workflow","title":"3. Customizing the Inference Workflow","text":"<p>You can gain full control over the data loading and preprocessing pipeline by creating the <code>DNADataset</code> and <code>DataLoader</code> yourself. This is useful for complex datasets or when you need to apply custom transformations.</p> <pre><code>from torch.utils.data import DataLoader\nfrom dnallm.datahandling.data import DNADataset\n\n# Assume inference_engine is already initialized\n\n# 1. Load data from a file into a DNADataset object\ndna_dataset = DNADataset.load_local_data(\n    \"data/my_sequences.fasta\",\n    tokenizer=inference_engine.tokenizer,\n    max_length=inference_engine.pred_config.max_length\n)\n\n# 2. Apply encoding\n# This step tokenizes the sequences and prepares them for the model\ndna_dataset.encode_sequences(\n    task=inference_engine.task_config.task_type,\n    remove_unused_columns=True\n)\n\n# 3. Create a custom DataLoader\ncustom_dataloader = DataLoader(\n    dna_dataset,\n    batch_size=32,  # Override config batch size\n    shuffle=False,  # Inference should not be shuffled\n    num_workers=inference_engine.pred_config.num_workers\n)\n\n# 4. Run inference with the custom dataloader\nlogits, predictions, _ = inference_engine.batch_infer(custom_dataloader)\n\nprint(f\"Processed {len(predictions)} sequences.\")\n</code></pre>"},{"location":"user_guide/inference/advanced_inference/#4-lora-model-inference","title":"4. LoRA-Model Inference","text":"<p>The <code>DNAInference</code> engine seamlessly supports inference with models fine-tuned using LoRA adapters. This allows you to switch between different \"personalities\" of a base model without loading a completely new one.</p> <p>To use a LoRA adapter, simply provide the path or hub ID to the <code>lora_adapter</code> argument during initialization.</p> <pre><code># 1. Load the base model\nbase_model_name = \"lgq12697/PlantCAD2-Small-l24-d0768\"\nmodel, tokenizer = load_model_and_tokenizer(base_model_name, task_config=configs['task'], source=\"modelscope\")\n\n# 2. Specify the LoRA adapter\nlora_adapter_id = \"lgq12697/cross_species_acr_train_on_arabidopsis_plantcad2_small\"\n\n# 3. Initialize the engine with the adapter\n# The engine will automatically download and apply the LoRA weights\nlora_inference_engine = DNAInference(\n    model=model,\n    tokenizer=tokenizer,\n    config=configs,\n    lora_adapter=lora_adapter_id\n)\n\n# Now, all inference calls will use the LoRA-adapted model\nresults = lora_inference_engine.infer(sequences=[\"GATTACA...\"])\nprint(results)\n</code></pre> <p>This is extremely powerful for comparing a base model's performance against its fine-tuned variants or for deploying multiple specialized models efficiently.</p>"},{"location":"user_guide/inference/advanced_inference/#5-advanced-result-post-processing","title":"5. Advanced Result Post-Processing","text":"<p>The <code>DNAInference</code> engine includes powerful visualization tools for model interpretability, which require <code>output_attentions=True</code> or <code>output_hidden_states=True</code>.</p>"},{"location":"user_guide/inference/advanced_inference/#visualizing-attention","title":"Visualizing Attention","text":"<p><code>plot_attentions()</code> helps you see which parts of a sequence the model focused on.</p> <pre><code># Run inference first with output_attentions=True\ninference_engine.infer(sequences=sequences, output_attentions=True)\n\n# Plot the attention map for the first sequence, last layer, and last head\nattention_figure = inference_engine.plot_attentions(\n    seq_idx=0,\n    layer=-1,\n    head=-1,\n    save_path=\"./results/attention_map.png\"\n)\n</code></pre>"},{"location":"user_guide/inference/advanced_inference/#visualizing-embeddings","title":"Visualizing Embeddings","text":"<p><code>plot_hidden_states()</code> uses dimensionality reduction (t-SNE, PCA, UMAP) to visualize the sequence embeddings from each layer, which can reveal how the model separates different classes.</p> <pre><code># Run inference first with output_hidden_states=True\ninference_engine.infer(file_path=\"data/labeled_data.csv\", output_hidden_states=True, evaluate=True)\n\n# Plot the embeddings using t-SNE\nembedding_figure = inference_engine.plot_hidden_states(\n    reducer=\"t-SNE\",\n    save_path=\"./results/embedding_plot.png\"\n)\n</code></pre> <p>Next, learn how to squeeze every drop of performance out of your hardware in the Performance Optimization guide.</p>"},{"location":"user_guide/inference/advanced_inference/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Optimization - Optimize inference performance</li> <li>Visualization - Learn about result visualization</li> <li>Mutagenesis Analysis - Analyze mutation effects</li> <li>Inference Troubleshooting - Common inference issues and solutions</li> </ul>"},{"location":"user_guide/inference/basic_inference/","title":"Basic Inference with DNALLM","text":"<p>This tutorial walks you through the complete process of running inference using the <code>DNAInference</code> engine. We will cover loading a model, preparing data, and making predictions on both individual sequences and files.</p>"},{"location":"user_guide/inference/basic_inference/#1-the-core-workflow","title":"1. The Core Workflow","text":"<p>The inference process in DNALLM follows these steps: 1.  Load Configuration: Read the <code>inference_config.yaml</code> file. 2.  Load Model &amp; Tokenizer: Fetch a pre-trained model and its corresponding tokenizer. 3.  Initialize <code>DNAInference</code>: Create an inference engine instance with the model, tokenizer, and config. 4.  Run Inference: Use the engine's <code>infer()</code> method to get predictions. 5.  Interpret Results: Analyze the output.</p>"},{"location":"user_guide/inference/basic_inference/#2-a-complete-example","title":"2. A Complete Example","text":"<p>Let's put everything together in a Python script. This example demonstrates loading a promoter prediction model and using it to classify DNA sequences.</p> <pre><code>import os\nfrom dnallm import load_config, load_model_and_tokenizer, DNAInference\n\ndef main():\n    # 1. Load Configuration\n    # Assumes 'inference_config.yaml' is in the same directory\n    try:\n        configs = load_config(\"inference_config.yaml\")\n    except FileNotFoundError:\n        print(\"Error: 'inference_config.yaml' not found. Please create it.\")\n        return\n\n    # 2. Load Model and Tokenizer\n    # This example uses a model from ModelScope. You can also use 'huggingface'.\n    model_name = \"zhangtaolab/plant-dnagpt-BPE-promoter\"\n    print(f\"Loading model '{model_name}'...\")\n    model, tokenizer = load_model_and_tokenizer(\n        model_name, \n        task_config=configs['task'], \n        source=\"modelscope\"\n    )\n\n    # 3. Initialize DNAInference Engine\n    print(\"Initializing inference engine...\")\n    inference_engine = DNAInference(\n        model=model,\n        tokenizer=tokenizer,\n        config=configs\n    )\n\n    # --- 4. Run Inference ---\n\n    # Example 1: Infer from a list of sequences\n    print(\"\\n--- Predicting from a list of sequences ---\")\n    seqs_list = [\n        \"GCACTTTACTTAAAGTAAAAAGAAAAAAACTGTGCGCTCTCCAACTACCGCAGCAACGTGTCGAGCACAGGAACACGTGTCACTTCAGTTCTTCCAATTGCTGGGGCCCACCACTGTTTACTTCTGTACAGGCAGGTGGCCATGCTGATGACACTCCACACTCCTCGACTTTCGTAGCAGCAAGCCACGCGTGACCGAGAAGCCTCGCG\",\n        \"TTGTCATCACATTTGATCAACTACGATTTATGTTGTACTATTCATCTGTTTTCTCCTTTTTTTTTCCCTTATTGACAGGTTGTGGAGGTTCACAACGAACAGAATACAAGAAATTTTGGTAATCATTTGAGGACTTTCATGGGGTATGAATTGTGTGCTATAATAAATTAA\"\n    ]\n    results_from_list = inference_engine.infer(sequences=seqs_list)\n    print(\"Results:\")\n    print(results_from_list)\n\n    # Example 2: Infer from a file\n    print(\"\\n--- Predicting from a file ---\")\n    # Create a dummy CSV file for demonstration\n    seq_file = 'test_data.csv'\n    with open(seq_file, 'w') as f:\n        f.write(\"sequence,label\\n\")\n        f.write(f\"{seqs_list[0]},1\\n\")\n        f.write(f\"{seqs_list[1]},0\\n\")\n\n    # Run inference and evaluation\n    try:\n        results_from_file, metrics = inference_engine.infer(\n            file_path=seq_file,\n            evaluate=True,  # Enable evaluation since the file has labels\n            label_col='label' # Specify the column containing labels\n        )\n        print(\"\\nResults from file (first 2 entries):\")\n        print({k: results_from_file[k] for k in list(results_from_file)[:2]})\n\n        print(\"\\nEvaluation Metrics:\")\n        print(metrics)\n\n    except FileNotFoundError:\n        print(f\"Error: The file '{seq_file}' was not found.\")\n    finally:\n        # Clean up the dummy file\n        if os.path.exists(seq_file):\n            os.remove(seq_file)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>After create the <code>inference.py</code> script, run the following code to do inference: <pre><code>python inference.py\n</code></pre></p> <p>A user-friendly Jupyter Notebook is also provided: example/notebooks/inference/inference.ipynb.</p>"},{"location":"user_guide/inference/basic_inference/#3-understanding-the-output","title":"3. Understanding the Output","text":"<p>The <code>infer()</code> method returns a dictionary where each key is the index of a sequence and the value contains its prediction details.</p> <pre><code>{\n    \"0\": {\n        \"sequence\": \"GCACTTTACTTAAAGTA...\",\n        \"label\": \"positive\",\n        \"scores\": {\n            \"negative\": 0.02738,\n            \"positive\": 0.97261\n        }\n    },\n    \"1\": {\n        \"sequence\": \"TTGTCATCACATTTGAT...\",\n        \"label\": \"negative\",\n        \"scores\": {\n            \"negative\": 0.99983,\n            \"positive\": 0.00016\n        }\n    }\n}\n</code></pre> <ul> <li><code>sequence</code>: The input DNA sequence (if <code>keep_seqs</code> is <code>True</code> during data loading).</li> <li><code>label</code>: The final predicted label, based on the <code>task.threshold</code> from your config. For a binary task, this would be one of the <code>label_names</code>.</li> <li><code>scores</code>: The raw probabilities for each class. This gives you a measure of the model's confidence.</li> </ul> <p>If <code>evaluate=True</code>, a second dictionary containing performance metrics (like accuracy, F1-score, AUROC) is also returned.</p>"},{"location":"user_guide/inference/basic_inference/#4-best-practices-and-performance","title":"4. Best Practices and Performance","text":""},{"location":"user_guide/inference/basic_inference/#error-handling","title":"Error Handling","text":"<ul> <li>FileNotFoundError: Always wrap file-based inference in a <code>try...except</code> block to handle cases where the input file doesn't exist.</li> <li>OutOfMemoryError: If you get a CUDA out-of-memory error, the primary solution is to reduce <code>batch_size</code> in your <code>inference_config.yaml</code>.</li> </ul>"},{"location":"user_guide/inference/basic_inference/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Use a GPU: For any serious workload, a GPU is essential. Set <code>device: auto</code> or <code>device: cuda</code>.</li> <li>Tune <code>batch_size</code>: Find the largest <code>batch_size</code> that fits in your GPU memory to maximize throughput.</li> <li>Enable FP16/BF16: If you have a modern NVIDIA GPU (Ampere architecture or newer), setting <code>use_fp16: true</code> or <code>use_bf16: true</code> can provide a significant speedup with minimal impact on accuracy.</li> <li>Increase <code>num_workers</code>: If you notice your GPU is often waiting for data, increasing <code>num_workers</code> can help speed up data loading, especially for large files.</li> </ul>"},{"location":"user_guide/inference/basic_inference/#5-common-questions-faq","title":"5. Common Questions (FAQ)","text":"<p>Q: Why are my predictions all the same? A: This can happen if the model is not well-suited for your data or if the input sequences are too different from what it was trained on. Check that the model you loaded is appropriate for your task.</p> <p>Q: How do I get hidden states or attention weights for model interpretability? A: The <code>infer()</code> method has <code>output_hidden_states=True</code> and <code>output_attentions=True</code> flags. Setting these will return embeddings and attention scores, which can be accessed via <code>inference_engine.embeddings</code>. Be aware that this consumes a large amount of memory.</p> <p>Q: Can I run inference on a FASTA file? A: Yes. The <code>infer_file</code> method automatically handles <code>.fasta</code>, <code>.fa</code>, <code>.csv</code>, <code>.tsv</code>, and <code>.txt</code> files. For FASTA, the sequence is read directly. For CSV/TSV, you must specify the <code>seq_col</code>. Other structured formats such as pickle, arrow, parquet, etc. are also supported.</p>"},{"location":"user_guide/inference/basic_inference/#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Inference - Explore advanced inference features</li> <li>Performance Optimization - Optimize inference performance</li> <li>Visualization - Learn about result visualization</li> <li>Inference Troubleshooting - Common inference issues and solutions</li> </ul>"},{"location":"user_guide/inference/getting_started/","title":"Getting Started with DNALLM Inference","text":"<p>This guide provides a comprehensive introduction to performing inference with DNALLM. You will learn what inference is, why it's crucial for DNA sequence analysis, and how to set up your environment to get started.</p>"},{"location":"user_guide/inference/getting_started/#1-overview","title":"1. Overview","text":""},{"location":"user_guide/inference/getting_started/#what-is-inference","title":"What is Inference?","text":"<p>Inference is the process of using a trained machine learning model to make predictions on new, unseen data. In the context of DNALLM, it means applying a pre-trained DNA language model to analyze DNA sequences, predict their functional properties (like whether a sequence is a promoter), or extract meaningful biological features.</p>"},{"location":"user_guide/inference/getting_started/#why-is-it-important","title":"Why is it Important?","text":"<p>Inference is the key to unlocking the practical value of a trained model. For genomics, it enables:</p> <ul> <li>High-throughput sequence analysis: Rapidly screen thousands or millions of DNA sequences.</li> <li>Functional annotation: Predict the function of unknown DNA elements.</li> <li>Feature extraction: Convert raw DNA sequences into rich numerical representations (embeddings) for downstream tasks.</li> <li>In-silico experiments: Test hypotheses about sequence function without costly and time-consuming lab work.</li> </ul>"},{"location":"user_guide/inference/getting_started/#2-environment-setup","title":"2. Environment Setup","text":"<p>Before you can run inference, you need to install DNALLM and its dependencies.</p>"},{"location":"user_guide/inference/getting_started/#installation","title":"Installation","text":"<p>We recommend creating a virtual environment to avoid conflicts with other packages.</p> <pre><code># Create and activate a virtual environment (optional but recommended)\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install DNALLM from the source\n# Make sure you are at the root of the DNALLM project\npip install .\n</code></pre> <p>For GPU support, ensure you have a compatible version of PyTorch installed for your CUDA version. You can find installation instructions on the official PyTorch website.</p>"},{"location":"user_guide/inference/getting_started/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>CPU: Inference can run on a standard CPU, but it will be significantly slower for large datasets or models.</li> <li>GPU: A modern NVIDIA GPU with at least 8GB of VRAM is highly recommended for optimal performance. DNALLM also supports Apple Silicon (MPS).</li> </ul>"},{"location":"user_guide/inference/getting_started/#3-configuration","title":"3. Configuration","text":"<p>Inference behavior is controlled via a YAML configuration file. This file allows you to specify the model, task, and inference parameters without changing the code.</p> <p>Here is a basic configuration file (<code>inference_config.yaml</code>):</p> <pre><code># example/notebooks/inference/inference_config.yaml\n\ninference:\n  batch_size: 16         # Number of sequences to process at once. Adjust based on GPU memory.\n  device: auto           # 'auto', 'cpu', 'cuda', 'mps'. 'auto' will pick the best available.\n  max_length: 512        # Maximum sequence length. Sequences longer than this will be truncated.\n  num_workers: 4         # Number of CPU workers for data loading.\n  output_dir: ./results  # Directory to save predictions and metrics.\n  use_fp16: false        # Use half-precision for faster inference on compatible GPUs (e.g., NVIDIA Ampere).\n\ntask:\n  num_labels: 2          # Number of output classes (e.g., 2 for binary classification).\n  task_type: binary      # 'binary', 'multiclass', 'multilabel', 'regression'.\n  threshold: 0.5         # Probability threshold for binary/multilabel classification.\n  label_names:           # Optional: Human-readable names for labels.\n    - \"negative\"\n    - \"positive\"\n</code></pre> <p>Key Parameters:</p> <ul> <li><code>inference.batch_size</code>: The most critical parameter for performance and memory. Increase it to maximize GPU utilization, but decrease it if you encounter out-of-memory errors.</li> <li><code>inference.device</code>: Set to <code>auto</code> for automatic detection.</li> <li><code>task.task_type</code>: Must match the task the model was fine-tuned for.</li> </ul> <p>Next, proceed to the Basic Inference Tutorial to see how to use this configuration to run predictions.</p>"},{"location":"user_guide/inference/getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Inference - Learn basic inference techniques</li> <li>Advanced Inference - Explore advanced inference features</li> <li>Performance Optimization - Optimize inference performance</li> <li>Inference Troubleshooting - Common inference issues and solutions</li> </ul>"},{"location":"user_guide/inference/mutagenesis_analysis/","title":"Sequence Mutation Analysis with DNALLM","text":"<p>This tutorial provides a comprehensive guide to performing in silico mutagenesis analysis using the <code>dnallm.Mutagenesis</code> class. This powerful tool allows you to systematically introduce mutations into a DNA sequence and evaluate their impact on a model's predictions, providing deep insights into sequence-function relationships and model interpretability.</p> <p>We will cover: - Zero-shot Inference: Using pre-trained models without fine-tuning. - Mutation Analysis with Base Models: Scoring mutations with Masked Language Models (MLM) and Causal Language Models (CLM). - Mutation Analysis with Fine-tuned Models: Analyzing mutation effects on classification and regression tasks. - Specialized Models: Handling unique models like EVO for scoring.</p> <p>Prerequisites: - Familiarity with Basic Inference. - An understanding of Advanced Inference concepts is helpful.</p>"},{"location":"user_guide/inference/mutagenesis_analysis/#1-zero-shot-inference-overview","title":"1. Zero-shot Inference Overview","text":"<p>Zero-shot inference is the ability of a large language model to perform tasks it was not explicitly trained for. In genomics, this means we can use a base, pre-trained model (like a CLM or MLM) to infer the \"fitness\" or \"naturalness\" of a DNA sequence without fine-tuning it on a specific labeled dataset.</p> <p>The core idea is that a model trained on a vast corpus of genomic data has learned the underlying \"grammar\" of DNA. Sequences that are more \"grammatical\" or \"likely\" according to the model are often more biologically functional. The <code>Mutagenesis</code> class leverages this by measuring how much a mutation perturbs a sequence's likelihood score.</p>"},{"location":"user_guide/inference/mutagenesis_analysis/#2-in-silico-mutagenesis-overview","title":"2. In Silico Mutagenesis Overview","text":"<p>In silico mutagenesis is the computational equivalent of saturation mutagenesis in the lab. Instead of physically creating and testing every possible mutation, we generate them computationally and use a trained model to predict their effects.</p> <p>The <code>dnallm.Mutagenesis</code> class automates this process: 1.  Mutation Generation: It takes a reference sequence and creates a dataset of mutated sequences, including single nucleotide substitutions, deletions, and insertions. 2.  Model-based Scoring: It runs inference on the original and all mutated sequences. 3.  Effect Calculation: It compares the prediction scores of mutated sequences to the original to quantify the impact of each mutation. 4.  Visualization: It provides tools to plot the results as intuitive heatmaps and effect plots.</p>"},{"location":"user_guide/inference/mutagenesis_analysis/#3-mutation-analysis-with-base-models-clm-mlm","title":"3. Mutation Analysis with Base Models (CLM &amp; MLM)","text":"<p>Using pre-trained base models is a powerful zero-shot approach to identify functionally important regions in a sequence. The <code>Mutagenesis</code> class supports two primary scoring algorithms for this purpose.</p>"},{"location":"user_guide/inference/mutagenesis_analysis/#scoring-with-masked-language-models-mlm","title":"Scoring with Masked Language Models (MLM)","text":"<p>For MLMs (e.g., DNABERT), we use a Pseudo-Log-Likelihood (PLL) score. The <code>mlm_evaluate()</code> method calculates this by: 1. Iterating through each token in the sequence. 2. Masking one token at a time. 3. Asking the model to predict the original token. 4. Summing the log-probabilities of the correct predictions.</p> <p>A higher PLL score indicates the sequence is more \"expected\" by the model. A mutation that causes a large drop in PLL is likely deleterious.</p> <p>Usage: Set <code>task_type: \"mask\"</code> in your configuration.</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer, Mutagenesis\n\n# Use a config with task_type: \"mask\"\nconfigs = load_config(\"config_mlm.yaml\") \n\nmodel, tokenizer = load_model_and_tokenizer(\n    \"InstaDeepAI/nucleotide-transformer-500m-human-ref\", \n    task_config=configs['task']\n)\n\nmut_analyzer = Mutagenesis(model=model, tokenizer=tokenizer, config=configs)\n\nsequence = \"GATTACA...\" # Your sequence of interest\nmut_analyzer.mutate_sequence(sequence, replace_mut=True)\n\n# The evaluate() method will automatically use mlm_evaluate()\npredictions = mut_analyzer.evaluate()\n\nmut_analyzer.plot(predictions, save_path=\"./results/mlm_mut_effects.pdf\")\n</code></pre>"},{"location":"user_guide/inference/mutagenesis_analysis/#scoring-with-causal-language-models-clm","title":"Scoring with Causal Language Models (CLM)","text":"<p>For CLMs (e.g., DNAGPT, Evo), we calculate the log-probability of the entire sequence. The <code>clm_evaluate()</code> method does this by: 1. Processing the sequence token by token. 2. At each position, calculating the log-probability of the correct next token given the preceding context. 3. Summing these log-probabilities.</p> <p>This score represents how likely the model thinks the sequence is, from start to finish.</p> <p>Usage: Set <code>task_type: \"generation\"</code> in your configuration.</p> <pre><code># Use a config with task_type: \"generation\"\nconfigs = load_config(\"config_clm.yaml\")\n\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnagpt-BPE-promoter\", \n    task_config=configs['task']\n)\n\nmut_analyzer = Mutagenesis(model=model, tokenizer=tokenizer, config=configs)\n\nsequence = \"AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC\"\nmut_analyzer.mutate_sequence(sequence, replace_mut=True)\n\n# The evaluate() method will automatically use clm_evaluate()\npredictions = mut_analyzer.evaluate()\n\nmut_analyzer.plot(predictions, save_path=\"./results/clm_mut_effects.pdf\")\n</code></pre>"},{"location":"user_guide/inference/mutagenesis_analysis/#4-mutation-analysis-with-fine-tuned-models","title":"4. Mutation Analysis with Fine-tuned Models","text":"<p>When you have a model fine-tuned for a specific task (e.g., predicting promoter strength), you can measure how mutations affect its output. This is a direct way to map sequence positions to functional outcomes.</p> <p>The <code>evaluate()</code> method's <code>strategy</code> parameter is key here. It defines how the final \"mutation effect score\" is calculated from the model's output (which can be multi-dimensional for multi-class or multi-label tasks).</p>"},{"location":"user_guide/inference/mutagenesis_analysis/#score-normalization-strategies","title":"Score Normalization Strategies","text":"<p>The effect of a mutation is measured as the log2 fold change (<code>logfc</code>) between the mutated sequence's score and the original sequence's score. The <code>strategy</code> parameter determines which value from the <code>logfc</code> array to use as the final score for plotting.</p> <ul> <li><code>strategy=\"last\"</code> (default): Uses the <code>logfc</code> of the last class. Ideal for binary or regression tasks.</li> <li><code>strategy=\"first\"</code>: Uses the <code>logfc</code> of the first class.</li> <li><code>strategy=\"mean\"</code>: Averages the <code>logfc</code> across all classes.</li> <li><code>strategy=\"sum\"</code>: Sums the <code>logfc</code> across all classes.</li> <li><code>strategy=&lt;int&gt;</code>: Uses the <code>logfc</code> at a specific class index.</li> <li><code>strategy=\"max\"</code>: Uses the <code>logfc</code> of the class that had the highest raw score in the original prediction.</li> </ul> <p>Example: Regression Model</p> <p>Let's analyze a model that predicts promoter strength (a regression task).</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer, Mutagenesis\n\n# Config for a regression task\nconfigs = load_config(\"inference_config.yaml\")\n\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnagpt-BPE-promoter_strength_protoplast\",\n    task_config=configs['task'],\n    source=\"modelscope\"\n)\n\nmut_analyzer = Mutagenesis(model=model, tokenizer=tokenizer, config=configs)\n\nsequence = \"AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC\"\nmut_analyzer.mutate_sequence(sequence, replace_mut=True)\n\n# For regression, 'last' or 'mean' are good choices.\npredictions = mut_analyzer.evaluate(strategy=\"mean\")\n\nmut_analyzer.plot(predictions, save_path=\"./results/finetuned_mut_effects.pdf\")\n</code></pre>"},{"location":"user_guide/inference/mutagenesis_analysis/#5-special-models-eg-evo","title":"5. Special Models (e.g., EVO)","text":"<p>The <code>Mutagenesis</code> class has built-in support for specialized generative models like Evo-1 and Evo-2. These models have their own optimized <code>scoring</code> methods.</p> <p>When an Evo model is detected, <code>mutagenesis.evaluate()</code> automatically calls <code>inference_engine.scoring()</code> instead of the standard <code>batch_infer()</code>. The <code>strategy</code> parameter is passed to the <code>reduce_method</code> of the scoring function, typically with <code>\"mean\"</code> or <code>\"sum\"</code> being the most relevant options.</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer, Mutagenesis\n\n# Config for a generation task\nconfigs = load_config(\"inference_evo_config.yaml\")\n\n# Load an Evo model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"lgq12697/evo2_1b_base\", \n    task_config=configs['task'], \n    source=\"modelscope\"\n)\n\nmut_analyzer = Mutagenesis(model=model, tokenizer=tokenizer, config=configs)\n\nsequence = \"GATTACAGATTACAGATTACA\"\nmut_analyzer.mutate_sequence(sequence, replace_mut=True)\n\n# 'mean' or 'sum' are the most effective strategies for Evo models\npredictions = mut_analyzer.evaluate(strategy=\"mean\")\n\nmut_analyzer.plot(predictions, save_path=\"./results/evo_mut_effects.pdf\")\n</code></pre>"},{"location":"user_guide/inference/mutagenesis_analysis/#6-troubleshooting","title":"6. Troubleshooting","text":""},{"location":"user_guide/inference/mutagenesis_analysis/#problem-slow-performance","title":"Problem: Slow Performance","text":"<ul> <li>Cause: Mutagenesis generates many sequences (<code>len(seq) * 3</code> for substitutions).</li> <li>Solution:<ul> <li>Ensure you are using a GPU (<code>device: cuda</code>).</li> <li>Increase <code>inference.batch_size</code> in your config to the largest value that fits in VRAM.</li> <li>For very long sequences, consider analyzing only a specific region of interest by passing a subsequence to <code>mutate_sequence()</code>.</li> </ul> </li> </ul>"},{"location":"user_guide/inference/mutagenesis_analysis/#problem-out-of-memory-oom-errors","title":"Problem: Out-of-Memory (OOM) Errors","text":"<ul> <li>Solution: Reduce <code>batch_size</code>. This is the most common fix. Start with a small value like 4 or 8 and increase it gradually.</li> </ul>"},{"location":"user_guide/inference/mutagenesis_analysis/#problem-unexpected-or-flat-scores","title":"Problem: Unexpected or Flat Scores","text":"<ul> <li>Check Model &amp; Task Type: Ensure the <code>task_type</code> in your config matches the model you are using. Using a <code>regression</code> config with a base MLM model will not produce meaningful results.</li> <li>Check Sequence Length: If your sequence is much longer than the model's <code>max_length</code>, it will be truncated, and mutations outside the context window will have no effect.</li> <li>Model Sensitivity: Some models may not be sensitive to single-nucleotide changes. This is an insight in itself! You might need a model with higher resolution or one fine-tuned on a relevant task.</li> </ul>"},{"location":"user_guide/inference/mutagenesis_analysis/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Optimization - Optimize inference performance</li> <li>Visualization - Learn about result visualization</li> <li>Inference Troubleshooting - Common inference issues and solutions</li> </ul>"},{"location":"user_guide/inference/performance_optimization/","title":"Performance Optimization for DNALLM Inference","text":"<p>This guide provides advanced techniques and best practices for optimizing the performance of <code>DNAInference</code>. We will cover hardware acceleration, batching strategies, memory management, and profiling to help you achieve maximum throughput and efficiency.</p>"},{"location":"user_guide/inference/performance_optimization/#1-hardware-acceleration","title":"1. Hardware Acceleration","text":"<p>The choice of hardware and its configuration is the single most important factor for inference speed.</p>"},{"location":"user_guide/inference/performance_optimization/#gpu-utilization","title":"GPU Utilization","text":"<ul> <li>Priority: Always use a GPU if available. Set <code>device: cuda</code> (for NVIDIA) or <code>device: mps</code> (for Apple Silicon) in your <code>inference_config.yaml</code>. The <code>auto</code> setting is convenient but explicitly setting it is safer.</li> <li>FP16/BF16 (Half-Precision): For modern NVIDIA GPUs (Ampere architecture or newer), using half-precision can provide a 1.5-2x speedup with minimal accuracy loss.</li> <li><code>use_fp16: true</code>: Uses 16-bit floating-point. It is widely supported and offers a good balance of speed and precision.</li> <li><code>use_bf16: true</code>: Uses bfloat16. It is more resilient to underflow/overflow issues than FP16 but requires newer hardware (e.g., A100, H100).</li> </ul> <pre><code># In inference_config.yaml\ninference:\n  device: cuda\n  use_fp16: true # Enable for significant speedup on compatible GPUs\n</code></pre>"},{"location":"user_guide/inference/performance_optimization/#cpu-workers","title":"CPU Workers","text":"<p>The <code>num_workers</code> parameter in <code>inference_config.yaml</code> specifies how many CPU processes to use for data loading.</p> <ul> <li>If your GPU is waiting (underutilized), it might be bottlenecked by data preparation. Increase <code>num_workers</code>. A good starting point is half the number of your CPU cores.</li> <li>If you experience system instability or high CPU usage, decrease <code>num_workers</code>.</li> </ul> <pre><code>inference:\n  num_workers: 8 # Adjust based on your CPU cores and I/O speed\n</code></pre>"},{"location":"user_guide/inference/performance_optimization/#2-batching-and-sequence-length","title":"2. Batching and Sequence Length","text":""},{"location":"user_guide/inference/performance_optimization/#optimal-batch-size","title":"Optimal Batch Size","text":"<p>The <code>batch_size</code> is the most critical parameter to tune.</p> <ul> <li>Goal: Find the largest <code>batch_size</code> that fits into your GPU's VRAM without causing an out-of-memory (OOM) error.</li> <li>Strategy: Start with a moderate size (e.g., 16 or 32) and double it until you get an OOM error. Then, back off slightly.</li> <li>Dynamic Batching: For datasets with highly variable sequence lengths, processing sequences of similar lengths together can improve performance by reducing padding. You can implement this by pre-sorting your data before passing it to the inference engine.</li> </ul>"},{"location":"user_guide/inference/performance_optimization/#managing-sequence-length-max_length","title":"Managing Sequence Length (<code>max_length</code>)","text":"<p>The <code>max_length</code> parameter has a quadratic impact on memory usage and computation time for Transformer-based models.</p> <ul> <li>Set <code>max_length</code> appropriately: Do not set it to an unnecessarily large value. Analyze your dataset's length distribution and choose a value that covers the majority (e.g., 95th percentile) of your sequences.</li> <li>Truncation vs. Sliding Window: For sequences longer than <code>max_length</code>, the default behavior is truncation. For critical analysis where information at the end of a long sequence is important, you may need to implement a sliding window strategy manually.</li> </ul>"},{"location":"user_guide/inference/performance_optimization/#3-memory-management","title":"3. Memory Management","text":"<p>Inference with large models can be memory-intensive. Here are some techniques to manage VRAM usage.</p>"},{"location":"user_guide/inference/performance_optimization/#disabling-gradients","title":"Disabling Gradients","text":"<p>The <code>DNAInference</code> engine automatically uses <code>torch.no_grad()</code> during <code>batch_infer</code>, which is the most effective way to reduce memory consumption by disabling gradient calculations. You don't need to do this manually.</p>"},{"location":"user_guide/inference/performance_optimization/#selective-outputs","title":"Selective Outputs","text":"<p>Extracting hidden states or attention weights is extremely memory-intensive.</p> <ul> <li><code>output_hidden_states=True</code>: Stores the output of every layer for every token. Memory usage scales with <code>batch_size * num_layers * seq_len * hidden_dim</code>.</li> <li><code>output_attentions=True</code>: Stores attention matrices. Memory usage scales with <code>batch_size * num_layers * num_heads * seq_len * seq_len</code>.</li> </ul> <p>Recommendation: Only enable these options when you are actively performing model interpretability analysis on a small subset of your data. Do not use them for large-scale inference runs.</p>"},{"location":"user_guide/inference/performance_optimization/#estimating-memory-usage","title":"Estimating Memory Usage","text":"<p>Use the <code>estimate_memory_usage()</code> method to get a rough idea of how much VRAM a model will require for a given batch size and sequence length. This can help you anticipate and prevent OOM errors.</p> <pre><code># Assume inference_engine is initialized\nmemory_estimate = inference_engine.estimate_memory_usage(\n    batch_size=32,\n    sequence_length=1024\n)\nprint(memory_estimate)\n\n# Output might look like:\n# {\n#     'total_estimated_mb': '4586.7',\n#     'parameter_memory_mb': '1245.1',\n...\n# }\n</code></pre>"},{"location":"user_guide/inference/performance_optimization/#4-troubleshooting-performance-issues","title":"4. Troubleshooting Performance Issues","text":""},{"location":"user_guide/inference/performance_optimization/#problem-cuda-out-of-memory-oom","title":"Problem: CUDA Out-of-Memory (OOM)","text":"<ul> <li>Primary Solution: Reduce <code>batch_size</code>. This is almost always the fix.</li> <li>Secondary Solution: Reduce <code>max_length</code> if it's set excessively high.</li> <li>Last Resort: Disable <code>output_hidden_states</code> or <code>output_attentions</code> if they are enabled.</li> </ul>"},{"location":"user_guide/inference/performance_optimization/#problem-inference-is-slow-low-gpu-utilization","title":"Problem: Inference is Slow / Low GPU Utilization","text":"<ul> <li>Check <code>device</code>: Ensure the model is actually running on the GPU. Use <code>inference_engine.get_model_info()</code> to check the device.</li> <li>Increase <code>batch_size</code>: A small batch size may not be enough to saturate the GPU's computational capacity.</li> <li>Increase <code>num_workers</code>: The bottleneck might be data loading from the disk.</li> <li>Enable <code>use_fp16</code>: If your hardware supports it, this is a free performance boost.</li> <li>Check for I/O bottlenecks: If you are reading from a slow network drive or hard disk, data loading can become the main bottleneck.</li> </ul>"},{"location":"user_guide/inference/performance_optimization/#problem-sdpa-does-not-support-output_attentionstrue","title":"Problem: \"SDPA does not support <code>output_attentions=True</code>\"","text":"<p>This error occurs because the default high-performance attention implementation (<code>scaled_dot_product_attention</code>) is not compatible with outputting attention weights.</p> <ul> <li>Solution: The <code>DNAInference</code> engine attempts to handle this automatically by switching to a compatible implementation (<code>eager</code>).</li> <li>Manual Fix: You can force this change yourself for debugging:   <pre><code>inference_engine.force_eager_attention()\n</code></pre>   Be aware that the <code>eager</code> implementation is slower. Only use it when you need attention weights.</li> </ul> <p>By applying these optimization techniques, you can significantly improve the speed and efficiency of your DNALLM inference pipelines, enabling you to process larger datasets and iterate on your research faster.</p>"},{"location":"user_guide/inference/performance_optimization/#next-steps","title":"Next Steps","text":"<ul> <li>Visualization - Learn about result visualization</li> <li>Mutagenesis Analysis - Analyze mutation effects</li> <li>Inference Troubleshooting - Common inference issues and solutions</li> </ul>"},{"location":"user_guide/inference/visualization/","title":"Visualizing Inference Results","text":"<p>DNALLM provides a powerful and flexible plotting module built on top of Altair to help you visualize model performance, interpretability, and the effects of mutations. This tutorial will guide you through the various visualization types available and how to customize them.</p> <p>Prerequisites: - Familiarity with Basic Inference and running evaluations. - Understanding of Advanced Inference for interpretability plots. - Knowledge of Mutation Analysis for mutation effect plots.</p>"},{"location":"user_guide/inference/visualization/#1-overview-of-visualizable-metrics","title":"1. Overview of Visualizable Metrics","text":"<p>The type of visualization available depends on the task you are performing. DNALLM automatically prepares the correct data structures for plotting based on your task type.</p> <ul> <li> <p>Classification Tasks (<code>binary</code>, <code>multiclass</code>, <code>multilabel</code>):</p> <ul> <li>Bar Charts: For scalar metrics like <code>accuracy</code>, <code>precision</code>, <code>recall</code>, <code>f1</code>, <code>mcc</code>, <code>AUROC</code>, and <code>AUPRC</code>.</li> <li>Curve Plots: For Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves.</li> </ul> </li> <li> <p>Regression Tasks (<code>regression</code>):</p> <ul> <li>Bar Charts: For scalar metrics like <code>mse</code>, <code>mae</code>, and <code>r2</code>.</li> <li>Scatter Plots: To compare predicted values against true (experimental) values.</li> </ul> </li> <li> <p>Model Interpretability (from <code>DNAInference</code>):</p> <ul> <li>Heatmaps: For visualizing attention weights between tokens (<code>plot_attentions</code>).</li> <li>Scatter Plots: For visualizing high-dimensional embeddings in 2D using dimensionality reduction (<code>plot_embeddings</code>).</li> </ul> </li> <li> <p>Mutation Analysis (from <code>Mutagenesis</code>):</p> <ul> <li>Combined Plots: A set of vertically concatenated plots including a substitution heatmap, a max-effect bar chart, and a gain/loss line plot (<code>plot_muts</code>).</li> </ul> </li> </ul>"},{"location":"user_guide/inference/visualization/#2-supported-visualization-types","title":"2. Supported Visualization Types","text":"<p>The plotting functions are primarily accessed through the <code>Benchmark</code> and <code>Mutagenesis</code> classes, or directly from an <code>DNAInference</code> instance for interpretability plots.</p>"},{"location":"user_guide/inference/visualization/#bar-and-curvescatter-plots-from-benchmark","title":"Bar and Curve/Scatter Plots (from <code>Benchmark</code>)","text":"<p>When you run a benchmark using the <code>Benchmark</code> class, you can easily plot the results. The <code>benchmark.plot()</code> method intelligently creates the appropriate charts based on the task type defined in your configuration.</p> <p>Basic Usage:</p> <pre><code>from dnallm import load_config, Benchmark\n\n# Assume you have a benchmark_config.yaml\nconfigs = load_config(\"benchmark_config.yaml\")\nbenchmark = Benchmark(config=configs)\n\n# Run the benchmark to get results\nresults = benchmark.run()\n\n# Generate and save plots\n# This will create 'benchmark_metrics.pdf' and 'benchmark_roc.pdf'\npbar, pline = benchmark.plot(results, save_path=\"benchmark.pdf\")\n\n# To display in a notebook\npbar\n</code></pre> <p>This will produce a grid of bar charts for all scalar metrics and a combined plot for ROC and PR curves.</p>"},{"location":"user_guide/inference/visualization/#attention-map-heatmaps-from-dnainference","title":"Attention Map Heatmaps (from <code>DNAInference</code>)","text":"<p>To understand which parts of a sequence a model focuses on, you can visualize its attention weights. This requires running inference with <code>output_attentions=True</code>.</p> <p>Basic Usage:</p> <pre><code># Assume 'inference_engine' is an initialized DNAInference instance\n\n# 1. Run inference and collect attentions\nsequences = [\"GATTACAGATTACAGATTACA...\"]\ninference_engine.infer(sequences=sequences, output_attentions=True)\n\n# 2. Plot the attention map\nattention_plot = inference_engine.plot_attentions(\n    seq_idx=0,      # Plot for the first sequence\n    layer=-1,       # Last layer\n    head=-1,        # Last attention head\n    save_path=\"./results/attention_map.png\"\n)\n\n# To display in a notebook\nattention_plot\n</code></pre>"},{"location":"user_guide/inference/visualization/#embedding-visualization-from-dnainference","title":"Embedding Visualization (from <code>DNAInference</code>)","text":"<p>Visualize how a model represents sequences in its hidden layers. This can reveal how the model separates different classes. This requires running inference with <code>output_hidden_states=True</code>.</p> <p>Basic Usage:</p> <pre><code># Assume 'inference_engine' is initialized\n\n# 1. Run inference on a labeled file and collect hidden states\ninference_engine.infer(\n    file_path=\"path/to/labeled_data.csv\", \n    evaluate=True, \n    output_hidden_states=True\n)\n\n# 2. Plot the embeddings using t-SNE\nembedding_plot = inference_engine.plot_hidden_states(\n    reducer=\"t-SNE\",\n    save_path=\"./results/embedding_plot.png\"\n)\n\n# To display in a notebook\nembedding_plot\n</code></pre>"},{"location":"user_guide/inference/visualization/#mutation-effect-plots-from-mutagenesis","title":"Mutation Effect Plots (from <code>Mutagenesis</code>)","text":"<p>After performing an in silico mutagenesis experiment, you can visualize the impact of each mutation.</p> <p>Basic Usage:</p> <pre><code># Assume 'mut_analyzer' is an initialized Mutagenesis instance\n# and you have already run mutate_sequence() and evaluate()\n\npredictions = mut_analyzer.evaluate(strategy=\"mean\")\n\n# Generate and save the plot\nmutation_plot = mut_analyzer.plot(\n    predictions, \n    save_path=\"./results/mutation_effects.pdf\"\n)\n\n# To display in a notebook\nmutation_plot\n</code></pre>"},{"location":"user_guide/inference/visualization/#3-customizing-plot-parameters","title":"3. Customizing Plot Parameters","text":"<p>All plotting functions in <code>dnallm.inference.plot</code> share a set of common parameters that allow you to customize the appearance and output of your visualizations.</p> Parameter Type Description Example <code>width</code> <code>int</code> Sets the width of each individual plot in pixels. <code>width=300</code> <code>height</code> <code>int</code> Sets the height of each individual plot in pixels. <code>height=200</code> <code>ncols</code> <code>int</code> For grid plots (bars, embeddings), sets the number of columns. <code>ncols=3</code> <code>save_path</code> <code>str</code> Path to save the plot. The file extension (<code>.pdf</code>, <code>.png</code>, <code>.svg</code>, <code>.json</code>) determines the output format. If <code>None</code>, the plot is displayed. <code>save_path=\"my_plot.svg\"</code> <code>separate</code> <code>bool</code> If <code>True</code>, returns a dictionary of separate Altair chart objects instead of a single combined chart. Useful for custom layouts. <code>separate=True</code> <code>show_score</code> <code>bool</code> For bar plots, toggles the display of the metric value on each bar. <code>show_score=False</code>"},{"location":"user_guide/inference/visualization/#advanced-usage-example-customizing-benchmark-plots","title":"Advanced Usage Example: Customizing Benchmark Plots","text":"<p>Let's customize the output of our benchmark plot. We want larger, separate plots for each metric, saved in SVG format for high quality.</p> <pre><code># Assume 'benchmark' and 'results' are available\n\npbar_dict, pline_dict = benchmark.plot(\n    results,\n    width=400,\n    height=150,\n    separate=True, # Return separate plots\n    show_score=True\n)\n\n# Now you can save each plot individually\nfor metric, chart in pbar_dict.items():\n    chart.save(f\"./results/{metric}_chart.svg\")\n\nfor curve_type, chart in pline_dict.items():\n    chart.save(f\"./results/{curve_type}_curve.svg\")\n</code></pre>"},{"location":"user_guide/inference/visualization/#4-troubleshooting","title":"4. Troubleshooting","text":""},{"location":"user_guide/inference/visualization/#problem-plots-are-not-displayed-in-my-notebook","title":"Problem: Plots are not displayed in my notebook.","text":"<ul> <li>Solution: Ensure you are in a Jupyter Notebook or JupyterLab environment. The Altair library requires a rich frontend to render charts. If you are in a different environment, you must use the <code>save_path</code> argument to save the plot to a file.</li> </ul>"},{"location":"user_guide/inference/visualization/#problem-importerror-not-installed-eg-umap-sklearn","title":"Problem: <code>ImportError: ... not installed</code> (e.g., <code>umap</code>, <code>sklearn</code>).","text":"<ul> <li>Solution: Some plotting features have optional dependencies. <code>plot_embeddings</code> requires <code>scikit-learn</code> for PCA/t-SNE and <code>umap-learn</code> for UMAP. Install the required package:   <pre><code>pip install scikit-learn umap-learn\n</code></pre></li> </ul>"},{"location":"user_guide/inference/visualization/#problem-saved-plots-are-too-small-or-have-overlapping-text","title":"Problem: Saved plots are too small or have overlapping text.","text":"<ul> <li>Solution: Adjust the <code>width</code> and <code>height</code> parameters. For plots with many items (e.g., a bar chart with many models, or a mutation plot for a long sequence), you will need to increase the <code>width</code> significantly to prevent labels from overlapping.</li> </ul>"},{"location":"user_guide/inference/visualization/#next-steps","title":"Next Steps","text":"<ul> <li>Mutagenesis Analysis - Analyze mutation effects</li> <li>Performance Optimization - Optimize inference performance</li> <li>Inference Troubleshooting - Common inference issues and solutions</li> </ul>"},{"location":"user_guide/mcp/configuration/","title":"MCP Server Configuration","text":"<p>This guide provides comprehensive documentation for configuring the DNALLM MCP server, including all available options and best practices.</p>"},{"location":"user_guide/mcp/configuration/#configuration-structure","title":"Configuration Structure","text":"<p>The MCP server uses a hierarchical configuration system with two main files:</p> <ol> <li>Main Server Configuration (<code>mcp_server_config.yaml</code>) - Server settings and model definitions</li> <li>Individual Model Configurations - Task-specific settings for each model</li> </ol>"},{"location":"user_guide/mcp/configuration/#main-server-configuration","title":"Main Server Configuration","text":""},{"location":"user_guide/mcp/configuration/#server-settings","title":"Server Settings","text":"<pre><code>server:\n  host: \"0.0.0.0\"              # Host address to bind to\n  port: 8000                   # Port number\n  workers: 1                   # Number of worker processes\n  log_level: \"INFO\"            # Logging level\n  debug: false                 # Debug mode\n</code></pre> <p>Options: - <code>host</code>: IP address to bind to (<code>\"0.0.0.0\"</code> for all interfaces, <code>\"127.0.0.1\"</code> for localhost only) - <code>port</code>: Port number (1-65535) - <code>workers</code>: Number of worker processes (typically 1 for MCP servers) - <code>log_level</code>: <code>\"DEBUG\"</code>, <code>\"INFO\"</code>, <code>\"WARNING\"</code>, <code>\"ERROR\"</code>, <code>\"CRITICAL\"</code> - <code>debug</code>: Enable debug mode for detailed logging</p>"},{"location":"user_guide/mcp/configuration/#mcp-protocol-settings","title":"MCP Protocol Settings","text":"<pre><code>mcp:\n  name: \"DNALLM MCP Server\"     # Server name\n  version: \"0.1.0\"              # Server version\n  description: \"MCP server for DNA sequence prediction using fine-tuned models\"\n</code></pre> <p>Options: - <code>name</code>: Human-readable server name - <code>version</code>: Semantic version string - <code>description</code>: Server description for clients</p>"},{"location":"user_guide/mcp/configuration/#model-configuration","title":"Model Configuration","text":"<pre><code>models:\n  promoter_model:\n    name: \"promoter_model\"                    # Internal model name\n    model_name: \"Plant DNABERT BPE promoter\"  # Display name\n    config_path: \"./promoter_inference_config.yaml\"  # Path to model config\n    enabled: true                             # Whether model is loaded\n    priority: 1                               # Loading priority (1=highest)\n</code></pre> <p>Model Options: - <code>name</code>: Unique identifier for the model (used in API calls) - <code>model_name</code>: Human-readable name from model_info.yaml - <code>config_path</code>: Path to individual model configuration file - <code>enabled</code>: Whether to load this model at startup - <code>priority</code>: Loading order (lower numbers load first)</p>"},{"location":"user_guide/mcp/configuration/#multi-model-analysis-groups","title":"Multi-Model Analysis Groups","text":"<pre><code>multi_model:\n  promoter_analysis:\n    name: \"promoter_analysis\"\n    description: \"Comprehensive promoter analysis using multiple models\"\n    models: [\"promoter_model\", \"conservation_model\"]\n    enabled: true\n</code></pre> <p>Options: - <code>name</code>: Group identifier - <code>description</code>: Human-readable description - <code>models</code>: List of model names to include in this group - <code>enabled</code>: Whether this group is active</p>"},{"location":"user_guide/mcp/configuration/#sse-server-sent-events-configuration","title":"SSE (Server-Sent Events) Configuration","text":"<pre><code>sse:\n  heartbeat_interval: 30        # Heartbeat interval in seconds\n  max_connections: 100          # Maximum concurrent connections\n  connection_timeout: 300       # Connection timeout in seconds\n  enable_compression: true      # Enable gzip compression\n  mount_path: \"/mcp\"            # URL mount path\n  cors_origins: [\"*\"]           # CORS allowed origins\n  enable_heartbeat: true        # Enable heartbeat messages\n</code></pre> <p>SSE Options: - <code>heartbeat_interval</code>: How often to send heartbeat messages - <code>max_connections</code>: Maximum number of concurrent SSE connections - <code>connection_timeout</code>: How long to keep idle connections open - <code>enable_compression</code>: Enable gzip compression for SSE streams - <code>mount_path</code>: URL path where MCP endpoints are mounted - <code>cors_origins</code>: List of allowed CORS origins (<code>[\"*\"]</code> for all) - <code>enable_heartbeat</code>: Send periodic heartbeat messages to keep connections alive</p>"},{"location":"user_guide/mcp/configuration/#logging-configuration","title":"Logging Configuration","text":"<pre><code>logging:\n  level: \"INFO\"                 # Log level\n  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n  file: \"./logs/mcp_server.log\" # Log file path\n  max_size: \"10MB\"              # Maximum log file size\n  backup_count: 5               # Number of backup files to keep\n</code></pre> <p>Logging Options: - <code>level</code>: Minimum log level to record - <code>format</code>: Log message format string - <code>file</code>: Path to log file (relative to server working directory) - <code>max_size</code>: Maximum size before rotating log file - <code>backup_count</code>: Number of old log files to keep</p>"},{"location":"user_guide/mcp/configuration/#individual-model-configuration","title":"Individual Model Configuration","text":"<p>Each model requires its own configuration file. Here's the complete structure:</p>"},{"location":"user_guide/mcp/configuration/#task-configuration","title":"Task Configuration","text":"<pre><code>task:\n  task_type: \"binary\"           # Task type: binary, multiclass, regression, token\n  num_labels: 2                 # Number of output labels\n  label_names: [\"Not promoter\", \"Core promoter\"]  # Label names\n  threshold: 0.5                # Classification threshold\n  description: \"Predict whether a DNA sequence is a core promoter in plants\"\n</code></pre> <p>Task Types: - <code>binary</code>: Two-class classification (e.g., promoter vs non-promoter) - <code>multiclass</code>: Multi-class classification (e.g., open chromatin states) - <code>regression</code>: Continuous value prediction (e.g., promoter strength) - <code>token</code>: Token-level prediction (e.g., NER tasks)</p>"},{"location":"user_guide/mcp/configuration/#inference-configuration","title":"Inference Configuration","text":"<pre><code>inference:\n  batch_size: 16                # Batch size for inference\n  max_length: 512               # Maximum sequence length\n  device: \"cpu\"                 # Device: \"cpu\" or \"cuda\"\n  num_workers: 4                # Number of data loading workers\n  precision: \"float16\"          # Precision: \"float16\", \"float32\", \"bfloat16\"\n  output_dir: \"./outputs/promoter_predictions\"  # Output directory\n  save_predictions: true        # Save prediction results\n  save_hidden_states: false     # Save model hidden states\n  save_attentions: false        # Save attention weights\n</code></pre> <p>Inference Options: - <code>batch_size</code>: Number of sequences processed together (higher = faster, more memory) - <code>max_length</code>: Maximum input sequence length in tokens - <code>device</code>: <code>\"cpu\"</code> for CPU inference, <code>\"cuda\"</code> for GPU - <code>num_workers</code>: Number of parallel data loading processes - <code>precision</code>: Numerical precision (<code>\"float16\"</code> for speed, <code>\"float32\"</code> for accuracy) - <code>output_dir</code>: Directory to save prediction outputs - <code>save_predictions</code>: Whether to save prediction results to files - <code>save_hidden_states</code>: Whether to save model hidden states (for analysis) - <code>save_attentions</code>: Whether to save attention weights (for visualization)</p>"},{"location":"user_guide/mcp/configuration/#model-configuration_1","title":"Model Configuration","text":"<pre><code>model:\n  name: \"Plant DNABERT BPE promoter\"  # Model display name\n  path: \"zhangtaolab/plant-dnabert-BPE-promoter\"  # Model path/ID\n  source: \"modelscope\"               # Source: \"modelscope\" or \"huggingface\"\n  task_info:\n    architecture: \"DNABERT\"          # Model architecture\n    tokenizer: \"BPE\"                 # Tokenizer type\n    species: \"plant\"                 # Target species\n    task_category: \"promoter_prediction\"  # Task category\n    performance_metrics:\n      accuracy: 0.85                 # Model accuracy\n      f1_score: 0.82                # F1 score\n      precision: 0.80               # Precision\n      recall: 0.85                  # Recall\n</code></pre> <p>Model Options: - <code>name</code>: Human-readable model name - <code>path</code>: Model identifier (HuggingFace model ID or ModelScope path) - <code>source</code>: Model source platform - <code>task_info</code>: Metadata about the model and task - <code>performance_metrics</code>: Model performance statistics</p>"},{"location":"user_guide/mcp/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"user_guide/mcp/configuration/#complete-server-configuration","title":"Complete Server Configuration","text":"<pre><code># mcp_server_config.yaml\nserver:\n  host: \"0.0.0.0\"\n  port: 8000\n  workers: 1\n  log_level: \"INFO\"\n  debug: false\n\nmcp:\n  name: \"DNALLM MCP Server\"\n  version: \"0.1.0\"\n  description: \"MCP server for DNA sequence prediction using fine-tuned models\"\n\nmodels:\n  promoter_model:\n    name: \"promoter_model\"\n    model_name: \"Plant DNABERT BPE promoter\"\n    config_path: \"./promoter_inference_config.yaml\"\n    enabled: true\n    priority: 1\n\n  conservation_model:\n    name: \"conservation_model\"\n    model_name: \"Plant DNABERT BPE conservation\"\n    config_path: \"./conservation_inference_config.yaml\"\n    enabled: true\n    priority: 2\n\n  open_chromatin_model:\n    name: \"open_chromatin_model\"\n    model_name: \"Plant DNAMamba BPE open chromatin\"\n    config_path: \"./open_chromatin_inference_config.yaml\"\n    enabled: true\n    priority: 3\n\nmulti_model:\n  promoter_analysis:\n    name: \"promoter_analysis\"\n    description: \"Comprehensive promoter analysis using multiple models\"\n    models: [\"promoter_model\", \"conservation_model\"]\n    enabled: true\n\n  chromatin_analysis:\n    name: \"chromatin_analysis\"\n    description: \"Chromatin state analysis using multiple models\"\n    models: [\"open_chromatin_model\", \"conservation_model\"]\n    enabled: true\n\nsse:\n  heartbeat_interval: 30\n  max_connections: 100\n  connection_timeout: 300\n  enable_compression: true\n  mount_path: \"/mcp\"\n  cors_origins: [\"*\"]\n  enable_heartbeat: true\n\nlogging:\n  level: \"INFO\"\n  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n  file: \"./logs/mcp_server.log\"\n  max_size: \"10MB\"\n  backup_count: 5\n</code></pre>"},{"location":"user_guide/mcp/configuration/#binary-classification-model-config","title":"Binary Classification Model Config","text":"<pre><code># promoter_inference_config.yaml\ntask:\n  task_type: \"binary\"\n  num_labels: 2\n  label_names: [\"Not promoter\", \"Core promoter\"]\n  threshold: 0.5\n  description: \"Predict whether a DNA sequence is a core promoter in plants\"\n\ninference:\n  batch_size: 16\n  max_length: 512\n  device: \"cpu\"\n  num_workers: 4\n  precision: \"float16\"\n  output_dir: \"./outputs/promoter_predictions\"\n  save_predictions: true\n  save_hidden_states: false\n  save_attentions: false\n\nmodel:\n  name: \"Plant DNABERT BPE promoter\"\n  path: \"zhangtaolab/plant-dnabert-BPE-promoter\"\n  source: \"modelscope\"\n  task_info:\n    architecture: \"DNABERT\"\n    tokenizer: \"BPE\"\n    species: \"plant\"\n    task_category: \"promoter_prediction\"\n    performance_metrics:\n      accuracy: 0.85\n      f1_score: 0.82\n      precision: 0.80\n      recall: 0.85\n</code></pre>"},{"location":"user_guide/mcp/configuration/#multi-class-classification-model-config","title":"Multi-class Classification Model Config","text":"<pre><code># open_chromatin_inference_config.yaml\ntask:\n  task_type: \"multiclass\"\n  num_labels: 3\n  label_names: [\"Not open\", \"Full open\", \"Partial open\"]\n  threshold: 0.5\n  description: \"Predict chromatin accessibility state in plants\"\n\ninference:\n  batch_size: 8\n  max_length: 1024\n  device: \"cuda\"\n  num_workers: 2\n  precision: \"float16\"\n  output_dir: \"./outputs/open_chromatin_predictions\"\n  save_predictions: true\n  save_hidden_states: false\n  save_attentions: false\n\nmodel:\n  name: \"Plant DNAMamba BPE open chromatin\"\n  path: \"zhangtaolab/plant-dnamamba-BPE-open_chromatin\"\n  source: \"modelscope\"\n  task_info:\n    architecture: \"DNAMamba\"\n    tokenizer: \"BPE\"\n    species: \"plant\"\n    task_category: \"chromatin_prediction\"\n    performance_metrics:\n      accuracy: 0.82\n      f1_score: 0.79\n      precision: 0.78\n      recall: 0.80\n</code></pre>"},{"location":"user_guide/mcp/configuration/#regression-model-config","title":"Regression Model Config","text":"<pre><code># promoter_strength_inference_config.yaml\ntask:\n  task_type: \"regression\"\n  num_labels: 1\n  label_names: \"promoter strength in tobacco leaves\"\n  threshold: 0.5\n  description: \"Predict promoter strength in tobacco leaves\"\n\ninference:\n  batch_size: 32\n  max_length: 512\n  device: \"cuda\"\n  num_workers: 4\n  precision: \"float16\"\n  output_dir: \"./outputs/promoter_strength_predictions\"\n  save_predictions: true\n  save_hidden_states: false\n  save_attentions: false\n\nmodel:\n  name: \"Plant DNABERT BPE promoter strength leaf\"\n  path: \"zhangtaolab/plant-dnabert-BPE-promoter_strength_leaf\"\n  source: \"modelscope\"\n  task_info:\n    architecture: \"DNABERT\"\n    tokenizer: \"BPE\"\n    species: \"plant\"\n    task_category: \"promoter_strength_prediction\"\n    performance_metrics:\n      mse: 0.15\n      r2_score: 0.78\n      mae: 0.32\n</code></pre>"},{"location":"user_guide/mcp/configuration/#environment-variables","title":"Environment Variables","text":"<p>You can override configuration values using environment variables:</p> <pre><code># Server settings\nexport DNALLM_MCP_HOST=\"0.0.0.0\"\nexport DNALLM_MCP_PORT=\"8000\"\nexport DNALLM_MCP_LOG_LEVEL=\"DEBUG\"\n\n# Model settings\nexport DNALLM_MODEL_DEVICE=\"cuda\"\nexport DNALLM_MODEL_BATCH_SIZE=\"32\"\nexport DNALLM_MODEL_PRECISION=\"float16\"\n\n# Start server\npython -m dnallm.mcp.start_server\n</code></pre>"},{"location":"user_guide/mcp/configuration/#configuration-validation","title":"Configuration Validation","text":"<p>The server validates configuration files on startup. Common validation errors:</p> <ol> <li>Invalid Model Path: Ensure model paths exist in model_info.yaml</li> <li>Missing Configuration Files: Check that all referenced config files exist</li> <li>Invalid Task Type: Use only supported task types</li> <li>Invalid Device: Use only \"cpu\" or \"cuda\"</li> <li>Invalid Precision: Use only \"float16\", \"float32\", or \"bfloat16\"</li> </ol>"},{"location":"user_guide/mcp/configuration/#best-practices","title":"Best Practices","text":""},{"location":"user_guide/mcp/configuration/#1-model-selection","title":"1. Model Selection","text":"<ul> <li>Start with Plant DNABERT BPE models for general use</li> <li>Use Plant DNAMamba for long sequences or speed requirements</li> <li>Use Plant DNAGemma for maximum accuracy</li> <li>Choose models specifically fine-tuned for your task</li> </ul>"},{"location":"user_guide/mcp/configuration/#2-performance-optimization","title":"2. Performance Optimization","text":"<ul> <li>Use GPU (<code>device: \"cuda\"</code>) when available</li> <li>Increase batch size for GPU inference</li> <li>Use <code>float16</code> precision for speed</li> <li>Adjust <code>max_length</code> based on your sequence lengths</li> </ul>"},{"location":"user_guide/mcp/configuration/#3-memory-management","title":"3. Memory Management","text":"<ul> <li>Reduce batch size if running out of memory</li> <li>Use CPU inference for memory-constrained environments</li> <li>Enable model offloading if available</li> <li>Monitor memory usage with system tools</li> </ul>"},{"location":"user_guide/mcp/configuration/#4-production-deployment","title":"4. Production Deployment","text":"<ul> <li>Set appropriate log levels (<code>INFO</code> or <code>WARNING</code>)</li> <li>Configure proper CORS origins for security</li> <li>Use absolute paths for configuration files</li> <li>Set up log rotation and monitoring</li> <li>Configure proper resource limits</li> </ul>"},{"location":"user_guide/mcp/configuration/#troubleshooting-configuration","title":"Troubleshooting Configuration","text":""},{"location":"user_guide/mcp/configuration/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Configuration File Not Found <pre><code># Use absolute paths\npython -m dnallm.mcp.start_server --config /absolute/path/to/config.yaml\n</code></pre></p> </li> <li> <p>Model Loading Failed</p> </li> <li>Check model path in configuration</li> <li>Verify internet connection for model download</li> <li>Check available disk space</li> <li> <p>Review model_info.yaml for correct model names</p> </li> <li> <p>Invalid Configuration</p> </li> <li>Use YAML validator to check syntax</li> <li>Check indentation (use spaces, not tabs)</li> <li> <p>Verify all required fields are present</p> </li> <li> <p>Performance Issues</p> </li> <li>Reduce batch size if memory limited</li> <li>Use CPU if GPU memory insufficient</li> <li>Check device availability (<code>nvidia-smi</code> for GPU)</li> </ol>"},{"location":"user_guide/mcp/configuration/#configuration-testing","title":"Configuration Testing","text":"<p>Test your configuration before deployment:</p> <pre><code># Validate configuration syntax\npython -c \"import yaml; yaml.safe_load(open('mcp_server_config.yaml'))\"\n\n# Test model loading\npython -c \"from dnallm.mcp.config_manager import MCPConfigManager; MCPConfigManager('.', 'mcp_server_config.yaml')\"\n</code></pre>"},{"location":"user_guide/mcp/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Start the MCP Server with your configuration</li> <li>Read the Usage Guide for API documentation</li> <li>Review Troubleshooting for common issues</li> </ul>"},{"location":"user_guide/mcp/startserver/","title":"Starting the MCP Server","text":"<p>This guide covers how to start the DNALLM MCP (Model Context Protocol) server, including configuration setup, model selection, and different transport options.</p>"},{"location":"user_guide/mcp/startserver/#prerequisites","title":"Prerequisites","text":"<p>Before starting the MCP server, ensure you have:</p> <ul> <li>Python 3.8+ installed</li> <li>DNALLM package installed</li> <li>Sufficient system resources (RAM, disk space)</li> <li>Network access for model downloading (if using remote models)</li> </ul>"},{"location":"user_guide/mcp/startserver/#quick-start","title":"Quick Start","text":""},{"location":"user_guide/mcp/startserver/#1-basic-server-start","title":"1. Basic Server Start","text":"<p>Start the server with default configuration:</p> <pre><code># Using the module directly\npython -m dnallm.mcp.start_server\n\n# Or using the CLI entry point\ndnallm-mcp-server\n</code></pre>"},{"location":"user_guide/mcp/startserver/#2-start-with-custom-configuration","title":"2. Start with Custom Configuration","text":"<pre><code>python -m dnallm.mcp.start_server --config /path/to/your/config.yaml\n</code></pre>"},{"location":"user_guide/mcp/startserver/#3-start-with-different-transport-protocols","title":"3. Start with Different Transport Protocols","text":"<pre><code># STDIO transport (default) - for CLI tools\npython -m dnallm.mcp.start_server --transport stdio\n\n# SSE transport - for web applications\npython -m dnallm.mcp.start_server --transport sse --host 0.0.0.0 --port 8000\n\n# Streamable HTTP transport - for REST APIs\npython -m dnallm.mcp.start_server --transport streamable-http --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"user_guide/mcp/startserver/#configuration-setup","title":"Configuration Setup","text":""},{"location":"user_guide/mcp/startserver/#1-main-server-configuration","title":"1. Main Server Configuration","text":"<p>Create a main server configuration file (e.g., <code>mcp_server_config.yaml</code>):</p> <pre><code># MCP Server Configuration\nserver:\n  host: \"0.0.0.0\"\n  port: 8000\n  workers: 1\n  log_level: \"INFO\"\n  debug: false\n\nmcp:\n  name: \"DNALLM MCP Server\"\n  version: \"0.1.0\"\n  description: \"MCP server for DNA sequence prediction using fine-tuned models\"\n\n# Model configurations\nmodels:\n  promoter_model:\n    name: \"promoter_model\"\n    model_name: \"Plant DNABERT BPE promoter\"\n    config_path: \"./promoter_inference_config.yaml\"\n    enabled: true\n    priority: 1\n\n  conservation_model:\n    name: \"conservation_model\"\n    model_name: \"Plant DNABERT BPE conservation\"\n    config_path: \"./conservation_inference_config.yaml\"\n    enabled: true\n    priority: 2\n\n  open_chromatin_model:\n    name: \"open_chromatin_model\"\n    model_name: \"Plant DNAMamba BPE open chromatin\"\n    config_path: \"./open_chromatin_inference_config.yaml\"\n    enabled: true\n    priority: 3\n\n# SSE configuration\nsse:\n  heartbeat_interval: 30\n  max_connections: 100\n  connection_timeout: 300\n  enable_compression: true\n  mount_path: \"/mcp\"\n  cors_origins: [\"*\"]\n  enable_heartbeat: true\n\n# Logging configuration\nlogging:\n  level: \"INFO\"\n  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n  file: \"./logs/mcp_server.log\"\n  max_size: \"10MB\"\n  backup_count: 5\n</code></pre>"},{"location":"user_guide/mcp/startserver/#2-individual-model-configuration","title":"2. Individual Model Configuration","text":"<p>For each model, create a separate inference configuration file. Example for promoter prediction (<code>promoter_inference_config.yaml</code>):</p> <pre><code># Inference Model Configuration for Promoter Prediction\ntask:\n  task_type: \"binary\"\n  num_labels: 2\n  label_names: [\"Not promoter\", \"Core promoter\"]\n  threshold: 0.5\n  description: \"Predict whether a DNA sequence is a core promoter in plants\"\n\ninference:\n  batch_size: 16\n  max_length: 512\n  device: \"cpu\"  # or \"cuda\" for GPU\n  num_workers: 4\n  precision: \"float16\"\n  output_dir: \"./outputs/promoter_predictions\"\n  save_predictions: true\n  save_hidden_states: false\n  save_attentions: false\n\nmodel:\n  name: \"Plant DNABERT BPE promoter\"\n  path: \"zhangtaolab/plant-dnabert-BPE-promoter\"\n  source: \"modelscope\"  # or \"huggingface\"\n  task_info:\n    architecture: \"DNABERT\"\n    tokenizer: \"BPE\"\n    species: \"plant\"\n    task_category: \"promoter_prediction\"\n    performance_metrics:\n      accuracy: 0.85\n      f1_score: 0.82\n      precision: 0.80\n      recall: 0.85\n</code></pre>"},{"location":"user_guide/mcp/startserver/#model-selection","title":"Model Selection","text":""},{"location":"user_guide/mcp/startserver/#available-models","title":"Available Models","text":"<p>The DNALLM MCP server supports a wide range of pre-trained and fine-tuned models. Here are the recommended models from the zhangtaolab organization:</p>"},{"location":"user_guide/mcp/startserver/#binary-classification-models","title":"Binary Classification Models","text":"<p>Promoter Prediction: - <code>zhangtaolab/plant-dnabert-BPE-promoter</code> (Recommended) - <code>zhangtaolab/plant-dnagpt-BPE-promoter</code> - <code>zhangtaolab/plant-dnamamba-BPE-promoter</code></p> <p>Conservation Prediction: - <code>zhangtaolab/plant-dnabert-BPE-conservation</code> (Recommended) - <code>zhangtaolab/plant-dnagpt-BPE-conservation</code> - <code>zhangtaolab/plant-dnamamba-BPE-conservation</code></p> <p>lncRNA Prediction: - <code>zhangtaolab/plant-dnabert-BPE-lncRNAs</code> (Recommended) - <code>zhangtaolab/plant-dnagpt-BPE-lncRNAs</code></p>"},{"location":"user_guide/mcp/startserver/#multi-class-classification-models","title":"Multi-class Classification Models","text":"<p>Open Chromatin Prediction: - <code>zhangtaolab/plant-dnabert-BPE-open_chromatin</code> (Recommended) - <code>zhangtaolab/plant-dnamamba-BPE-open_chromatin</code></p> <p>tRNA Detection: - <code>zhangtaolab/tRNADetector</code> (Plant DNAMamba-based)</p>"},{"location":"user_guide/mcp/startserver/#regression-models","title":"Regression Models","text":"<p>Promoter Strength Prediction: - <code>zhangtaolab/plant-dnabert-BPE-promoter_strength_leaf</code> (Tobacco leaves) - <code>zhangtaolab/plant-dnabert-BPE-promoter_strength_protoplast</code> (Maize protoplasts)</p>"},{"location":"user_guide/mcp/startserver/#model-selection-guidelines","title":"Model Selection Guidelines","text":"<ol> <li>For General Use: Start with Plant DNABERT BPE models - they offer good balance of performance and speed</li> <li>For Speed: Use Plant DNAMamba models - they're faster for long sequences</li> <li>For Accuracy: Use Plant DNAGemma models - they often provide the best accuracy</li> <li>For Specific Tasks: Choose models specifically fine-tuned for your task</li> </ol>"},{"location":"user_guide/mcp/startserver/#example-model-configuration","title":"Example Model Configuration","text":"<pre><code>models:\n  # Promoter prediction with Plant DNABERT\n  promoter_model:\n    name: \"promoter_model\"\n    model_name: \"Plant DNABERT BPE promoter\"\n    config_path: \"./promoter_inference_config.yaml\"\n    enabled: true\n    priority: 1\n\n  # Conservation prediction with Plant DNAMamba\n  conservation_model:\n    name: \"conservation_model\"\n    model_name: \"Plant DNAMamba BPE conservation\"\n    config_path: \"./conservation_inference_config.yaml\"\n    enabled: true\n    priority: 2\n\n  # Open chromatin with Plant DNABERT\n  open_chromatin_model:\n    name: \"open_chromatin_model\"\n    model_name: \"Plant DNABERT BPE open chromatin\"\n    config_path: \"./open_chromatin_inference_config.yaml\"\n    enabled: true\n    priority: 3\n</code></pre>"},{"location":"user_guide/mcp/startserver/#transport-protocols","title":"Transport Protocols","text":""},{"location":"user_guide/mcp/startserver/#1-stdio-transport-default","title":"1. STDIO Transport (Default)","text":"<p>Use Case: Command-line tools, automation scripts</p> <pre><code>python -m dnallm.mcp.start_server --transport stdio\n</code></pre> <p>Features: - Standard input/output communication - Suitable for CLI integration - No network configuration needed</p>"},{"location":"user_guide/mcp/startserver/#2-sse-transport-recommended-for-web-apps","title":"2. SSE Transport (Recommended for Web Apps)","text":"<p>Use Case: Real-time web applications, interactive tools</p> <pre><code>python -m dnallm.mcp.start_server --transport sse --host 0.0.0.0 --port 8000\n</code></pre> <p>Features: - Real-time progress updates - Web-friendly protocol - Supports streaming predictions - Endpoints: <code>/sse</code> and <code>/mcp/messages/</code></p>"},{"location":"user_guide/mcp/startserver/#3-streamable-http-transport","title":"3. Streamable HTTP Transport","text":"<p>Use Case: REST API integration, HTTP clients</p> <pre><code>python -m dnallm.mcp.start_server --transport streamable-http --host 0.0.0.0 --port 8000\n</code></pre> <p>Features: - Standard HTTP protocol - RESTful API interface - Easy integration with existing HTTP clients</p>"},{"location":"user_guide/mcp/startserver/#command-line-options","title":"Command Line Options","text":"<pre><code>python -m dnallm.mcp.start_server [OPTIONS]\n\nOptions:\n  --config, -c PATH          Path to MCP server configuration file\n  --host TEXT                Host to bind the server to (default: 0.0.0.0)\n  --port INTEGER             Port to bind the server to (default: 8000)\n  --transport [stdio|sse|streamable-http]  Transport protocol (default: stdio)\n  --log-level [DEBUG|INFO|WARNING|ERROR|CRITICAL]  Logging level (default: INFO)\n  --version                  Show version information\n  --help                     Show help message\n</code></pre>"},{"location":"user_guide/mcp/startserver/#testing-the-server","title":"Testing the Server","text":""},{"location":"user_guide/mcp/startserver/#1-health-check","title":"1. Health Check","text":"<pre><code># For SSE/HTTP transports\ncurl http://localhost:8000/mcp/messages/?session_id=test \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/call\", \"params\": {\"name\": \"health_check\", \"arguments\": {}}}'\n\n# For STDIO transport\necho '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/call\", \"params\": {\"name\": \"health_check\", \"arguments\": {}}}' | python -m dnallm.mcp.start_server\n</code></pre>"},{"location":"user_guide/mcp/startserver/#2-list-available-models","title":"2. List Available Models","text":"<pre><code>curl http://localhost:8000/mcp/messages/?session_id=test \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/call\", \"params\": {\"name\": \"list_loaded_models\", \"arguments\": {}}}'\n</code></pre>"},{"location":"user_guide/mcp/startserver/#3-test-dna-prediction","title":"3. Test DNA Prediction","text":"<pre><code>curl http://localhost:8000/mcp/messages/?session_id=test \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/call\", \"params\": {\"name\": \"dna_sequence_predict\", \"arguments\": {\"sequence\": \"ATCGATCGATCG\", \"model_name\": \"promoter_model\"}}}'\n</code></pre>"},{"location":"user_guide/mcp/startserver/#python-client-example","title":"Python Client Example","text":"<p>Here's how to connect to the MCP server using Python:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ollama import OllamaProvider\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\n# Create MCP server connection\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')\n\n# Create agent with MCP server tools\nagent = Agent(\n    OpenAIChatModel(\n        model_name='qwen3:latest',\n        provider=OllamaProvider(base_url='http://localhost:11434/v1'),\n    ),\n    toolsets=[server],\n    system_prompt='''You are a DNA analysis assistant with access to specialized DNA analysis tools via MCP server.\n\nWhen analyzing a DNA sequence, you should:\n1. First call _list_loaded_models to see what models are available\n2. Then call _dna_multi_model_predict with the DNA sequence and appropriate model names\n3. Interpret and explain the results in a comprehensive way\n\nAvailable tools should include:\n- _list_loaded_models: Lists available DNA analysis models\n- _dna_multi_model_predict: Predicts DNA sequence properties using multiple models\n\nAlways use the tools to provide accurate analysis.'''\n)\n\n# Analyze DNA sequence\nasync def analyze_dna_sequence():\n    async with agent:\n        result = await agent.run(\n            'What is the function of following DNA sequence? Please analyze it thoroughly using all available models: AGAAAAAACATGACAAGAAATCGATAATAATACAAAAGCTATGATGGTGTGCAATGTCCGTGTGCATGCGTGCACGCATTGCAACCGGCCCAAATCAAGGCCCATCGATCAGTGAATACTCATGGGCCGGCGGCCCACCACCGCTTCATCTCCTCCTCCGACGACGGGAGCACCCCCGCCGCATCGCCACCGACGAGGAGGAGGCCATTGCCGGCGGCGCCCCCGGTGAGCCGCTGCACCACGTCCCTGA'\n        )\n        return result\n\n# Run the analysis\nresult = await analyze_dna_sequence()\nprint(result.output)\n</code></pre>"},{"location":"user_guide/mcp/startserver/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide/mcp/startserver/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Port Already in Use <pre><code># Check what's using the port\nlsof -i :8000\n\n# Use a different port\npython -m dnallm.mcp.start_server --port 8001\n</code></pre></p> </li> <li> <p>Model Loading Failed</p> </li> <li>Check internet connection</li> <li>Verify model name in configuration</li> <li>Check available disk space</li> <li> <p>Review logs for specific error messages</p> </li> <li> <p>Configuration File Not Found <pre><code># Use absolute path\npython -m dnallm.mcp.start_server --config /absolute/path/to/config.yaml\n\n# Or create a default config\npython -m dnallm.mcp.start_server --config ./mcp_server_config.yaml\n</code></pre></p> </li> <li> <p>Memory Issues</p> </li> <li>Reduce batch size in model configuration</li> <li>Use CPU instead of GPU if memory is limited</li> <li>Enable model offloading if available</li> </ol>"},{"location":"user_guide/mcp/startserver/#log-files","title":"Log Files","text":"<p>Check the log files for detailed error information:</p> <pre><code># Server logs\ntail -f logs/mcp_server.log\n\n# Model loading logs\ntail -f logs/dnallm.log\n</code></pre>"},{"location":"user_guide/mcp/startserver/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user_guide/mcp/startserver/#1-gpu-acceleration","title":"1. GPU Acceleration","text":"<p>Enable GPU acceleration in model configuration:</p> <pre><code>inference:\n  device: \"cuda\"  # Use GPU\n  precision: \"float16\"  # Use half precision\n  batch_size: 32  # Increase batch size for GPU\n</code></pre>"},{"location":"user_guide/mcp/startserver/#2-memory-optimization","title":"2. Memory Optimization","text":"<pre><code>inference:\n  device: \"cpu\"\n  precision: \"float32\"\n  batch_size: 8  # Reduce batch size\n  num_workers: 2  # Reduce workers\n</code></pre>"},{"location":"user_guide/mcp/startserver/#3-model-caching","title":"3. Model Caching","text":"<p>Models are automatically cached after first download. To clear cache:</p> <pre><code># Clear model cache\nrm -rf ~/.cache/huggingface/\nrm -rf ~/.cache/modelscope/\n</code></pre>"},{"location":"user_guide/mcp/startserver/#next-steps","title":"Next Steps","text":"<p>After successfully starting the MCP server:</p> <ol> <li>Read the Usage Guide for detailed API documentation</li> <li>Check the Configuration Reference for advanced settings</li> <li>Review Troubleshooting for common issues</li> </ol>"},{"location":"user_guide/mcp/startserver/#support","title":"Support","text":"<p>For additional help:</p> <ul> <li>Check the FAQ</li> <li>Review GitHub Issues</li> <li>Join our Discord Community</li> <li>Read the API Documentation</li> </ul>"},{"location":"user_guide/mcp/usage/","title":"MCP Server Usage Guide","text":"<p>This guide covers how to use the DNALLM MCP server, including API reference, client examples, and practical usage patterns.</p>"},{"location":"user_guide/mcp/usage/#overview","title":"Overview","text":"<p>The DNALLM MCP server provides DNA sequence analysis capabilities through the Model Context Protocol (MCP). It supports multiple transport protocols and offers both basic and streaming prediction modes.</p>"},{"location":"user_guide/mcp/usage/#available-tools","title":"Available Tools","text":""},{"location":"user_guide/mcp/usage/#basic-prediction-tools","title":"Basic Prediction Tools","text":""},{"location":"user_guide/mcp/usage/#dna_sequence_predict","title":"<code>dna_sequence_predict</code>","text":"<p>Predict a single DNA sequence using a specific model.</p> <p>Parameters: - <code>sequence</code> (string): DNA sequence to analyze (A, T, G, C only) - <code>model_name</code> (string): Name of the model to use</p> <p>Returns: - Prediction results with confidence scores and labels</p> <p>Example: <pre><code>{\n  \"sequence\": \"ATCGATCGATCG\",\n  \"model_name\": \"promoter_model\",\n  \"result\": {\n    \"prediction\": \"Core promoter\",\n    \"confidence\": 0.85,\n    \"probabilities\": {\n      \"Not promoter\": 0.15,\n      \"Core promoter\": 0.85\n    }\n  }\n}\n</code></pre></p>"},{"location":"user_guide/mcp/usage/#dna_batch_predict","title":"<code>dna_batch_predict</code>","text":"<p>Predict multiple DNA sequences using a single model.</p> <p>Parameters: - <code>sequences</code> (array): List of DNA sequences to analyze - <code>model_name</code> (string): Name of the model to use</p> <p>Returns: - Batch prediction results for all sequences</p> <p>Example: <pre><code>{\n  \"sequences\": [\"ATCGATCG\", \"GCTAGCTA\", \"TTAACCGG\"],\n  \"model_name\": \"promoter_model\",\n  \"results\": [\n    {\n      \"sequence\": \"ATCGATCG\",\n      \"prediction\": \"Core promoter\",\n      \"confidence\": 0.85\n    },\n    {\n      \"sequence\": \"GCTAGCTA\", \n      \"prediction\": \"Not promoter\",\n      \"confidence\": 0.72\n    },\n    {\n      \"sequence\": \"TTAACCGG\",\n      \"prediction\": \"Core promoter\", \n      \"confidence\": 0.91\n    }\n  ]\n}\n</code></pre></p>"},{"location":"user_guide/mcp/usage/#dna_multi_model_predict","title":"<code>dna_multi_model_predict</code>","text":"<p>Predict a single sequence using multiple models for comparison.</p> <p>Parameters: - <code>sequence</code> (string): DNA sequence to analyze - <code>model_names</code> (array, optional): List of model names to use (uses all loaded models if not specified)</p> <p>Returns: - Multi-model prediction results with consensus analysis</p> <p>Example: <pre><code>{\n  \"sequence\": \"ATCGATCGATCG\",\n  \"model_names\": [\"promoter_model\", \"conservation_model\"],\n  \"results\": {\n    \"promoter_model\": {\n      \"prediction\": \"Core promoter\",\n      \"confidence\": 0.85\n    },\n    \"conservation_model\": {\n      \"prediction\": \"Conserved\",\n      \"confidence\": 0.92\n    }\n  },\n  \"consensus\": {\n    \"promoter_consensus\": \"Core promoter\",\n    \"conservation_consensus\": \"Conserved\",\n    \"overall_confidence\": 0.88\n  }\n}\n</code></pre></p>"},{"location":"user_guide/mcp/usage/#streaming-tools-real-time-progress","title":"Streaming Tools (Real-time Progress)","text":""},{"location":"user_guide/mcp/usage/#dna_stream_predict","title":"<code>dna_stream_predict</code>","text":"<p>Stream single sequence prediction with real-time progress updates.</p> <p>Parameters: - <code>sequence</code> (string): DNA sequence to analyze - <code>model_name</code> (string): Name of the model to use - <code>stream_progress</code> (boolean, optional): Enable progress streaming (default: true)</p> <p>Returns: - Streaming prediction results with progress updates</p> <p>Progress Updates: - 0%: Starting prediction - 25%: Loading model and tokenizer - 75%: Processing prediction results - 100%: Prediction completed</p>"},{"location":"user_guide/mcp/usage/#dna_stream_batch_predict","title":"<code>dna_stream_batch_predict</code>","text":"<p>Stream batch prediction with progress updates.</p> <p>Parameters: - <code>sequences</code> (array): List of DNA sequences to analyze - <code>model_name</code> (string): Name of the model to use - <code>stream_progress</code> (boolean, optional): Enable progress streaming (default: true)</p> <p>Returns: - Streaming batch prediction results with per-sequence progress</p>"},{"location":"user_guide/mcp/usage/#dna_stream_multi_model_predict","title":"<code>dna_stream_multi_model_predict</code>","text":"<p>Stream multi-model prediction with progress updates.</p> <p>Parameters: - <code>sequence</code> (string): DNA sequence to analyze - <code>model_names</code> (array, optional): List of model names to use - <code>stream_progress</code> (boolean, optional): Enable progress streaming (default: true)</p> <p>Returns: - Streaming multi-model prediction results with per-model progress</p>"},{"location":"user_guide/mcp/usage/#model-management-tools","title":"Model Management Tools","text":""},{"location":"user_guide/mcp/usage/#list_loaded_models","title":"<code>list_loaded_models</code>","text":"<p>List all currently loaded models with their information.</p> <p>Parameters: None</p> <p>Returns: - List of loaded models with metadata</p> <p>Example: <pre><code>{\n  \"loaded_models\": [\n    {\n      \"name\": \"promoter_model\",\n      \"display_name\": \"Plant DNABERT BPE promoter\",\n      \"task_type\": \"binary\",\n      \"num_labels\": 2,\n      \"architecture\": \"DNABERT\",\n      \"tokenizer\": \"BPE\",\n      \"performance\": {\n        \"accuracy\": 0.85,\n        \"f1_score\": 0.82\n      }\n    }\n  ]\n}\n</code></pre></p>"},{"location":"user_guide/mcp/usage/#get_model_info","title":"<code>get_model_info</code>","text":"<p>Get detailed information about a specific model.</p> <p>Parameters: - <code>model_name</code> (string): Name of the model</p> <p>Returns: - Detailed model information including configuration and performance metrics</p>"},{"location":"user_guide/mcp/usage/#list_models_by_task_type","title":"<code>list_models_by_task_type</code>","text":"<p>List models filtered by task type.</p> <p>Parameters: - <code>task_type</code> (string): Task type to filter by (\"binary\", \"multiclass\", \"regression\", \"token\")</p> <p>Returns: - List of models matching the specified task type</p>"},{"location":"user_guide/mcp/usage/#get_all_available_models","title":"<code>get_all_available_models</code>","text":"<p>Get information about all available models (not just loaded ones).</p> <p>Parameters: None</p> <p>Returns: - Complete list of available models from model_info.yaml</p>"},{"location":"user_guide/mcp/usage/#health_check","title":"<code>health_check</code>","text":"<p>Perform health check on the MCP server.</p> <p>Parameters: None</p> <p>Returns: - Server health status and statistics</p> <p>Example: <pre><code>{\n  \"status\": \"healthy\",\n  \"loaded_models\": 3,\n  \"total_configured_models\": 3,\n  \"server_name\": \"DNALLM MCP Server\",\n  \"server_version\": \"0.1.0\",\n  \"uptime\": \"2h 15m 30s\"\n}\n</code></pre></p>"},{"location":"user_guide/mcp/usage/#client-examples","title":"Client Examples","text":""},{"location":"user_guide/mcp/usage/#python-with-pydantic-ai","title":"Python with Pydantic AI","text":"<pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ollama import OllamaProvider\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\n# Create MCP server connection\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')\n\n# Create agent with MCP server tools\nagent = Agent(\n    OpenAIChatModel(\n        model_name='qwen3:latest',\n        provider=OllamaProvider(base_url='http://localhost:11434/v1'),\n    ),\n    toolsets=[server],\n    system_prompt='''You are a DNA analysis assistant with access to specialized DNA analysis tools via MCP server.\n\nWhen analyzing a DNA sequence, you should:\n1. First call _list_loaded_models to see what models are available\n2. Then call _dna_multi_model_predict with the DNA sequence and appropriate model names\n3. Interpret and explain the results in a comprehensive way\n\nAvailable tools should include:\n- _list_loaded_models: Lists available DNA analysis models\n- _dna_multi_model_predict: Predicts DNA sequence properties using multiple models\n\nAlways use the tools to provide accurate analysis.'''\n)\n\n# Analyze DNA sequence\nasync def analyze_dna_sequence():\n    async with agent:\n        result = await agent.run(\n            'What is the function of following DNA sequence? Please analyze it thoroughly using all available models: AGAAAAAACATGACAAGAAATCGATAATAATACAAAAGCTATGATGGTGTGCAATGTCCGTGTGCATGCGTGCACGCATTGCAACCGGCCCAAATCAAGGCCCATCGATCAGTGAATACTCATGGGCCGGCGGCCCACCACCGCTTCATCTCCTCCTCCGACGACGGGAGCACCCCCGCCGCATCGCCACCGACGAGGAGGAGGCCATTGCCGGCGGCGCCCCCGGTGAGCCGCTGCACCACGTCCCTGA'\n        )\n        return result\n\n# Run the analysis\nresult = await analyze_dna_sequence()\nprint(result.output)\n</code></pre>"},{"location":"user_guide/mcp/usage/#python-with-mcp-client","title":"Python with MCP Client","text":"<pre><code>import asyncio\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nasync def main():\n    # Connect to MCP server via STDIO\n    server_params = StdioServerParameters(\n        command=\"python\",\n        args=[\"-m\", \"dnallm.mcp.start_server\"]\n    )\n\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            # Initialize the session\n            await session.initialize()\n\n            # List available models\n            models = await session.call_tool(\"list_loaded_models\", {})\n            print(f\"Available models: {models}\")\n\n            # Predict DNA sequence\n            result = await session.call_tool(\n                \"dna_sequence_predict\",\n                {\n                    \"sequence\": \"ATCGATCGATCG\",\n                    \"model_name\": \"promoter_model\"\n                }\n            )\n            print(f\"Prediction result: {result}\")\n\n            # Multi-model prediction\n            multi_result = await session.call_tool(\n                \"dna_multi_model_predict\",\n                {\n                    \"sequence\": \"ATCGATCGATCG\",\n                    \"model_names\": [\"promoter_model\", \"conservation_model\"]\n                }\n            )\n            print(f\"Multi-model result: {multi_result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"user_guide/mcp/usage/#python-with-sse-client","title":"Python with SSE Client","text":"<pre><code>import asyncio\nfrom mcp.client.sse import sse_client\n\nasync def main():\n    # Connect to SSE server\n    async with sse_client(\"http://localhost:8000/sse\") as (read, write):\n        # List available models\n        models = await read.call_tool(\"list_loaded_models\", {})\n        print(f\"Available models: {models}\")\n\n        # Test streaming prediction\n        result = await read.call_tool(\n            \"dna_stream_predict\",\n            {\n                \"sequence\": \"ATCGATCGATCG\",\n                \"model_name\": \"promoter_model\",\n                \"stream_progress\": True\n            }\n        )\n        print(f\"Streaming prediction result: {result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"user_guide/mcp/usage/#javascriptnodejs-client","title":"JavaScript/Node.js Client","text":"<pre><code>const { EventSource } = require('eventsource');\n\n// Connect to SSE server\nconst eventSource = new EventSource('http://localhost:8000/sse');\n\neventSource.onmessage = function(event) {\n    const data = JSON.parse(event.data);\n    console.log('Received:', data);\n};\n\n// Send MCP tool call\nasync function callTool(toolName, arguments) {\n    const response = await fetch('http://localhost:8000/mcp/messages/?session_id=test', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({\n            jsonrpc: \"2.0\",\n            id: 1,\n            method: \"tools/call\",\n            params: {\n                name: toolName,\n                arguments: arguments\n            }\n        })\n    });\n\n    return await response.json();\n}\n\n// Example usage\ncallTool(\"list_loaded_models\", {})\n    .then(result =&gt; console.log(\"Models:\", result))\n    .catch(error =&gt; console.error(\"Error:\", error));\n\ncallTool(\"dna_sequence_predict\", {\n    sequence: \"ATCGATCGATCG\",\n    model_name: \"promoter_model\"\n})\n    .then(result =&gt; console.log(\"Prediction:\", result))\n    .catch(error =&gt; console.error(\"Error:\", error));\n</code></pre>"},{"location":"user_guide/mcp/usage/#curl-examples","title":"cURL Examples","text":""},{"location":"user_guide/mcp/usage/#health-check","title":"Health Check","text":"<pre><code>curl -X POST \"http://localhost:8000/mcp/messages/?session_id=test\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/call\", \"params\": {\"name\": \"health_check\", \"arguments\": {}}}'\n</code></pre>"},{"location":"user_guide/mcp/usage/#list-models","title":"List Models","text":"<pre><code>curl -X POST \"http://localhost:8000/mcp/messages/?session_id=test\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/call\", \"params\": {\"name\": \"list_loaded_models\", \"arguments\": {}}}'\n</code></pre>"},{"location":"user_guide/mcp/usage/#single-sequence-prediction","title":"Single Sequence Prediction","text":"<pre><code>curl -X POST \"http://localhost:8000/mcp/messages/?session_id=test\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/call\", \"params\": {\"name\": \"dna_sequence_predict\", \"arguments\": {\"sequence\": \"ATCGATCGATCG\", \"model_name\": \"promoter_model\"}}}'\n</code></pre>"},{"location":"user_guide/mcp/usage/#batch-prediction","title":"Batch Prediction","text":"<pre><code>curl -X POST \"http://localhost:8000/mcp/messages/?session_id=test\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/call\", \"params\": {\"name\": \"dna_batch_predict\", \"arguments\": {\"sequences\": [\"ATCGATCG\", \"GCTAGCTA\"], \"model_name\": \"promoter_model\"}}}'\n</code></pre>"},{"location":"user_guide/mcp/usage/#multi-model-prediction","title":"Multi-Model Prediction","text":"<pre><code>curl -X POST \"http://localhost:8000/mcp/messages/?session_id=test\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/call\", \"params\": {\"name\": \"dna_multi_model_predict\", \"arguments\": {\"sequence\": \"ATCGATCGATCG\", \"model_names\": [\"promoter_model\", \"conservation_model\"]}}}'\n</code></pre>"},{"location":"user_guide/mcp/usage/#usage-patterns","title":"Usage Patterns","text":""},{"location":"user_guide/mcp/usage/#1-basic-dna-analysis-workflow","title":"1. Basic DNA Analysis Workflow","text":"<pre><code>async def basic_dna_analysis(sequence):\n    async with sse_client(\"http://localhost:8000/sse\") as (read, write):\n        # 1. Check available models\n        models = await read.call_tool(\"list_loaded_models\", {})\n        print(f\"Available models: {models}\")\n\n        # 2. Single model prediction\n        result = await read.call_tool(\n            \"dna_sequence_predict\",\n            {\"sequence\": sequence, \"model_name\": \"promoter_model\"}\n        )\n        print(f\"Promoter prediction: {result}\")\n\n        # 3. Multi-model analysis\n        multi_result = await read.call_tool(\n            \"dna_multi_model_predict\",\n            {\"sequence\": sequence}\n        )\n        print(f\"Multi-model analysis: {multi_result}\")\n\n        return multi_result\n</code></pre>"},{"location":"user_guide/mcp/usage/#2-batch-processing-workflow","title":"2. Batch Processing Workflow","text":"<pre><code>async def batch_dna_analysis(sequences):\n    async with sse_client(\"http://localhost:8000/sse\") as (read, write):\n        # Process sequences in batches\n        batch_size = 10\n        results = []\n\n        for i in range(0, len(sequences), batch_size):\n            batch = sequences[i:i + batch_size]\n\n            # Use streaming for progress updates\n            result = await read.call_tool(\n                \"dna_stream_batch_predict\",\n                {\n                    \"sequences\": batch,\n                    \"model_name\": \"promoter_model\",\n                    \"stream_progress\": True\n                }\n            )\n            results.extend(result.get(\"results\", []))\n\n        return results\n</code></pre>"},{"location":"user_guide/mcp/usage/#3-real-time-analysis-with-progress","title":"3. Real-time Analysis with Progress","text":"<pre><code>async def real_time_analysis(sequence):\n    async with sse_client(\"http://localhost:8000/sse\") as (read, write):\n        # Use streaming prediction for real-time updates\n        result = await read.call_tool(\n            \"dna_stream_predict\",\n            {\n                \"sequence\": sequence,\n                \"model_name\": \"promoter_model\",\n                \"stream_progress\": True\n            }\n        )\n\n        # Process streaming results\n        if result.get(\"streamed\"):\n            print(\"Prediction completed with progress updates\")\n\n        return result\n</code></pre>"},{"location":"user_guide/mcp/usage/#4-model-comparison-workflow","title":"4. Model Comparison Workflow","text":"<pre><code>async def model_comparison(sequence):\n    async with sse_client(\"http://localhost:8000/sse\") as (read, write):\n        # Get all available models\n        models = await read.call_tool(\"list_loaded_models\", {})\n        model_names = [model[\"name\"] for model in models[\"loaded_models\"]]\n\n        # Compare all models\n        comparison = await read.call_tool(\n            \"dna_multi_model_predict\",\n            {\n                \"sequence\": sequence,\n                \"model_names\": model_names\n            }\n        )\n\n        # Analyze consensus\n        results = comparison.get(\"results\", {})\n        consensus = comparison.get(\"consensus\", {})\n\n        print(f\"Model comparison for sequence: {sequence}\")\n        for model_name, result in results.items():\n            print(f\"{model_name}: {result['prediction']} (confidence: {result['confidence']:.3f})\")\n\n        print(f\"Consensus: {consensus}\")\n\n        return comparison\n</code></pre>"},{"location":"user_guide/mcp/usage/#error-handling","title":"Error Handling","text":""},{"location":"user_guide/mcp/usage/#common-error-responses","title":"Common Error Responses","text":"<pre><code>{\n  \"error\": \"Model promoter_model not available or prediction failed\",\n  \"isError\": true\n}\n</code></pre> <pre><code>{\n  \"error\": \"Invalid DNA sequence: contains invalid characters\",\n  \"isError\": true\n}\n</code></pre> <pre><code>{\n  \"error\": \"Model not found: unknown_model\",\n  \"isError\": true\n}\n</code></pre>"},{"location":"user_guide/mcp/usage/#error-handling-in-python","title":"Error Handling in Python","text":"<pre><code>async def safe_prediction(sequence, model_name):\n    try:\n        async with sse_client(\"http://localhost:8000/sse\") as (read, write):\n            result = await read.call_tool(\n                \"dna_sequence_predict\",\n                {\"sequence\": sequence, \"model_name\": model_name}\n            )\n\n            if result.get(\"isError\"):\n                print(f\"Prediction error: {result.get('error')}\")\n                return None\n\n            return result\n\n    except Exception as e:\n        print(f\"Connection error: {e}\")\n        return None\n</code></pre>"},{"location":"user_guide/mcp/usage/#performance-tips","title":"Performance Tips","text":""},{"location":"user_guide/mcp/usage/#1-batch-processing","title":"1. Batch Processing","text":"<ul> <li>Use <code>dna_batch_predict</code> for multiple sequences</li> <li>Process sequences in batches of 10-50 for optimal performance</li> <li>Use streaming for progress updates on large batches</li> </ul>"},{"location":"user_guide/mcp/usage/#2-model-selection","title":"2. Model Selection","text":"<ul> <li>Use faster models (DNAMamba) for real-time applications</li> <li>Use more accurate models (DNABERT) for critical analysis</li> <li>Consider model-specific optimizations</li> </ul>"},{"location":"user_guide/mcp/usage/#3-memory-management","title":"3. Memory Management","text":"<ul> <li>Monitor memory usage with large batches</li> <li>Use appropriate batch sizes for your hardware</li> <li>Consider CPU inference for memory-constrained environments</li> </ul>"},{"location":"user_guide/mcp/usage/#4-network-optimization","title":"4. Network Optimization","text":"<ul> <li>Use local models when possible</li> <li>Enable compression for SSE connections</li> <li>Use appropriate connection timeouts</li> </ul>"},{"location":"user_guide/mcp/usage/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide for advanced setup</li> <li>Troubleshooting for common issues</li> <li>API Reference for detailed documentation</li> </ul>"},{"location":"user_guide/performance/gpu_optimization/","title":"GPU Performance Optimization","text":"<p>Training and running large DNA language models can be computationally intensive. Optimizing GPU usage is key to achieving faster results and handling larger models. This guide covers several techniques to boost GPU performance within the DNALLM framework.</p>"},{"location":"user_guide/performance/gpu_optimization/#1-mixed-precision-training-fp16bf16","title":"1. Mixed-Precision Training (FP16/BF16)","text":"<p>Mixed-precision training uses a combination of 16-bit (half-precision) and 32-bit (full-precision) floating-point types to speed up training and reduce memory usage.</p>"},{"location":"user_guide/performance/gpu_optimization/#the-problem","title":"The Problem","text":"<p>By default, models are trained using 32-bit precision (FP32). This provides high accuracy but consumes significant VRAM and can be slow on modern GPUs with specialized Tensor Cores.</p>"},{"location":"user_guide/performance/gpu_optimization/#how-to-optimize","title":"How to Optimize","text":"<p>Modern GPUs (NVIDIA Ampere and newer) are highly optimized for 16-bit computations.</p> <ul> <li>FP16 (Half-Precision): Offers a significant speedup and memory reduction. It's a great general-purpose choice.</li> <li>BF16 (Bfloat16): Offers a wider dynamic range than FP16, making it more stable for training large models, but requires Ampere or newer GPUs.</li> </ul> <p>You can enable mixed-precision training in your DNALLM configuration file or via the command line.</p> <p>Configuration (<code>config.yaml</code>): <pre><code>training_args:\n  fp16: true\n  # Or for Ampere/Hopper GPUs:\n  # bf16: true\n</code></pre></p> <p>CLI Argument: <pre><code>dnallm finetune --fp16 ...\n# Or\ndnallm finetune --bf16 ...\n</code></pre></p>"},{"location":"user_guide/performance/gpu_optimization/#2-multi-gpu-training-ddp","title":"2. Multi-GPU Training (DDP)","text":"<p>If you have access to multiple GPUs, you can use Data-Parallel training to significantly reduce training time.</p>"},{"location":"user_guide/performance/gpu_optimization/#the-problem_1","title":"The Problem","text":"<p>Training on a single GPU can be a bottleneck, especially with large datasets and models.</p>"},{"location":"user_guide/performance/gpu_optimization/#how-to-optimize_1","title":"How to Optimize","text":"<p>The <code>transformers.Trainer</code> used by DNALLM seamlessly supports PyTorch's Distributed Data Parallel (DDP). It automatically distributes data batches across all available GPUs, computes gradients in parallel, and synchronizes them.</p> <p>To enable multi-GPU training, simply run your training command using <code>torchrun</code>.</p> <p>CLI Command: <pre><code># Assuming you have 4 GPUs available\ntorchrun --nproc_per_node=4 -m dnallm.cli.finetune --config_file /path/to/your/config.yaml\n</code></pre></p> <p><code>torchrun</code> will handle the setup, and the <code>Trainer</code> will automatically detect the distributed environment. No changes to your configuration file are needed.</p>"},{"location":"user_guide/performance/gpu_optimization/#3-using-flash-attention","title":"3. Using Flash Attention","text":"<p>For supported models, Flash Attention is a highly optimized attention implementation that is both faster and more memory-efficient than the standard implementation.</p>"},{"location":"user_guide/performance/gpu_optimization/#the-problem_2","title":"The Problem","text":"<p>The self-attention mechanism in Transformers has a quadratic memory and time complexity with respect to sequence length, making it a major bottleneck for long DNA sequences.</p>"},{"location":"user_guide/performance/gpu_optimization/#how-to-optimize_2","title":"How to Optimize","text":"<p>Flash Attention re-orders the computation to reduce the number of memory read/write operations to HBM (High Bandwidth Memory).</p> <p>Many modern models in the DNALLM ecosystem, such as <code>HyenaDNA</code>, <code>Evo</code>, and recent <code>Llama</code> variants, can use Flash Attention. You can enable it by installing the required package and setting the <code>attn_implementation</code> flag.</p> <ol> <li> <p>Install Flash Attention: <pre><code>pip install flash-attn\n</code></pre> Note: <code>flash-attn</code> has specific CUDA and GPU architecture requirements. Please check its official repository.</p> </li> <li> <p>Enable in Configuration: <pre><code>model_args:\n  attn_implementation: \"flash_attention_2\"\n</code></pre></p> </li> </ol> <p>The <code>Trainer</code> will automatically use this implementation if the model architecture supports it.</p>"},{"location":"user_guide/performance/inference_speed/","title":"Inference Speed Optimization","text":"<p>Fast inference is critical for deploying DNA language models in real-world applications, from large-scale genomic screening to interactive analysis. This guide covers key techniques to accelerate model inference.</p>"},{"location":"user_guide/performance/inference_speed/#1-use-half-precision-fp16bf16","title":"1. Use Half-Precision (FP16/BF16)","text":""},{"location":"user_guide/performance/inference_speed/#the-problem","title":"The Problem","text":"<p>Running inference in full 32-bit precision (FP32) is often unnecessarily slow and memory-intensive.</p>"},{"location":"user_guide/performance/inference_speed/#how-to-optimize","title":"How to Optimize","text":"<p>Just like in training, using 16-bit floating-point numbers can provide a significant speedup for inference, especially on GPUs with Tensor Cores.</p> <ul> <li>FP16: Best for general-purpose speedup.</li> <li>BF16: Best for stability on Ampere and newer GPUs.</li> </ul> <p>You can load the model directly in half-precision.</p> <pre><code>import torch\nfrom dnallm.utils.load import load_model_and_tokenizer\n\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhihan1996/DNABERT-2-117M\",\n    model_type=\"bert\",\n    torch_dtype=torch.float16, # Use float16 for inference\n    device_map=\"auto\" # Automatically move to GPU\n)\n\n# The model is now on the GPU in FP16 format\n</code></pre>"},{"location":"user_guide/performance/inference_speed/#2-batching-inference-requests","title":"2. Batching Inference Requests","text":""},{"location":"user_guide/performance/inference_speed/#the-problem_1","title":"The Problem","text":"<p>Processing sequences one by one is highly inefficient. The overhead of launching the model for a single sequence dominates the actual computation time.</p>"},{"location":"user_guide/performance/inference_speed/#how-to-optimize_1","title":"How to Optimize","text":"<p>Group multiple DNA sequences together and process them as a single batch. This allows the GPU to perform computations in parallel, dramatically increasing throughput.</p> <pre><code>import torch\n\ndna_sequences = [\n    \"GATTACA\" * 10,\n    \"ACGT\" * 20,\n    \"TTTAAA\" * 15\n]\n\n# Tokenize all sequences together with padding\ninputs = tokenizer(\n    dna_sequences,\n    return_tensors=\"pt\",\n    padding=True, # Pad to the length of the longest sequence in the batch\n    truncation=True\n).to(model.device)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nembeddings = outputs.last_hidden_state\nprint(\"Processed batch of size:\", len(dna_sequences))\nprint(\"Output shape:\", embeddings.shape)\n</code></pre>"},{"location":"user_guide/performance/inference_speed/#3-compile-the-model-with-torchcompile","title":"3. Compile the Model with <code>torch.compile</code>","text":""},{"location":"user_guide/performance/inference_speed/#the-problem_2","title":"The Problem","text":"<p>Standard PyTorch execution involves Python overhead that can slow down model execution.</p>"},{"location":"user_guide/performance/inference_speed/#how-to-optimize_2","title":"How to Optimize","text":"<p><code>torch.compile()</code> is a feature in PyTorch 2.0+ that JIT (Just-In-Time) compiles your model into optimized kernel code. It can provide significant speedups (1.3x-2x) with a single line of code.</p> <pre><code># Before your inference loop, compile the model\ncompiled_model = torch.compile(model)\n\n# Use the compiled model for inference\nwith torch.no_grad():\n    outputs = compiled_model(**inputs)\n</code></pre> <p>Note: The first run after <code>torch.compile()</code> will be slow as the compilation happens. Subsequent runs will be much faster.</p>"},{"location":"user_guide/performance/memory_optimization/","title":"Memory Optimization","text":"<p>One of the most common errors when working with large models is the <code>CUDA out of memory</code> error. This guide provides several strategies to reduce the VRAM footprint of your training and inference jobs.</p>"},{"location":"user_guide/performance/memory_optimization/#1-gradient-accumulation","title":"1. Gradient Accumulation","text":""},{"location":"user_guide/performance/memory_optimization/#the-problem","title":"The Problem","text":"<p>Large models require a sufficiently large batch size for stable training, but fitting a large batch into GPU memory is often impossible.</p>"},{"location":"user_guide/performance/memory_optimization/#how-to-optimize","title":"How to Optimize","text":"<p>Gradient accumulation allows you to simulate a larger batch size. It works by accumulating gradients over several smaller batches (micro-batches) before performing a weight update. The effective batch size becomes <code>batch_size * gradient_accumulation_steps</code>.</p> <p>This technique trades a small amount of computation for a massive reduction in memory, as only one micro-batch needs to fit in VRAM at a time.</p> <p>Configuration (<code>config.yaml</code>): <pre><code>training_args:\n  per_device_train_batch_size: 4 # A small batch size that fits in memory\n  gradient_accumulation_steps: 8 # Accumulate gradients over 8 steps\n  # Effective batch size = 4 * 8 = 32\n</code></pre></p> <p>CLI Argument: <pre><code>dnallm finetune --per_device_train_batch_size 4 --gradient_accumulation_steps 8 ...\n</code></pre></p>"},{"location":"user_guide/performance/memory_optimization/#2-gradient-checkpointing","title":"2. Gradient Checkpointing","text":""},{"location":"user_guide/performance/memory_optimization/#the-problem_1","title":"The Problem","text":"<p>During the forward pass, all intermediate activations are stored in memory to be used for gradient calculation in the backward pass. For very long sequences or deep models, these activations can consume a huge amount of VRAM.</p>"},{"location":"user_guide/performance/memory_optimization/#how-to-optimize_1","title":"How to Optimize","text":"<p>Gradient checkpointing (also known as activation checkpointing) saves memory by not storing all intermediate activations. Instead, it re-computes them during the backward pass where needed.</p> <p>This is another trade-off: it saves a significant amount of memory at the cost of increased computation time (typically around 20-30% slower). It is extremely effective for models with very long sequences.</p> <p>Configuration (<code>config.yaml</code>): <pre><code>training_args:\n  gradient_checkpointing: true\n</code></pre></p> <p>CLI Argument: <pre><code>dnallm finetune --gradient_checkpointing ...\n</code></pre></p>"},{"location":"user_guide/performance/memory_optimization/#3-8-bit-optimizers","title":"3. 8-bit Optimizers","text":""},{"location":"user_guide/performance/memory_optimization/#the-problem_2","title":"The Problem","text":"<p>The optimizer states (e.g., momentum and variance for Adam) are typically stored in 32-bit precision and can take up a large portion of VRAM, sometimes as much as the model weights themselves.</p>"},{"location":"user_guide/performance/memory_optimization/#how-to-optimize_2","title":"How to Optimize","text":"<p>Using an 8-bit optimizer, such as <code>bitsandbytes.optim.AdamW8bit</code>, can drastically reduce this memory overhead. It stores optimizer states in 8-bit format, de-quantizing them only when needed for the weight update.</p> <ol> <li> <p>Install <code>bitsandbytes</code>: <pre><code>pip install bitsandbytes\n</code></pre></p> </li> <li> <p>Enable in Configuration: <pre><code>training_args:\n  optim: \"adamw_8bit\"\n</code></pre></p> </li> </ol> <p>This can reduce optimizer memory usage by up to 75% with minimal impact on training performance.</p>"},{"location":"user_guide/performance/memory_optimization/#4-cpu-offloading-with-deepspeed","title":"4. CPU Offloading (with DeepSpeed)","text":"<p>For extreme cases where a model or its optimizer states still don't fit in VRAM, you can use CPU offloading.</p>"},{"location":"user_guide/performance/memory_optimization/#the-problem_3","title":"The Problem","text":"<p>You want to train a model that is fundamentally too large for your GPU's VRAM.</p>"},{"location":"user_guide/performance/memory_optimization/#how-to-optimize_3","title":"How to Optimize","text":"<p>DeepSpeed is a powerful library that provides advanced optimization strategies. Its \"ZeRO\" (Zero Redundancy Optimizer) stages can offload model parameters, gradients, and optimizer states to regular CPU RAM.</p> <p>DNALLM supports DeepSpeed integration. To use it, you need to provide a DeepSpeed configuration file.</p> <ol> <li> <p>Create a DeepSpeed config file (e.g., <code>ds_config.json</code>): <pre><code>{\n  \"fp16\": { \"enabled\": true },\n  \"optimizer\": {\n    \"type\": \"AdamW\",\n    \"params\": { \"lr\": \"auto\", \"betas\": \"auto\", \"eps\": \"auto\", \"weight_decay\": \"auto\" }\n  },\n  \"zero_optimization\": {\n    \"stage\": 2,\n    \"offload_optimizer\": {\n      \"device\": \"cpu\",\n      \"pin_memory\": true\n    }\n  }\n}\n</code></pre></p> </li> <li> <p>Specify the config in your training command: <pre><code>dnallm finetune --deepspeed /path/to/ds_config.json ...\n</code></pre></p> </li> </ol> <p>This is an advanced technique that can enable training of massive models on hardware with limited VRAM, but it will be significantly slower due to the data transfer between CPU and GPU.</p>"},{"location":"user_guide/performance/model_quantization/","title":"Model Quantization","text":"<p>Quantization is an advanced optimization technique that can dramatically reduce model size and accelerate inference speed, especially on CPUs. It involves converting the model's floating-point weights to lower-precision integers, such as 8-bit integers (INT8).</p>"},{"location":"user_guide/performance/model_quantization/#1-what-is-quantization","title":"1. What is Quantization?","text":""},{"location":"user_guide/performance/model_quantization/#the-problem","title":"The Problem","text":"<p>Large language models have millions or billions of parameters, typically stored as 32-bit floating-point numbers (FP32). This results in: - Large Model Size: A 1-billion parameter model can take up 4 GB of storage. - High Memory Bandwidth: Moving these large weights from RAM to the processor is slow. - Slow Computation: Floating-point arithmetic is more complex than integer arithmetic.</p>"},{"location":"user_guide/performance/model_quantization/#how-to-optimize","title":"How to Optimize","text":"<p>Quantization maps the FP32 weights to a smaller set of INT8 values. This provides several benefits: - 4x Smaller Model: INT8 uses 4 times less space than FP32. - Faster Memory Access: Smaller weights can be moved more quickly. - Faster Computation: Integer arithmetic is much faster, especially on modern CPUs with specialized instructions (e.g., AVX2, AVX512).</p> <p>There is a small trade-off in model accuracy, but for many tasks, this is negligible.</p>"},{"location":"user_guide/performance/model_quantization/#2-how-to-use-quantization-in-dnallm","title":"2. How to Use Quantization in DNALLM","text":"<p>The <code>transformers</code> library, which DNALLM is built on, provides easy-to-use tools for quantization through the <code>bitsandbytes</code> library.</p>"},{"location":"user_guide/performance/model_quantization/#8-bit-quantization-for-inference","title":"8-bit Quantization for Inference","text":"<p>You can load a model directly in 8-bit precision. This is the easiest way to get started.</p> <ol> <li> <p>Install <code>bitsandbytes</code>: <pre><code>pip install bitsandbytes\n</code></pre></p> </li> <li> <p>Load the model with <code>load_in_8bit=True</code>: <pre><code>from dnallm.utils.load import load_model_and_tokenizer\n\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhihan1996/DNABERT-2-117M\",\n    model_type=\"bert\",\n    load_in_8bit=True, # Enable 8-bit quantization\n    device_map=\"auto\"\n)\n\nprint(f\"Model loaded on device: {model.device}\")\nprint(\"Model footprint:\", model.get_memory_footprint())\n</code></pre>     This will load the model onto the GPU (if available) with its <code>nn.Linear</code> layers converted to 8-bit.</p> </li> </ol>"},{"location":"user_guide/performance/model_quantization/#4-bit-quantization-nf4","title":"4-bit Quantization (NF4)","text":"<p>For even greater memory savings, you can use 4-bit quantization (NF4 - NormalFloat 4), which is particularly effective for fine-tuning very large models on consumer GPUs (a technique known as QLoRA).</p> <ol> <li>Load the model with <code>load_in_4bit=True</code>: <pre><code>model, tokenizer = load_model_and_tokenizer(\n    \"GenerTeam/GENERator-eukaryote-1.2b-base\",\n    model_type=\"llama\",\n    load_in_4bit=True, # Enable 4-bit quantization\n    device_map=\"auto\"\n)\n\nprint(\"Model footprint:\", model.get_memory_footprint())\n</code></pre></li> </ol> <p>When to use Quantization: - Inference on CPU: Quantization provides the biggest speed benefits on CPU. - Fitting Large Models: Use 8-bit or 4-bit loading to fit massive models into limited VRAM for inference or QLoRA fine-tuning. - Reducing Model Size: If you need to deploy a model in a size-constrained environment.</p>"},{"location":"user_guide/tutorial/end_to_end_workflow/","title":"DNALLM End-to-End Tutorial: From Data to MCP Service Deployment","text":"<p>This tutorial will guide you through a complete DNA sequence analysis project, including data preparation, model training, validation evaluation, and MCP service deployment.</p>"},{"location":"user_guide/tutorial/end_to_end_workflow/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Tutorial Overview</li> <li>Task 1: Binary Classification - Promoter Strength Prediction</li> <li>Task 2: Multi-label Classification - Sequence Functional Annotation</li> <li>Task 3: Named Entity Recognition - Genomic Element Localization</li> <li>Task 4: Using LoRA for Efficient Fine-tuning</li> <li>Task 5: Model Inference and Mutagenesis Analysis</li> <li>Task 6: MCP Service Deployment</li> <li>Advanced Tips and Best Practices</li> <li>Frequently Asked Questions</li> </ol>"},{"location":"user_guide/tutorial/end_to_end_workflow/#1-tutorial-overview","title":"1. Tutorial Overview","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#11-learning-objectives","title":"1.1 Learning Objectives","text":"<p>After completing this tutorial, you will be able to:</p> <ul> <li>Prepare and validate DNA sequence data</li> <li>Configure and execute model training</li> <li>Evaluate model performance and perform inference</li> <li>Use LoRA for parameter-efficient fine-tuning</li> <li>Deploy models as MCP services</li> <li>Understand best practices for different task types</li> </ul>"},{"location":"user_guide/tutorial/end_to_end_workflow/#12-project-structure","title":"1.2 Project Structure","text":"<pre><code>tutorial_project/\n\u251c\u2500\u2500 data/                    # Data directory\n\u2502   \u251c\u2500\u2500 raw/                 # Raw data\n\u2502   \u251c\u2500\u2500 processed/           # Processed data\n\u2502   \u2514\u2500\u2500 test.csv             # Test data\n\u251c\u2500\u2500 configs/                 # Configuration file directory\n\u2502   \u251c\u2500\u2500 finetune_config.yaml\n\u2502   \u2514\u2500\u2500 inference_config.yaml\n\u251c\u2500\u2500 models/                  # Model save directory\n\u2502   \u2514\u2500\u2500 checkpoint-100/\n\u251c\u2500\u2500 mcp_server/              # MCP service configuration\n\u2502   \u251c\u2500\u2500 mcp_server_config.yaml\n\u2502   \u2514\u2500\u2500 inference_config.yaml\n\u251c\u2500\u2500 notebooks/               # Jupyter notebooks\n\u2514\u2500\u2500 outputs/                 # Training output\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#13-environment-setup","title":"1.3 Environment Setup","text":"<p>First, ensure you have correctly installed DNALLM following the Installation Guide.</p> <pre><code># Activate environment\nconda activate dnallm\n\n# Verify installation\npython -c \"\nimport dnallm\nfrom dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.datahandling import DNADataset\nfrom dnallm.finetune import DNATrainer\nfrom dnallm.inference import DNAInference\nprint('\u2705 All modules imported successfully')\n\"\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#2-task-1-promoter-strength-prediction","title":"2. Task 1: Promoter Strength Prediction","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#21-task-description","title":"2.1 Task Description","text":"<p>This task will train a binary classification model to predict whether a DNA sequence has promoter activity.</p> <p>Dataset: Use <code>zhangtaolab/plant-multi-species-core-promoters</code> dataset Model: Plant DNABERT BPE Task Type: binary classification</p>"},{"location":"user_guide/tutorial/end_to_end_workflow/#22-data-preparation","title":"2.2 Data Preparation","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#221-download-and-view-data","title":"2.2.1 Download and View Data","text":"<pre><code>from dnallm import load_config\nfrom dnallm.datahandling import DNADataset\n\n# Download dataset from ModelScope\ndata_name = \"zhangtaolab/plant-multi-species-core-promoters\"\ndatasets = DNADataset.from_modelscope(\n    data_name, \n    seq_col=\"sequence\", \n    label_col=\"label\"\n)\n\n# View dataset information\nprint(f\"Dataset size: {len(datasets)}\")\nprint(f\"Label distribution: {datasets.get_label_distribution()}\")\n\n# Data sampling (for quick testing)\nsampled_datasets = datasets.sampling(0.1, overwrite=True)\nprint(f\"Sampled size: {len(sampled_datasets)}\")\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#222-local-data-format","title":"2.2.2 Local Data Format","text":"<p>If using local data, ensure the format is as follows:</p> <p>CSV Format (<code>data.csv</code>): <pre><code>sequence,label\nATGCGT...,0\nATCGAT...,1\nGCTAGC...,0\n...\n</code></pre></p> <p>TSV Format (<code>data.tsv</code>): <pre><code>sequence    label\nATGCGT...   0\nATCGAT...   1\nGCTAGC...   0\n</code></pre></p>"},{"location":"user_guide/tutorial/end_to_end_workflow/#223-data-quality-check","title":"2.2.3 Data Quality Check","text":"<pre><code>from dnallm.datahandling import DNADataset\n\n# Load local data\ndataset = DNADataset.load_local_data(\n    file_paths=\"./data/train.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\"\n)\n\n# Validate data quality\ndataset.validate_sequences(\n    min_length=50,\n    max_length=1000,\n    valid_chars=[\"A\", \"T\", \"G\", \"C\"]\n)\n\n# Check label distribution\nprint(f\"Positive samples: {dataset.label_counts.get(1, 0)}\")\nprint(f\"Negative samples: {dataset.label_counts.get(0, 0)}\")\n\n# Data augmentation (add reverse complement sequences)\ndataset.augment_reverse_complement()\nprint(f\"Size after augmentation: {len(dataset)}\")\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#23-configure-training-parameters","title":"2.3 Configure Training Parameters","text":"<p>Create <code>finetune_config.yaml</code> configuration file:</p> <pre><code># File: configs/finetune_config.yaml\n\n# Task configuration (required)\ntask:\n  task_type: \"binary\"                    # Binary classification task\n  num_labels: 2                          # Number of labels\n  label_names: [\"negative\", \"positive\"]  # Label names\n  threshold: 0.5                         # Classification threshold\n\n# Fine-tuning configuration\nfinetune:\n  # Output configuration\n  output_dir: \"./models/promoter_classifier\"\n\n  # Training parameters\n  num_train_epochs: 3\n  per_device_train_batch_size: 16\n  per_device_eval_batch_size: 16\n  gradient_accumulation_steps: 1\n\n  # Optimizer parameters\n  learning_rate: 2e-5\n  weight_decay: 0.01\n  adam_beta1: 0.9\n  adam_beta2: 0.999\n  adam_epsilon: 1e-8\n  max_grad_norm: 1.0\n\n  # Learning rate scheduler\n  warmup_ratio: 0.1\n  lr_scheduler_type: \"linear\"\n  lr_scheduler_kwargs: {}\n\n  # Logging and evaluation\n  logging_strategy: \"steps\"\n  logging_steps: 100\n  eval_strategy: \"steps\"\n  eval_steps: 500\n\n  # Model saving\n  save_strategy: \"steps\"\n  save_steps: 500\n  save_total_limit: 3\n  save_safetensors: True\n\n  # Performance optimization\n  fp16: True                              # Mixed precision training\n  load_best_model_at_end: True\n  metric_for_best_model: \"eval_loss\"\n  report_to: \"tensorboard\"\n  seed: 42\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#24-execute-training","title":"2.4 Execute Training","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.datahandling import DNADataset\nfrom dnallm.finetune import DNATrainer\n\n# 1. Load configuration\nconfigs = load_config(\"./configs/finetune_config.yaml\")\n\n# 2. Load model and tokenizer\nmodel_name = \"zhangtaolab/plant-dnabert-BPE\"\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name,\n    task_config=configs[\"task\"],\n    source=\"modelscope\"  # or \"huggingface\"\n)\n\n# 3. Load and process dataset\ndatasets = DNADataset.from_modelscope(\n    data_name=\"zhangtaolab/plant-multi-species-core-promoters\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Sampling (optional, for quick testing)\nsampled_datasets = datasets.sampling(0.1, overwrite=True)\n\n# Encode sequences\nsampled_datasets.encode_sequences(remove_unused_columns=True)\n\n# 4. Initialize trainer\ntrainer = DNATrainer(\n    config=configs,\n    model=model,\n    datasets=sampled_datasets\n)\n\n# 5. Start training\nprint(\"\ud83d\ude80 Starting training...\")\nmetrics = trainer.train()\nprint(f\"Training complete! Final metrics: {metrics}\")\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#25-model-validation","title":"2.5 Model Validation","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.datahandling import DNADataset\nfrom dnallm.finetune import DNATrainer\n\n# Load best model\ntrainer.load_best_model()\n\n# Evaluate on test set\ntest_metrics = trainer.evaluate(test_dataset=sampled_datasets.test)\nprint(f\"Test set metrics: {test_metrics}\")\n\n# Save evaluation report\ntrainer.save_evaluation_report(test_metrics, save_path=\"./evaluation_report.json\")\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#26-complete-training-script","title":"2.6 Complete Training Script","text":"<p>Save the above code as <code>train_promoter.py</code>:</p> <pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nPromoter strength prediction model training script\n\"\"\"\n\nimport os\nimport argparse\nfrom dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.datahandling import DNADataset\nfrom dnallm.finetune import DNATrainer\n\ndef main():\n    parser = argparse.ArgumentParser(description='Train promoter prediction model')\n    parser.add_argument('--config', type=str, default='./configs/finetune_config.yaml')\n    parser.add_argument('--model', type=str, default='zhangtaolab/plant-dnabert-BPE')\n    parser.add_argument('--data', type=str, default='zhangtaolab/plant-multi-species-core-promoters')\n    parser.add_argument('--sample-ratio', type=float, default=0.1)\n    parser.add_argument('--source', type=str, default='modelscope')\n    args = parser.parse_args()\n\n    # Load configuration\n    configs = load_config(args.config)\n\n    # Load model\n    model, tokenizer = load_model_and_tokenizer(\n        args.model,\n        task_config=configs[\"task\"],\n        source=args.source\n    )\n\n    # Load data\n    datasets = DNADataset.from_modelscope(\n        data_name=args.data,\n        seq_col=\"sequence\",\n        label_col=\"label\",\n        tokenizer=tokenizer,\n        max_length=512\n    )\n\n    # Sampling\n    if args.sample_ratio &lt; 1.0:\n        datasets = datasets.sampling(args.sample_ratio, overwrite=True)\n\n    # Encode\n    datasets.encode_sequences(remove_unused_columns=True)\n\n    # Train\n    trainer = DNATrainer(config=configs, model=model, datasets=datasets)\n    metrics = trainer.train()\n\n    # Save model\n    trainer.save_model(\"./models/final_model\")\n    print(f\"\u2705 Model saved to ./models/final_model\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run the script:</p> <pre><code>python train_promoter.py --config ./configs/finetune_config.yaml --sample-ratio 0.1\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#3-task-2-multi-label-classification","title":"3. Task 2: Multi-label Classification","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#31-task-description","title":"3.1 Task Description","text":"<p>This task will train a multi-label classification model to predict multiple functional attributes of sequences simultaneously (e.g., both promoter and enhancer).</p> <p>Dataset: Custom multi-label dataset Model: Plant DNABERT BPE Task Type: multilabel classification</p>"},{"location":"user_guide/tutorial/end_to_end_workflow/#32-multi-label-data-format","title":"3.2 Multi-label Data Format","text":"<pre><code>sequence,promoter,enhancer,repressor,silencer\nATGCGT...,1,0,1,0\nATCGAT...,0,1,0,1\nGCTAGC...,1,1,0,0\n...\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#33-multi-label-configuration","title":"3.3 Multi-label Configuration","text":"<pre><code># File: configs/multilabel_config.yaml\n\ntask:\n  task_type: \"multilabel\"              # Multi-label classification\n  num_labels: 4                         # Multiple labels\n  label_names: [\"promoter\", \"enhancer\", \"repressor\", \"silencer\"]\n  threshold: 0.5\n\nfinetune:\n  output_dir: \"./models/multilabel_classifier\"\n  num_train_epochs: 3\n  per_device_train_batch_size: 8\n  learning_rate: 2e-5\n  # Multi-label specific loss function configuration\n  loss_function: \"binary_cross_entropy\"\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#34-multi-label-training-code","title":"3.4 Multi-label Training Code","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.datahandling import DNADataset\nfrom dnallm.finetune import DNATrainer\n\n# Load configuration\nconfigs = load_config(\"./configs/multilabel_config.yaml\")\n\n# Load model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=configs[\"task\"],\n    source=\"modelscope\"\n)\n\n# Load multi-label data\ndataset = DNADataset.load_local_data(\n    file_paths=\"./data/multilabel_data.csv\",\n    seq_col=\"sequence\",\n    label_col=[\"promoter\", \"enhancer\", \"repressor\", \"silencer\"],\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Encode\ndataset.encode_sequences(remove_unused_columns=True)\n\n# Train\ntrainer = DNATrainer(config=configs, model=model, datasets=dataset)\nmetrics = trainer.train()\n\n# Multi-label evaluation metrics\nprint(\"Multi-label evaluation metrics:\")\nprint(f\"  - Hamming Loss: {metrics['eval_hamming_loss']:.4f}\")\nprint(f\"  - Accuracy: {metrics['eval_accuracy']:.4f}\")\nprint(f\"  - F1 Score: {metrics['eval_f1']:.4f}\")\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#4-task-3-named-entity-recognition","title":"4. Task 3: Named Entity Recognition","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#41-task-description","title":"4.1 Task Description","text":"<p>This task will train an NER model to identify genomic element positions in DNA sequences (e.g., genes, TSS, CDS).</p> <p>Dataset: Custom NER dataset Model: DNABERT-2 Task Type: token classification (NER)</p>"},{"location":"user_guide/tutorial/end_to_end_workflow/#42-ner-data-format","title":"4.2 NER Data Format","text":"<p>BIO Format: <pre><code>ATGCGT... O\nATG I-GENE\nCTA I-GENE\n...\n</code></pre></p> <p>CSV Format: <pre><code>sequence,tags\nATGCGT...,[\"O\",\"O\",\"O\",\"B-GENE\",\"I-GENE\",\"I-GENE\",\"O\",...]\n</code></pre></p>"},{"location":"user_guide/tutorial/end_to_end_workflow/#43-ner-configuration","title":"4.3 NER Configuration","text":"<pre><code># File: configs/ner_config.yaml\n\ntask:\n  task_type: \"token\"                   # Token classification (NER)\n  num_labels: 5                         # Number of labels\n  label_names: [\"O\", \"B-GENE\", \"I-GENE\", \"B-TS\", \"I-TS\"]\n\nfinetune:\n  output_dir: \"./models/ner_model\"\n  num_train_epochs: 3\n  per_device_train_batch_size: 8\n  learning_rate: 3e-5\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#44-ner-training-code","title":"4.4 NER Training Code","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.datahandling import DNADataset\nfrom dnallm.finetune import DNATrainer\n\n# Load configuration\nconfigs = load_config(\"./configs/ner_config.yaml\")\n\n# Load NER model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhijunliao/dnabert-2-embedded-35m\",  # DNABERT-2\n    task_config=configs[\"task\"],\n    source=\"huggingface\"\n)\n\n# Load NER dataset\ndataset = DNADataset.load_local_data(\n    file_paths=\"./data/ner_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"tags\",\n    tokenizer=tokenizer,\n    max_length=128,\n    task_type=\"token\"  # Specify as token classification task\n)\n\n# Encode\ndataset.encode_sequences(remove_unused_columns=True)\n\n# Train\ntrainer = DNATrainer(config=configs, model=model, datasets=dataset)\nmetrics = trainer.train()\n\n# NER evaluation\nprint(\"NER evaluation metrics:\")\nprint(f\"  - Precision: {metrics['eval_precision']:.4f}\")\nprint(f\"  - Recall: {metrics['eval_recall']:.4f}\")\nprint(f\"  - F1: {metrics['eval_f1']:.4f}\")\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#5-task-4-using-lora-for-efficient-fine-tuning","title":"5. Task 4: Using LoRA for Efficient Fine-tuning","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#51-lora-introduction","title":"5.1 LoRA Introduction","text":"<p>LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that requires training only a small number of parameters to achieve full-model fine-tuning results.</p>"},{"location":"user_guide/tutorial/end_to_end_workflow/#52-lora-configuration","title":"5.2 LoRA Configuration","text":"<p>Add LoRA configuration in <code>finetune_config.yaml</code>:</p> <pre><code># File: configs/lora_config.yaml\n\ntask:\n  task_type: \"binary\"\n  num_labels: 2\n  label_names: [\"negative\", \"positive\"]\n  threshold: 0.5\n\nfinetune:\n  output_dir: \"./models/lora_promoter\"\n  num_train_epochs: 3\n  per_device_train_batch_size: 16\n  learning_rate: 2e-5\n  bf16: True                            # Enable BF16 acceleration\n  report_to: \"tensorboard\"\n\n# LoRA specific configuration\nlora:\n  r: 8                                  # LoRA rank\n  lora_alpha: 32                        # LoRA scaling factor\n  lora_dropout: 0.1                     # Dropout ratio\n  target_modules:                       # Target modules\n    - \"x_proj\"\n    - \"in_proj\"\n    - \"out_proj\"\n  task_type: \"SEQ_CLS\"                  # Task type\n  bias: \"none\"                          # Bias handling\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#53-lora-training-code","title":"5.3 LoRA Training Code","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.datahandling import DNADataset\nfrom dnallm.finetune import DNATrainer\n\n# Load configuration\nconfigs = load_config(\"./configs/lora_config.yaml\")\n\n# Load model (supports Mamba, Caduceus, etc. architectures)\nmodel_name = \"kuleshov-group/PlantCAD2-Small-l24-d0768\"\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name,\n    task_config=configs[\"task\"],\n    source=\"huggingface\"\n)\n\n# Load dataset\ndatasets = DNADataset.from_modelscope(\n    data_name=\"zhangtaolab/plant-multi-species-core-promoters\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Sampling and encoding\nsampled_datasets = datasets.sampling(0.05, overwrite=True)\nsampled_datasets.encode_sequences(remove_unused_columns=True)\n\n# Initialize trainer (enable LoRA)\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=sampled_datasets,\n    use_lora=True  # Enable LoRA, will automatically read lora parameters from config\n)\n\n# Train\nmetrics = trainer.train()\nprint(f\"LoRA training complete!\")\nprint(f\"Trainable parameters ratio: {trainer.get_trainable_parameters_ratio():.2%}\")\n\n# Save LoRA adapter\ntrainer.save_lora_adapter(\"./models/lora_adapter\")\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#54-loading-lora-model-for-inference","title":"5.4 Loading LoRA Model for Inference","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm import DNAInference\n\n# Load configuration\nconfigs = load_config(\"./configs/lora_config.yaml\")\n\n# Load base model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"kuleshov-group/PlantCAD2-Small-l24-d0768\",\n    task_config=configs[\"task\"],\n    source=\"huggingface\"\n)\n\n# Load LoRA adapter\nfrom peft import PeftModel\nmodel = PeftModel.from_pretrained(model, \"./models/lora_adapter\")\nmodel = model.merge_and_unload()  # Merge LoRA weights\n\n# Inference\ninference_engine = DNAInference(config=configs, model=model, tokenizer=tokenizer)\nresult = inference_engine.infer(\"ATGCGT...\")\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#6-task-5-model-inference-and-mutagenesis-analysis","title":"6. Task 5: Model Inference and Mutagenesis Analysis","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#61-inference-configuration","title":"6.1 Inference Configuration","text":"<p>Create <code>inference_config.yaml</code>:</p> <pre><code># File: configs/inference_config.yaml\n\ninference:\n  batch_size: 16\n  device: \"auto\"                         # Auto-select GPU/CPU\n  max_length: 512\n  num_workers: 4\n  output_dir: \"./results\"\n  use_fp16: false\n\ntask:\n  num_labels: 2\n  task_type: binary\n  threshold: 0.5\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#62-single-sequence-inference","title":"6.2 Single Sequence Inference","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm import DNAInference\n\n# Load configuration and model\nconfigs = load_config(\"./configs/inference_config.yaml\")\nmodel, tokenizer = load_model_and_tokenizer(\n    \"./models/final_model\",  # Use trained model\n    task_config=configs[\"task\"]\n)\n\n# Initialize inference engine\ninference_engine = DNAInference(config=configs, model=model, tokenizer=tokenizer)\n\n# Single sequence inference\nsequence = \"ATGCGTACGTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGC\"\nresult = inference_engine.infer(sequence)\nprint(f\"Inference result: {result}\")\n\n# Get probabilities\nprint(f\"Negative class probability: {result['probabilities'][0]:.4f}\")\nprint(f\"Positive class probability: {result['probabilities'][1]:.4f}\")\nprint(f\"Predicted label: {result['predicted_label']}\")\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#63-batch-inference","title":"6.3 Batch Inference","text":"<pre><code>from dnallm import DNAInference\nimport pandas as pd\n\n# Load test data\ntest_data = pd.read_csv(\"./data/test.csv\")\nsequences = test_data[\"sequence\"].tolist()\n\n# Batch inference\nresults = inference_engine.batch_infer(sequences, show_progress=True)\n\n# Save results\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"./results/predictions.csv\", index=False)\nprint(f\"Batch inference complete! Processed {len(sequences)} sequences\")\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#64-mutagenesis-analysis-in-silico-mutagenesis","title":"6.4 Mutagenesis Analysis (In-silico Mutagenesis)","text":"<p>Mutagenesis analysis is used to identify the contribution of each position in the sequence to the prediction.</p> <pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.inference import Mutagenesis\n\n# Load configuration and model\nconfigs = load_config(\"./configs/inference_config.yaml\")\nmodel, tokenizer = load_model_and_tokenizer(\n    \"./models/final_model\",\n    task_config=configs[\"task\"]\n)\n\n# Initialize mutagenesis analyzer\nmutagenesis = Mutagenesis(config=configs, model=model, tokenizer=tokenizer)\n\n# Mutagenesis analysis\nsequence = \"ATGCGTACGTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGC\"\nmutagenesis.mutate_sequence(sequence, replace_mut=True)\n\n# Evaluate mutation effects\npredictions = mutagenesis.evaluate(strategy=\"mean\")\n\n# Visualization\nplot = mutagenesis.plot(predictions, save_path=\"./results/mutation_effects.pdf\")\n\n# Get important positions\nimportant_positions = mutagenesis.get_important_positions(top_k=10)\nprint(f\"Top 10 most important positions: {important_positions}\")\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#65-model-interpretability","title":"6.5 Model Interpretability","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer\nfrom dnallm.inference import DNAInterpreter\n\n# Load configuration and model\nconfigs = load_config(\"./configs/inference_config.yaml\")\nmodel, tokenizer = load_model_and_tokenizer(\n    \"./models/final_model\",\n    task_config=configs[\"task\"]\n)\n\n# Initialize interpreter\ninterpreter = DNAInterpreter(config=configs, model=model, tokenizer=tokenizer)\n\n# Get attention weights\nsequence = \"ATGCGTACGTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGC\"\nattention_weights = interpreter.get_attention(sequence)\n\n# Get embedding vectors\nembeddings = interpreter.get_embedding(sequence)\n\n# Generate comprehensive report\nreport = interpreter.generate_report(sequence)\nprint(report)\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#7-task-6-mcp-service-deployment","title":"7. Task 6: MCP Service Deployment","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#71-mcp-service-overview","title":"7.1 MCP Service Overview","text":"<p>MCP (Model Context Protocol) is a standardized model service protocol that supports deploying DNA sequence analysis models as API services.</p>"},{"location":"user_guide/tutorial/end_to_end_workflow/#72-mcp-server-configuration","title":"7.2 MCP Server Configuration","text":"<p>Create <code>mcp_server_config.yaml</code>:</p> <pre><code># File: mcp_server/mcp_server_config.yaml\n\n# Server configuration\nserver:\n  host: \"0.0.0.0\"\n  port: 8000\n  workers: 1\n  log_level: \"INFO\"\n  debug: false\n\n# MCP metadata\nmcp:\n  name: \"DNA Sequence Analysis Server\"\n  version: \"0.1.0\"\n  description: \"DNA sequence analysis MCP server, supports promoter prediction, mutagenesis analysis, and more\"\n\n# Model configuration - one-to-many relationship\nmodels:\n  promoter_model:\n    name: \"promoter_model\"\n    model_name: \"Plant DNABERT BPE promoter\"\n    config_path: \"./inference_config.yaml\"\n    enabled: true\n    priority: 1\n\n  ner_model:\n    name: \"ner_model\"\n    model_name: \"Custom NER Model\"\n    config_path: \"./ner_inference_config.yaml\"\n    enabled: true\n    priority: 2\n\n# Multi-model combined analysis\nmulti_model:\n  comprehensive_analysis:\n    name: \"comprehensive_analysis\"\n    description: \"Comprehensive sequence analysis\"\n    models: [\"promoter_model\"]\n    enabled: true\n\n# SSE configuration\nsse:\n  heartbeat_interval: 30\n  max_connections: 100\n  connection_timeout: 300\n  enable_compression: true\n  mount_path: \"/mcp\"\n  cors_origins: [\"*\"]\n  enable_heartbeat: true\n\n# Logging configuration\nlogging:\n  level: \"INFO\"\n  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n  file: \"./logs/mcp_server.log\"\n  max_size: \"10MB\"\n  backup_count: 5\n</code></pre> <p>Create corresponding inference configuration file <code>inference_config.yaml</code>:</p> <pre><code># File: mcp_server/inference_config.yaml\n\ninference:\n  batch_size: 16\n  device: \"auto\"\n  max_length: 512\n  num_workers: 4\n  output_dir: ./results\n  use_fp16: false\n\ntask:\n  num_labels: 2\n  task_type: binary\n  threshold: 0.5\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#73-start-mcp-server","title":"7.3 Start MCP Server","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#731-using-command-line","title":"7.3.1 Using Command Line","text":"<pre><code># Start with configuration file\ndnallm-mcp-server --config ./mcp_server/mcp_server_config.yaml\n\n# Run in background\nnohup dnallm-mcp-server --config ./mcp_server/mcp_server_config.yaml &gt; ./logs/mcp_server.log 2&gt;&amp;1 &amp;\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#732-using-python-code","title":"7.3.2 Using Python Code","text":"<pre><code>import asyncio\nfrom dnallm.mcp import DNALLMMCPServer\n\nasync def main():\n    # Initialize server\n    server = DNALLMMCPServer(\"./mcp_server/mcp_server_config.yaml\")\n    await server.initialize()\n\n    # Start server\n    print(\"\ud83d\ude80 MCP server starting...\")\n    await server.start_server(host=\"0.0.0.0\", port=8000, transport=\"sse\")\n    print(\"\u2705 MCP server started: http://0.0.0.0:8000/mcp\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#74-mcp-client-usage","title":"7.4 MCP Client Usage","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#741-using-curl-for-testing","title":"7.4.1 Using curl for Testing","text":"<pre><code># Health check\ncurl http://localhost:8000/health\n\n# Single sequence prediction\ncurl -X POST http://localhost:8000/predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"sequence\": \"ATGCGTACGTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGC\", \"model\": \"promoter_model\"}'\n\n# SSE real-time prediction\ncurl -N http://localhost:8000/mcp/stream \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"sequence\": \"ATGCGTACGTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGC\"}'\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#742-python-client-example","title":"7.4.2 Python Client Example","text":"<pre><code>import asyncio\nimport aiohttp\n\nclass DNALLMClient:\n    def __init__(self, base_url=\"http://localhost:8000\"):\n        self.base_url = base_url\n\n    async def health_check(self):\n        \"\"\"Health check\"\"\"\n        async with aiohttp.ClientSession() as session:\n            async with session.get(f\"{self.base_url}/health\") as response:\n                return await response.json()\n\n    async def predict(self, sequence, model=\"promoter_model\"):\n        \"\"\"Single sequence prediction\"\"\"\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{self.base_url}/predict\",\n                json={\"sequence\": sequence, \"model\": model}\n            ) as response:\n                return await response.json()\n\n    async def batch_predict(self, sequences, model=\"promoter_model\"):\n        \"\"\"Batch prediction\"\"\"\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{self.base_url}/batch_predict\",\n                json={\"sequences\": sequences, \"model\": model}\n            ) as response:\n                return await response.json()\n\n    async def list_models(self):\n        \"\"\"List available models\"\"\"\n        async with aiohttp.ClientSession() as session:\n            async with session.get(f\"{self.base_url}/models\") as response:\n                return await response.json()\n\n# Usage example\nasync def main():\n    client = DNALLMClient()\n\n    # Health check\n    health = await client.health_check()\n    print(f\"Server status: {health}\")\n\n    # Single sequence prediction\n    result = await client.predict(\n        \"ATGCGTACGTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGC\"\n    )\n    print(f\"Prediction result: {result}\")\n\n    # Batch prediction\n    sequences = [\n        \"ATGCGTACGTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGC\",\n        \"GCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTA\"\n    ]\n    results = await client.batch_predict(sequences)\n    print(f\"Batch prediction results: {results}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#75-docker-deployment","title":"7.5 Docker Deployment","text":"<p>Create <code>Dockerfile</code>:</p> <pre><code>FROM python:3.12-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    build-essential \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy dependency files\nCOPY pyproject.toml ./\nCOPY README.md ./\n\n# Install Python dependencies\nRUN pip install --no-cache-dir uv &amp;&amp; \\\n    uv pip install -e '.[base,mcp,cuda124]' --system\n\n# Copy application code\nCOPY dnallm/ ./dnallm/\nCOPY mcp_server/ ./mcp_server/\nCOPY models/ ./models/  # Pre-trained models\n\n# Expose port\nEXPOSE 8000\n\n# Start command\nCMD [\"dnallm-mcp-server\", \"--config\", \"./mcp_server/mcp_server_config.yaml\"]\n</code></pre> <p>Build and run:</p> <pre><code># Build image\ndocker build -t dnallm-mcp-server .\n\n# Run container\ndocker run -p 8000:8000 -v ./models:/app/models dnallm-mcp-server\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#8-advanced-tips-and-best-practices","title":"8. Advanced Tips and Best Practices","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#81-training-optimization-tips","title":"8.1 Training Optimization Tips","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#811-mixed-precision-training","title":"8.1.1 Mixed Precision Training","text":"<pre><code>finetune:\n  fp16: True              # FP16 mixed precision\n  bf16: False             # BF16 (newer GPUs)\n  # or\n  bf16: True              # A100, H100, etc.\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#812-gradient-accumulation","title":"8.1.2 Gradient Accumulation","text":"<pre><code>finetune:\n  per_device_train_batch_size: 4\n  gradient_accumulation_steps: 4  # Effective batch size = 4 * 4 = 16\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#813-learning-rate-scheduling","title":"8.1.3 Learning Rate Scheduling","text":"<pre><code>finetune:\n  learning_rate: 2e-5\n  warmup_ratio: 0.1              # Warmup ratio\n  lr_scheduler_type: \"cosine_with_restarts\"\n  lr_scheduler_kwargs:\n    num_restarts: 2\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#82-model-selection-guide","title":"8.2 Model Selection Guide","text":"Task Type Recommended Model Features Classification/Feature Extraction DNABERT, Nucleotide Transformer Strong sequence context understanding Sequence Generation DNAGPT, Evo Next token prediction Long sequences (&gt;5kb) Caduceus, HyenaDNA High efficiency, low memory usage Multimodal megaDNA, LucaOne Multi-species, multimodal support"},{"location":"user_guide/tutorial/end_to_end_workflow/#83-data-augmentation-strategies","title":"8.3 Data Augmentation Strategies","text":"<pre><code>from dnallm.datahandling import DNADataset\n\n# Reverse complement augmentation\ndataset.augment_reverse_complement()\n\n# Random mutation\ndataset.augment_random_mutation(rate=0.01)\n\n# K-mer augmentation\ndataset.augment_kmer(k=3)\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#84-distributed-training","title":"8.4 Distributed Training","text":"<pre><code>from dnallm.finetune import DNATrainer\nfrom accelerate import Accelerator\n\n# Initialize accelerator\naccelerator = Accelerator(\n    mixed_precision=\"fp16\",\n    gradient_accumulation_steps=4\n)\n\n# Prepare data and model\nmodel, optimizer, dataloader = accelerator.prepare(\n    model, optimizer, dataloader\n)\n\n# Train\ntrainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=datasets,\n    accelerator=accelerator\n)\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#85-model-saving-and-loading","title":"8.5 Model Saving and Loading","text":"<pre><code># Save full model\ntrainer.save_model(\"./models/full_model\")\n\n# Save weights only\ntrainer.save_pretrained(\"./models/weights_only\")\n\n# Save as Safetensors format (recommended, faster and safer)\ntrainer.save_model(\"./models/safetensors_model\", safe_serialization=True)\n\n# Load model\nfrom dnallm import load_model_and_tokenizer\nmodel, tokenizer = load_model_and_tokenizer(\n    \"./models/final_model\",\n    task_config=configs[\"task\"]\n)\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#9-frequently-asked-questions","title":"9. Frequently Asked Questions","text":""},{"location":"user_guide/tutorial/end_to_end_workflow/#q1-what-to-do-if-cuda-out-of-memory","title":"Q1: What to do if CUDA out of memory?","text":"<p>Solutions: 1. Decrease <code>per_device_train_batch_size</code> 2. Increase <code>gradient_accumulation_steps</code> 3. Enable <code>fp16: true</code> 4. Use <code>gradient_checkpointing: true</code></p> <pre><code>finetune:\n  per_device_train_batch_size: 4\n  gradient_accumulation_steps: 8\n  fp16: true\n  gradient_checkpointing: true\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#q2-what-to-do-if-model-training-doesnt-converge","title":"Q2: What to do if model training doesn't converge?","text":"<p>Solutions: 1. Check data quality (are labels correct?) 2. Adjust learning rate (typically 1e-5 to 1e-4) 3. Increase training data 4. Use learning rate warmup</p> <pre><code>finetune:\n  learning_rate: 5e-5\n  warmup_ratio: 0.2\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#q3-how-to-choose-the-right-task-type","title":"Q3: How to choose the right task type?","text":"<p>Decision Tree:</p> <pre><code>Task type selection:\n\u251c\u2500\u2500 Binary classification \u2192 task_type: \"binary\"\n\u251c\u2500\u2500 Multi-class classification (mutually exclusive) \u2192 task_type: \"multiclass\"\n\u251c\u2500\u2500 Multi-class classification (not mutually exclusive) \u2192 task_type: \"multilabel\"\n\u251c\u2500\u2500 Predict continuous values \u2192 task_type: \"regression\"\n\u251c\u2500\u2500 Sequence labeling \u2192 task_type: \"token\"\n\u251c\u2500\u2500 Feature extraction \u2192 task_type: \"embedding\"\n\u251c\u2500\u2500 Mask prediction \u2192 task_type: \"mask\"\n\u2514\u2500\u2500 Text generation \u2192 task_type: \"generation\"\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#q4-how-to-resume-training-from-checkpoint","title":"Q4: How to resume training from checkpoint?","text":"<pre><code>trainer = DNATrainer(\n    model=model,\n    config=configs,\n    datasets=datasets,\n    resume_from_checkpoint=\"./outputs/checkpoint-1000\"\n)\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#q5-how-to-monitor-training-process","title":"Q5: How to monitor training process?","text":"<pre><code>finetune:\n  report_to: \"wandb\"  # or \"tensorboard\"\n\n  # Logging configuration\n  logging_strategy: \"steps\"\n  logging_steps: 100\n  eval_strategy: \"steps\"\n  eval_steps: 500\n</code></pre>"},{"location":"user_guide/tutorial/end_to_end_workflow/#q6-what-to-do-if-mcp-server-fails-to-start","title":"Q6: What to do if MCP server fails to start?","text":"<p>Troubleshooting Steps: 1. Check if port is occupied: <code>lsof -i :8000</code> 2. Check if model file exists 3. View log file: <code>cat ./logs/mcp_server.log</code> 4. Verify configuration file syntax: <code>python -c \"import yaml; yaml.safe_load(open('config.yaml'))\"</code></p>"},{"location":"user_guide/tutorial/end_to_end_workflow/#related-resources","title":"Related Resources","text":"<ul> <li>API Documentation - Detailed API documentation</li> <li>Configuration Documentation - Configuration file details</li> <li>Model Selection Guide - How to choose the right model</li> <li>FAQ - More frequently asked questions</li> </ul> <p>\ud83d\udca1 Tip: All code in this tutorial has been tested. It is recommended to complete each task in order to gradually master the core functionality of DNALLM.</p>"}]}