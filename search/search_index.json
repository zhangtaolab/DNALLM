{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DNALLM - DNA Large Language Model Toolkit","text":"<p>DNALLM is an open-source toolkit designed for large language model (LLM) applications in DNA sequence analysis and bioinformatics. It provides a comprehensive suite for model training, fine-tuning, inference, benchmarking, and evaluation, specifically tailored for DNA and genomics tasks.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Model Training &amp; Fine-tuning: Supports a variety of DNA-related tasks, including classification, regression, named entity recognition (NER), masked language modeling (MLM), and more.</li> <li>Inference &amp; Benchmarking: Enables efficient model inference, batch prediction, mutagenesis effect analysis, and multi-model benchmarking with visualization tools.</li> <li>Data Processing: Tools for dataset generation, cleaning, formatting, and adaptation to various DNA sequence formats.</li> <li>Model Management: Flexible loading and switching between different DNA language models, supporting both native mamba and transformer-compatible architectures.</li> <li>Extensibility: Modular design with utility functions and configuration modules for easy integration and secondary development.</li> <li>Protocol Support: Implements Model Context Protocol (MCP) for server/client deployment and integration into larger systems.</li> <li>Rich Examples &amp; Documentation: Includes interactive examples (marimo, notebooks) and detailed documentation to help users get started quickly.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li>Install dependencies (recommended: uv)</li> </ol> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n\ngit clone https://github.com/zhangtaolab/DNALLM.git\n\ncd DNALLM\n\nuv venv\n\nsource .venv/bin/activate\n\nuv pip install -e '.[base]'\n</code></pre> <ol> <li>Launch Jupyter Lab or Marimo for interactive development:</li> </ol> <pre><code>uv run jupyter lab\n   # or\nuv run marimo run xxx.py\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<ul> <li><code>dnallm/</code> : Core library (CLI, configuration, datasets, finetune, inference, models, tasks, utils, MCP)</li> <li><code>example/</code> : Interactive and notebook-based examples</li> <li><code>docs/</code> : Documentation</li> <li><code>scripts/</code> : Utility scripts</li> <li><code>tests/</code> : Test suite</li> </ul> <p>For more details, please refer to the README.md and contribution guidelines.</p>"},{"location":"api/datahandling/data/","title":"datahandling/data API","text":"<p>DNA Dataset handling and processing utilities.</p> <p>This module provides comprehensive tools for loading, processing, and managing DNA sequence datasets. It supports various file formats, data augmentation techniques, and statistical analysis.</p>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset","title":"<code>DNADataset</code>","text":"<p>A comprehensive wrapper for DNA sequence datasets with advanced processing capabilities.</p> <p>This class provides methods for loading DNA datasets from various sources (local files, Hugging Face Hub, ModelScope), encoding sequences with tokenizers, data augmentation, statistical analysis, and more.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <p>The underlying Hugging Face Dataset or DatasetDict</p> <code>tokenizer</code> <p>Tokenizer for sequence encoding</p> <code>max_length</code> <p>Maximum sequence length for tokenization</p> <code>sep</code> <p>Separator for multi-label data</p> <code>multi_label_sep</code> <p>Separator for multi-label sequences</p> <code>data_type</code> <p>Type of the dataset (classification, regression, etc.)</p> <code>stats</code> <p>Cached dataset statistics</p> <code>stats_for_plot</code> <p>Cached statistics for plotting</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>class DNADataset:\n    \"\"\"A comprehensive wrapper for DNA sequence datasets with advanced processing capabilities.\n\n    This class provides methods for loading DNA datasets from various sources (local files,\n    Hugging Face Hub, ModelScope), encoding sequences with tokenizers, data augmentation,\n    statistical analysis, and more.\n\n    Attributes:\n        dataset: The underlying Hugging Face Dataset or DatasetDict\n        tokenizer: Tokenizer for sequence encoding\n        max_length: Maximum sequence length for tokenization\n        sep: Separator for multi-label data\n        multi_label_sep: Separator for multi-label sequences\n        data_type: Type of the dataset (classification, regression, etc.)\n        stats: Cached dataset statistics\n        stats_for_plot: Cached statistics for plotting\n    \"\"\"\n\n    def __init__(self, ds: Union[Dataset, DatasetDict], tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512):\n        \"\"\"Initialize the DNADataset.\n\n        Args:\n            ds: A Hugging Face Dataset containing at least 'sequence' and 'label' fields\n            tokenizer: A Hugging Face tokenizer for encoding sequences\n            max_length: Maximum length for tokenization\n        \"\"\"\n        self.dataset = ds\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.sep = None\n        self.multi_label_sep = None\n        self.data_type = None\n        self.stats = None\n        self.stats_for_plot = None\n        self.__data_type__()  # Determine the data type of the dataset\n\n    @classmethod\n    def load_local_data(cls, file_paths, seq_col: str = \"sequence\", label_col: str = \"labels\",\n                        sep: str = None, fasta_sep: str = \"|\",\n                        multi_label_sep: Union[str, None] = None,\n                        tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512) -&gt; 'DNADataset':\n        \"\"\"Load DNA sequence datasets from one or multiple local files.\n\n        Supports input formats: csv, tsv, json, parquet, arrow, dict, fasta, txt.\n\n        Args:\n            file_paths: Single dataset: Provide one file path (e.g., \"data.csv\").\n                       Pre-split datasets: Provide a dict like {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n            seq_col: Column name for DNA sequences\n            label_col: Column name for labels\n            sep: Delimiter for CSV, TSV, or TXT\n            fasta_sep: Delimiter for FASTA files\n            multi_label_sep: Delimiter for multi-label sequences\n            tokenizer: A tokenizer for sequence encoding\n            max_length: Max token length\n\n        Returns:\n            DNADataset: An instance wrapping a Dataset or DatasetDict\n\n        Raises:\n            ValueError: If file type is not supported\n        \"\"\"\n        # Set separators\n        cls.sep = sep\n        cls.multi_label_sep = multi_label_sep\n        # Check if input is a list or dict\n        if isinstance(file_paths, dict):  # Handling multiple files (pre-split datasets)\n            ds_dict = {}\n            for split, path in file_paths.items():\n                ds_dict[split] = cls._load_single_data(path, seq_col, label_col, sep, fasta_sep, multi_label_sep)\n            dataset = DatasetDict(ds_dict)\n        else:  # Handling a single file\n            dataset = cls._load_single_data(file_paths, seq_col, label_col, sep, fasta_sep, multi_label_sep)\n        dataset.stats = None  # Initialize stats as None\n\n        return cls(dataset, tokenizer=tokenizer, max_length=max_length)\n\n    @classmethod\n    def _load_single_data(cls, file_path, seq_col: str = \"sequence\", label_col: str = \"labels\",\n                          sep: str = None, fasta_sep: str = \"|\",\n                          multi_label_sep: Union[str, None] = None) -&gt; Dataset:\n        \"\"\"Load DNA data (sequences and labels) from a local file.\n\n        Supported file types: \n          - For structured formats (CSV, TSV, JSON, Parquet, Arrow, dict), uses load_dataset from datasets.\n          - For FASTA and TXT, uses custom parsing.\n\n        Args:\n            file_path: For most file types, a path (or pattern) to the file(s). For 'dict', a dictionary.\n            seq_col: Name of the column containing the DNA sequence\n            label_col: Name of the column containing the label\n            sep: Delimiter for CSV, TSV, or TXT files\n            fasta_sep: Delimiter for FASTA files\n            multi_label_sep: Delimiter for multi-label sequences\n\n        Returns:\n            Dataset: A Hugging Face Dataset with 'sequence' and 'labels' columns\n\n        Raises:\n            ValueError: If file type is not supported\n        \"\"\"\n        if isinstance(file_path, list):\n            file_path = [os.path.expanduser(fpath) for fpath in file_path]\n            file_type = os.path.basename(file_path[0]).split(\".\")[-1].lower()\n        else:\n            file_path = os.path.expanduser(file_path)\n            file_type = os.path.basename(file_path).split(\".\")[-1].lower()\n        # Define data type\n        default_types = [\"csv\", \"tsv\", \"json\", \"parquet\", \"arrow\"]\n        dict_types = [\"pkl\", \"pickle\", \"dict\"]\n        fasta_types = [\"fa\", \"fna\", \"fas\", \"fasta\"]\n        # Check if the file contains a header\n        if file_type in [\"csv\", \"tsv\", \"txt\"]:\n            if file_type == \"csv\":\n                sep = sep if sep else \",\"\n            with open(file_path, \"r\") as f:\n                header = f.readline().strip()\n                if not header or (seq_col not in header and label_col not in header):\n                    file_type = \"txt\"  # Treat as TXT if no header found\n        # For structured formats that load via datasets.load_dataset\n        if file_type in default_types:\n            if file_type in [\"csv\", \"tsv\"]:\n                sep = sep or (\",\" if file_type == \"csv\" else \"\\t\")\n                ds = load_dataset(\"csv\", data_files=file_path, split=\"train\", delimiter=sep)\n            elif file_type == \"json\":\n                ds = load_dataset(\"json\", data_files=file_path, split=\"train\")\n            elif file_type in [\"parquet\", \"arrow\"]:\n                ds = load_dataset(file_type, data_files=file_path, split=\"train\")\n            # Rename columns if needed\n            if seq_col != \"sequence\":\n                ds = ds.rename_column(seq_col, \"sequence\")\n            if label_col != \"labels\":\n                ds = ds.rename_column(label_col, \"labels\")\n        elif file_type in dict_types:\n            # Here, file_path is assumed to be a dictionary.\n            import pickle\n            data = pickle.load(open(file_path, 'rb'))\n            ds = Dataset.from_dict(data)\n            if seq_col != \"sequence\" or label_col != \"labels\":\n                if seq_col in ds.column_names:\n                    if \"sequence\" not in ds.features:\n                        ds = ds.rename_column(seq_col, \"sequence\")\n                if label_col in ds.column_names:\n                    if \"labels\" not in ds.features:\n                        ds = ds.rename_column(label_col, \"labels\")\n        elif file_type in fasta_types:\n            sequences, labels = [], []\n            with open(file_path, \"r\") as f:\n                seq = \"\"\n                lab = None\n                for line in f:\n                    line = line.strip()\n                    if line.startswith(\"&gt;\"):\n                        if seq and lab is not None:\n                            sequences.append(seq)\n                            labels.append(lab)\n                        lab = line[1:].strip().split(fasta_sep)[-1]  # Assume label is separated by `fasta_sep` in the header\n                        seq = \"\"\n                    else:\n                        seq += line.strip()\n                if seq and lab is not None:\n                    sequences.append(seq)\n                    labels.append(lab)\n            ds = Dataset.from_dict({\"sequence\": sequences, \"labels\": labels})\n        elif file_type == \"txt\":\n            # Assume each line contains a sequence and a label separated by whitespace or a custom sep.\n            sequences, labels = [], []\n            with open(file_path, \"r\") as f:\n                for i,line in enumerate(f):\n                    if i == 0:\n                        # Contain header, use load_dataset with csv method\n                        if seq_col in line and label_col in line:\n                            ds = load_dataset(\"csv\", data_files=file_path, split=\"train\", delimiter=sep)\n                            break\n                    record = line.strip().split(sep) if sep else line.strip().split()\n                    if len(record) &gt;= 2:\n                        sequences.append(record[0])\n                        labels.append(record[1])\n                    else:\n                        continue\n            ds = Dataset.from_dict({\"sequence\": sequences, \"labels\": labels})\n        else:\n            raise ValueError(f\"Unsupported file type: {file_type}\")\n        # Convert string labels to integer\n        def format_labels(example):\n            labels = example['labels']\n            if isinstance(labels, str):\n                if multi_label_sep:\n                    example['labels'] = [float(x) for x in labels.split(multi_label_sep)]\n                else:\n                    example['labels'] = float(labels) if '.' in labels else int(labels)\n            return example\n        if 'labels' in ds.column_names:\n            ds = ds.map(format_labels, desc=\"Format labels\")\n        # Return processed dataset\n        return ds\n\n    @classmethod\n    def from_huggingface(cls, dataset_name: str,\n                         seq_col: str = \"sequence\", label_col: str = \"labels\",\n                         data_dir: Union[str, None]=None,\n                         tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512) -&gt; 'DNADataset':\n        \"\"\"Load a dataset from the Hugging Face Hub.\n\n        Args:\n            dataset_name: Name of the dataset\n            seq_col: Column name for the DNA sequence\n            label_col: Column name for the label\n            data_dir: Data directory in a dataset\n            tokenizer: Tokenizer for sequence encoding\n            max_length: Max token length\n\n        Returns:\n            DNADataset: An instance wrapping a datasets.Dataset\n        \"\"\"\n        if data_dir:\n            ds = load_dataset(dataset_name, data_dir=data_dir)\n        else:\n            ds = load_dataset(dataset_name)\n        # Rename columns if necessary\n        if seq_col != \"sequence\":\n            ds = ds.rename_column(seq_col, \"sequence\")\n        if label_col != \"labels\":\n            ds = ds.rename_column(label_col, \"labels\")\n        return cls(ds, tokenizer=tokenizer, max_length=max_length)\n\n    @classmethod\n    def from_modelscope(cls, dataset_name: str,\n                        seq_col: str = \"sequence\", label_col: str = \"labels\",\n                        data_dir: Union[str, None]=None,\n                        tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512) -&gt; 'DNADataset':\n        \"\"\"Load a dataset from the ModelScope.\n\n        Args:\n            dataset_name: Name of the dataset\n            seq_col: Column name for the DNA sequence\n            label_col: Column name for the label\n            data_dir: Data directory in a dataset\n            tokenizer: Tokenizer for sequence encoding\n            max_length: Max token length\n\n        Returns:\n            DNADataset: An instance wrapping a datasets.Dataset\n        \"\"\"\n        from modelscope import MsDataset\n\n        if data_dir:\n            ds = MsDataset.load(dataset_name, data_dir=data_dir)\n        else:\n            ds = MsDataset.load(dataset_name)\n        # Rename columns if necessary\n        if seq_col != \"sequence\":\n            ds = ds.rename_column(seq_col, \"sequence\")\n        if label_col != \"labels\":\n            ds = ds.rename_column(label_col, \"labels\")\n        return cls(ds, tokenizer=tokenizer, max_length=max_length)\n\n    def encode_sequences(self, padding: str = \"max_length\", return_tensors: str = \"pt\",\n                         remove_unused_columns: bool = False,\n                         uppercase: bool=False, lowercase: bool=False,\n                         task: Optional[str] = 'SequenceClassification'):\n        \"\"\"Encode all sequences using the provided tokenizer.\n\n        The dataset is mapped to include tokenized fields along with the label,\n        making it directly usable with Hugging Face Trainer.\n\n        Args:\n            padding: Padding strategy for sequences. Can be 'max_length' or 'longest'.\n                    Use 'longest' to pad to the length of the longest sequence in case of memory outage\n            return_tensors: Returned tensor types, can be 'pt', 'tf', 'np', or 'jax'\n            remove_unused_columns: Whether to remove the original 'sequence' and 'label' columns\n            uppercase: Whether to convert sequences to uppercase\n            lowercase: Whether to convert sequences to lowercase\n            task: Task type for the tokenizer. If not provided, defaults to 'SequenceClassification'\n\n        Raises:\n            ValueError: If tokenizer is not provided\n        \"\"\"\n        if self.tokenizer:\n            sp_token_map = self.tokenizer.special_tokens_map\n            pad_token = sp_token_map['pad_token'] if 'pad_token' in sp_token_map else None\n            pad_id = self.tokenizer.encode(pad_token)[-1] if pad_token else None\n            cls_token = sp_token_map['cls_token'] if 'cls_token' in sp_token_map else None\n            sep_token = sp_token_map['sep_token'] if 'sep_token' in sp_token_map else None\n            eos_token = sp_token_map['eos_token'] if 'eos_token' in sp_token_map else None\n            max_length = self.max_length\n        else:\n            raise ValueError(\"Tokenizer not provided.\")\n        def tokenize_for_sequence_classification(example):\n            sequences = example[\"sequence\"]\n            if uppercase:\n                sequences = [x.upper() for x in sequences]\n            if lowercase:\n                sequences = [x.lower() for x in sequences]\n            tokenized = self.tokenizer(\n                sequences,\n                truncation=True,\n                padding=padding,\n                max_length=max_length\n            )\n            return tokenized\n        def tokenize_for_token_classification(examples):\n\n            tokenized_examples = {'sequence': [],\n                                  'input_ids': [],\n                                  # 'token_type_ids': [],\n                                  'attention_mask': []}\n            if 'labels' in examples:\n                tokenized_examples['labels'] = []\n            input_seqs = examples['sequence']\n            if isinstance(input_seqs, str):\n                input_seqs = input_seqs.split(self.multi_label_sep)\n            for i, example_tokens in enumerate(input_seqs):\n                all_ids = [x for x in self.tokenizer.encode(example_tokens, is_split_into_words=True)]\n                if 'labels' in examples:\n                    example_ner_tags = examples['labels'][i]\n                else:\n                    example_ner_tags = [0] * len(example_tokens)\n                pad_len = max_length - len(all_ids)\n                if pad_len &gt;= 0:\n                    all_masks = [1] * len(all_ids) + [0] * pad_len\n                    all_ids = all_ids + [pad_id] * pad_len\n                    if cls_token:\n                        if sep_token:\n                            example_tokens = [cls_token] + example_tokens + [sep_token] + [pad_token] * pad_len\n                            example_ner_tags = [-100] + example_ner_tags + [-100] * (pad_len + 1)\n                        elif eos_token:\n                            example_tokens = [cls_token] + example_tokens + [eos_token] + [pad_token] * pad_len\n                            example_ner_tags = [-100] + example_ner_tags + [-100] * (pad_len + 1)\n                        else:\n                            example_tokens = [cls_token] + example_tokens + [pad_token] * pad_len\n                            example_ner_tags = [-100] + example_ner_tags + [-100] * pad_len\n                    else:\n                        example_tokens = example_tokens + [pad_token] * pad_len\n                        example_ner_tags = example_ner_tags + [-100] * pad_len\n                elif pad_len &lt; 0:\n                    all_ids = all_ids[:max_length]\n                    all_masks = [1] * (max_length)\n                    if cls_token:\n                        if sep_token:\n                            example_tokens = [cls_token] + example_tokens[:max_length - 2] + [sep_token]\n                            example_ner_tags = [-100] + example_ner_tags[:max_length - 2] + [-100]\n                        else:\n                            example_tokens = [cls_token] + example_tokens[:max_length - 1]\n                            example_ner_tags = [-100] + example_ner_tags[:max_length - 1]\n                    else:\n                        example_tokens = example_tokens[:max_length]\n                        example_ner_tags = example_ner_tags[:max_length]\n                tokenized_examples['sequence'].append(example_tokens)\n                tokenized_examples['input_ids'].append(all_ids)\n                # tokenized_examples['token_type_ids'].append([0] * max_length)\n                tokenized_examples['attention_mask'].append(all_masks)\n                if 'labels' in examples:\n                    tokenized_examples['labels'].append(example_ner_tags)\n            return BatchEncoding(tokenized_examples)\n        # Judge the task type\n        task = task.lower()\n        if task in ['sequenceclassification', 'binary', 'multiclass', 'multilabel', 'regression']:\n            self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True, desc=\"Encoding inputs\")\n        elif task in ['tokenclassification', 'token', 'ner']:\n            from transformers.tokenization_utils_base import BatchEncoding\n            self.dataset = self.dataset.map(tokenize_for_token_classification, batched=True, desc=\"Encoding inputs\")\n        elif task in ['maskedlm', 'mlm', 'mask', 'embedding']:\n            self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True, desc=\"Encoding inputs\")\n        elif task in ['causallm', 'clm', 'causal', 'generation', 'embedding']:\n            self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True)\n        else:\n            self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True, desc=\"Encoding inputs\")\n        if remove_unused_columns:\n            used_cols = ['labels', 'input_ids', 'attention_mask']\n            if isinstance(self.dataset, DatasetDict):\n                for dt in self.dataset:\n                    unused_cols = [f for f in self.dataset[dt].features if f not in used_cols]\n                    self.dataset[dt] = self.dataset[dt].remove_columns(unused_cols)\n            else:\n                unused_cols = [f for f in self.dataset.features if f not in used_cols]\n                self.dataset = self.dataset.remove_columns(unused_cols)\n        if return_tensors == \"tf\":\n            self.dataset.set_format(type=\"tensorflow\")\n        elif return_tensors == \"jax\":\n            self.dataset.set_format(type=\"jax\")\n        elif return_tensors == \"np\":\n            self.dataset.set_format(type=\"numpy\")\n        else:\n            self.dataset.set_format(type=\"torch\")\n        self.dataset._is_encoded = True  # Mark the dataset as encoded\n\n    def split_data(self, test_size: float = 0.2, val_size: float = 0.1, seed: int = None):\n        \"\"\"Split the dataset into train, test, and validation sets.\n\n        Args:\n            test_size: Proportion of the dataset to include in the test split\n            val_size: Proportion of the dataset to include in the validation split\n            seed: Random seed for reproducibility\n        \"\"\"\n        # First, split off test+validation from training data\n        split_result = self.dataset.train_test_split(test_size=test_size + val_size, seed=seed)\n        train_ds = split_result['train']\n        temp_ds = split_result['test']\n        # Further split temp_ds into test and validation sets\n        if val_size &gt; 0:\n            rel_val_size = val_size / (test_size + val_size)\n            temp_split = temp_ds.train_test_split(test_size=rel_val_size, seed=seed)\n            test_ds = temp_split['train']\n            val_ds = temp_split['test']\n            self.dataset = DatasetDict({'train': train_ds, 'test': test_ds, 'val': val_ds})\n        else:\n            self.dataset = DatasetDict({'train': train_ds, 'test': temp_ds})\n\n    def shuffle(self, seed: int = None):\n        \"\"\"Shuffle the dataset.\n\n        Args:\n            seed: Random seed for reproducibility\n        \"\"\"\n        self.dataset.shuffle(seed=seed)\n\n    def validate_sequences(self, minl: int = 20, maxl: int = 6000, gc: tuple = (0,1), valid_chars: str = \"ACGTN\"):\n        \"\"\"Filter the dataset to keep sequences containing valid DNA bases or allowed length.\n\n        Args:\n            minl: Minimum length of the sequences\n            maxl: Maximum length of the sequences\n            gc: GC content range between 0 and 1\n            valid_chars: Allowed characters in the sequences\n        \"\"\"\n        self.dataset = self.dataset.filter(\n            lambda example: check_sequence(example[\"sequence\"], minl, maxl, gc, valid_chars)\n        )\n\n    def random_generate(self, minl: int, maxl: int = 0, samples: int = 1,\n                              gc: tuple = (0,1), N_ratio: float = 0.0,\n                              padding_size: int = 0, seed: int = None,\n                              label_func = None, append: bool = False):\n        \"\"\"Replace the current dataset with randomly generated DNA sequences.\n\n        Args:\n            minl: Minimum length of the sequences\n            maxl: Maximum length of the sequences, default is the same as minl\n            samples: Number of sequences to generate, default 1\n            gc: GC content range, default (0,1)\n            N_ratio: Include N base in the generated sequence, default 0.0\n            padding_size: Padding size for sequence length, default 0\n            seed: Random seed, default None\n            label_func: A function that generates a label from a sequence\n            append: Append the random generated data to the existing dataset or use the data as a dataset\n        \"\"\"\n        def process(minl, maxl, number, gc, N_ratio, padding_size, seed, label_func):\n            sequences = random_generate_sequences(minl=minl, maxl=maxl, samples=number,\n                                                gc=gc, N_ratio=N_ratio,\n                                                padding_size=padding_size, seed=seed)\n            labels = []\n            for seq in sequences:\n                labels.append(label_func(seq) if label_func else 0)\n            random_ds = Dataset.from_dict({\"sequence\": sequences, \"labels\": labels})\n            return random_ds\n        if append:\n            if isinstance(self.dataset, DatasetDict):\n                for dt in self.dataset:\n                    number = round(samples * len(self.dataset[dt]) / sum(self.__len__().values()))\n                    random_ds = process(minl, maxl, number, gc, N_ratio, padding_size, seed, label_func)\n                    self.dataset[dt] = concatenate_datasets([self.dataset[dt], random_ds])\n            else:\n                random_ds = process(minl, maxl, samples, gc, N_ratio, padding_size, seed, label_func)\n                self.dataset = concatenate_datasets([self.dataset, random_ds])\n        else:\n            self.dataset = process(minl, maxl, samples, gc, N_ratio, padding_size, seed, label_func)\n\n    def process_missing_data(self):\n        \"\"\"Filter out samples with missing or empty sequences or labels.\"\"\"\n        def non_missing(example):\n            return example[\"sequence\"] and example[\"labels\"] is not None and example[\"sequence\"].strip() != \"\"\n        self.dataset = self.dataset.filter(non_missing)\n\n    def raw_reverse_complement(self, ratio: float = 0.5, seed: int = None):\n        \"\"\"Do reverse complement of sequences in the dataset.\n\n        Args:\n            ratio: Ratio of sequences to reverse complement\n            seed: Random seed for reproducibility\n        \"\"\"\n        def process(ds, ratio, seed):\n            random.seed(seed)\n            number = len(ds[\"sequence\"])\n            idxlist = set(random.sample(range(number), int(number * ratio)))\n            def concat_fn(example, idx):\n                rc = reverse_complement(example[\"sequence\"])\n                if idx in idxlist:\n                    example[\"sequence\"] = rc\n                return example\n            # Create a dataset with random reverse complement.\n            ds.map(concat_fn, with_indices=True, desc=\"Reverse complementary\")\n            return ds\n        if isinstance(self.dataset, DatasetDict):\n            for dt in self.dataset:\n                self.dataset[dt] = process(self.dataset[dt], ratio, seed)\n        else:\n            self.dataset = process(self.dataset, ratio, seed)\n\n    def augment_reverse_complement(self, reverse=True, complement=True):\n        \"\"\"Augment the dataset by adding reverse complement sequences.\n\n        This method doubles the dataset size.\n\n        Args:\n            reverse: Whether to do reverse\n            complement: Whether to do complement\n        \"\"\"\n        def process(ds, reverse, complement):\n            # Create a dataset with an extra field for the reverse complement.\n            def add_rc(example):\n                example[\"rc_sequence\"] = reverse_complement(\n                    example[\"sequence\"], reverse=reverse, complement=complement\n                )\n                return example\n            ds_with_rc = ds.map(add_rc, desc=\"Reverse complementary\")\n            # Build a new dataset where the reverse complement becomes the 'sequence'\n            rc_ds = ds_with_rc.map(lambda ex: {\"sequence\": ex[\"rc_sequence\"], \"labels\": ex[\"labels\"]}, desc=\"Data augment\")\n            ds = concatenate_datasets([ds, rc_ds])\n            ds.remove_columns([\"rc_sequence\"])\n            return ds\n        if isinstance(self.dataset, DatasetDict):\n            for dt in self.dataset:\n                self.dataset[dt] = process(self.dataset[dt], reverse, complement)\n        else:\n            self.dataset = process(self.dataset, reverse, complement)\n\n    def concat_reverse_complement(self, reverse=True, complement=True, sep: str = \"\"):\n        \"\"\"Augment each sample by concatenating the sequence with its reverse complement.\n\n        Args:\n            reverse: Whether to do reverse\n            complement: Whether to do complement\n            sep: Separator between the original and reverse complement sequences\n        \"\"\"\n        def process(ds, reverse, complement, sep):\n            def concat_fn(example):\n                rc = reverse_complement(example[\"sequence\"], reverse=reverse, complement=complement)\n                example[\"sequence\"] = example[\"sequence\"] + sep + rc\n                return example\n            ds = ds.map(concat_fn, desc=\"Data augment\")\n            return ds\n        if isinstance(self.dataset, DatasetDict):\n            for dt in self.dataset:\n                self.dataset[dt] = process(self.dataset[dt], reverse, complement, sep)\n        else:\n            self.dataset = process(self.dataset, reverse, complement, sep)\n\n    def sampling(self, ratio: float=1.0, seed: int = None, overwrite: bool=False) -&gt; Union[Dataset, DatasetDict]:\n        \"\"\"Randomly sample a fraction of the dataset.\n\n        Args:\n            ratio: Fraction of the dataset to sample. Default is 1.0 (no sampling)\n            seed: Random seed for reproducibility\n            overwrite: Whether to overwrite the original dataset with the sampled one\n\n        Returns:\n            A sampled dataset if overwrite=False, otherwise None\n        \"\"\"\n        dataset = self.dataset\n        if isinstance(dataset, DatasetDict):\n            for dt in dataset.keys():\n                random.seed(seed)\n                random_idx = random.sample(range(len(dataset[dt])), int(len(dataset[dt]) * ratio))\n                dataset[dt] = dataset[dt].select(random_idx)\n        else:\n            random_idx = random.sample(range(len(dataset)), int(len(dataset) * ratio))\n            dataset = dataset.select(random_idx)\n        if overwrite:\n            self.dataset = dataset\n        else:\n            return dataset\n\n    def head(self, head: int=10, show: bool=False) -&gt; Union[dict, None]:\n        \"\"\"Fetch the head n data from the dataset.\n\n        Args:\n            head: Number of samples to fetch\n            show: Whether to print the data or return it\n\n        Returns:\n            A dictionary containing the first n samples if show=False, otherwise None\n        \"\"\"\n        import pprint\n        def format_convert(data):\n            df = {}\n            length = len(data[\"sequence\"])\n            for i in range(length):\n                df[i] = {}\n                for key in data.keys():\n                    df[i][key] = data[key][i]\n            return df\n        dataset = self.dataset\n        if isinstance(dataset, DatasetDict):\n            df = {}\n            for dt in dataset.keys():\n                data = dataset[dt][:head]\n                if show:\n                    print(f\"Dataset: {dt}\")\n                    pprint.pp(format_convert(data))\n                else:\n                    df[dt] = data\n                    return df\n        else:\n            data = dataset[dt][:head]\n            if show:\n                pprint.pp(format_convert(data))\n            else:\n                return data\n\n    def show(self, head: int=10):\n        \"\"\"Display the dataset.\n\n        Args:\n            head: Number of samples to display\n        \"\"\"\n        self.head(head=head, show=True)            \n\n    def iter_batches(self, batch_size: int):\n        \"\"\"Generator that yields batches of examples from the dataset.\n\n        Args:\n            batch_size: Size of each batch\n\n        Yields:\n            A batch of examples\n\n        Raises:\n            ValueError: If dataset is a DatasetDict\n        \"\"\"\n        if isinstance(self.dataset, DatasetDict):\n            raise ValueError(\"Dataset is a DatasetDict Object, please use `DNADataset.dataset[datatype].iter_batches(batch_size)` instead.\")\n        else:\n            for i in range(0, len(self.dataset), batch_size):\n                yield self.dataset[i: i + batch_size]\n\n    def __len__(self):\n        \"\"\"Return the length of the dataset.\n\n        Returns:\n            Length of the dataset or dict of lengths for DatasetDict\n        \"\"\"\n        if isinstance(self.dataset, DatasetDict):\n            return {dt: len(self.dataset[dt]) for dt in self.dataset}\n        else:\n            return len(self.dataset)\n\n    def __getitem__(self, idx):\n        \"\"\"Get an item from the dataset.\n\n        Args:\n            idx: Index of the item to retrieve\n\n        Returns:\n            The item at the specified index\n\n        Raises:\n            ValueError: If dataset is a DatasetDict\n        \"\"\"\n        if isinstance(self.dataset, DatasetDict):\n            raise ValueError(\"Dataset is a DatasetDict Object, please use `DNADataset.dataset[datatype].__getitem__(idx)` instead.\")\n        else:\n            return self.dataset[idx]\n\n    def __data_type__(self):\n        \"\"\"Get the data type of the dataset (classification, regression, etc.).\n\n        This method analyzes the labels to determine if the dataset is for:\n        - classification (integer or string labels)\n        - regression (float labels)\n        - multi-label (multiple labels per sample)\n        - multi-regression (multiple float values per sample)\n        \"\"\"\n        if isinstance(self.dataset, DatasetDict):\n            keys = list(self.dataset.keys())\n            if not keys:\n                raise ValueError(\"DatasetDict is empty.\")\n            if \"labels\" in self.dataset[keys[0]].column_names:\n                labels = self.dataset[keys[0]][\"labels\"]\n            else:\n                labels = None\n        else:\n            if \"labels\" in self.dataset.column_names:\n                labels = self.dataset[\"labels\"]\n            else:\n                labels = None\n        if labels is not None:\n            if isinstance(labels[0], str):\n                if self.multi_label_sep and self.multi_label_sep in labels:\n                    multi_labels = labels.split(self.multi_label_sep)\n                    if '.' in multi_labels[0]:\n                        self.data_type = \"multi_regression\"\n                    else:\n                        self.data_type = \"multi_label\"\n                else:\n                    if '.' in labels[0]:\n                        self.data_type = \"regression\"\n                    else:\n                        self.data_type = \"classification\"\n            else:\n                if isinstance(labels[0], int):\n                    self.data_type = \"classification\"\n                else:\n                    self.data_type = \"regression\"\n\n    def statistics(self) -&gt; dict:\n        \"\"\"Get statistics of the dataset.\n\n        Includes number of samples, sequence length (min, max, average, median), \n        label distribution, GC content (by labels), nucleotide composition (by labels).\n\n        Returns:\n            A dictionary containing statistics of the dataset\n\n        Raises:\n            ValueError: If statistics have not been computed yet\n        \"\"\"\n\n        def prepare_dataframe(dataset) -&gt; pd.DataFrame:\n            \"\"\"Convert a datasets.Dataset to pandas DataFrame if needed.\n\n            If the input is already a pandas DataFrame, return a copy.\n            \"\"\"\n            # avoid importing datasets at top-level to keep dependency optional\n            try:\n                from datasets import Dataset\n                is_dataset = isinstance(dataset, Dataset)\n            except Exception:\n                is_dataset = False\n\n            if is_dataset:\n                df = dataset.to_pandas()\n            elif isinstance(dataset, pd.DataFrame):\n                df = dataset.copy()\n            else:\n                raise ValueError('prepare_dataframe expects a datasets.Dataset or pandas.DataFrame')\n            return df\n\n        def compute_basic_stats(df: pd.DataFrame, seq_col: str = 'sequence') -&gt; dict:\n            \"\"\"Compute number of samples and sequence length statistics.\"\"\"\n            seqs = df[seq_col].fillna('').astype(str)\n            lens = seqs.str.len()\n            return {\n                'n_samples': int(len(lens)),\n                'min_len': int(lens.min()) if len(lens) &gt; 0 else 0,\n                'max_len': int(lens.max()) if len(lens) &gt; 0 else 0,\n                'mean_len': float(lens.mean()) if len(lens) &gt; 0 else float('nan'),\n                'median_len': float(lens.median()) if len(lens) &gt; 0 else float('nan'),\n            }\n\n        stats = {}\n        seq_col = \"sequence\"\n        label_col = \"labels\"\n        if isinstance(self.dataset, DatasetDict):\n            for split_name, split_ds in self.dataset.items():\n                df = prepare_dataframe(split_ds)\n                data_type = self.data_type\n                basic = compute_basic_stats(df, seq_col)\n                stats[split_name] = {'data_type': data_type, **basic}\n        else:\n            df = prepare_dataframe(self.dataset)\n            data_type = self.data_type\n            basic = compute_basic_stats(df, seq_col)\n            stats['full'] = {'data_type': data_type, **basic}\n\n        self.stats = stats  # Store stats in the instance for later use\n        self.stats_for_plot = df\n\n        return stats\n\n    def plot_statistics(self, save_path: str = None):\n        \"\"\"Plot statistics of the dataset.\n\n        Includes sequence length distribution (histogram), \n        GC content distribution (box plot) for each sequence.\n        If dataset is a DatasetDict, length plots and GC content plots from different datasets will be \n        concatenated into a single chart, respectively.\n        Sequence length distribution is shown as a histogram, with min and max lengths for its' limit.\n\n        Args:\n            save_path: Path to save the plots. If None, plots will be shown interactively\n\n        Raises:\n            ValueError: If statistics have not been computed yet\n        \"\"\"\n\n        import altair as alt\n        from typing import Tuple\n        alt.data_transformers.enable(\"vegafusion\")\n\n        def parse_multi_labels(series: pd.Series) -&gt; pd.DataFrame:\n            \"\"\"Split semicolon-separated labels in a Series into a dataframe of columns.\n\n            Example: '0;1;1' -&gt; columns ['label_0','label_1','label_2']\n            \"\"\"\n            rows = []\n            maxlen = 0\n            for v in series.fillna(''):\n                if v == '':\n                    parts = []\n                else:\n                    parts = [p.strip() for p in str(v).split(';')]\n                rows.append(parts)\n                if len(parts) &gt; maxlen:\n                    maxlen = len(parts)\n            cols = [f'label_{i}' for i in range(maxlen)]\n            parsed = [r + [''] * (maxlen - len(r)) for r in rows]\n            df = pd.DataFrame(parsed, columns=cols)\n\n            # try convert numeric types\n            for c in df.columns:\n                df[c] = pd.to_numeric(df[c].replace('', np.nan))\n            return df\n\n        def classification_plots(df: pd.DataFrame, label_col: str = 'labels', seq_col: str = 'sequence') -&gt; alt.Chart:\n            \"\"\"Build histogram of seq lengths colorized by label and GC boxplot grouped by label.\n\n            For multi-label (where label column contains semicolon), this function expects the\n            caller to have split and called per-sublabel as necessary.\n            \"\"\"\n            # ensure label is a categorical column\n            df = df.copy()\n            df['label_str'] = df[label_col].astype(str)\n            df['seq_len'] = df[seq_col].fillna('').astype(str).str.len()\n            df['gc'] = df[seq_col].fillna('').astype(str).map(calc_gc_content)\n\n            # histogram: seq length, colored by label\n            hist = alt.Chart(df).mark_bar(opacity=0.7).encode(\n                x=alt.X('seq_len:Q', bin=alt.Bin(maxbins=60), title='Sequence length'),\n                y=alt.Y('count():Q', title='Count'),\n                color=alt.Color('label_str:N', title='Label')\n            ).properties(width=300, height=240)\n\n            # GC boxplot grouped by label\n            box = alt.Chart(df).mark_boxplot(size=20).encode(\n                x=alt.X('label_str:N', title='Label'),\n                y=alt.Y('gc:Q', title='GC content'),\n                color=alt.Color('label_str:N', legend=None)\n            ).properties(width=300, height=240)\n\n            return hist, box\n\n        def regression_plots(df: pd.DataFrame, label_col: str = 'labels', seq_col: str = 'sequence') -&gt; Tuple[alt.Chart, alt.Chart]:\n            \"\"\"Build histogram of seq lengths (ungrouped) and GC scatter (GC vs label value).\n\n            For multi-regression, caller should split and call per target.\n            \"\"\"\n            df = df.copy()\n            df['seq_len'] = df[seq_col].fillna('').astype(str).str.len()\n            df['gc'] = df[seq_col].fillna('').astype(str).map(calc_gc_content)\n\n            hist = alt.Chart(df).mark_bar(opacity=0.7).encode(\n                x=alt.X('seq_len:Q', bin=alt.Bin(maxbins=60), title='Sequence length'),\n                y=alt.Y('count():Q', title='Count')\n            ).properties(width=300, height=240)\n\n            # ensure numeric label\n            df['label_val'] = pd.to_numeric(df[label_col], errors='coerce')\n            scatter = alt.Chart(df).mark_point().encode(\n                x=alt.X('gc:Q', title='GC content'),\n                y=alt.Y('label_val:Q', title='Label value'),\n                tooltip=[alt.Tooltip('seq_len:Q'), alt.Tooltip('gc:Q'), alt.Tooltip('label_val:Q')]\n            ).properties(width=300, height=240)\n\n            return hist, scatter\n\n        def per_split_charts(df: pd.DataFrame, data_type: str, seq_col: str, label_col: str) -&gt; alt.Chart:\n            \"\"\"Return a combined Altair chart for a single split (DataFrame) based on data_type.\n\n            Behavior aligned with user requirement:\n            - For 'classification' or 'regression' (single-label): seq_len and GC plots are concatenated horizontally.\n            - For 'multi-classification' and 'multi-regression': sublabels' results are concatenated horizontally\n            and the pair (seq_len, GC) for each sublabel are concatenated vertically.\n            \"\"\"\n            if data_type == 'classification':\n                hist, box = classification_plots(df, label_col, seq_col)\n                combined = alt.hconcat(hist, box)\n                return combined.properties(title='Classification stats')\n\n            if data_type == 'regression':\n                hist, scatter = regression_plots(df, label_col, seq_col)\n                combined = alt.hconcat(hist, scatter)\n                return combined.properties(title='Regression stats')\n\n            if data_type in ('multi-classification', 'multi-regression'):\n                # split labels into subcolumns\n                lbls_df = parse_multi_labels(df[label_col])\n                per_subcharts = []\n                for c in lbls_df.columns:\n                    subdf = df.copy()\n                    subdf[c] = lbls_df[c]\n                    # drop nan labels (optional) but keep sequences\n                    if data_type == 'multi-classification':\n                        # treat each sublabel like single classification\n                        subdf_for_plot = subdf.copy()\n                        subdf_for_plot['labels_for_plot'] = subdf[c].astype('Int64').astype(str)\n                        hist = alt.Chart(subdf_for_plot).mark_bar(opacity=0.7).encode(\n                            x=alt.X('seq_len:Q', bin=alt.Bin(maxbins=50), title='Sequence length'),\n                            y='count():Q',\n                            color=alt.Color('labels_for_plot:N', title=f'{c}')\n                        ).properties(width=260, height=200)\n\n                        box = alt.Chart(subdf_for_plot).mark_boxplot(size=20).encode(\n                            x=alt.X('labels_for_plot:N', title=f'{c}'),\n                            y=alt.Y('gc:Q', title='GC content'),\n                            color=alt.Color('labels_for_plot:N', legend=None)\n                        ).properties(width=260, height=200)\n                        pair = alt.vconcat(hist, box).properties(title=f'Sub-label {c}')\n                        per_subcharts.append(pair)\n\n                    else:  # multi-regression\n                        subdf_for_plot = subdf.copy()\n                        subdf_for_plot['label_val'] = pd.to_numeric(subdf[c], errors='coerce')\n                        hist = alt.Chart(subdf_for_plot).mark_bar(opacity=0.7).encode(\n                            x=alt.X('seq_len:Q', bin=alt.Bin(maxbins=50), title='Sequence length'),\n                            y='count():Q'\n                        ).properties(width=260, height=200)\n\n                        scatter = alt.Chart(subdf_for_plot).mark_point().encode(\n                            x=alt.X('gc:Q', title='GC content'),\n                            y=alt.Y('label_val:Q', title='Label value'),\n                            tooltip=[alt.Tooltip('seq_len:Q'), alt.Tooltip('gc:Q'), alt.Tooltip('label_val:Q')]\n                        ).properties(width=260, height=200)\n                        pair = alt.vconcat(hist, scatter).properties(title=f'Sub-target {c}')\n                        per_subcharts.append(pair)\n\n                # concat all subcharts horizontally\n                combined = alt.hconcat(*per_subcharts)\n                return combined.properties(title='Multi-target stats')\n\n            raise ValueError(f'Unknown data_type: {data_type}')\n\n        if self.stats is None or self.stats_for_plot is None:\n            raise ValueError(\"Statistics have not been computed yet. Please call `statistics()` method first.\")\n        task_type = self.data_type\n        df = self.stats_for_plot.copy()\n        seq_col = \"sequence\"\n        label_col = \"labels\"\n        split_charts = []\n        if isinstance(self.stats, dict):\n            for split_name, split_stats in self.stats.items():\n                chart = per_split_charts(df, task_type, seq_col, label_col).properties(title=split_name)\n                split_charts.append(chart)\n            # concatenate splits horizontally\n            final = alt.hconcat(*split_charts).properties(title='Dataset splits')\n        else:\n            final = per_split_charts(df, task_type, seq_col, label_col).properties(title='Full dataset')\n\n        if save_path:\n            final.save(save_path)\n        else:\n            final.show()\n            print(\"Successfully plotted dataset statistics.\")\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.__data_type__","title":"<code>__data_type__()</code>","text":"<p>Get the data type of the dataset (classification, regression, etc.).</p> <p>This method analyzes the labels to determine if the dataset is for: - classification (integer or string labels) - regression (float labels) - multi-label (multiple labels per sample) - multi-regression (multiple float values per sample)</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def __data_type__(self):\n    \"\"\"Get the data type of the dataset (classification, regression, etc.).\n\n    This method analyzes the labels to determine if the dataset is for:\n    - classification (integer or string labels)\n    - regression (float labels)\n    - multi-label (multiple labels per sample)\n    - multi-regression (multiple float values per sample)\n    \"\"\"\n    if isinstance(self.dataset, DatasetDict):\n        keys = list(self.dataset.keys())\n        if not keys:\n            raise ValueError(\"DatasetDict is empty.\")\n        if \"labels\" in self.dataset[keys[0]].column_names:\n            labels = self.dataset[keys[0]][\"labels\"]\n        else:\n            labels = None\n    else:\n        if \"labels\" in self.dataset.column_names:\n            labels = self.dataset[\"labels\"]\n        else:\n            labels = None\n    if labels is not None:\n        if isinstance(labels[0], str):\n            if self.multi_label_sep and self.multi_label_sep in labels:\n                multi_labels = labels.split(self.multi_label_sep)\n                if '.' in multi_labels[0]:\n                    self.data_type = \"multi_regression\"\n                else:\n                    self.data_type = \"multi_label\"\n            else:\n                if '.' in labels[0]:\n                    self.data_type = \"regression\"\n                else:\n                    self.data_type = \"classification\"\n        else:\n            if isinstance(labels[0], int):\n                self.data_type = \"classification\"\n            else:\n                self.data_type = \"regression\"\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get an item from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <p>Index of the item to retrieve</p> required <p>Returns:</p> Type Description <p>The item at the specified index</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset is a DatasetDict</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"Get an item from the dataset.\n\n    Args:\n        idx: Index of the item to retrieve\n\n    Returns:\n        The item at the specified index\n\n    Raises:\n        ValueError: If dataset is a DatasetDict\n    \"\"\"\n    if isinstance(self.dataset, DatasetDict):\n        raise ValueError(\"Dataset is a DatasetDict Object, please use `DNADataset.dataset[datatype].__getitem__(idx)` instead.\")\n    else:\n        return self.dataset[idx]\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.__init__","title":"<code>__init__(ds, tokenizer=None, max_length=512)</code>","text":"<p>Initialize the DNADataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Union[Dataset, DatasetDict]</code> <p>A Hugging Face Dataset containing at least 'sequence' and 'label' fields</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>A Hugging Face tokenizer for encoding sequences</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Maximum length for tokenization</p> <code>512</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def __init__(self, ds: Union[Dataset, DatasetDict], tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512):\n    \"\"\"Initialize the DNADataset.\n\n    Args:\n        ds: A Hugging Face Dataset containing at least 'sequence' and 'label' fields\n        tokenizer: A Hugging Face tokenizer for encoding sequences\n        max_length: Maximum length for tokenization\n    \"\"\"\n    self.dataset = ds\n    self.tokenizer = tokenizer\n    self.max_length = max_length\n    self.sep = None\n    self.multi_label_sep = None\n    self.data_type = None\n    self.stats = None\n    self.stats_for_plot = None\n    self.__data_type__()  # Determine the data type of the dataset\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the dataset.</p> <p>Returns:</p> Type Description <p>Length of the dataset or dict of lengths for DatasetDict</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the length of the dataset.\n\n    Returns:\n        Length of the dataset or dict of lengths for DatasetDict\n    \"\"\"\n    if isinstance(self.dataset, DatasetDict):\n        return {dt: len(self.dataset[dt]) for dt in self.dataset}\n    else:\n        return len(self.dataset)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.augment_reverse_complement","title":"<code>augment_reverse_complement(reverse=True, complement=True)</code>","text":"<p>Augment the dataset by adding reverse complement sequences.</p> <p>This method doubles the dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>reverse</code> <p>Whether to do reverse</p> <code>True</code> <code>complement</code> <p>Whether to do complement</p> <code>True</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def augment_reverse_complement(self, reverse=True, complement=True):\n    \"\"\"Augment the dataset by adding reverse complement sequences.\n\n    This method doubles the dataset size.\n\n    Args:\n        reverse: Whether to do reverse\n        complement: Whether to do complement\n    \"\"\"\n    def process(ds, reverse, complement):\n        # Create a dataset with an extra field for the reverse complement.\n        def add_rc(example):\n            example[\"rc_sequence\"] = reverse_complement(\n                example[\"sequence\"], reverse=reverse, complement=complement\n            )\n            return example\n        ds_with_rc = ds.map(add_rc, desc=\"Reverse complementary\")\n        # Build a new dataset where the reverse complement becomes the 'sequence'\n        rc_ds = ds_with_rc.map(lambda ex: {\"sequence\": ex[\"rc_sequence\"], \"labels\": ex[\"labels\"]}, desc=\"Data augment\")\n        ds = concatenate_datasets([ds, rc_ds])\n        ds.remove_columns([\"rc_sequence\"])\n        return ds\n    if isinstance(self.dataset, DatasetDict):\n        for dt in self.dataset:\n            self.dataset[dt] = process(self.dataset[dt], reverse, complement)\n    else:\n        self.dataset = process(self.dataset, reverse, complement)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.concat_reverse_complement","title":"<code>concat_reverse_complement(reverse=True, complement=True, sep='')</code>","text":"<p>Augment each sample by concatenating the sequence with its reverse complement.</p> <p>Parameters:</p> Name Type Description Default <code>reverse</code> <p>Whether to do reverse</p> <code>True</code> <code>complement</code> <p>Whether to do complement</p> <code>True</code> <code>sep</code> <code>str</code> <p>Separator between the original and reverse complement sequences</p> <code>''</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def concat_reverse_complement(self, reverse=True, complement=True, sep: str = \"\"):\n    \"\"\"Augment each sample by concatenating the sequence with its reverse complement.\n\n    Args:\n        reverse: Whether to do reverse\n        complement: Whether to do complement\n        sep: Separator between the original and reverse complement sequences\n    \"\"\"\n    def process(ds, reverse, complement, sep):\n        def concat_fn(example):\n            rc = reverse_complement(example[\"sequence\"], reverse=reverse, complement=complement)\n            example[\"sequence\"] = example[\"sequence\"] + sep + rc\n            return example\n        ds = ds.map(concat_fn, desc=\"Data augment\")\n        return ds\n    if isinstance(self.dataset, DatasetDict):\n        for dt in self.dataset:\n            self.dataset[dt] = process(self.dataset[dt], reverse, complement, sep)\n    else:\n        self.dataset = process(self.dataset, reverse, complement, sep)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.encode_sequences","title":"<code>encode_sequences(padding='max_length', return_tensors='pt', remove_unused_columns=False, uppercase=False, lowercase=False, task='SequenceClassification')</code>","text":"<p>Encode all sequences using the provided tokenizer.</p> <p>The dataset is mapped to include tokenized fields along with the label, making it directly usable with Hugging Face Trainer.</p> <p>Parameters:</p> Name Type Description Default <code>padding</code> <code>str</code> <p>Padding strategy for sequences. Can be 'max_length' or 'longest'.     Use 'longest' to pad to the length of the longest sequence in case of memory outage</p> <code>'max_length'</code> <code>return_tensors</code> <code>str</code> <p>Returned tensor types, can be 'pt', 'tf', 'np', or 'jax'</p> <code>'pt'</code> <code>remove_unused_columns</code> <code>bool</code> <p>Whether to remove the original 'sequence' and 'label' columns</p> <code>False</code> <code>uppercase</code> <code>bool</code> <p>Whether to convert sequences to uppercase</p> <code>False</code> <code>lowercase</code> <code>bool</code> <p>Whether to convert sequences to lowercase</p> <code>False</code> <code>task</code> <code>Optional[str]</code> <p>Task type for the tokenizer. If not provided, defaults to 'SequenceClassification'</p> <code>'SequenceClassification'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tokenizer is not provided</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def encode_sequences(self, padding: str = \"max_length\", return_tensors: str = \"pt\",\n                     remove_unused_columns: bool = False,\n                     uppercase: bool=False, lowercase: bool=False,\n                     task: Optional[str] = 'SequenceClassification'):\n    \"\"\"Encode all sequences using the provided tokenizer.\n\n    The dataset is mapped to include tokenized fields along with the label,\n    making it directly usable with Hugging Face Trainer.\n\n    Args:\n        padding: Padding strategy for sequences. Can be 'max_length' or 'longest'.\n                Use 'longest' to pad to the length of the longest sequence in case of memory outage\n        return_tensors: Returned tensor types, can be 'pt', 'tf', 'np', or 'jax'\n        remove_unused_columns: Whether to remove the original 'sequence' and 'label' columns\n        uppercase: Whether to convert sequences to uppercase\n        lowercase: Whether to convert sequences to lowercase\n        task: Task type for the tokenizer. If not provided, defaults to 'SequenceClassification'\n\n    Raises:\n        ValueError: If tokenizer is not provided\n    \"\"\"\n    if self.tokenizer:\n        sp_token_map = self.tokenizer.special_tokens_map\n        pad_token = sp_token_map['pad_token'] if 'pad_token' in sp_token_map else None\n        pad_id = self.tokenizer.encode(pad_token)[-1] if pad_token else None\n        cls_token = sp_token_map['cls_token'] if 'cls_token' in sp_token_map else None\n        sep_token = sp_token_map['sep_token'] if 'sep_token' in sp_token_map else None\n        eos_token = sp_token_map['eos_token'] if 'eos_token' in sp_token_map else None\n        max_length = self.max_length\n    else:\n        raise ValueError(\"Tokenizer not provided.\")\n    def tokenize_for_sequence_classification(example):\n        sequences = example[\"sequence\"]\n        if uppercase:\n            sequences = [x.upper() for x in sequences]\n        if lowercase:\n            sequences = [x.lower() for x in sequences]\n        tokenized = self.tokenizer(\n            sequences,\n            truncation=True,\n            padding=padding,\n            max_length=max_length\n        )\n        return tokenized\n    def tokenize_for_token_classification(examples):\n\n        tokenized_examples = {'sequence': [],\n                              'input_ids': [],\n                              # 'token_type_ids': [],\n                              'attention_mask': []}\n        if 'labels' in examples:\n            tokenized_examples['labels'] = []\n        input_seqs = examples['sequence']\n        if isinstance(input_seqs, str):\n            input_seqs = input_seqs.split(self.multi_label_sep)\n        for i, example_tokens in enumerate(input_seqs):\n            all_ids = [x for x in self.tokenizer.encode(example_tokens, is_split_into_words=True)]\n            if 'labels' in examples:\n                example_ner_tags = examples['labels'][i]\n            else:\n                example_ner_tags = [0] * len(example_tokens)\n            pad_len = max_length - len(all_ids)\n            if pad_len &gt;= 0:\n                all_masks = [1] * len(all_ids) + [0] * pad_len\n                all_ids = all_ids + [pad_id] * pad_len\n                if cls_token:\n                    if sep_token:\n                        example_tokens = [cls_token] + example_tokens + [sep_token] + [pad_token] * pad_len\n                        example_ner_tags = [-100] + example_ner_tags + [-100] * (pad_len + 1)\n                    elif eos_token:\n                        example_tokens = [cls_token] + example_tokens + [eos_token] + [pad_token] * pad_len\n                        example_ner_tags = [-100] + example_ner_tags + [-100] * (pad_len + 1)\n                    else:\n                        example_tokens = [cls_token] + example_tokens + [pad_token] * pad_len\n                        example_ner_tags = [-100] + example_ner_tags + [-100] * pad_len\n                else:\n                    example_tokens = example_tokens + [pad_token] * pad_len\n                    example_ner_tags = example_ner_tags + [-100] * pad_len\n            elif pad_len &lt; 0:\n                all_ids = all_ids[:max_length]\n                all_masks = [1] * (max_length)\n                if cls_token:\n                    if sep_token:\n                        example_tokens = [cls_token] + example_tokens[:max_length - 2] + [sep_token]\n                        example_ner_tags = [-100] + example_ner_tags[:max_length - 2] + [-100]\n                    else:\n                        example_tokens = [cls_token] + example_tokens[:max_length - 1]\n                        example_ner_tags = [-100] + example_ner_tags[:max_length - 1]\n                else:\n                    example_tokens = example_tokens[:max_length]\n                    example_ner_tags = example_ner_tags[:max_length]\n            tokenized_examples['sequence'].append(example_tokens)\n            tokenized_examples['input_ids'].append(all_ids)\n            # tokenized_examples['token_type_ids'].append([0] * max_length)\n            tokenized_examples['attention_mask'].append(all_masks)\n            if 'labels' in examples:\n                tokenized_examples['labels'].append(example_ner_tags)\n        return BatchEncoding(tokenized_examples)\n    # Judge the task type\n    task = task.lower()\n    if task in ['sequenceclassification', 'binary', 'multiclass', 'multilabel', 'regression']:\n        self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True, desc=\"Encoding inputs\")\n    elif task in ['tokenclassification', 'token', 'ner']:\n        from transformers.tokenization_utils_base import BatchEncoding\n        self.dataset = self.dataset.map(tokenize_for_token_classification, batched=True, desc=\"Encoding inputs\")\n    elif task in ['maskedlm', 'mlm', 'mask', 'embedding']:\n        self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True, desc=\"Encoding inputs\")\n    elif task in ['causallm', 'clm', 'causal', 'generation', 'embedding']:\n        self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True)\n    else:\n        self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True, desc=\"Encoding inputs\")\n    if remove_unused_columns:\n        used_cols = ['labels', 'input_ids', 'attention_mask']\n        if isinstance(self.dataset, DatasetDict):\n            for dt in self.dataset:\n                unused_cols = [f for f in self.dataset[dt].features if f not in used_cols]\n                self.dataset[dt] = self.dataset[dt].remove_columns(unused_cols)\n        else:\n            unused_cols = [f for f in self.dataset.features if f not in used_cols]\n            self.dataset = self.dataset.remove_columns(unused_cols)\n    if return_tensors == \"tf\":\n        self.dataset.set_format(type=\"tensorflow\")\n    elif return_tensors == \"jax\":\n        self.dataset.set_format(type=\"jax\")\n    elif return_tensors == \"np\":\n        self.dataset.set_format(type=\"numpy\")\n    else:\n        self.dataset.set_format(type=\"torch\")\n    self.dataset._is_encoded = True  # Mark the dataset as encoded\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.from_huggingface","title":"<code>from_huggingface(dataset_name, seq_col='sequence', label_col='labels', data_dir=None, tokenizer=None, max_length=512)</code>  <code>classmethod</code>","text":"<p>Load a dataset from the Hugging Face Hub.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset</p> required <code>seq_col</code> <code>str</code> <p>Column name for the DNA sequence</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for the label</p> <code>'labels'</code> <code>data_dir</code> <code>Union[str, None]</code> <p>Data directory in a dataset</p> <code>None</code> <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>Tokenizer for sequence encoding</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length</p> <code>512</code> <p>Returns:</p> Name Type Description <code>DNADataset</code> <code>DNADataset</code> <p>An instance wrapping a datasets.Dataset</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>@classmethod\ndef from_huggingface(cls, dataset_name: str,\n                     seq_col: str = \"sequence\", label_col: str = \"labels\",\n                     data_dir: Union[str, None]=None,\n                     tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512) -&gt; 'DNADataset':\n    \"\"\"Load a dataset from the Hugging Face Hub.\n\n    Args:\n        dataset_name: Name of the dataset\n        seq_col: Column name for the DNA sequence\n        label_col: Column name for the label\n        data_dir: Data directory in a dataset\n        tokenizer: Tokenizer for sequence encoding\n        max_length: Max token length\n\n    Returns:\n        DNADataset: An instance wrapping a datasets.Dataset\n    \"\"\"\n    if data_dir:\n        ds = load_dataset(dataset_name, data_dir=data_dir)\n    else:\n        ds = load_dataset(dataset_name)\n    # Rename columns if necessary\n    if seq_col != \"sequence\":\n        ds = ds.rename_column(seq_col, \"sequence\")\n    if label_col != \"labels\":\n        ds = ds.rename_column(label_col, \"labels\")\n    return cls(ds, tokenizer=tokenizer, max_length=max_length)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.from_modelscope","title":"<code>from_modelscope(dataset_name, seq_col='sequence', label_col='labels', data_dir=None, tokenizer=None, max_length=512)</code>  <code>classmethod</code>","text":"<p>Load a dataset from the ModelScope.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset</p> required <code>seq_col</code> <code>str</code> <p>Column name for the DNA sequence</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for the label</p> <code>'labels'</code> <code>data_dir</code> <code>Union[str, None]</code> <p>Data directory in a dataset</p> <code>None</code> <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>Tokenizer for sequence encoding</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length</p> <code>512</code> <p>Returns:</p> Name Type Description <code>DNADataset</code> <code>DNADataset</code> <p>An instance wrapping a datasets.Dataset</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>@classmethod\ndef from_modelscope(cls, dataset_name: str,\n                    seq_col: str = \"sequence\", label_col: str = \"labels\",\n                    data_dir: Union[str, None]=None,\n                    tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512) -&gt; 'DNADataset':\n    \"\"\"Load a dataset from the ModelScope.\n\n    Args:\n        dataset_name: Name of the dataset\n        seq_col: Column name for the DNA sequence\n        label_col: Column name for the label\n        data_dir: Data directory in a dataset\n        tokenizer: Tokenizer for sequence encoding\n        max_length: Max token length\n\n    Returns:\n        DNADataset: An instance wrapping a datasets.Dataset\n    \"\"\"\n    from modelscope import MsDataset\n\n    if data_dir:\n        ds = MsDataset.load(dataset_name, data_dir=data_dir)\n    else:\n        ds = MsDataset.load(dataset_name)\n    # Rename columns if necessary\n    if seq_col != \"sequence\":\n        ds = ds.rename_column(seq_col, \"sequence\")\n    if label_col != \"labels\":\n        ds = ds.rename_column(label_col, \"labels\")\n    return cls(ds, tokenizer=tokenizer, max_length=max_length)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.head","title":"<code>head(head=10, show=False)</code>","text":"<p>Fetch the head n data from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>head</code> <code>int</code> <p>Number of samples to fetch</p> <code>10</code> <code>show</code> <code>bool</code> <p>Whether to print the data or return it</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>A dictionary containing the first n samples if show=False, otherwise None</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def head(self, head: int=10, show: bool=False) -&gt; Union[dict, None]:\n    \"\"\"Fetch the head n data from the dataset.\n\n    Args:\n        head: Number of samples to fetch\n        show: Whether to print the data or return it\n\n    Returns:\n        A dictionary containing the first n samples if show=False, otherwise None\n    \"\"\"\n    import pprint\n    def format_convert(data):\n        df = {}\n        length = len(data[\"sequence\"])\n        for i in range(length):\n            df[i] = {}\n            for key in data.keys():\n                df[i][key] = data[key][i]\n        return df\n    dataset = self.dataset\n    if isinstance(dataset, DatasetDict):\n        df = {}\n        for dt in dataset.keys():\n            data = dataset[dt][:head]\n            if show:\n                print(f\"Dataset: {dt}\")\n                pprint.pp(format_convert(data))\n            else:\n                df[dt] = data\n                return df\n    else:\n        data = dataset[dt][:head]\n        if show:\n            pprint.pp(format_convert(data))\n        else:\n            return data\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.iter_batches","title":"<code>iter_batches(batch_size)</code>","text":"<p>Generator that yields batches of examples from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Size of each batch</p> required <p>Yields:</p> Type Description <p>A batch of examples</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset is a DatasetDict</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def iter_batches(self, batch_size: int):\n    \"\"\"Generator that yields batches of examples from the dataset.\n\n    Args:\n        batch_size: Size of each batch\n\n    Yields:\n        A batch of examples\n\n    Raises:\n        ValueError: If dataset is a DatasetDict\n    \"\"\"\n    if isinstance(self.dataset, DatasetDict):\n        raise ValueError(\"Dataset is a DatasetDict Object, please use `DNADataset.dataset[datatype].iter_batches(batch_size)` instead.\")\n    else:\n        for i in range(0, len(self.dataset), batch_size):\n            yield self.dataset[i: i + batch_size]\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.load_local_data","title":"<code>load_local_data(file_paths, seq_col='sequence', label_col='labels', sep=None, fasta_sep='|', multi_label_sep=None, tokenizer=None, max_length=512)</code>  <code>classmethod</code>","text":"<p>Load DNA sequence datasets from one or multiple local files.</p> <p>Supports input formats: csv, tsv, json, parquet, arrow, dict, fasta, txt.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <p>Single dataset: Provide one file path (e.g., \"data.csv\").        Pre-split datasets: Provide a dict like {\"train\": \"train.csv\", \"test\": \"test.csv\"}</p> required <code>seq_col</code> <code>str</code> <p>Column name for DNA sequences</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for labels</p> <code>'labels'</code> <code>sep</code> <code>str</code> <p>Delimiter for CSV, TSV, or TXT</p> <code>None</code> <code>fasta_sep</code> <code>str</code> <p>Delimiter for FASTA files</p> <code>'|'</code> <code>multi_label_sep</code> <code>Union[str, None]</code> <p>Delimiter for multi-label sequences</p> <code>None</code> <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>A tokenizer for sequence encoding</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length</p> <code>512</code> <p>Returns:</p> Name Type Description <code>DNADataset</code> <code>DNADataset</code> <p>An instance wrapping a Dataset or DatasetDict</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If file type is not supported</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>@classmethod\ndef load_local_data(cls, file_paths, seq_col: str = \"sequence\", label_col: str = \"labels\",\n                    sep: str = None, fasta_sep: str = \"|\",\n                    multi_label_sep: Union[str, None] = None,\n                    tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512) -&gt; 'DNADataset':\n    \"\"\"Load DNA sequence datasets from one or multiple local files.\n\n    Supports input formats: csv, tsv, json, parquet, arrow, dict, fasta, txt.\n\n    Args:\n        file_paths: Single dataset: Provide one file path (e.g., \"data.csv\").\n                   Pre-split datasets: Provide a dict like {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n        seq_col: Column name for DNA sequences\n        label_col: Column name for labels\n        sep: Delimiter for CSV, TSV, or TXT\n        fasta_sep: Delimiter for FASTA files\n        multi_label_sep: Delimiter for multi-label sequences\n        tokenizer: A tokenizer for sequence encoding\n        max_length: Max token length\n\n    Returns:\n        DNADataset: An instance wrapping a Dataset or DatasetDict\n\n    Raises:\n        ValueError: If file type is not supported\n    \"\"\"\n    # Set separators\n    cls.sep = sep\n    cls.multi_label_sep = multi_label_sep\n    # Check if input is a list or dict\n    if isinstance(file_paths, dict):  # Handling multiple files (pre-split datasets)\n        ds_dict = {}\n        for split, path in file_paths.items():\n            ds_dict[split] = cls._load_single_data(path, seq_col, label_col, sep, fasta_sep, multi_label_sep)\n        dataset = DatasetDict(ds_dict)\n    else:  # Handling a single file\n        dataset = cls._load_single_data(file_paths, seq_col, label_col, sep, fasta_sep, multi_label_sep)\n    dataset.stats = None  # Initialize stats as None\n\n    return cls(dataset, tokenizer=tokenizer, max_length=max_length)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.plot_statistics","title":"<code>plot_statistics(save_path=None)</code>","text":"<p>Plot statistics of the dataset.</p> <p>Includes sequence length distribution (histogram),  GC content distribution (box plot) for each sequence. If dataset is a DatasetDict, length plots and GC content plots from different datasets will be  concatenated into a single chart, respectively. Sequence length distribution is shown as a histogram, with min and max lengths for its' limit.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to save the plots. If None, plots will be shown interactively</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If statistics have not been computed yet</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def plot_statistics(self, save_path: str = None):\n    \"\"\"Plot statistics of the dataset.\n\n    Includes sequence length distribution (histogram), \n    GC content distribution (box plot) for each sequence.\n    If dataset is a DatasetDict, length plots and GC content plots from different datasets will be \n    concatenated into a single chart, respectively.\n    Sequence length distribution is shown as a histogram, with min and max lengths for its' limit.\n\n    Args:\n        save_path: Path to save the plots. If None, plots will be shown interactively\n\n    Raises:\n        ValueError: If statistics have not been computed yet\n    \"\"\"\n\n    import altair as alt\n    from typing import Tuple\n    alt.data_transformers.enable(\"vegafusion\")\n\n    def parse_multi_labels(series: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"Split semicolon-separated labels in a Series into a dataframe of columns.\n\n        Example: '0;1;1' -&gt; columns ['label_0','label_1','label_2']\n        \"\"\"\n        rows = []\n        maxlen = 0\n        for v in series.fillna(''):\n            if v == '':\n                parts = []\n            else:\n                parts = [p.strip() for p in str(v).split(';')]\n            rows.append(parts)\n            if len(parts) &gt; maxlen:\n                maxlen = len(parts)\n        cols = [f'label_{i}' for i in range(maxlen)]\n        parsed = [r + [''] * (maxlen - len(r)) for r in rows]\n        df = pd.DataFrame(parsed, columns=cols)\n\n        # try convert numeric types\n        for c in df.columns:\n            df[c] = pd.to_numeric(df[c].replace('', np.nan))\n        return df\n\n    def classification_plots(df: pd.DataFrame, label_col: str = 'labels', seq_col: str = 'sequence') -&gt; alt.Chart:\n        \"\"\"Build histogram of seq lengths colorized by label and GC boxplot grouped by label.\n\n        For multi-label (where label column contains semicolon), this function expects the\n        caller to have split and called per-sublabel as necessary.\n        \"\"\"\n        # ensure label is a categorical column\n        df = df.copy()\n        df['label_str'] = df[label_col].astype(str)\n        df['seq_len'] = df[seq_col].fillna('').astype(str).str.len()\n        df['gc'] = df[seq_col].fillna('').astype(str).map(calc_gc_content)\n\n        # histogram: seq length, colored by label\n        hist = alt.Chart(df).mark_bar(opacity=0.7).encode(\n            x=alt.X('seq_len:Q', bin=alt.Bin(maxbins=60), title='Sequence length'),\n            y=alt.Y('count():Q', title='Count'),\n            color=alt.Color('label_str:N', title='Label')\n        ).properties(width=300, height=240)\n\n        # GC boxplot grouped by label\n        box = alt.Chart(df).mark_boxplot(size=20).encode(\n            x=alt.X('label_str:N', title='Label'),\n            y=alt.Y('gc:Q', title='GC content'),\n            color=alt.Color('label_str:N', legend=None)\n        ).properties(width=300, height=240)\n\n        return hist, box\n\n    def regression_plots(df: pd.DataFrame, label_col: str = 'labels', seq_col: str = 'sequence') -&gt; Tuple[alt.Chart, alt.Chart]:\n        \"\"\"Build histogram of seq lengths (ungrouped) and GC scatter (GC vs label value).\n\n        For multi-regression, caller should split and call per target.\n        \"\"\"\n        df = df.copy()\n        df['seq_len'] = df[seq_col].fillna('').astype(str).str.len()\n        df['gc'] = df[seq_col].fillna('').astype(str).map(calc_gc_content)\n\n        hist = alt.Chart(df).mark_bar(opacity=0.7).encode(\n            x=alt.X('seq_len:Q', bin=alt.Bin(maxbins=60), title='Sequence length'),\n            y=alt.Y('count():Q', title='Count')\n        ).properties(width=300, height=240)\n\n        # ensure numeric label\n        df['label_val'] = pd.to_numeric(df[label_col], errors='coerce')\n        scatter = alt.Chart(df).mark_point().encode(\n            x=alt.X('gc:Q', title='GC content'),\n            y=alt.Y('label_val:Q', title='Label value'),\n            tooltip=[alt.Tooltip('seq_len:Q'), alt.Tooltip('gc:Q'), alt.Tooltip('label_val:Q')]\n        ).properties(width=300, height=240)\n\n        return hist, scatter\n\n    def per_split_charts(df: pd.DataFrame, data_type: str, seq_col: str, label_col: str) -&gt; alt.Chart:\n        \"\"\"Return a combined Altair chart for a single split (DataFrame) based on data_type.\n\n        Behavior aligned with user requirement:\n        - For 'classification' or 'regression' (single-label): seq_len and GC plots are concatenated horizontally.\n        - For 'multi-classification' and 'multi-regression': sublabels' results are concatenated horizontally\n        and the pair (seq_len, GC) for each sublabel are concatenated vertically.\n        \"\"\"\n        if data_type == 'classification':\n            hist, box = classification_plots(df, label_col, seq_col)\n            combined = alt.hconcat(hist, box)\n            return combined.properties(title='Classification stats')\n\n        if data_type == 'regression':\n            hist, scatter = regression_plots(df, label_col, seq_col)\n            combined = alt.hconcat(hist, scatter)\n            return combined.properties(title='Regression stats')\n\n        if data_type in ('multi-classification', 'multi-regression'):\n            # split labels into subcolumns\n            lbls_df = parse_multi_labels(df[label_col])\n            per_subcharts = []\n            for c in lbls_df.columns:\n                subdf = df.copy()\n                subdf[c] = lbls_df[c]\n                # drop nan labels (optional) but keep sequences\n                if data_type == 'multi-classification':\n                    # treat each sublabel like single classification\n                    subdf_for_plot = subdf.copy()\n                    subdf_for_plot['labels_for_plot'] = subdf[c].astype('Int64').astype(str)\n                    hist = alt.Chart(subdf_for_plot).mark_bar(opacity=0.7).encode(\n                        x=alt.X('seq_len:Q', bin=alt.Bin(maxbins=50), title='Sequence length'),\n                        y='count():Q',\n                        color=alt.Color('labels_for_plot:N', title=f'{c}')\n                    ).properties(width=260, height=200)\n\n                    box = alt.Chart(subdf_for_plot).mark_boxplot(size=20).encode(\n                        x=alt.X('labels_for_plot:N', title=f'{c}'),\n                        y=alt.Y('gc:Q', title='GC content'),\n                        color=alt.Color('labels_for_plot:N', legend=None)\n                    ).properties(width=260, height=200)\n                    pair = alt.vconcat(hist, box).properties(title=f'Sub-label {c}')\n                    per_subcharts.append(pair)\n\n                else:  # multi-regression\n                    subdf_for_plot = subdf.copy()\n                    subdf_for_plot['label_val'] = pd.to_numeric(subdf[c], errors='coerce')\n                    hist = alt.Chart(subdf_for_plot).mark_bar(opacity=0.7).encode(\n                        x=alt.X('seq_len:Q', bin=alt.Bin(maxbins=50), title='Sequence length'),\n                        y='count():Q'\n                    ).properties(width=260, height=200)\n\n                    scatter = alt.Chart(subdf_for_plot).mark_point().encode(\n                        x=alt.X('gc:Q', title='GC content'),\n                        y=alt.Y('label_val:Q', title='Label value'),\n                        tooltip=[alt.Tooltip('seq_len:Q'), alt.Tooltip('gc:Q'), alt.Tooltip('label_val:Q')]\n                    ).properties(width=260, height=200)\n                    pair = alt.vconcat(hist, scatter).properties(title=f'Sub-target {c}')\n                    per_subcharts.append(pair)\n\n            # concat all subcharts horizontally\n            combined = alt.hconcat(*per_subcharts)\n            return combined.properties(title='Multi-target stats')\n\n        raise ValueError(f'Unknown data_type: {data_type}')\n\n    if self.stats is None or self.stats_for_plot is None:\n        raise ValueError(\"Statistics have not been computed yet. Please call `statistics()` method first.\")\n    task_type = self.data_type\n    df = self.stats_for_plot.copy()\n    seq_col = \"sequence\"\n    label_col = \"labels\"\n    split_charts = []\n    if isinstance(self.stats, dict):\n        for split_name, split_stats in self.stats.items():\n            chart = per_split_charts(df, task_type, seq_col, label_col).properties(title=split_name)\n            split_charts.append(chart)\n        # concatenate splits horizontally\n        final = alt.hconcat(*split_charts).properties(title='Dataset splits')\n    else:\n        final = per_split_charts(df, task_type, seq_col, label_col).properties(title='Full dataset')\n\n    if save_path:\n        final.save(save_path)\n    else:\n        final.show()\n        print(\"Successfully plotted dataset statistics.\")\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.process_missing_data","title":"<code>process_missing_data()</code>","text":"<p>Filter out samples with missing or empty sequences or labels.</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def process_missing_data(self):\n    \"\"\"Filter out samples with missing or empty sequences or labels.\"\"\"\n    def non_missing(example):\n        return example[\"sequence\"] and example[\"labels\"] is not None and example[\"sequence\"].strip() != \"\"\n    self.dataset = self.dataset.filter(non_missing)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.random_generate","title":"<code>random_generate(minl, maxl=0, samples=1, gc=(0, 1), N_ratio=0.0, padding_size=0, seed=None, label_func=None, append=False)</code>","text":"<p>Replace the current dataset with randomly generated DNA sequences.</p> <p>Parameters:</p> Name Type Description Default <code>minl</code> <code>int</code> <p>Minimum length of the sequences</p> required <code>maxl</code> <code>int</code> <p>Maximum length of the sequences, default is the same as minl</p> <code>0</code> <code>samples</code> <code>int</code> <p>Number of sequences to generate, default 1</p> <code>1</code> <code>gc</code> <code>tuple</code> <p>GC content range, default (0,1)</p> <code>(0, 1)</code> <code>N_ratio</code> <code>float</code> <p>Include N base in the generated sequence, default 0.0</p> <code>0.0</code> <code>padding_size</code> <code>int</code> <p>Padding size for sequence length, default 0</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed, default None</p> <code>None</code> <code>label_func</code> <p>A function that generates a label from a sequence</p> <code>None</code> <code>append</code> <code>bool</code> <p>Append the random generated data to the existing dataset or use the data as a dataset</p> <code>False</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def random_generate(self, minl: int, maxl: int = 0, samples: int = 1,\n                          gc: tuple = (0,1), N_ratio: float = 0.0,\n                          padding_size: int = 0, seed: int = None,\n                          label_func = None, append: bool = False):\n    \"\"\"Replace the current dataset with randomly generated DNA sequences.\n\n    Args:\n        minl: Minimum length of the sequences\n        maxl: Maximum length of the sequences, default is the same as minl\n        samples: Number of sequences to generate, default 1\n        gc: GC content range, default (0,1)\n        N_ratio: Include N base in the generated sequence, default 0.0\n        padding_size: Padding size for sequence length, default 0\n        seed: Random seed, default None\n        label_func: A function that generates a label from a sequence\n        append: Append the random generated data to the existing dataset or use the data as a dataset\n    \"\"\"\n    def process(minl, maxl, number, gc, N_ratio, padding_size, seed, label_func):\n        sequences = random_generate_sequences(minl=minl, maxl=maxl, samples=number,\n                                            gc=gc, N_ratio=N_ratio,\n                                            padding_size=padding_size, seed=seed)\n        labels = []\n        for seq in sequences:\n            labels.append(label_func(seq) if label_func else 0)\n        random_ds = Dataset.from_dict({\"sequence\": sequences, \"labels\": labels})\n        return random_ds\n    if append:\n        if isinstance(self.dataset, DatasetDict):\n            for dt in self.dataset:\n                number = round(samples * len(self.dataset[dt]) / sum(self.__len__().values()))\n                random_ds = process(minl, maxl, number, gc, N_ratio, padding_size, seed, label_func)\n                self.dataset[dt] = concatenate_datasets([self.dataset[dt], random_ds])\n        else:\n            random_ds = process(minl, maxl, samples, gc, N_ratio, padding_size, seed, label_func)\n            self.dataset = concatenate_datasets([self.dataset, random_ds])\n    else:\n        self.dataset = process(minl, maxl, samples, gc, N_ratio, padding_size, seed, label_func)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.raw_reverse_complement","title":"<code>raw_reverse_complement(ratio=0.5, seed=None)</code>","text":"<p>Do reverse complement of sequences in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>Ratio of sequences to reverse complement</p> <code>0.5</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility</p> <code>None</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def raw_reverse_complement(self, ratio: float = 0.5, seed: int = None):\n    \"\"\"Do reverse complement of sequences in the dataset.\n\n    Args:\n        ratio: Ratio of sequences to reverse complement\n        seed: Random seed for reproducibility\n    \"\"\"\n    def process(ds, ratio, seed):\n        random.seed(seed)\n        number = len(ds[\"sequence\"])\n        idxlist = set(random.sample(range(number), int(number * ratio)))\n        def concat_fn(example, idx):\n            rc = reverse_complement(example[\"sequence\"])\n            if idx in idxlist:\n                example[\"sequence\"] = rc\n            return example\n        # Create a dataset with random reverse complement.\n        ds.map(concat_fn, with_indices=True, desc=\"Reverse complementary\")\n        return ds\n    if isinstance(self.dataset, DatasetDict):\n        for dt in self.dataset:\n            self.dataset[dt] = process(self.dataset[dt], ratio, seed)\n    else:\n        self.dataset = process(self.dataset, ratio, seed)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.sampling","title":"<code>sampling(ratio=1.0, seed=None, overwrite=False)</code>","text":"<p>Randomly sample a fraction of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>Fraction of the dataset to sample. Default is 1.0 (no sampling)</p> <code>1.0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the original dataset with the sampled one</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Dataset, DatasetDict]</code> <p>A sampled dataset if overwrite=False, otherwise None</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def sampling(self, ratio: float=1.0, seed: int = None, overwrite: bool=False) -&gt; Union[Dataset, DatasetDict]:\n    \"\"\"Randomly sample a fraction of the dataset.\n\n    Args:\n        ratio: Fraction of the dataset to sample. Default is 1.0 (no sampling)\n        seed: Random seed for reproducibility\n        overwrite: Whether to overwrite the original dataset with the sampled one\n\n    Returns:\n        A sampled dataset if overwrite=False, otherwise None\n    \"\"\"\n    dataset = self.dataset\n    if isinstance(dataset, DatasetDict):\n        for dt in dataset.keys():\n            random.seed(seed)\n            random_idx = random.sample(range(len(dataset[dt])), int(len(dataset[dt]) * ratio))\n            dataset[dt] = dataset[dt].select(random_idx)\n    else:\n        random_idx = random.sample(range(len(dataset)), int(len(dataset) * ratio))\n        dataset = dataset.select(random_idx)\n    if overwrite:\n        self.dataset = dataset\n    else:\n        return dataset\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.show","title":"<code>show(head=10)</code>","text":"<p>Display the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>head</code> <code>int</code> <p>Number of samples to display</p> <code>10</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def show(self, head: int=10):\n    \"\"\"Display the dataset.\n\n    Args:\n        head: Number of samples to display\n    \"\"\"\n    self.head(head=head, show=True)            \n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.shuffle","title":"<code>shuffle(seed=None)</code>","text":"<p>Shuffle the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed for reproducibility</p> <code>None</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def shuffle(self, seed: int = None):\n    \"\"\"Shuffle the dataset.\n\n    Args:\n        seed: Random seed for reproducibility\n    \"\"\"\n    self.dataset.shuffle(seed=seed)\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.split_data","title":"<code>split_data(test_size=0.2, val_size=0.1, seed=None)</code>","text":"<p>Split the dataset into train, test, and validation sets.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>float</code> <p>Proportion of the dataset to include in the test split</p> <code>0.2</code> <code>val_size</code> <code>float</code> <p>Proportion of the dataset to include in the validation split</p> <code>0.1</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility</p> <code>None</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def split_data(self, test_size: float = 0.2, val_size: float = 0.1, seed: int = None):\n    \"\"\"Split the dataset into train, test, and validation sets.\n\n    Args:\n        test_size: Proportion of the dataset to include in the test split\n        val_size: Proportion of the dataset to include in the validation split\n        seed: Random seed for reproducibility\n    \"\"\"\n    # First, split off test+validation from training data\n    split_result = self.dataset.train_test_split(test_size=test_size + val_size, seed=seed)\n    train_ds = split_result['train']\n    temp_ds = split_result['test']\n    # Further split temp_ds into test and validation sets\n    if val_size &gt; 0:\n        rel_val_size = val_size / (test_size + val_size)\n        temp_split = temp_ds.train_test_split(test_size=rel_val_size, seed=seed)\n        test_ds = temp_split['train']\n        val_ds = temp_split['test']\n        self.dataset = DatasetDict({'train': train_ds, 'test': test_ds, 'val': val_ds})\n    else:\n        self.dataset = DatasetDict({'train': train_ds, 'test': temp_ds})\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.statistics","title":"<code>statistics()</code>","text":"<p>Get statistics of the dataset.</p> <p>Includes number of samples, sequence length (min, max, average, median),  label distribution, GC content (by labels), nucleotide composition (by labels).</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing statistics of the dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If statistics have not been computed yet</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def statistics(self) -&gt; dict:\n    \"\"\"Get statistics of the dataset.\n\n    Includes number of samples, sequence length (min, max, average, median), \n    label distribution, GC content (by labels), nucleotide composition (by labels).\n\n    Returns:\n        A dictionary containing statistics of the dataset\n\n    Raises:\n        ValueError: If statistics have not been computed yet\n    \"\"\"\n\n    def prepare_dataframe(dataset) -&gt; pd.DataFrame:\n        \"\"\"Convert a datasets.Dataset to pandas DataFrame if needed.\n\n        If the input is already a pandas DataFrame, return a copy.\n        \"\"\"\n        # avoid importing datasets at top-level to keep dependency optional\n        try:\n            from datasets import Dataset\n            is_dataset = isinstance(dataset, Dataset)\n        except Exception:\n            is_dataset = False\n\n        if is_dataset:\n            df = dataset.to_pandas()\n        elif isinstance(dataset, pd.DataFrame):\n            df = dataset.copy()\n        else:\n            raise ValueError('prepare_dataframe expects a datasets.Dataset or pandas.DataFrame')\n        return df\n\n    def compute_basic_stats(df: pd.DataFrame, seq_col: str = 'sequence') -&gt; dict:\n        \"\"\"Compute number of samples and sequence length statistics.\"\"\"\n        seqs = df[seq_col].fillna('').astype(str)\n        lens = seqs.str.len()\n        return {\n            'n_samples': int(len(lens)),\n            'min_len': int(lens.min()) if len(lens) &gt; 0 else 0,\n            'max_len': int(lens.max()) if len(lens) &gt; 0 else 0,\n            'mean_len': float(lens.mean()) if len(lens) &gt; 0 else float('nan'),\n            'median_len': float(lens.median()) if len(lens) &gt; 0 else float('nan'),\n        }\n\n    stats = {}\n    seq_col = \"sequence\"\n    label_col = \"labels\"\n    if isinstance(self.dataset, DatasetDict):\n        for split_name, split_ds in self.dataset.items():\n            df = prepare_dataframe(split_ds)\n            data_type = self.data_type\n            basic = compute_basic_stats(df, seq_col)\n            stats[split_name] = {'data_type': data_type, **basic}\n    else:\n        df = prepare_dataframe(self.dataset)\n        data_type = self.data_type\n        basic = compute_basic_stats(df, seq_col)\n        stats['full'] = {'data_type': data_type, **basic}\n\n    self.stats = stats  # Store stats in the instance for later use\n    self.stats_for_plot = df\n\n    return stats\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.DNADataset.validate_sequences","title":"<code>validate_sequences(minl=20, maxl=6000, gc=(0, 1), valid_chars='ACGTN')</code>","text":"<p>Filter the dataset to keep sequences containing valid DNA bases or allowed length.</p> <p>Parameters:</p> Name Type Description Default <code>minl</code> <code>int</code> <p>Minimum length of the sequences</p> <code>20</code> <code>maxl</code> <code>int</code> <p>Maximum length of the sequences</p> <code>6000</code> <code>gc</code> <code>tuple</code> <p>GC content range between 0 and 1</p> <code>(0, 1)</code> <code>valid_chars</code> <code>str</code> <p>Allowed characters in the sequences</p> <code>'ACGTN'</code> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def validate_sequences(self, minl: int = 20, maxl: int = 6000, gc: tuple = (0,1), valid_chars: str = \"ACGTN\"):\n    \"\"\"Filter the dataset to keep sequences containing valid DNA bases or allowed length.\n\n    Args:\n        minl: Minimum length of the sequences\n        maxl: Maximum length of the sequences\n        gc: GC content range between 0 and 1\n        valid_chars: Allowed characters in the sequences\n    \"\"\"\n    self.dataset = self.dataset.filter(\n        lambda example: check_sequence(example[\"sequence\"], minl, maxl, gc, valid_chars)\n    )\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.load_preset_dataset","title":"<code>load_preset_dataset(dataset_name, task=None)</code>","text":"<p>Load a preset dataset from Hugging Face or ModelScope.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset</p> required <code>task</code> <code>str</code> <p>Task directory in a dataset</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DNADataset</code> <code>DNADataset</code> <p>An instance wrapping a datasets.Dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset is not found in preset datasets</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def load_preset_dataset(dataset_name: str, task: str=None) -&gt; 'DNADataset':\n    \"\"\"Load a preset dataset from Hugging Face or ModelScope.\n\n    Args:\n        dataset_name: Name of the dataset\n        task: Task directory in a dataset\n\n    Returns:\n        DNADataset: An instance wrapping a datasets.Dataset\n\n    Raises:\n        ValueError: If dataset is not found in preset datasets\n    \"\"\"\n    from modelscope import MsDataset\n    from .dataset_auto import PRESET_DATASETS\n\n    if dataset_name in PRESET_DATASETS:\n        ds_info = PRESET_DATASETS[dataset_name]\n        dataset_name = ds_info[\"name\"]\n        if task in ds_info[\"tasks\"]:\n            ds = MsDataset.load(dataset_name, data_dir=task)\n        else:\n            ds = MsDataset.load(dataset_name)\n    else:\n        raise ValueError(f\"Dataset {dataset_name} not found in preset datasets.\")\n\n    seq_cols = [\"s\", \"seq\", \"sequence\", \"sequences\"]\n    label_cols = [\"l\", \"label\", \"labels\", \"target\", \"targets\"]\n    seq_col = \"sequence\"\n    label_col = \"labels\"\n    if isinstance(ds, DatasetDict):\n        # Check if the dataset is a DatasetDict\n        for dt in ds:\n            # Rename columns if necessary\n            for s in seq_cols:\n                if s in ds[dt].column_names:\n                    seq_col = s\n                    break\n            for l in label_cols:\n                if l in ds[dt].column_names:\n                    label_col = l\n                    break\n            if seq_col != \"sequence\":\n                ds[dt] = ds[dt].rename_column(seq_col, \"sequence\")\n            if label_col != \"labels\":\n                ds[dt] = ds[dt].rename_column(label_col, \"labels\")\n    else:\n        # If it's a single dataset, rename columns directly\n        if seq_col != \"sequence\":\n            ds = ds.rename_column(seq_col, \"sequence\")\n        if label_col != \"labels\":\n            ds = ds.rename_column(label_col, \"labels\")\n\n    dna_ds = DNADataset(ds, tokenizer=None, max_length=1024)\n    dna_ds.sep = ds_info.get(\"separator\", \",\")\n    dna_ds.multi_label_sep = ds_info.get(\"multi_separator\", \";\")\n\n    return dna_ds\n</code></pre>"},{"location":"api/datahandling/data/#dnallm.datahandling.data.show_preset_dataset","title":"<code>show_preset_dataset()</code>","text":"<p>Show all preset datasets available in Hugging Face or ModelScope.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing dataset names and their descriptions</p> Source code in <code>dnallm/datahandling/data.py</code> <pre><code>def show_preset_dataset() -&gt; dict:\n    \"\"\"Show all preset datasets available in Hugging Face or ModelScope.\n\n    Returns:\n        A dictionary containing dataset names and their descriptions\n    \"\"\"\n    from .dataset_auto import PRESET_DATASETS\n    return PRESET_DATASETS\n</code></pre>"},{"location":"api/finetune/trainer/","title":"finetune/trainer API","text":"<p>DNA Language Model Trainer Module.</p> <p>This module implements the training process management for DNA language models, with the following main features:</p> <ol> <li>DNATrainer Class</li> <li>Unified management of model training, evaluation, and prediction processes</li> <li>Support for multiple task types (classification, regression, masked language modeling)</li> <li>Integration of task-specific prediction heads</li> <li>Training parameter configuration</li> <li> <p>Training process monitoring and model saving</p> </li> <li> <p>Core Features:</p> </li> <li>Model initialization and device management</li> <li>Training parameter configuration</li> <li>Training loop control</li> <li>Evaluation metrics calculation</li> <li>Model saving and loading</li> <li> <p>Prediction result generation</p> </li> <li> <p>Supported Training Features:</p> </li> <li>Automatic evaluation and best model saving</li> <li>Training log recording</li> <li>Flexible batch size settings</li> <li>Learning rate and weight decay configuration</li> <li>Distributed training support</li> <li>LoRA (Low-Rank Adaptation) for efficient fine-tuning</li> </ol> Usage Example <pre><code>trainer = DNATrainer(\n    model=model,\n    config=config,\n    datasets=datasets\n)\nmetrics = trainer.train()\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer","title":"<code>DNATrainer</code>","text":"<p>DNA Language Model Trainer that supports multiple model types.</p> <p>This trainer class provides a unified interface for training, evaluating, and predicting with DNA language models. It supports various task types including classification, regression, and masked language modeling.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The DNA language model to be trained</p> <code>task_config</code> <p>Configuration for the specific task</p> <code>train_config</code> <p>Configuration for training parameters</p> <code>datasets</code> <p>Dataset for training and evaluation</p> <code>extra_args</code> <p>Additional training arguments</p> <code>trainer</code> <p>HuggingFace Trainer instance</p> <code>training_args</code> <p>Training arguments configuration</p> <code>data_split</code> <p>Available dataset splits</p> <p>Examples:</p> <pre><code>trainer = DNATrainer(\n    model=model,\n    config=config,\n    datasets=datasets\n)\nmetrics = trainer.train()\n</code></pre> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>class DNATrainer:\n    \"\"\"DNA Language Model Trainer that supports multiple model types.\n\n    This trainer class provides a unified interface for training, evaluating, and predicting\n    with DNA language models. It supports various task types including classification,\n    regression, and masked language modeling.\n\n    Attributes:\n        model: The DNA language model to be trained\n        task_config: Configuration for the specific task\n        train_config: Configuration for training parameters\n        datasets: Dataset for training and evaluation\n        extra_args: Additional training arguments\n        trainer: HuggingFace Trainer instance\n        training_args: Training arguments configuration\n        data_split: Available dataset splits\n\n    Examples:\n        ```python\n        trainer = DNATrainer(\n            model=model,\n            config=config,\n            datasets=datasets\n        )\n        metrics = trainer.train()\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Any,\n        config: dict,\n        datasets: Optional[DNADataset] = None,\n        extra_args: Optional[Dict] = None,\n        use_lora: bool = False,\n    ):\n        \"\"\"Initialize the DNA trainer.\n\n        Args:\n            model: The DNA language model to be trained\n            config: Configuration dictionary containing task and training settings\n            datasets: Dataset for training and evaluation\n            extra_args: Additional training arguments to override defaults\n            use_lora: Whether to use LoRA for efficient fine-tuning\n        \"\"\"\n        self.model = model\n        self.task_config = config['task']\n        self.train_config = config['finetune']\n        self.datasets = datasets\n        self.extra_args = extra_args\n\n        # LoRA\n        if use_lora:\n            print(\"[Info] Applying LoRA to the model...\")\n            lora_config = LoraConfig(\n                **config[\"lora\"]\n            )\n            self.model = get_peft_model(self.model, lora_config)\n            self.model.print_trainable_parameters()\n\n        # Multi-GPU support\n        if torch.cuda.device_count() &gt; 1:\n            print(f\"[Info] Using {torch.cuda.device_count()} GPUs.\")\n            self.model = torch.nn.DataParallel(self.model)\n\n        self.set_up_trainer()\n\n    def set_up_trainer(self):\n        \"\"\"Set up the HuggingFace Trainer with appropriate configurations.\n\n        This method configures the training environment by:\n        1. Setting up training arguments from configuration\n        2. Configuring dataset splits (train/eval/test)\n        3. Setting up task-specific metrics computation\n        4. Configuring appropriate data collator for different task types\n        5. Initializing the HuggingFace Trainer instance\n\n        The method automatically handles:\n        - Dataset split detection and validation\n        - Task-specific data collator selection\n        - Evaluation strategy configuration\n        - Metrics computation setup\n        \"\"\"\n        # Setup training arguments\n        training_args = self.train_config.model_dump()\n        if self.extra_args:\n            training_args.update(self.extra_args)\n        self.training_args = TrainingArguments(\n            **training_args,\n        )\n        # Check if the dataset has been split\n        if isinstance(self.datasets.dataset, DatasetDict):        \n            self.data_split = self.datasets.dataset.keys()\n        else:\n            self.data_split = []\n        # Get datasets\n        if \"train\" in self.data_split:\n            train_dataset = self.datasets.dataset[\"train\"]\n        else:\n            if len(self.data_split) == 0:\n                train_dataset = self.datasets.dataset\n            else:\n                raise KeyError(\"Cannot find train data.\")\n        eval_key = [x for x in self.data_split if x not in ['train', 'test']]\n        if eval_key:\n            eval_dataset = self.datasets.dataset[eval_key[0]]\n        elif \"test\" in self.data_split:\n            eval_dataset = self.datasets.dataset['test']\n        else:\n            eval_dataset = None\n            self.training_args.eval_strategy = \"no\"\n\n        # Get compute metrics\n        compute_metrics = self.compute_task_metrics()\n        # Set data collator\n        if self.task_config.task_type == \"mask\":\n            from transformers import DataCollatorForLanguageModeling\n            mlm_probability = self.task_config.mlm_probability\n            mlm_probability = mlm_probability if mlm_probability else 0.15\n            data_collator = DataCollatorForLanguageModeling(\n                tokenizer=self.datasets.tokenizer,\n                mlm=True, mlm_probability=mlm_probability\n            )\n        elif self.task_config.task_type == \"generation\":\n            from transformers import DataCollatorForLanguageModeling\n            data_collator = DataCollatorForLanguageModeling(\n                tokenizer=self.datasets.tokenizer,\n                mlm=False\n            )\n        else:\n            data_collator = None\n        # Initialize trainer\n        self.trainer = Trainer(\n            model=self.model,\n            args=self.training_args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            compute_metrics=compute_metrics,\n            data_collator=data_collator,\n        )\n\n    def compute_task_metrics(self) -&gt; Callable:\n        \"\"\"Compute task-specific evaluation metrics.\n\n        This method returns a callable function that computes appropriate metrics\n        for the specific task type (classification, regression, etc.).\n\n        Returns:\n            Callable: A function that computes metrics for the specific task type\n        \"\"\"\n        return compute_metrics(self.task_config)\n\n    def train(self, save_tokenizer: bool = True) -&gt; Dict[str, float]:\n        \"\"\"Train the model and return training metrics.\n\n        This method executes the training process using the configured HuggingFace Trainer,\n        automatically saving the best model and optionally the tokenizer.\n\n        Args:\n            save_tokenizer: Whether to save the tokenizer along with the model, default True\n\n        Returns:\n            Dictionary containing training metrics including loss, learning rate, etc.\n        \"\"\"\n        self.model.train()\n        train_result = self.trainer.train()\n        metrics = train_result.metrics\n        # Save the model\n        self.trainer.save_model()\n        if save_tokenizer:\n            self.datasets.tokenizer.save_pretrained(self.train_config.output_dir)\n        return metrics\n\n    def evaluate(self) -&gt; Dict[str, float]:\n        \"\"\"Evaluate the model on the evaluation dataset.\n\n        This method runs evaluation on the configured evaluation dataset and returns\n        task-specific metrics.\n\n        Returns:\n            Dictionary containing evaluation metrics for the current model state\n        \"\"\"\n        self.model.eval()\n        result = self.trainer.evaluate()\n        return result\n\n    def predict(self) -&gt; Dict[str, float]:\n        \"\"\"Generate predictions on the test dataset.\n\n        This method generates predictions on the test dataset if available and returns\n        both predictions and evaluation metrics.\n\n        Returns:\n            Dictionary containing prediction results and metrics if test dataset exists,\n            otherwise empty dictionary\n        \"\"\"\n        self.model.eval()\n        result = {}\n        if \"test\" in self.data_split:\n            test_dataset = self.datasets.dataset['test']\n            result = self.trainer.predict(test_dataset)\n        return result\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.__init__","title":"<code>__init__(model, config, datasets=None, extra_args=None, use_lora=False)</code>","text":"<p>Initialize the DNA trainer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The DNA language model to be trained</p> required <code>config</code> <code>dict</code> <p>Configuration dictionary containing task and training settings</p> required <code>datasets</code> <code>Optional[DNADataset]</code> <p>Dataset for training and evaluation</p> <code>None</code> <code>extra_args</code> <code>Optional[Dict]</code> <p>Additional training arguments to override defaults</p> <code>None</code> <code>use_lora</code> <code>bool</code> <p>Whether to use LoRA for efficient fine-tuning</p> <code>False</code> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def __init__(\n    self,\n    model: Any,\n    config: dict,\n    datasets: Optional[DNADataset] = None,\n    extra_args: Optional[Dict] = None,\n    use_lora: bool = False,\n):\n    \"\"\"Initialize the DNA trainer.\n\n    Args:\n        model: The DNA language model to be trained\n        config: Configuration dictionary containing task and training settings\n        datasets: Dataset for training and evaluation\n        extra_args: Additional training arguments to override defaults\n        use_lora: Whether to use LoRA for efficient fine-tuning\n    \"\"\"\n    self.model = model\n    self.task_config = config['task']\n    self.train_config = config['finetune']\n    self.datasets = datasets\n    self.extra_args = extra_args\n\n    # LoRA\n    if use_lora:\n        print(\"[Info] Applying LoRA to the model...\")\n        lora_config = LoraConfig(\n            **config[\"lora\"]\n        )\n        self.model = get_peft_model(self.model, lora_config)\n        self.model.print_trainable_parameters()\n\n    # Multi-GPU support\n    if torch.cuda.device_count() &gt; 1:\n        print(f\"[Info] Using {torch.cuda.device_count()} GPUs.\")\n        self.model = torch.nn.DataParallel(self.model)\n\n    self.set_up_trainer()\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.compute_task_metrics","title":"<code>compute_task_metrics()</code>","text":"<p>Compute task-specific evaluation metrics.</p> <p>This method returns a callable function that computes appropriate metrics for the specific task type (classification, regression, etc.).</p> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A function that computes metrics for the specific task type</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def compute_task_metrics(self) -&gt; Callable:\n    \"\"\"Compute task-specific evaluation metrics.\n\n    This method returns a callable function that computes appropriate metrics\n    for the specific task type (classification, regression, etc.).\n\n    Returns:\n        Callable: A function that computes metrics for the specific task type\n    \"\"\"\n    return compute_metrics(self.task_config)\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.evaluate","title":"<code>evaluate()</code>","text":"<p>Evaluate the model on the evaluation dataset.</p> <p>This method runs evaluation on the configured evaluation dataset and returns task-specific metrics.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary containing evaluation metrics for the current model state</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def evaluate(self) -&gt; Dict[str, float]:\n    \"\"\"Evaluate the model on the evaluation dataset.\n\n    This method runs evaluation on the configured evaluation dataset and returns\n    task-specific metrics.\n\n    Returns:\n        Dictionary containing evaluation metrics for the current model state\n    \"\"\"\n    self.model.eval()\n    result = self.trainer.evaluate()\n    return result\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.predict","title":"<code>predict()</code>","text":"<p>Generate predictions on the test dataset.</p> <p>This method generates predictions on the test dataset if available and returns both predictions and evaluation metrics.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary containing prediction results and metrics if test dataset exists,</p> <code>Dict[str, float]</code> <p>otherwise empty dictionary</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def predict(self) -&gt; Dict[str, float]:\n    \"\"\"Generate predictions on the test dataset.\n\n    This method generates predictions on the test dataset if available and returns\n    both predictions and evaluation metrics.\n\n    Returns:\n        Dictionary containing prediction results and metrics if test dataset exists,\n        otherwise empty dictionary\n    \"\"\"\n    self.model.eval()\n    result = {}\n    if \"test\" in self.data_split:\n        test_dataset = self.datasets.dataset['test']\n        result = self.trainer.predict(test_dataset)\n    return result\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.set_up_trainer","title":"<code>set_up_trainer()</code>","text":"<p>Set up the HuggingFace Trainer with appropriate configurations.</p> <p>This method configures the training environment by: 1. Setting up training arguments from configuration 2. Configuring dataset splits (train/eval/test) 3. Setting up task-specific metrics computation 4. Configuring appropriate data collator for different task types 5. Initializing the HuggingFace Trainer instance</p> <p>The method automatically handles: - Dataset split detection and validation - Task-specific data collator selection - Evaluation strategy configuration - Metrics computation setup</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def set_up_trainer(self):\n    \"\"\"Set up the HuggingFace Trainer with appropriate configurations.\n\n    This method configures the training environment by:\n    1. Setting up training arguments from configuration\n    2. Configuring dataset splits (train/eval/test)\n    3. Setting up task-specific metrics computation\n    4. Configuring appropriate data collator for different task types\n    5. Initializing the HuggingFace Trainer instance\n\n    The method automatically handles:\n    - Dataset split detection and validation\n    - Task-specific data collator selection\n    - Evaluation strategy configuration\n    - Metrics computation setup\n    \"\"\"\n    # Setup training arguments\n    training_args = self.train_config.model_dump()\n    if self.extra_args:\n        training_args.update(self.extra_args)\n    self.training_args = TrainingArguments(\n        **training_args,\n    )\n    # Check if the dataset has been split\n    if isinstance(self.datasets.dataset, DatasetDict):        \n        self.data_split = self.datasets.dataset.keys()\n    else:\n        self.data_split = []\n    # Get datasets\n    if \"train\" in self.data_split:\n        train_dataset = self.datasets.dataset[\"train\"]\n    else:\n        if len(self.data_split) == 0:\n            train_dataset = self.datasets.dataset\n        else:\n            raise KeyError(\"Cannot find train data.\")\n    eval_key = [x for x in self.data_split if x not in ['train', 'test']]\n    if eval_key:\n        eval_dataset = self.datasets.dataset[eval_key[0]]\n    elif \"test\" in self.data_split:\n        eval_dataset = self.datasets.dataset['test']\n    else:\n        eval_dataset = None\n        self.training_args.eval_strategy = \"no\"\n\n    # Get compute metrics\n    compute_metrics = self.compute_task_metrics()\n    # Set data collator\n    if self.task_config.task_type == \"mask\":\n        from transformers import DataCollatorForLanguageModeling\n        mlm_probability = self.task_config.mlm_probability\n        mlm_probability = mlm_probability if mlm_probability else 0.15\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.datasets.tokenizer,\n            mlm=True, mlm_probability=mlm_probability\n        )\n    elif self.task_config.task_type == \"generation\":\n        from transformers import DataCollatorForLanguageModeling\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.datasets.tokenizer,\n            mlm=False\n        )\n    else:\n        data_collator = None\n    # Initialize trainer\n    self.trainer = Trainer(\n        model=self.model,\n        args=self.training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.train","title":"<code>train(save_tokenizer=True)</code>","text":"<p>Train the model and return training metrics.</p> <p>This method executes the training process using the configured HuggingFace Trainer, automatically saving the best model and optionally the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>save_tokenizer</code> <code>bool</code> <p>Whether to save the tokenizer along with the model, default True</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary containing training metrics including loss, learning rate, etc.</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def train(self, save_tokenizer: bool = True) -&gt; Dict[str, float]:\n    \"\"\"Train the model and return training metrics.\n\n    This method executes the training process using the configured HuggingFace Trainer,\n    automatically saving the best model and optionally the tokenizer.\n\n    Args:\n        save_tokenizer: Whether to save the tokenizer along with the model, default True\n\n    Returns:\n        Dictionary containing training metrics including loss, learning rate, etc.\n    \"\"\"\n    self.model.train()\n    train_result = self.trainer.train()\n    metrics = train_result.metrics\n    # Save the model\n    self.trainer.save_model()\n    if save_tokenizer:\n        self.datasets.tokenizer.save_pretrained(self.train_config.output_dir)\n    return metrics\n</code></pre>"},{"location":"api/inference/predictor/","title":"inference/predictor API","text":"<p>DNA Language Model Inference Module.</p> <p>This module implements core model inference functionality, including:</p> <ol> <li>DNAPredictor class</li> <li>Model loading and initialization</li> <li>Batch sequence prediction</li> <li>Result post-processing</li> <li>Device management</li> <li> <p>Half-precision inference support</p> </li> <li> <p>Core features:</p> </li> <li>Model state management</li> <li>Batch prediction</li> <li>Result merging</li> <li>Prediction result saving</li> <li> <p>Memory optimization</p> </li> <li> <p>Inference optimization:</p> </li> <li>Batch parallelization</li> <li>GPU acceleration</li> <li>Half-precision computation</li> <li>Memory efficiency optimization</li> </ol> Example <pre><code>predictor = DNAPredictor(\n    model=model,\n    tokenizer=tokenizer,\n    config=config\n)\nresults = predictor.predict(sequences)\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor","title":"<code>DNAPredictor</code>","text":"<p>DNA sequence predictor using fine-tuned models.</p> <p>This class provides comprehensive functionality for making predictions using DNA language models. It handles model loading, inference, result processing, and various output formats including hidden states and attention weights for model interpretability.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>Fine-tuned model instance for inference</p> <code>tokenizer</code> <p>Tokenizer for encoding DNA sequences</p> <code>task_config</code> <p>Configuration object containing task settings</p> <code>pred_config</code> <p>Configuration object containing inference parameters</p> <code>device</code> <p>Device (CPU/GPU/MPS) for model inference</p> <code>sequences</code> <p>List of input sequences</p> <code>labels</code> <p>List of true labels (if available)</p> <code>embeddings</code> <p>Dictionary containing hidden states and attention weights</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>class DNAPredictor:\n    \"\"\"DNA sequence predictor using fine-tuned models.\n\n    This class provides comprehensive functionality for making predictions using DNA language models.\n    It handles model loading, inference, result processing, and various output formats including\n    hidden states and attention weights for model interpretability.\n\n    Attributes:\n        model: Fine-tuned model instance for inference\n        tokenizer: Tokenizer for encoding DNA sequences\n        task_config: Configuration object containing task settings\n        pred_config: Configuration object containing inference parameters\n        device: Device (CPU/GPU/MPS) for model inference\n        sequences: List of input sequences\n        labels: List of true labels (if available)\n        embeddings: Dictionary containing hidden states and attention weights\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Any,\n        tokenizer: Any,\n        config: dict\n    ):\n        \"\"\"Initialize the predictor.\n\n        Args:\n            model: Fine-tuned model instance for inference\n            tokenizer: Tokenizer for encoding DNA sequences\n            config: Configuration dictionary containing task settings and inference parameters\n        \"\"\"\n\n        self.model = model\n        self.tokenizer = tokenizer\n        self.task_config = config['task']\n        self.pred_config = config['inference']\n        self.device = self._get_device()\n        if model:\n            self.model.to(self.device)\n            print(f\"Use device: {self.device}\")\n        self.sequences = []\n        self.labels = []\n\n    def _get_device(self) -&gt; torch.device:\n        \"\"\"Get the appropriate device for model inference.\n\n        This method automatically detects and selects the best available device for inference,\n        supporting CPU, CUDA (NVIDIA), MPS (Apple Silicon), ROCm (AMD), TPU, and XPU (Intel).\n\n        Returns:\n            torch.device: The device to use for model inference\n\n        Raises:\n            ValueError: If the specified device type is not supported\n        \"\"\"\n        # Get the device type\n        device = self.pred_config.device.lower()\n        if device == 'cpu':\n            return torch.device('cpu')\n        elif device in ['cuda', 'nvidia']:\n            if not torch.cuda.is_available():\n                warnings.warn(\"CUDA is not available. Please check your installation. Use CPU instead.\")\n                return torch.device('cpu')\n            else:\n                return torch.device('cuda')\n        elif device in ['mps', 'apple', 'mac']:\n            if not torch.backends.mps.is_available():\n                warnings.warn(\"MPS is not available. Please check your installation. Use CPU instead.\")\n                return torch.device('cpu')\n            else:\n                return torch.device('mps')\n        elif device in ['rocm', 'amd']:\n            if not torch.cuda.is_available():\n                warnings.warn(\"ROCm is not available. Please check your installation. Use CPU instead.\")\n                return torch.device('cpu')\n            else:\n                return torch.device('cuda')\n        elif device == ['tpu', 'xla', 'google']:\n            try:\n                import torch_xla.core.xla_model as xm\n                return torch.device('xla')\n            except:\n                warnings.warn(\"TPU is not available. Please check your installation. Use CPU instead.\")\n                return torch.device('cpu')\n        elif device == ['xpu', 'intel']:\n            if not torch.xpu.is_available():\n                warnings.warn(\"XPU is not available. Please check your installation. Use CPU instead.\")\n                return torch.device('cpu')\n            else:\n                return torch.device('xpu')\n        elif device == 'auto':\n            if torch.cuda.is_available():\n                return torch.device('cuda')\n            elif torch.backends.mps.is_available():\n                return torch.device('mps')\n            elif torch.xpu.is_available():\n                return torch.device('xpu')\n            else:\n                return torch.device('cpu')\n        else:\n            raise ValueError(f\"Unsupported device type: {device}\")\n\n    def generate_dataset(self, seq_or_path: Union[str, List[str]], batch_size: int = 1,\n                         seq_col: str = \"sequence\", label_col: str = \"labels\",\n                         sep: str = None, fasta_sep: str = \"|\",\n                         multi_label_sep: Union[str, None] = None,\n                         uppercase: bool = False, lowercase: bool = False,\n                         keep_seqs: bool = True, do_encode: bool = True) -&gt; Tuple[DNADataset, DataLoader]:\n        \"\"\"Generate dataset from sequences or file path.\n\n        This method creates a DNADataset and DataLoader from either a list of sequences\n        or a file path, supporting various file formats and preprocessing options.\n\n        Args:\n            seq_or_path: Single sequence, list of sequences, or path to a file containing sequences\n            batch_size: Batch size for DataLoader\n            seq_col: Column name for sequences in the file\n            label_col: Column name for labels in the file\n            sep: Delimiter for CSV, TSV, or TXT files\n            fasta_sep: Delimiter for FASTA files\n            multi_label_sep: Delimiter for multi-label sequences\n            uppercase: Whether to convert sequences to uppercase\n            lowercase: Whether to convert sequences to lowercase\n            keep_seqs: Whether to keep sequences in the dataset for later use\n            do_encode: Whether to encode sequences for the model\n\n        Returns:\n            Tuple containing:\n                - DNADataset: Dataset object with sequences and labels\n                - DataLoader: DataLoader object for batch processing\n\n        Raises:\n            ValueError: If input is neither a file path nor a list of sequences\n        \"\"\"\n        if isinstance(seq_or_path, str):\n            suffix = seq_or_path.split(\".\")[-1]\n            if suffix and os.path.isfile(seq_or_path):\n                sequences = []\n                dataset = DNADataset.load_local_data(seq_or_path, seq_col=seq_col, label_col=label_col,\n                                                     sep=sep, fasta_sep=fasta_sep, multi_label_sep=multi_label_sep,\n                                                     tokenizer=self.tokenizer, max_length=self.pred_config.max_length)\n            else:\n                sequences = [seq_or_path]\n        elif isinstance(seq_or_path, list):\n            sequences = seq_or_path\n        else:\n            raise ValueError(\"Input should be a file path or a list of sequences.\")\n        if len(sequences) &gt; 0:\n            ds = Dataset.from_dict({\"sequence\": sequences})\n            dataset = DNADataset(ds, self.tokenizer, max_length=self.pred_config.max_length)\n        # If labels are provided, keep labels\n        if keep_seqs:\n            self.sequences = dataset.dataset[\"sequence\"]\n        # Encode sequences\n        if do_encode:\n            task_type = self.task_config.task_type\n            dataset.encode_sequences(remove_unused_columns=True, task=task_type, uppercase=uppercase, lowercase=lowercase)\n        if \"labels\" in dataset.dataset.features:\n            self.labels = dataset.dataset[\"labels\"]\n        # Create DataLoader\n        dataloader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            num_workers=self.pred_config.num_workers\n        )\n\n        return dataset, dataloader\n\n    def logits_to_preds(self, logits: torch.Tensor) -&gt; Tuple[torch.Tensor, List]:\n        \"\"\"Convert model logits to predictions and human-readable labels.\n\n        This method processes raw model outputs based on the task type to generate\n        appropriate predictions and convert them to human-readable labels.\n\n        Args:\n            logits: Model output logits tensor\n\n        Returns:\n            Tuple containing:\n                - torch.Tensor: Model predictions (probabilities or raw values)\n                - List: Human-readable labels corresponding to predictions\n\n        Raises:\n            ValueError: If task type is not supported\n        \"\"\"\n        # Get task type and threshold from config\n        task_type = self.task_config.task_type\n        threshold = self.task_config.threshold\n        label_names = self.task_config.label_names\n        # Convert logits to predictions based on task type\n        if task_type == \"binary\":\n            probs = torch.softmax(logits, dim=-1)\n            preds = (probs[:, 1] &gt; threshold).long()\n            labels = [label_names[pred] for pred in preds]\n        elif task_type == \"multiclass\":\n            probs = torch.softmax(logits, dim=-1)\n            preds = torch.argmax(probs, dim=-1)\n            labels = [label_names[pred] for pred in preds]\n        elif task_type == \"multilabel\":\n            probs = torch.sigmoid(logits)\n            preds = (probs &gt; threshold).long()\n            labels = []\n            for pred in preds:\n                label = [label_names[i] for i in range(len(pred)) if pred[i] == 1]\n                labels.append(label)\n        elif task_type == \"regression\":\n            preds = logits.squeeze(-1)\n            probs = preds\n            labels = preds.tolist()\n        elif task_type == \"token\":\n            probs = torch.softmax(logits, dim=-1)\n            preds = torch.argmax(logits, dim=-1)\n            labels = []\n            for pred in preds:\n                label = [label_names[pred[i]] for i in range(len(pred))]\n                labels.append(label)\n        else:\n            raise ValueError(f\"Unsupported task type: {task_type}\")\n        return probs, labels\n\n    def format_output(self, predictions: Tuple[torch.Tensor, List]) -&gt; Dict:\n        \"\"\"Format output predictions into a structured dictionary.\n\n        This method converts raw predictions into a user-friendly format with\n        sequences, labels, and confidence scores.\n\n        Args:\n            predictions: Tuple containing (probabilities, labels)\n\n        Returns:\n            Dictionary containing formatted predictions with structure:\n            {index: {'sequence': str, 'label': str/list, 'scores': dict/list}}\n        \"\"\"\n        # Get task type from config\n        task_type = self.task_config.task_type\n        formatted_predictions = {}\n        probs, labels = predictions\n        probs = probs.numpy().tolist()\n        keep_seqs = True if len(self.sequences) else False\n        label_names = self.task_config.label_names\n        for i, label in enumerate(labels):\n            prob = probs[i]\n            formatted_predictions[i] = {\n                'sequence': self.sequences[i] if keep_seqs else '',\n                'label': label,\n                'scores': {label_names[j]: p for j, p in enumerate(prob)} if task_type != \"token\"\n                          else [max(x) for x in prob],\n            }\n        return formatted_predictions\n\n    @torch.no_grad()\n    def batch_predict(self, dataloader: DataLoader, do_pred: bool = True,\n                      output_hidden_states: bool = False,\n                      output_attentions: bool = False) -&gt; Tuple[torch.Tensor, Optional[Dict], Dict]:\n        \"\"\"Perform batch prediction on sequences.\n\n        This method runs inference on batches of sequences and optionally extracts\n        hidden states and attention weights for model interpretability.\n\n        Args:\n            dataloader: DataLoader object containing sequences for inference\n            do_pred: Whether to convert logits to predictions\n            output_hidden_states: Whether to output hidden states from all layers\n            output_attentions: Whether to output attention weights from all layers\n\n        Returns:\n            Tuple containing:\n                - torch.Tensor: All logits from the model\n                - Optional[Dict]: Predictions dictionary if do_pred=True, otherwise None\n                - Dict: Embeddings dictionary containing hidden states and/or attention weights\n\n        Note:\n            Setting output_hidden_states or output_attentions to True will consume\n            significant memory, especially for long sequences or large models.\n        \"\"\"\n        # Set model to evaluation mode\n        self.model.eval()\n        all_logits = []\n        # Whether or not to output hidden states\n        params = None\n        embeddings = {}\n        if output_hidden_states:\n            import inspect\n            sig = inspect.signature(self.model.forward)\n            params = sig.parameters\n            if 'output_hidden_states' in params:\n                self.model.config.output_hidden_states = True\n            embeddings['hidden_states'] = None\n            embeddings['attention_mask'] = []\n            embeddings['labels'] = []\n        if output_attentions:\n            if not params:\n                import inspect\n                sig = inspect.signature(self.model.forward)\n                params = sig.parameters\n            if 'output_attentions' in params:\n                self.model.config.output_attentions = True\n            embeddings['attentions'] = None\n        # Iterate over batches\n        for batch in tqdm(dataloader, desc=\"Predicting\"):\n            inputs = {k: v.to(self.device) for k, v in batch.items()}\n            if self.pred_config.use_fp16:\n                self.model = self.model.half()\n                with torch.amp.autocast('cuda'):\n                    outputs = self.model(**inputs)\n            else:\n                outputs = self.model(**inputs)\n            # Get logits\n            logits = outputs.logits.cpu().detach()\n            all_logits.append(logits)\n            # Get hidden states\n            if output_hidden_states:\n                hidden_states = [h.cpu().detach() for h in outputs.hidden_states] if hasattr(outputs, 'hidden_states') else None\n                if embeddings['hidden_states'] is None:\n                    embeddings['hidden_states'] = [[h] for h in hidden_states]\n                else:\n                    for i, h in enumerate(hidden_states):\n                        embeddings['hidden_states'][i].append(h)\n                attention_mask = inputs['attention_mask'].cpu().detach() if 'attention_mask' in inputs else None\n                embeddings['attention_mask'].append(attention_mask)\n                labels = inputs['labels'].cpu().detach() if 'labels' in inputs else None\n                embeddings['labels'].append(labels)\n            # Get attentions\n            if output_attentions:\n                attentions = [a.cpu().detach() for a in outputs.attentions] if hasattr(outputs, 'attentions') else None\n                if attentions:\n                    if embeddings['attentions'] is None:\n                        embeddings['attentions'] = [[a] for a in attentions]\n                    else:\n                        for i, a in enumerate(attentions):\n                            embeddings['attentions'][i].append(a)\n        # Concatenate logits\n        all_logits = torch.cat(all_logits, dim=0)\n        if output_hidden_states:\n            if embeddings['hidden_states']:\n                embeddings['hidden_states'] = tuple(torch.cat(lst, dim=0) for lst in embeddings['hidden_states'])\n            if embeddings['attention_mask']:\n                embeddings['attention_mask'] = torch.cat(embeddings['attention_mask'], dim=0)\n            if embeddings['labels']:\n                embeddings['labels'] = torch.cat(embeddings['labels'], dim=0)\n        if output_attentions:\n            if embeddings['attentions']:\n                embeddings['attentions'] = tuple(torch.cat(lst, dim=0) for lst in embeddings['attentions'])\n        # Get predictions\n        predictions = None\n        if do_pred:\n            predictions = self.logits_to_preds(all_logits)\n            predictions = self.format_output(predictions)\n        return all_logits, predictions, embeddings\n\n    def predict_seqs(self, sequences: Union[str, List[str]],\n                     evaluate: bool = False,\n                     output_hidden_states: bool = False,\n                     output_attentions: bool = False,\n                     save_to_file: bool = False) -&gt; Union[Dict, Tuple[Dict, Dict]]:\n        \"\"\"Predict for a list of sequences.\n\n        This method provides a convenient interface for predicting on sequences,\n        with optional evaluation and saving capabilities.\n\n        Args:\n            sequences: Single sequence or list of sequences for prediction\n            evaluate: Whether to evaluate predictions against true labels\n            output_hidden_states: Whether to output hidden states for visualization\n            output_attentions: Whether to output attention weights for visualization\n            save_to_file: Whether to save predictions to output directory\n\n        Returns:\n            Either:\n                - Dict: Dictionary containing predictions\n                - Tuple[Dict, Dict]: (predictions, metrics) if evaluate=True\n\n        Note:\n            Evaluation requires that labels are available in the dataset\n        \"\"\"\n        # Get dataset and dataloader from sequences\n        _, dataloader = self.generate_dataset(sequences, batch_size=self.pred_config.batch_size)\n        # Do batch prediction\n        logits, predictions, embeddings = self.batch_predict(dataloader,\n                                                             output_hidden_states=output_hidden_states,\n                                                             output_attentions=output_attentions)\n        # Keep hidden states\n        if output_hidden_states or output_attentions:\n            self.embeddings = embeddings\n        # Save predictions\n        if save_to_file and self.pred_config.output_dir:\n            save_predictions(predictions, Path(self.pred_config.output_dir))\n        # Do evaluation\n        if len(self.labels) == len(logits) and evaluate:\n            metrics = self.calculate_metrics(logits, self.labels)\n            metrics_save = dict(metrics)\n            if 'curve' in metrics_save:\n                del metrics_save['curve']\n            if 'scatter' in metrics_save:\n                del metrics_save['scatter']\n            if save_to_file and self.pred_config.output_dir:\n                save_metrics(metrics_save, Path(self.pred_config.output_dir))\n            return predictions, metrics\n\n        return predictions\n\n\n    def predict_file(self, file_path: str, evaluate: bool = False,\n                     output_hidden_states: bool = False,\n                     output_attentions: bool = False,\n                     seq_col: str = \"sequence\", label_col: str = \"labels\",\n                     sep: str = None, fasta_sep: str = \"|\",\n                     multi_label_sep: Union[str, None] = None,\n                     uppercase: bool = False, lowercase: bool = False,\n                     save_to_file: bool = False, plot_metrics: bool = False) -&gt; Union[Dict, Tuple[Dict, Dict]]:\n        \"\"\"Predict from a file containing sequences.\n\n        This method loads sequences from a file and performs prediction,\n        with optional evaluation, visualization, and saving capabilities.\n\n        Args:\n            file_path: Path to the file containing sequences\n            evaluate: Whether to evaluate predictions against true labels\n            output_hidden_states: Whether to output hidden states for visualization\n            output_attentions: Whether to output attention weights for visualization\n            seq_col: Column name for sequences in the file\n            label_col: Column name for labels in the file\n            sep: Delimiter for CSV, TSV, or TXT files\n            fasta_sep: Delimiter for FASTA files\n            multi_label_sep: Delimiter for multi-label sequences\n            uppercase: Whether to convert sequences to uppercase\n            lowercase: Whether to convert sequences to lowercase\n            save_to_file: Whether to save predictions and metrics to output directory\n            plot_metrics: Whether to generate metric plots\n\n        Returns:\n            Either:\n                - Dict: Dictionary containing predictions\n                - Tuple[Dict, Dict]: (predictions, metrics) if evaluate=True\n\n        Note:\n            Setting output_attentions=True may consume significant memory\n        \"\"\"\n        # Get dataset and dataloader from file\n        _, dataloader = self.generate_dataset(file_path, seq_col=seq_col, label_col=label_col,\n                                              sep=sep, fasta_sep=fasta_sep, multi_label_sep=multi_label_sep,\n                                              uppercase=uppercase, lowercase=lowercase,\n                                              batch_size=self.pred_config.batch_size)\n        # Do batch prediction\n        if output_attentions:\n            warnings.warn(\"Cautions: output_attentions may consume a lot of memory.\\n\")\n        logits, predictions, embeddings = self.batch_predict(dataloader,\n                                                             output_hidden_states=output_hidden_states,\n                                                             output_attentions=output_attentions)\n        # Keep hidden states\n        if output_hidden_states or output_attentions:\n            self.embeddings = embeddings\n        # Save predictions\n        if save_to_file and self.pred_config.output_dir:\n            save_predictions(predictions, Path(self.pred_config.output_dir))\n        # Do evaluation\n        if len(self.labels) == len(logits) and evaluate:\n            metrics = self.calculate_metrics(logits, self.labels, plot=plot_metrics)\n            metrics_save = dict(metrics)\n            if 'curve' in metrics_save:\n                del metrics_save['curve']\n            if 'scatter' in metrics_save:\n                del metrics_save['scatter']\n            if save_to_file and self.pred_config.output_dir:\n                save_metrics(metrics, Path(self.pred_config.output_dir))\n            # Whether to plot metrics\n            if plot_metrics:\n                return predictions, metrics\n            else:\n                return predictions, metrics_save\n\n        return predictions\n\n    def calculate_metrics(self, logits: Union[List, torch.Tensor],\n                          labels: Union[List, torch.Tensor], plot: bool = False) -&gt; Dict:\n        \"\"\"Calculate evaluation metrics for model predictions.\n\n        This method computes task-specific evaluation metrics using the configured\n        metrics computation module.\n\n        Args:\n            logits: Model predictions (logits or probabilities)\n            labels: True labels for evaluation\n            plot: Whether to generate metric plots\n\n        Returns:\n            Dictionary containing evaluation metrics for the task\n        \"\"\"\n        # Calculate metrics based on task type\n        compute_metrics = Metrics(self.task_config, plot=plot)\n        metrics = compute_metrics((logits, labels))\n\n        return metrics\n\n    def plot_attentions(self, seq_idx: int = 0, layer: int = -1, head: int = -1,\n                        width: int = 800, height: int = 800,\n                        save_path: Optional[str] = None) -&gt; Optional[Any]:\n        \"\"\"Plot attention map visualization.\n\n        This method creates a heatmap visualization of attention weights between tokens\n        in a sequence, showing how the model attends to different parts of the input.\n\n        Args:\n            seq_idx: Index of the sequence to plot, default 0\n            layer: Layer index to visualize, default -1 (last layer)\n            head: Attention head index to visualize, default -1 (last head)\n            width: Width of the plot\n            height: Height of the plot\n            save_path: Path to save the plot. If None, plot will be shown interactively\n\n        Returns:\n            Attention map visualization if available, otherwise None\n\n        Note:\n            This method requires that attention weights were collected during inference\n            by setting output_attentions=True in prediction methods\n        \"\"\"\n        if hasattr(self, 'embeddings'):\n            attentions = self.embeddings['attentions']\n            if save_path:\n                suffix = os.path.splitext(save_path)[-1]\n                if suffix:\n                    heatmap = save_path.replace(suffix, \"_heatmap\" + suffix)\n                else:\n                    heatmap = os.path.join(save_path, \"heatmap.pdf\")\n            else:\n                heatmap = None\n            # Plot attention map\n            attn_map = plot_attention_map(attentions, self.sequences, self.tokenizer,\n                                          seq_idx=seq_idx, layer=layer, head=head,\n                                          width=width, height=height,\n                                          save_path=heatmap)\n            return attn_map\n        else:\n            print(\"No attention weights available to plot.\")\n\n    def plot_hidden_states(self, reducer: str = \"t-SNE\",\n                           ncols: int = 4, width: int = 300, height: int = 300,\n                           save_path: Optional[str] = None) -&gt; Optional[Any]:\n        \"\"\"Visualize embeddings using dimensionality reduction.\n\n        This method creates 2D visualizations of high-dimensional embeddings from\n        different model layers using PCA, t-SNE, or UMAP dimensionality reduction.\n\n        Args:\n            reducer: Dimensionality reduction method to use ('PCA', 't-SNE', 'UMAP')\n            ncols: Number of columns in the plot grid\n            width: Width of each plot\n            height: Height of each plot\n            save_path: Path to save the plot. If None, plot will be shown interactively\n\n        Returns:\n            Embedding visualization if available, otherwise None\n\n        Note:\n            This method requires that hidden states were collected during inference\n            by setting output_hidden_states=True in prediction methods\n        \"\"\"\n        if hasattr(self, 'embeddings'):\n            hidden_states = self.embeddings['hidden_states']\n            attention_mask = torch.unsqueeze(self.embeddings['attention_mask'], dim=-1)\n            labels = self.embeddings['labels']\n            if save_path:\n                suffix = os.path.splitext(save_path)[-1]\n                if suffix:\n                    embedding = save_path.replace(suffix, \"_embedding\" + suffix)\n                else:\n                    embedding = os.path.join(save_path, \"embedding.pdf\")\n            else:\n                embedding = None\n            # Plot hidden states\n            label_names = self.task_config.label_names\n            embeddings_vis = plot_embeddings(hidden_states, attention_mask, reducer=reducer,\n                                             labels=labels, label_names=label_names,\n                                             ncols=ncols, width=width, height=height,\n                                             save_path=embedding)\n            return embeddings_vis\n        else:\n            print(\"No hidden states available to plot.\")\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.__init__","title":"<code>__init__(model, tokenizer, config)</code>","text":"<p>Initialize the predictor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Fine-tuned model instance for inference</p> required <code>tokenizer</code> <code>Any</code> <p>Tokenizer for encoding DNA sequences</p> required <code>config</code> <code>dict</code> <p>Configuration dictionary containing task settings and inference parameters</p> required Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def __init__(\n    self,\n    model: Any,\n    tokenizer: Any,\n    config: dict\n):\n    \"\"\"Initialize the predictor.\n\n    Args:\n        model: Fine-tuned model instance for inference\n        tokenizer: Tokenizer for encoding DNA sequences\n        config: Configuration dictionary containing task settings and inference parameters\n    \"\"\"\n\n    self.model = model\n    self.tokenizer = tokenizer\n    self.task_config = config['task']\n    self.pred_config = config['inference']\n    self.device = self._get_device()\n    if model:\n        self.model.to(self.device)\n        print(f\"Use device: {self.device}\")\n    self.sequences = []\n    self.labels = []\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.batch_predict","title":"<code>batch_predict(dataloader, do_pred=True, output_hidden_states=False, output_attentions=False)</code>","text":"<p>Perform batch prediction on sequences.</p> <p>This method runs inference on batches of sequences and optionally extracts hidden states and attention weights for model interpretability.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>DataLoader object containing sequences for inference</p> required <code>do_pred</code> <code>bool</code> <p>Whether to convert logits to predictions</p> <code>True</code> <code>output_hidden_states</code> <code>bool</code> <p>Whether to output hidden states from all layers</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>Whether to output attention weights from all layers</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Optional[Dict], Dict]</code> <p>Tuple containing: - torch.Tensor: All logits from the model - Optional[Dict]: Predictions dictionary if do_pred=True, otherwise None - Dict: Embeddings dictionary containing hidden states and/or attention weights</p> Note <p>Setting output_hidden_states or output_attentions to True will consume significant memory, especially for long sequences or large models.</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>@torch.no_grad()\ndef batch_predict(self, dataloader: DataLoader, do_pred: bool = True,\n                  output_hidden_states: bool = False,\n                  output_attentions: bool = False) -&gt; Tuple[torch.Tensor, Optional[Dict], Dict]:\n    \"\"\"Perform batch prediction on sequences.\n\n    This method runs inference on batches of sequences and optionally extracts\n    hidden states and attention weights for model interpretability.\n\n    Args:\n        dataloader: DataLoader object containing sequences for inference\n        do_pred: Whether to convert logits to predictions\n        output_hidden_states: Whether to output hidden states from all layers\n        output_attentions: Whether to output attention weights from all layers\n\n    Returns:\n        Tuple containing:\n            - torch.Tensor: All logits from the model\n            - Optional[Dict]: Predictions dictionary if do_pred=True, otherwise None\n            - Dict: Embeddings dictionary containing hidden states and/or attention weights\n\n    Note:\n        Setting output_hidden_states or output_attentions to True will consume\n        significant memory, especially for long sequences or large models.\n    \"\"\"\n    # Set model to evaluation mode\n    self.model.eval()\n    all_logits = []\n    # Whether or not to output hidden states\n    params = None\n    embeddings = {}\n    if output_hidden_states:\n        import inspect\n        sig = inspect.signature(self.model.forward)\n        params = sig.parameters\n        if 'output_hidden_states' in params:\n            self.model.config.output_hidden_states = True\n        embeddings['hidden_states'] = None\n        embeddings['attention_mask'] = []\n        embeddings['labels'] = []\n    if output_attentions:\n        if not params:\n            import inspect\n            sig = inspect.signature(self.model.forward)\n            params = sig.parameters\n        if 'output_attentions' in params:\n            self.model.config.output_attentions = True\n        embeddings['attentions'] = None\n    # Iterate over batches\n    for batch in tqdm(dataloader, desc=\"Predicting\"):\n        inputs = {k: v.to(self.device) for k, v in batch.items()}\n        if self.pred_config.use_fp16:\n            self.model = self.model.half()\n            with torch.amp.autocast('cuda'):\n                outputs = self.model(**inputs)\n        else:\n            outputs = self.model(**inputs)\n        # Get logits\n        logits = outputs.logits.cpu().detach()\n        all_logits.append(logits)\n        # Get hidden states\n        if output_hidden_states:\n            hidden_states = [h.cpu().detach() for h in outputs.hidden_states] if hasattr(outputs, 'hidden_states') else None\n            if embeddings['hidden_states'] is None:\n                embeddings['hidden_states'] = [[h] for h in hidden_states]\n            else:\n                for i, h in enumerate(hidden_states):\n                    embeddings['hidden_states'][i].append(h)\n            attention_mask = inputs['attention_mask'].cpu().detach() if 'attention_mask' in inputs else None\n            embeddings['attention_mask'].append(attention_mask)\n            labels = inputs['labels'].cpu().detach() if 'labels' in inputs else None\n            embeddings['labels'].append(labels)\n        # Get attentions\n        if output_attentions:\n            attentions = [a.cpu().detach() for a in outputs.attentions] if hasattr(outputs, 'attentions') else None\n            if attentions:\n                if embeddings['attentions'] is None:\n                    embeddings['attentions'] = [[a] for a in attentions]\n                else:\n                    for i, a in enumerate(attentions):\n                        embeddings['attentions'][i].append(a)\n    # Concatenate logits\n    all_logits = torch.cat(all_logits, dim=0)\n    if output_hidden_states:\n        if embeddings['hidden_states']:\n            embeddings['hidden_states'] = tuple(torch.cat(lst, dim=0) for lst in embeddings['hidden_states'])\n        if embeddings['attention_mask']:\n            embeddings['attention_mask'] = torch.cat(embeddings['attention_mask'], dim=0)\n        if embeddings['labels']:\n            embeddings['labels'] = torch.cat(embeddings['labels'], dim=0)\n    if output_attentions:\n        if embeddings['attentions']:\n            embeddings['attentions'] = tuple(torch.cat(lst, dim=0) for lst in embeddings['attentions'])\n    # Get predictions\n    predictions = None\n    if do_pred:\n        predictions = self.logits_to_preds(all_logits)\n        predictions = self.format_output(predictions)\n    return all_logits, predictions, embeddings\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.calculate_metrics","title":"<code>calculate_metrics(logits, labels, plot=False)</code>","text":"<p>Calculate evaluation metrics for model predictions.</p> <p>This method computes task-specific evaluation metrics using the configured metrics computation module.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Union[List, Tensor]</code> <p>Model predictions (logits or probabilities)</p> required <code>labels</code> <code>Union[List, Tensor]</code> <p>True labels for evaluation</p> required <code>plot</code> <code>bool</code> <p>Whether to generate metric plots</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary containing evaluation metrics for the task</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def calculate_metrics(self, logits: Union[List, torch.Tensor],\n                      labels: Union[List, torch.Tensor], plot: bool = False) -&gt; Dict:\n    \"\"\"Calculate evaluation metrics for model predictions.\n\n    This method computes task-specific evaluation metrics using the configured\n    metrics computation module.\n\n    Args:\n        logits: Model predictions (logits or probabilities)\n        labels: True labels for evaluation\n        plot: Whether to generate metric plots\n\n    Returns:\n        Dictionary containing evaluation metrics for the task\n    \"\"\"\n    # Calculate metrics based on task type\n    compute_metrics = Metrics(self.task_config, plot=plot)\n    metrics = compute_metrics((logits, labels))\n\n    return metrics\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.format_output","title":"<code>format_output(predictions)</code>","text":"<p>Format output predictions into a structured dictionary.</p> <p>This method converts raw predictions into a user-friendly format with sequences, labels, and confidence scores.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Tuple[Tensor, List]</code> <p>Tuple containing (probabilities, labels)</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary containing formatted predictions with structure:</p> <code>Dict</code> <p>{index: {'sequence': str, 'label': str/list, 'scores': dict/list}}</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def format_output(self, predictions: Tuple[torch.Tensor, List]) -&gt; Dict:\n    \"\"\"Format output predictions into a structured dictionary.\n\n    This method converts raw predictions into a user-friendly format with\n    sequences, labels, and confidence scores.\n\n    Args:\n        predictions: Tuple containing (probabilities, labels)\n\n    Returns:\n        Dictionary containing formatted predictions with structure:\n        {index: {'sequence': str, 'label': str/list, 'scores': dict/list}}\n    \"\"\"\n    # Get task type from config\n    task_type = self.task_config.task_type\n    formatted_predictions = {}\n    probs, labels = predictions\n    probs = probs.numpy().tolist()\n    keep_seqs = True if len(self.sequences) else False\n    label_names = self.task_config.label_names\n    for i, label in enumerate(labels):\n        prob = probs[i]\n        formatted_predictions[i] = {\n            'sequence': self.sequences[i] if keep_seqs else '',\n            'label': label,\n            'scores': {label_names[j]: p for j, p in enumerate(prob)} if task_type != \"token\"\n                      else [max(x) for x in prob],\n        }\n    return formatted_predictions\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.generate_dataset","title":"<code>generate_dataset(seq_or_path, batch_size=1, seq_col='sequence', label_col='labels', sep=None, fasta_sep='|', multi_label_sep=None, uppercase=False, lowercase=False, keep_seqs=True, do_encode=True)</code>","text":"<p>Generate dataset from sequences or file path.</p> <p>This method creates a DNADataset and DataLoader from either a list of sequences or a file path, supporting various file formats and preprocessing options.</p> <p>Parameters:</p> Name Type Description Default <code>seq_or_path</code> <code>Union[str, List[str]]</code> <p>Single sequence, list of sequences, or path to a file containing sequences</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoader</p> <code>1</code> <code>seq_col</code> <code>str</code> <p>Column name for sequences in the file</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for labels in the file</p> <code>'labels'</code> <code>sep</code> <code>str</code> <p>Delimiter for CSV, TSV, or TXT files</p> <code>None</code> <code>fasta_sep</code> <code>str</code> <p>Delimiter for FASTA files</p> <code>'|'</code> <code>multi_label_sep</code> <code>Union[str, None]</code> <p>Delimiter for multi-label sequences</p> <code>None</code> <code>uppercase</code> <code>bool</code> <p>Whether to convert sequences to uppercase</p> <code>False</code> <code>lowercase</code> <code>bool</code> <p>Whether to convert sequences to lowercase</p> <code>False</code> <code>keep_seqs</code> <code>bool</code> <p>Whether to keep sequences in the dataset for later use</p> <code>True</code> <code>do_encode</code> <code>bool</code> <p>Whether to encode sequences for the model</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[DNADataset, DataLoader]</code> <p>Tuple containing: - DNADataset: Dataset object with sequences and labels - DataLoader: DataLoader object for batch processing</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input is neither a file path nor a list of sequences</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def generate_dataset(self, seq_or_path: Union[str, List[str]], batch_size: int = 1,\n                     seq_col: str = \"sequence\", label_col: str = \"labels\",\n                     sep: str = None, fasta_sep: str = \"|\",\n                     multi_label_sep: Union[str, None] = None,\n                     uppercase: bool = False, lowercase: bool = False,\n                     keep_seqs: bool = True, do_encode: bool = True) -&gt; Tuple[DNADataset, DataLoader]:\n    \"\"\"Generate dataset from sequences or file path.\n\n    This method creates a DNADataset and DataLoader from either a list of sequences\n    or a file path, supporting various file formats and preprocessing options.\n\n    Args:\n        seq_or_path: Single sequence, list of sequences, or path to a file containing sequences\n        batch_size: Batch size for DataLoader\n        seq_col: Column name for sequences in the file\n        label_col: Column name for labels in the file\n        sep: Delimiter for CSV, TSV, or TXT files\n        fasta_sep: Delimiter for FASTA files\n        multi_label_sep: Delimiter for multi-label sequences\n        uppercase: Whether to convert sequences to uppercase\n        lowercase: Whether to convert sequences to lowercase\n        keep_seqs: Whether to keep sequences in the dataset for later use\n        do_encode: Whether to encode sequences for the model\n\n    Returns:\n        Tuple containing:\n            - DNADataset: Dataset object with sequences and labels\n            - DataLoader: DataLoader object for batch processing\n\n    Raises:\n        ValueError: If input is neither a file path nor a list of sequences\n    \"\"\"\n    if isinstance(seq_or_path, str):\n        suffix = seq_or_path.split(\".\")[-1]\n        if suffix and os.path.isfile(seq_or_path):\n            sequences = []\n            dataset = DNADataset.load_local_data(seq_or_path, seq_col=seq_col, label_col=label_col,\n                                                 sep=sep, fasta_sep=fasta_sep, multi_label_sep=multi_label_sep,\n                                                 tokenizer=self.tokenizer, max_length=self.pred_config.max_length)\n        else:\n            sequences = [seq_or_path]\n    elif isinstance(seq_or_path, list):\n        sequences = seq_or_path\n    else:\n        raise ValueError(\"Input should be a file path or a list of sequences.\")\n    if len(sequences) &gt; 0:\n        ds = Dataset.from_dict({\"sequence\": sequences})\n        dataset = DNADataset(ds, self.tokenizer, max_length=self.pred_config.max_length)\n    # If labels are provided, keep labels\n    if keep_seqs:\n        self.sequences = dataset.dataset[\"sequence\"]\n    # Encode sequences\n    if do_encode:\n        task_type = self.task_config.task_type\n        dataset.encode_sequences(remove_unused_columns=True, task=task_type, uppercase=uppercase, lowercase=lowercase)\n    if \"labels\" in dataset.dataset.features:\n        self.labels = dataset.dataset[\"labels\"]\n    # Create DataLoader\n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        num_workers=self.pred_config.num_workers\n    )\n\n    return dataset, dataloader\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.logits_to_preds","title":"<code>logits_to_preds(logits)</code>","text":"<p>Convert model logits to predictions and human-readable labels.</p> <p>This method processes raw model outputs based on the task type to generate appropriate predictions and convert them to human-readable labels.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Model output logits tensor</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, List]</code> <p>Tuple containing: - torch.Tensor: Model predictions (probabilities or raw values) - List: Human-readable labels corresponding to predictions</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If task type is not supported</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def logits_to_preds(self, logits: torch.Tensor) -&gt; Tuple[torch.Tensor, List]:\n    \"\"\"Convert model logits to predictions and human-readable labels.\n\n    This method processes raw model outputs based on the task type to generate\n    appropriate predictions and convert them to human-readable labels.\n\n    Args:\n        logits: Model output logits tensor\n\n    Returns:\n        Tuple containing:\n            - torch.Tensor: Model predictions (probabilities or raw values)\n            - List: Human-readable labels corresponding to predictions\n\n    Raises:\n        ValueError: If task type is not supported\n    \"\"\"\n    # Get task type and threshold from config\n    task_type = self.task_config.task_type\n    threshold = self.task_config.threshold\n    label_names = self.task_config.label_names\n    # Convert logits to predictions based on task type\n    if task_type == \"binary\":\n        probs = torch.softmax(logits, dim=-1)\n        preds = (probs[:, 1] &gt; threshold).long()\n        labels = [label_names[pred] for pred in preds]\n    elif task_type == \"multiclass\":\n        probs = torch.softmax(logits, dim=-1)\n        preds = torch.argmax(probs, dim=-1)\n        labels = [label_names[pred] for pred in preds]\n    elif task_type == \"multilabel\":\n        probs = torch.sigmoid(logits)\n        preds = (probs &gt; threshold).long()\n        labels = []\n        for pred in preds:\n            label = [label_names[i] for i in range(len(pred)) if pred[i] == 1]\n            labels.append(label)\n    elif task_type == \"regression\":\n        preds = logits.squeeze(-1)\n        probs = preds\n        labels = preds.tolist()\n    elif task_type == \"token\":\n        probs = torch.softmax(logits, dim=-1)\n        preds = torch.argmax(logits, dim=-1)\n        labels = []\n        for pred in preds:\n            label = [label_names[pred[i]] for i in range(len(pred))]\n            labels.append(label)\n    else:\n        raise ValueError(f\"Unsupported task type: {task_type}\")\n    return probs, labels\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.plot_attentions","title":"<code>plot_attentions(seq_idx=0, layer=-1, head=-1, width=800, height=800, save_path=None)</code>","text":"<p>Plot attention map visualization.</p> <p>This method creates a heatmap visualization of attention weights between tokens in a sequence, showing how the model attends to different parts of the input.</p> <p>Parameters:</p> Name Type Description Default <code>seq_idx</code> <code>int</code> <p>Index of the sequence to plot, default 0</p> <code>0</code> <code>layer</code> <code>int</code> <p>Layer index to visualize, default -1 (last layer)</p> <code>-1</code> <code>head</code> <code>int</code> <p>Attention head index to visualize, default -1 (last head)</p> <code>-1</code> <code>width</code> <code>int</code> <p>Width of the plot</p> <code>800</code> <code>height</code> <code>int</code> <p>Height of the plot</p> <code>800</code> <code>save_path</code> <code>Optional[str]</code> <p>Path to save the plot. If None, plot will be shown interactively</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>Attention map visualization if available, otherwise None</p> Note <p>This method requires that attention weights were collected during inference by setting output_attentions=True in prediction methods</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def plot_attentions(self, seq_idx: int = 0, layer: int = -1, head: int = -1,\n                    width: int = 800, height: int = 800,\n                    save_path: Optional[str] = None) -&gt; Optional[Any]:\n    \"\"\"Plot attention map visualization.\n\n    This method creates a heatmap visualization of attention weights between tokens\n    in a sequence, showing how the model attends to different parts of the input.\n\n    Args:\n        seq_idx: Index of the sequence to plot, default 0\n        layer: Layer index to visualize, default -1 (last layer)\n        head: Attention head index to visualize, default -1 (last head)\n        width: Width of the plot\n        height: Height of the plot\n        save_path: Path to save the plot. If None, plot will be shown interactively\n\n    Returns:\n        Attention map visualization if available, otherwise None\n\n    Note:\n        This method requires that attention weights were collected during inference\n        by setting output_attentions=True in prediction methods\n    \"\"\"\n    if hasattr(self, 'embeddings'):\n        attentions = self.embeddings['attentions']\n        if save_path:\n            suffix = os.path.splitext(save_path)[-1]\n            if suffix:\n                heatmap = save_path.replace(suffix, \"_heatmap\" + suffix)\n            else:\n                heatmap = os.path.join(save_path, \"heatmap.pdf\")\n        else:\n            heatmap = None\n        # Plot attention map\n        attn_map = plot_attention_map(attentions, self.sequences, self.tokenizer,\n                                      seq_idx=seq_idx, layer=layer, head=head,\n                                      width=width, height=height,\n                                      save_path=heatmap)\n        return attn_map\n    else:\n        print(\"No attention weights available to plot.\")\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.plot_hidden_states","title":"<code>plot_hidden_states(reducer='t-SNE', ncols=4, width=300, height=300, save_path=None)</code>","text":"<p>Visualize embeddings using dimensionality reduction.</p> <p>This method creates 2D visualizations of high-dimensional embeddings from different model layers using PCA, t-SNE, or UMAP dimensionality reduction.</p> <p>Parameters:</p> Name Type Description Default <code>reducer</code> <code>str</code> <p>Dimensionality reduction method to use ('PCA', 't-SNE', 'UMAP')</p> <code>'t-SNE'</code> <code>ncols</code> <code>int</code> <p>Number of columns in the plot grid</p> <code>4</code> <code>width</code> <code>int</code> <p>Width of each plot</p> <code>300</code> <code>height</code> <code>int</code> <p>Height of each plot</p> <code>300</code> <code>save_path</code> <code>Optional[str]</code> <p>Path to save the plot. If None, plot will be shown interactively</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>Embedding visualization if available, otherwise None</p> Note <p>This method requires that hidden states were collected during inference by setting output_hidden_states=True in prediction methods</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def plot_hidden_states(self, reducer: str = \"t-SNE\",\n                       ncols: int = 4, width: int = 300, height: int = 300,\n                       save_path: Optional[str] = None) -&gt; Optional[Any]:\n    \"\"\"Visualize embeddings using dimensionality reduction.\n\n    This method creates 2D visualizations of high-dimensional embeddings from\n    different model layers using PCA, t-SNE, or UMAP dimensionality reduction.\n\n    Args:\n        reducer: Dimensionality reduction method to use ('PCA', 't-SNE', 'UMAP')\n        ncols: Number of columns in the plot grid\n        width: Width of each plot\n        height: Height of each plot\n        save_path: Path to save the plot. If None, plot will be shown interactively\n\n    Returns:\n        Embedding visualization if available, otherwise None\n\n    Note:\n        This method requires that hidden states were collected during inference\n        by setting output_hidden_states=True in prediction methods\n    \"\"\"\n    if hasattr(self, 'embeddings'):\n        hidden_states = self.embeddings['hidden_states']\n        attention_mask = torch.unsqueeze(self.embeddings['attention_mask'], dim=-1)\n        labels = self.embeddings['labels']\n        if save_path:\n            suffix = os.path.splitext(save_path)[-1]\n            if suffix:\n                embedding = save_path.replace(suffix, \"_embedding\" + suffix)\n            else:\n                embedding = os.path.join(save_path, \"embedding.pdf\")\n        else:\n            embedding = None\n        # Plot hidden states\n        label_names = self.task_config.label_names\n        embeddings_vis = plot_embeddings(hidden_states, attention_mask, reducer=reducer,\n                                         labels=labels, label_names=label_names,\n                                         ncols=ncols, width=width, height=height,\n                                         save_path=embedding)\n        return embeddings_vis\n    else:\n        print(\"No hidden states available to plot.\")\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.predict_file","title":"<code>predict_file(file_path, evaluate=False, output_hidden_states=False, output_attentions=False, seq_col='sequence', label_col='labels', sep=None, fasta_sep='|', multi_label_sep=None, uppercase=False, lowercase=False, save_to_file=False, plot_metrics=False)</code>","text":"<p>Predict from a file containing sequences.</p> <p>This method loads sequences from a file and performs prediction, with optional evaluation, visualization, and saving capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file containing sequences</p> required <code>evaluate</code> <code>bool</code> <p>Whether to evaluate predictions against true labels</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>Whether to output hidden states for visualization</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>Whether to output attention weights for visualization</p> <code>False</code> <code>seq_col</code> <code>str</code> <p>Column name for sequences in the file</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for labels in the file</p> <code>'labels'</code> <code>sep</code> <code>str</code> <p>Delimiter for CSV, TSV, or TXT files</p> <code>None</code> <code>fasta_sep</code> <code>str</code> <p>Delimiter for FASTA files</p> <code>'|'</code> <code>multi_label_sep</code> <code>Union[str, None]</code> <p>Delimiter for multi-label sequences</p> <code>None</code> <code>uppercase</code> <code>bool</code> <p>Whether to convert sequences to uppercase</p> <code>False</code> <code>lowercase</code> <code>bool</code> <p>Whether to convert sequences to lowercase</p> <code>False</code> <code>save_to_file</code> <code>bool</code> <p>Whether to save predictions and metrics to output directory</p> <code>False</code> <code>plot_metrics</code> <code>bool</code> <p>Whether to generate metric plots</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Either</code> <code>Union[Dict, Tuple[Dict, Dict]]</code> <ul> <li>Dict: Dictionary containing predictions</li> <li>Tuple[Dict, Dict]: (predictions, metrics) if evaluate=True</li> </ul> Note <p>Setting output_attentions=True may consume significant memory</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def predict_file(self, file_path: str, evaluate: bool = False,\n                 output_hidden_states: bool = False,\n                 output_attentions: bool = False,\n                 seq_col: str = \"sequence\", label_col: str = \"labels\",\n                 sep: str = None, fasta_sep: str = \"|\",\n                 multi_label_sep: Union[str, None] = None,\n                 uppercase: bool = False, lowercase: bool = False,\n                 save_to_file: bool = False, plot_metrics: bool = False) -&gt; Union[Dict, Tuple[Dict, Dict]]:\n    \"\"\"Predict from a file containing sequences.\n\n    This method loads sequences from a file and performs prediction,\n    with optional evaluation, visualization, and saving capabilities.\n\n    Args:\n        file_path: Path to the file containing sequences\n        evaluate: Whether to evaluate predictions against true labels\n        output_hidden_states: Whether to output hidden states for visualization\n        output_attentions: Whether to output attention weights for visualization\n        seq_col: Column name for sequences in the file\n        label_col: Column name for labels in the file\n        sep: Delimiter for CSV, TSV, or TXT files\n        fasta_sep: Delimiter for FASTA files\n        multi_label_sep: Delimiter for multi-label sequences\n        uppercase: Whether to convert sequences to uppercase\n        lowercase: Whether to convert sequences to lowercase\n        save_to_file: Whether to save predictions and metrics to output directory\n        plot_metrics: Whether to generate metric plots\n\n    Returns:\n        Either:\n            - Dict: Dictionary containing predictions\n            - Tuple[Dict, Dict]: (predictions, metrics) if evaluate=True\n\n    Note:\n        Setting output_attentions=True may consume significant memory\n    \"\"\"\n    # Get dataset and dataloader from file\n    _, dataloader = self.generate_dataset(file_path, seq_col=seq_col, label_col=label_col,\n                                          sep=sep, fasta_sep=fasta_sep, multi_label_sep=multi_label_sep,\n                                          uppercase=uppercase, lowercase=lowercase,\n                                          batch_size=self.pred_config.batch_size)\n    # Do batch prediction\n    if output_attentions:\n        warnings.warn(\"Cautions: output_attentions may consume a lot of memory.\\n\")\n    logits, predictions, embeddings = self.batch_predict(dataloader,\n                                                         output_hidden_states=output_hidden_states,\n                                                         output_attentions=output_attentions)\n    # Keep hidden states\n    if output_hidden_states or output_attentions:\n        self.embeddings = embeddings\n    # Save predictions\n    if save_to_file and self.pred_config.output_dir:\n        save_predictions(predictions, Path(self.pred_config.output_dir))\n    # Do evaluation\n    if len(self.labels) == len(logits) and evaluate:\n        metrics = self.calculate_metrics(logits, self.labels, plot=plot_metrics)\n        metrics_save = dict(metrics)\n        if 'curve' in metrics_save:\n            del metrics_save['curve']\n        if 'scatter' in metrics_save:\n            del metrics_save['scatter']\n        if save_to_file and self.pred_config.output_dir:\n            save_metrics(metrics, Path(self.pred_config.output_dir))\n        # Whether to plot metrics\n        if plot_metrics:\n            return predictions, metrics\n        else:\n            return predictions, metrics_save\n\n    return predictions\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.predict_seqs","title":"<code>predict_seqs(sequences, evaluate=False, output_hidden_states=False, output_attentions=False, save_to_file=False)</code>","text":"<p>Predict for a list of sequences.</p> <p>This method provides a convenient interface for predicting on sequences, with optional evaluation and saving capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>sequences</code> <code>Union[str, List[str]]</code> <p>Single sequence or list of sequences for prediction</p> required <code>evaluate</code> <code>bool</code> <p>Whether to evaluate predictions against true labels</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>Whether to output hidden states for visualization</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>Whether to output attention weights for visualization</p> <code>False</code> <code>save_to_file</code> <code>bool</code> <p>Whether to save predictions to output directory</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Either</code> <code>Union[Dict, Tuple[Dict, Dict]]</code> <ul> <li>Dict: Dictionary containing predictions</li> <li>Tuple[Dict, Dict]: (predictions, metrics) if evaluate=True</li> </ul> Note <p>Evaluation requires that labels are available in the dataset</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def predict_seqs(self, sequences: Union[str, List[str]],\n                 evaluate: bool = False,\n                 output_hidden_states: bool = False,\n                 output_attentions: bool = False,\n                 save_to_file: bool = False) -&gt; Union[Dict, Tuple[Dict, Dict]]:\n    \"\"\"Predict for a list of sequences.\n\n    This method provides a convenient interface for predicting on sequences,\n    with optional evaluation and saving capabilities.\n\n    Args:\n        sequences: Single sequence or list of sequences for prediction\n        evaluate: Whether to evaluate predictions against true labels\n        output_hidden_states: Whether to output hidden states for visualization\n        output_attentions: Whether to output attention weights for visualization\n        save_to_file: Whether to save predictions to output directory\n\n    Returns:\n        Either:\n            - Dict: Dictionary containing predictions\n            - Tuple[Dict, Dict]: (predictions, metrics) if evaluate=True\n\n    Note:\n        Evaluation requires that labels are available in the dataset\n    \"\"\"\n    # Get dataset and dataloader from sequences\n    _, dataloader = self.generate_dataset(sequences, batch_size=self.pred_config.batch_size)\n    # Do batch prediction\n    logits, predictions, embeddings = self.batch_predict(dataloader,\n                                                         output_hidden_states=output_hidden_states,\n                                                         output_attentions=output_attentions)\n    # Keep hidden states\n    if output_hidden_states or output_attentions:\n        self.embeddings = embeddings\n    # Save predictions\n    if save_to_file and self.pred_config.output_dir:\n        save_predictions(predictions, Path(self.pred_config.output_dir))\n    # Do evaluation\n    if len(self.labels) == len(logits) and evaluate:\n        metrics = self.calculate_metrics(logits, self.labels)\n        metrics_save = dict(metrics)\n        if 'curve' in metrics_save:\n            del metrics_save['curve']\n        if 'scatter' in metrics_save:\n            del metrics_save['scatter']\n        if save_to_file and self.pred_config.output_dir:\n            save_metrics(metrics_save, Path(self.pred_config.output_dir))\n        return predictions, metrics\n\n    return predictions\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.generate","title":"<code>generate(self, dataloader, n_tokens=400, temperature=1.0, top_k=4)</code>","text":"<p>Generate DNA sequences using the model.</p> <p>This function performs sequence generation tasks using the loaded model, currently supporting EVO2 models for DNA sequence generation.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>DataLoader containing prompt sequences</p> required <code>n_tokens</code> <code>int</code> <p>Number of tokens to generate, default 400</p> <code>400</code> <code>temperature</code> <code>float</code> <p>Sampling temperature for generation, default 1.0</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k sampling parameter, default 4</p> <code>4</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary containing generated sequences</p> Note <p>Currently only supports EVO2 models for sequence generation</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def generate(self, dataloader: DataLoader, n_tokens: int = 400, temperature: float = 1.0,\n             top_k: int = 4) -&gt; Dict:\n    \"\"\"Generate DNA sequences using the model.\n\n    This function performs sequence generation tasks using the loaded model,\n    currently supporting EVO2 models for DNA sequence generation.\n\n    Args:\n        dataloader: DataLoader containing prompt sequences\n        n_tokens: Number of tokens to generate, default 400\n        temperature: Sampling temperature for generation, default 1.0\n        top_k: Top-k sampling parameter, default 4\n\n    Returns:\n        Dictionary containing generated sequences\n\n    Note:\n        Currently only supports EVO2 models for sequence generation\n    \"\"\"\n    if \"evo2\" in str(self.model):\n        for data in tqdm(dataloader, desc=\"Generating\"):\n            prompt_seqs = data['sequence']\n            if isinstance(prompt_seqs, list):\n                prompt_seqs = [seq for seq in prompt_seqs if seq]\n            if not prompt_seqs:\n                continue\n            # Generate sequences\n            output = self.model.generate(prompt_seqs=prompt_seqs, n_tokens=n_tokens,\n                                         temperature=temperature, top_k=top_k)\n            return output\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.plot_attention_map","title":"<code>plot_attention_map(attentions, sequences, tokenizer, seq_idx=0, layer=-1, head=-1, width=800, height=800, save_path=None)</code>","text":"<p>Plot attention map visualization for transformer models.</p> <p>This function creates a heatmap visualization of attention weights between tokens in a sequence, showing how the model attends to different parts of the input.</p> <p>Parameters:</p> Name Type Description Default <code>attentions</code> <code>Union[tuple, list]</code> <p>Tuple or list containing attention weights from model layers</p> required <code>sequences</code> <code>list</code> <p>List of input sequences</p> required <code>tokenizer</code> <p>Tokenizer object for converting tokens to readable text</p> required <code>seq_idx</code> <code>int</code> <p>Index of the sequence to plot, default 0</p> <code>0</code> <code>layer</code> <code>int</code> <p>Layer index to visualize, default -1 (last layer)</p> <code>-1</code> <code>head</code> <code>int</code> <p>Attention head index to visualize, default -1 (last head)</p> <code>-1</code> <code>width</code> <code>int</code> <p>Width of the plot</p> <code>800</code> <code>height</code> <code>int</code> <p>Height of the plot</p> <code>800</code> <code>save_path</code> <code>str</code> <p>Path to save the plot. If None, plot will be shown interactively</p> <code>None</code> <p>Returns:</p> Type Description <code>Chart</code> <p>Altair chart object showing the attention heatmap</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_attention_map(attentions: Union[tuple, list], sequences: list, tokenizer,\n                       seq_idx: int = 0, layer: int = -1, head: int = -1,\n                       width: int = 800, height: int = 800,\n                       save_path: str = None) -&gt; alt.Chart:\n    \"\"\"Plot attention map visualization for transformer models.\n\n    This function creates a heatmap visualization of attention weights between tokens\n    in a sequence, showing how the model attends to different parts of the input.\n\n    Args:\n        attentions: Tuple or list containing attention weights from model layers\n        sequences: List of input sequences\n        tokenizer: Tokenizer object for converting tokens to readable text\n        seq_idx: Index of the sequence to plot, default 0\n        layer: Layer index to visualize, default -1 (last layer)\n        head: Attention head index to visualize, default -1 (last head)\n        width: Width of the plot\n        height: Height of the plot\n        save_path: Path to save the plot. If None, plot will be shown interactively\n\n    Returns:\n        Altair chart object showing the attention heatmap\n    \"\"\"\n    # Plot attention map\n    attn_layer = attentions[layer].numpy()\n    attn_head = attn_layer[seq_idx][head]\n    # Get the tokens\n    seq = sequences[seq_idx]\n    tokens_id = tokenizer.encode(seq)\n    try:\n        tokens = tokenizer.convert_ids_to_tokens(tokens_id)\n    except:\n        tokens = tokenizer.decode(seq).split()\n    # Create a DataFrame for the attention map\n    num_tokens = len(tokens)\n    flen = len(str(num_tokens))\n    df = {\"token1\": [], 'token2': [], 'attn': []}\n    for i, t1 in enumerate(tokens):\n        for j, t2 in enumerate(tokens):\n            df[\"token1\"].append(str(i).zfill(flen)+t1)\n            df[\"token2\"].append(str(num_tokens-j).zfill(flen)+t2)\n            df[\"attn\"].append(attn_head[i][j])\n    source = pd.DataFrame(df)\n    # Enable VegaFusion for Altair\n    alt.data_transformers.enable(\"vegafusion\")\n    # Plot the attention map\n    attn_map = alt.Chart(source).mark_rect().encode(\n        x=alt.X('token1:O', axis=alt.Axis(\n                    labelExpr = f\"substring(datum.value, {flen}, 100)\",\n                    labelAngle=-45,\n                    )\n                ).title(None),\n        y=alt.Y('token2:O', axis=alt.Axis(\n                    labelExpr = f\"substring(datum.value, {flen}, 100)\",\n                    labelAngle=0,\n                    )\n                ).title(None),\n        color=alt.Color('attn:Q').scale(scheme='viridis'),\n    ).properties(\n        width=width,\n        height=height\n    ).configure_axis(grid=False)\n    # Save the plot\n    if save_path:\n        attn_map.save(save_path)\n        print(f\"Attention map saved to {save_path}\")\n    return attn_map\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.plot_bars","title":"<code>plot_bars(data, show_score=True, ncols=3, width=200, height=50, bar_width=30, domain=(0.0, 1.0), save_path=None, separate=False)</code>","text":"<p>Plot bar charts for model metrics comparison.</p> <p>This function creates bar charts to compare different metrics across multiple models. It supports automatic layout with multiple columns and optional score labels on bars.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing metrics data with 'models' as the first key</p> required <code>show_score</code> <code>bool</code> <p>Whether to show the score values on the bars</p> <code>True</code> <code>ncols</code> <code>int</code> <p>Number of columns to arrange the plots</p> <code>3</code> <code>width</code> <code>int</code> <p>Width of each individual plot</p> <code>200</code> <code>height</code> <code>int</code> <p>Height of each individual plot</p> <code>50</code> <code>bar_width</code> <code>int</code> <p>Width of the bars in the plot</p> <code>30</code> <code>domain</code> <code>Union[tuple, list]</code> <p>Y-axis domain range for the plots, default (0.0, 1.0)</p> <code>(0.0, 1.0)</code> <code>save_path</code> <code>str</code> <p>Path to save the plot. If None, plot will be shown interactively</p> <code>None</code> <code>separate</code> <code>bool</code> <p>Whether to return separate plots for each metric</p> <code>False</code> <p>Returns:</p> Type Description <code>Chart</code> <p>Altair chart object (combined or separate plots based on separate parameter)</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_bars(data: dict, show_score: bool = True, ncols: int = 3,\n              width: int = 200, height: int = 50, bar_width: int = 30,\n              domain: Union[tuple, list] = (0.0, 1.0),\n              save_path: str = None, separate: bool = False) -&gt; alt.Chart:\n    \"\"\"Plot bar charts for model metrics comparison.\n\n    This function creates bar charts to compare different metrics across multiple models.\n    It supports automatic layout with multiple columns and optional score labels on bars.\n\n    Args:\n        data: Dictionary containing metrics data with 'models' as the first key\n        show_score: Whether to show the score values on the bars\n        ncols: Number of columns to arrange the plots\n        width: Width of each individual plot\n        height: Height of each individual plot\n        bar_width: Width of the bars in the plot\n        domain: Y-axis domain range for the plots, default (0.0, 1.0)\n        save_path: Path to save the plot. If None, plot will be shown interactively\n        separate: Whether to return separate plots for each metric\n\n    Returns:\n        Altair chart object (combined or separate plots based on separate parameter)\n    \"\"\"\n    # Plot bar charts\n    dbar = pd.DataFrame(data)\n    pbar = {}\n    p_separate = {}\n    for n, metric in enumerate([x for x in data if x != 'models']):\n        if metric in ['mae', 'mse']:\n            domain_use = [0, dbar[metric].max()*1.1]\n        else:\n            domain_use = domain\n        bar = alt.Chart(dbar).mark_bar(size=bar_width).encode(\n            x=alt.X(metric + \":Q\").scale(domain=domain_use),\n            y=alt.Y(\"models\").title(None),\n            color=alt.Color('models').legend(None),\n        ).properties(width=width, height=height*len(dbar['models']))\n        if show_score:\n            text = alt.Chart(dbar).mark_text(\n                dx=-10,\n                color='white',\n                baseline='middle',\n                align='right').encode(\n                    x=alt.X(metric + \":Q\"),\n                    y=alt.Y(\"models\").title(None),\n                    text=alt.Text(metric, format='.3f')\n                    )\n            p = bar + text\n        else:\n            p = bar\n        if separate:\n            p_separate[metric] = p.configure_axis(grid=False)\n        idx = n // ncols\n        if n % ncols == 0:\n            pbar[idx] = p\n        else:\n            pbar[idx] |= p\n    # Combine the plots\n    for i, p in enumerate(pbar):\n        if i == 0:\n            pbars = pbar[p]\n        else:\n            pbars &amp;= pbar[p]\n    # Configure the chart\n    pbars = pbars.configure_axis(grid=False)\n    # Save the plot\n    if save_path:\n        pbars.save(save_path)\n        print(f\"Metrics bar charts saved to {save_path}\")\n    if separate:\n        return p_separate\n    else:\n        return pbars\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.plot_curve","title":"<code>plot_curve(data, show_score=True, width=400, height=400, save_path=None, separate=False)</code>","text":"<p>Plot ROC and PR curves for classification tasks.</p> <p>This function creates ROC (Receiver Operating Characteristic) and PR (Precision-Recall) curves to evaluate model performance on classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing ROC and PR curve data with 'ROC' and 'PR' keys</p> required <code>show_score</code> <code>bool</code> <p>Whether to show the score values on the plot (currently not implemented)</p> <code>True</code> <code>width</code> <code>int</code> <p>Width of each plot</p> <code>400</code> <code>height</code> <code>int</code> <p>Height of each plot</p> <code>400</code> <code>save_path</code> <code>str</code> <p>Path to save the plot. If None, plot will be shown interactively</p> <code>None</code> <code>separate</code> <code>bool</code> <p>Whether to return separate plots for ROC and PR curves</p> <code>False</code> <p>Returns:</p> Type Description <code>Chart</code> <p>Altair chart object (combined or separate plots based on separate parameter)</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_curve(data: dict, show_score: bool = True,\n               width: int = 400, height: int = 400,\n               save_path: str = None, separate: bool = False) -&gt; alt.Chart:\n    \"\"\"Plot ROC and PR curves for classification tasks.\n\n    This function creates ROC (Receiver Operating Characteristic) and PR (Precision-Recall)\n    curves to evaluate model performance on classification tasks.\n\n    Args:\n        data: Dictionary containing ROC and PR curve data with 'ROC' and 'PR' keys\n        show_score: Whether to show the score values on the plot (currently not implemented)\n        width: Width of each plot\n        height: Height of each plot\n        save_path: Path to save the plot. If None, plot will be shown interactively\n        separate: Whether to return separate plots for ROC and PR curves\n\n    Returns:\n        Altair chart object (combined or separate plots based on separate parameter)\n    \"\"\"\n    # Plot curves\n    pline = {}\n    p_separate = {}\n    # Plot ROC curve\n    roc_data = pd.DataFrame(data['ROC'])\n    pline[0] = alt.Chart(roc_data).mark_line().encode(\n        x=alt.X(\"fpr\").scale(domain=(0.0, 1.0)),\n        y=alt.Y(\"tpr\").scale(domain=(0.0, 1.0)),\n        color=\"models\",\n    ).properties(width=width, height=height)\n    if separate:\n        p_separate['ROC'] = pline[0]\n    # Plot PR curve\n    pr_data = pd.DataFrame(data['PR'])\n    pline[1] = alt.Chart(pr_data).mark_line().encode(\n        x=alt.X(\"recall\").scale(domain=(0.0, 1.0)),\n        y=alt.Y(\"precision\").scale(domain=(0.0, 1.0)),\n        color=\"models\",\n    ).properties(width=width, height=height)\n    if separate:\n        p_separate['PR'] = pline[1]\n    # Combine the plots\n    for i, p in enumerate(pline):\n        if i == 0:\n            plines = pline[i]\n        else:\n            plines |= pline[i]\n    # Configure the chart\n    plines = plines.configure_axis(grid=False)\n    # Save the plot\n    if save_path:\n        plines.save(save_path)\n        print(f\"ROC curves saved to {save_path}\")\n    if separate:\n        return p_separate\n    else:\n        return plines\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.plot_embeddings","title":"<code>plot_embeddings(hidden_states, attention_mask, reducer='t-SNE', labels=None, label_names=None, ncols=4, width=300, height=300, save_path=None, separate=False)</code>","text":"<p>Visualize embeddings using dimensionality reduction techniques.</p> <p>This function creates 2D visualizations of high-dimensional embeddings from different model layers using PCA, t-SNE, or UMAP dimensionality reduction methods.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_states</code> <code>Union[tuple, list]</code> <p>Tuple or list containing hidden states from model layers</p> required <code>attention_mask</code> <code>Union[tuple, list]</code> <p>Tuple or list containing attention masks for sequence padding</p> required <code>reducer</code> <code>str</code> <p>Dimensionality reduction method. Options: 'PCA', 't-SNE', 'UMAP'</p> <code>'t-SNE'</code> <code>labels</code> <code>Union[tuple, list]</code> <p>List of labels for the data points</p> <code>None</code> <code>label_names</code> <code>Union[str, list]</code> <p>List of label names for legend display</p> <code>None</code> <code>ncols</code> <code>int</code> <p>Number of columns to arrange the plots</p> <code>4</code> <code>width</code> <code>int</code> <p>Width of each plot</p> <code>300</code> <code>height</code> <code>int</code> <p>Height of each plot</p> <code>300</code> <code>save_path</code> <code>str</code> <p>Path to save the plot. If None, plot will be shown interactively</p> <code>None</code> <code>separate</code> <code>bool</code> <p>Whether to return separate plots for each layer</p> <code>False</code> <p>Returns:</p> Type Description <code>Chart</code> <p>Altair chart object (combined or separate plots based on separate parameter)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unsupported dimensionality reduction method is specified</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_embeddings(hidden_states: Union[tuple, list], attention_mask: Union[tuple, list], reducer: str = \"t-SNE\",\n                    labels: Union[tuple, list] = None, label_names: Union[str, list] = None,\n                    ncols: int = 4, width: int = 300, height: int = 300,\n                    save_path: str = None, separate: bool = False) -&gt; alt.Chart:\n    \"\"\"Visualize embeddings using dimensionality reduction techniques.\n\n    This function creates 2D visualizations of high-dimensional embeddings from different\n    model layers using PCA, t-SNE, or UMAP dimensionality reduction methods.\n\n    Args:\n        hidden_states: Tuple or list containing hidden states from model layers\n        attention_mask: Tuple or list containing attention masks for sequence padding\n        reducer: Dimensionality reduction method. Options: 'PCA', 't-SNE', 'UMAP'\n        labels: List of labels for the data points\n        label_names: List of label names for legend display\n        ncols: Number of columns to arrange the plots\n        width: Width of each plot\n        height: Height of each plot\n        save_path: Path to save the plot. If None, plot will be shown interactively\n        separate: Whether to return separate plots for each layer\n\n    Returns:\n        Altair chart object (combined or separate plots based on separate parameter)\n\n    Raises:\n        ValueError: If unsupported dimensionality reduction method is specified\n    \"\"\"\n    import torch\n    if reducer.lower() == \"pca\":\n        from sklearn.decomposition import PCA\n        dim_reducer = PCA(n_components=2)\n    elif reducer.lower() == \"t-sne\":\n        from sklearn.manifold import TSNE\n        dim_reducer = TSNE(n_components=2)\n    elif reducer.lower() == \"umap\":\n        from umap import UMAP\n        dim_reducer = UMAP(n_components=2)\n    else:\n        raise(\"Unsupported dim reducer, please try PCA, t-SNE or UMAP.\")\n\n    pdot = {}\n    p_separate = {}\n    for i, hidden in enumerate(hidden_states):\n        embeddings = hidden.numpy()\n        mean_sequence_embeddings = torch.sum(attention_mask*embeddings, axis=-2) / torch.sum(attention_mask, axis=1)\n        layer_dim_reduced_vectors = dim_reducer.fit_transform(mean_sequence_embeddings.numpy())\n        if len(labels) == 0:\n            labels = [\"Uncategorized\"] * layer_dim_reduced_vectors.shape[0]\n        df = {\n            'Dimension 1': layer_dim_reduced_vectors[:,0],\n            'Dimension 2': layer_dim_reduced_vectors[:,1],\n            'labels': [label_names[int(i)] for i in labels]\n        }\n        source = pd.DataFrame(df)\n        dot = alt.Chart(source, title=f\"Layer {i+1}\").mark_point(filled=True).encode(\n            x=alt.X(\"Dimension 1:Q\"),\n            y=alt.Y(\"Dimension 2:Q\"),\n            color=alt.Color(\"labels:N\", legend=alt.Legend(title=\"Labels\")),\n        ).properties(width=width, height=height)\n        if separate:\n            p_separate[f\"Layer{i+1}\"] = dot.configure_axis(grid=False)\n        idx = i // ncols\n        if i % ncols == 0:\n            pdot[idx] = dot\n        else:\n            pdot[idx] |= dot\n    # Combine the plots\n    for i, p in enumerate(pdot):\n        if i == 0:\n            pdots = pdot[p]\n        else:\n            pdots &amp;= pdot[p]\n    # Configure the chart\n    pdots = pdots.configure_axis(grid=False)\n    # Save the plot\n    if save_path:\n        pdots.save(save_path)\n        print(f\"Embeddings visualization saved to {save_path}\")\n    if separate:\n        return p_separate\n    else:\n        return pdots\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.plot_muts","title":"<code>plot_muts(data, show_score=False, width=None, height=100, save_path=None)</code>","text":"<p>Visualize mutation effects on model predictions.</p> <p>This function creates comprehensive visualizations of how different mutations affect model predictions, including: - Heatmap showing mutation effects at each position - Line plot showing gain/loss of function - Bar chart showing maximum effect mutations</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing mutation data with 'raw' and mutation keys</p> required <code>show_score</code> <code>bool</code> <p>Whether to show the score values on the plot (currently not implemented)</p> <code>False</code> <code>width</code> <code>int</code> <p>Width of the plot. If None, automatically calculated based on sequence length</p> <code>None</code> <code>height</code> <code>int</code> <p>Height of the plot</p> <code>100</code> <code>save_path</code> <code>str</code> <p>Path to save the plot. If None, plot will be shown interactively</p> <code>None</code> <p>Returns:</p> Type Description <code>Chart</code> <p>Altair chart object showing the combined mutation effects visualization</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_muts(data: dict, show_score: bool = False,\n              width: int = None, height: int = 100,\n              save_path: str = None) -&gt; alt.Chart:\n    \"\"\"Visualize mutation effects on model predictions.\n\n    This function creates comprehensive visualizations of how different mutations\n    affect model predictions, including:\n    - Heatmap showing mutation effects at each position\n    - Line plot showing gain/loss of function\n    - Bar chart showing maximum effect mutations\n\n    Args:\n        data: Dictionary containing mutation data with 'raw' and mutation keys\n        show_score: Whether to show the score values on the plot (currently not implemented)\n        width: Width of the plot. If None, automatically calculated based on sequence length\n        height: Height of the plot\n        save_path: Path to save the plot. If None, plot will be shown interactively\n\n    Returns:\n        Altair chart object showing the combined mutation effects visualization\n    \"\"\"\n    # Create dataframe\n    seqlen = len(data['raw']['sequence'])\n    flen = len(str(seqlen))\n    mut_list = [x for x in data.keys() if x != 'raw']\n    raw_bases = [base for base in data['raw']['sequence']]\n    dheat = {\"base\": [], 'mut': [], 'score': []}\n    dline = {\"x\": [str(i).zfill(flen)+x for i,x in enumerate(raw_bases)] * 2,\n             \"score\": [0.0]*seqlen*2,\n             \"type\": [\"gain\"]*seqlen + [\"loss\"]*seqlen}\n    dbar = {\"x\": [], \"score\": [], \"base\": []}\n    # Iterate through mutations\n    for i, base1 in enumerate(raw_bases):\n        ref = \"mut_\" + str(i) + \"_\" + base1 + \"_\" + base1\n        # replacement\n        mut_prefix = \"mut_\" + str(i) + \"_\" + base1 + \"_\"\n        maxabs = 0.0\n        maxscore = 0.0\n        maxabs_index = base1\n        for mut in sorted([x for x in mut_list if x.startswith(mut_prefix)] + [ref]):\n            if mut in data:\n                # for heatmap\n                base2 = mut.split(\"_\")[-1]\n                score = data[mut]['score']\n                dheat[\"base\"].append(str(i).zfill(flen)+base1)\n                dheat[\"mut\"].append(base2)\n                dheat[\"score\"].append(score)\n                # for line\n                if score &gt;= 0:\n                    dline[\"score\"][i] += score\n                elif score &lt; 0:\n                    dline[\"score\"][i+seqlen] -= score\n                # for bar chart\n                if abs(score) &gt; maxabs:\n                    maxabs = abs(score)\n                    maxscore = score\n                    maxabs_index = base2\n            else:\n                dheat[\"base\"].append(str(i).zfill(flen)+base1)\n                dheat[\"mut\"].append(base1)\n                dheat[\"score\"].append(0.0)\n        # for bar chart\n        dbar[\"x\"].append(str(i).zfill(flen)+base1)\n        dbar[\"score\"].append(maxscore)\n        dbar[\"base\"].append(maxabs_index)\n        # deletion\n        del_prefix = \"del_\" + str(i) + \"_\"\n        for mut in [x for x in mut_list if x.startswith(del_prefix)]:\n            base2 = \"del_\" + mut.split(\"_\")[-1]\n            score = data[mut]['score']\n            dheat[\"base\"].append(str(i).zfill(flen)+base1)\n            dheat[\"mut\"].append(base2)\n            dheat[\"score\"].append(score)\n        # insertion\n        ins_prefix = \"ins_\" + str(i) + \"_\"\n        for mut in [x for x in mut_list if x.startswith(ins_prefix)]:\n            base2 = \"ins_\" + mut.split(\"_\")[-1]\n            score = data[mut]['score']\n            dheat[\"base\"].append(str(i).zfill(flen)+base1)\n            dheat[\"mut\"].append(base2)\n            dheat[\"score\"].append(score)\n    # Set color domain and range\n    domain1_min = min([data[mut]['score'] for mut in data])\n    domain1_max = max([data[mut]['score'] for mut in data])\n    domain1 = [-max([abs(domain1_min), abs(domain1_max)]),\n               0.0,\n               max([abs(domain1_min), abs(domain1_max)])]\n    range1_ = ['#2166ac', '#f7f7f7', '#b2182b']\n    domain2 = sorted([x for x in set(dbar['base'])])\n    range2_ = [\"#33a02c\", \"#e31a1c\", \"#1f78b4\", \"#ff7f00\", \"#cab2d6\"][:len(domain2)]\n    # Enable VegaFusion for Altair\n    alt.data_transformers.enable(\"vegafusion\")\n    # Plot the heatmap\n    if width is None:\n        width = int(height * len(raw_bases) / len(set(dheat['mut'])))\n    if dheat['base']:\n        pheat = alt.Chart(pd.DataFrame(dheat)).mark_rect().encode(\n            x=alt.X('base:O', axis=alt.Axis(\n                    labelExpr = f\"substring(datum.value, {flen}, {flen}+1)\",\n                    labelAngle=0,\n                    )\n                ).title(None),\n            y=alt.Y('mut:O').title(\"mutation\"),\n            color=alt.Color('score:Q').scale(domain=domain1, range=range1_),\n        ).properties(\n            width=width, height=height\n        )\n        # Plot gain and loss\n        pline = alt.Chart(pd.DataFrame(dline)).mark_line().encode(\n            x=alt.X('x:O').title(None).axis(labels=False),\n            y=alt.Y('score:Q'),\n            color=alt.Color('type:N').scale(\n                domain=['gain', 'loss'], range=['#b2182b', '#2166ac']\n            ),\n        ).properties(\n            width=width, height=height\n        )\n        pbar = alt.Chart(pd.DataFrame(dbar)).mark_bar().encode(\n            x=alt.X('x:O').title(None).axis(labels=False),\n            y=alt.Y('score:Q'),\n            color=alt.Color('base:N').scale(\n                domain=domain2, range=range2_\n            ),\n        ).properties(\n            width=width, height=height\n        )\n        pmerge = pheat &amp; pbar &amp; pline\n        pmerge = pmerge.configure_axis(grid=False)\n        # Save the plot\n        if save_path:\n            pmerge.save(save_path)\n            print(f\"Mutation effects visualization saved to {save_path}\")\n    return pheat\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.plot_scatter","title":"<code>plot_scatter(data, show_score=True, ncols=3, width=400, height=400, save_path=None, separate=False)</code>","text":"<p>Plot scatter plots for regression task evaluation.</p> <p>This function creates scatter plots to compare predicted vs. experimental values for regression tasks, with optional R\u00b2 score display.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing scatter plot data for each model</p> required <code>show_score</code> <code>bool</code> <p>Whether to show the R\u00b2 score on the plot</p> <code>True</code> <code>ncols</code> <code>int</code> <p>Number of columns to arrange the plots</p> <code>3</code> <code>width</code> <code>int</code> <p>Width of each plot</p> <code>400</code> <code>height</code> <code>int</code> <p>Height of each plot</p> <code>400</code> <code>save_path</code> <code>str</code> <p>Path to save the plot. If None, plot will be shown interactively</p> <code>None</code> <code>separate</code> <code>bool</code> <p>Whether to return separate plots for each model</p> <code>False</code> <p>Returns:</p> Type Description <code>Chart</code> <p>Altair chart object (combined or separate plots based on separate parameter)</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_scatter(data: dict, show_score: bool = True, ncols: int = 3,\n                 width: int = 400, height: int = 400,\n                 save_path: str = None, separate: bool = False) -&gt; alt.Chart:\n    \"\"\"Plot scatter plots for regression task evaluation.\n\n    This function creates scatter plots to compare predicted vs. experimental values\n    for regression tasks, with optional R\u00b2 score display.\n\n    Args:\n        data: Dictionary containing scatter plot data for each model\n        show_score: Whether to show the R\u00b2 score on the plot\n        ncols: Number of columns to arrange the plots\n        width: Width of each plot\n        height: Height of each plot\n        save_path: Path to save the plot. If None, plot will be shown interactively\n        separate: Whether to return separate plots for each model\n\n    Returns:\n        Altair chart object (combined or separate plots based on separate parameter)\n    \"\"\"\n    # Plot bar charts\n    pdot = {}\n    p_separate = {}\n    for n, model in enumerate(data):\n        scatter_data = dict(data[model])\n        r2 = scatter_data['r2']\n        del scatter_data['r2']\n        ddot = pd.DataFrame(scatter_data)\n        dot = alt.Chart(ddot, title=model).mark_point(filled=True).encode(\n            x=alt.X(\"predicted:Q\"),\n            y=alt.Y(\"experiment:Q\"),\n        ).properties(width=width, height=height)\n        if show_score:\n            min_x = ddot['predicted'].min()\n            max_y = ddot['experiment'].max()\n            text = alt.Chart().mark_text(size=14, align=\"left\", baseline=\"bottom\").encode(\n                x=alt.datum(min_x + 0.5),\n                y=alt.datum(max_y - 0.5),\n                text=alt.datum(\"R\\u00b2=\" + str(r2))\n            )\n            p = dot + text\n        else:\n            p = dot\n        if separate:\n            p_separate[model] = p.configure_axis(grid=False)\n        idx = n // ncols\n        if n % ncols == 0:\n            pdot[idx] = p\n        else:\n            pdot[idx] |= p\n    # Combine the plots\n    for i, p in enumerate(pdot):\n        if i == 0:\n            pdots = pdot[p]\n        else:\n            pdots &amp;= pdot[p]\n    # Configure the chart\n    pdots = pdots.configure_axis(grid=False)\n    # Save the plot\n    if save_path:\n        pdots.save(save_path)\n        print(f\"Metrics scatter plots saved to {save_path}\")\n    if separate:\n        return p_separate\n    else:\n        return pdots\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.prepare_data","title":"<code>prepare_data(metrics, task_type='binary')</code>","text":"<p>Prepare data for plotting various types of visualizations.</p> <p>This function organizes model metrics data into formats suitable for different plot types: - Bar charts for classification and regression metrics - ROC and PR curves for classification tasks - Scatter plots for regression tasks</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict</code> <p>Dictionary containing model metrics for different models</p> required <code>task_type</code> <code>str</code> <p>Type of task ('binary', 'multiclass', 'multilabel', 'token', 'regression')</p> <code>'binary'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing:</p> <code>tuple</code> <ul> <li>bars_data: Data formatted for bar chart visualization</li> </ul> <code>tuple</code> <ul> <li>curves_data/scatter_data: Data formatted for curve or scatter plot visualization</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If task type is not supported for plotting</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def prepare_data(metrics: dict, task_type: str = \"binary\") -&gt; tuple:\n    \"\"\"Prepare data for plotting various types of visualizations.\n\n    This function organizes model metrics data into formats suitable for different plot types:\n    - Bar charts for classification and regression metrics\n    - ROC and PR curves for classification tasks\n    - Scatter plots for regression tasks\n\n    Args:\n        metrics: Dictionary containing model metrics for different models\n        task_type: Type of task ('binary', 'multiclass', 'multilabel', 'token', 'regression')\n\n    Returns:\n        Tuple containing:\n        - bars_data: Data formatted for bar chart visualization\n        - curves_data/scatter_data: Data formatted for curve or scatter plot visualization\n\n    Raises:\n        ValueError: If task type is not supported for plotting\n    \"\"\"\n    # Load the data\n    bars_data = {'models': []}\n    if task_type in ['binary', 'multiclass', 'multilabel', 'token']:\n        curves_data = {'ROC': {'models': [], 'fpr': [], 'tpr': []},\n                    'PR': {'models': [], 'recall': [], 'precision': []}}\n        for model in metrics:\n            if model not in bars_data['models']:\n                bars_data['models'].append(model)\n            for metric in metrics[model]:\n                if metric == 'curve':\n                    for score in metrics[model][metric]:\n                        if score.endswith('pr'):\n                            if score == 'fpr':\n                                curves_data['ROC']['models'].extend([model] * len(metrics[model][metric][score]))\n                            curves_data['ROC'][score].extend(metrics[model][metric][score])\n                        else:\n                            if score == 'precision':\n                                curves_data['PR']['models'].extend([model] * len(metrics[model][metric][score]))\n                            curves_data['PR'][score].extend(metrics[model][metric][score])\n                else:\n                    if metric not in bars_data:\n                        bars_data[metric] = []\n                    bars_data[metric].append(metrics[model][metric])\n        return bars_data, curves_data\n    elif task_type == \"regression\":\n        scatter_data = {}\n        for model in metrics:\n            if model not in bars_data['models']:\n                bars_data['models'].append(model)\n            scatter_data[model] = {'predicted': [], 'experiment': []}\n            for metric in metrics[model]:\n                if metric == 'scatter':\n                    for score in metrics[model][metric]:\n                        scatter_data[model][score].extend(metrics[model][metric][score])\n                else:\n                    if metric not in bars_data:\n                        bars_data[metric] = []\n                    bars_data[metric].append(metrics[model][metric])\n                    if metric == 'r2':\n                        scatter_data[model][metric] = metrics[model][metric]\n        return bars_data, scatter_data\n    else:\n        raise ValueError(f\"Unsupport task type {task_type} for ploting\")\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.save_metrics","title":"<code>save_metrics(metrics, output_dir)</code>","text":"<p>Save evaluation metrics to JSON file.</p> <p>This function saves computed evaluation metrics in JSON format to the specified output directory.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Dict</code> <p>Dictionary containing metrics to save</p> required <code>output_dir</code> <code>Path</code> <p>Directory path where metrics will be saved</p> required Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def save_metrics(metrics: Dict, output_dir: Path) -&gt; None:\n    \"\"\"Save evaluation metrics to JSON file.\n\n    This function saves computed evaluation metrics in JSON format to the specified output directory.\n\n    Args:\n        metrics: Dictionary containing metrics to save\n        output_dir: Directory path where metrics will be saved\n    \"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save metrics\n    with open(output_dir / \"metrics.json\", \"w\") as f:\n        json.dump(metrics, f, indent=4)\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.save_predictions","title":"<code>save_predictions(predictions, output_dir)</code>","text":"<p>Save predictions to JSON file.</p> <p>This function saves model predictions in JSON format to the specified output directory.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Dict</code> <p>Dictionary containing predictions to save</p> required <code>output_dir</code> <code>Path</code> <p>Directory path where predictions will be saved</p> required Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def save_predictions(predictions: Dict, output_dir: Path) -&gt; None:\n    \"\"\"Save predictions to JSON file.\n\n    This function saves model predictions in JSON format to the specified output directory.\n\n    Args:\n        predictions: Dictionary containing predictions to save\n        output_dir: Directory path where predictions will be saved\n    \"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save predictions\n    with open(output_dir / \"predictions.json\", \"w\") as f:\n        json.dump(predictions, f, indent=4)\n</code></pre>"},{"location":"api/utils/sequence/","title":"utils/sequence API","text":"<p>Sequence utility functions for DNA sequence analysis and generation.</p> <p>This module provides functions for: - Calculating GC content - Generating reverse complements - Converting sequences to k-mers - Validating DNA sequences - Randomly generating DNA sequences with constraints</p> <p>All functions are designed for use in DNA language modeling and bioinformatics pipelines.</p>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.calc_gc_content","title":"<code>calc_gc_content(seq)</code>","text":"<p>Calculate the GC content of a DNA sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>str</code> <p>DNA sequence (A/C/G/T/U/N, case-insensitive).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>GC content (0.0 ~ 1.0). Returns 0.0 if sequence is empty.</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def calc_gc_content(seq: str) -&gt; float:\n    \"\"\"\n    Calculate the GC content of a DNA sequence.\n\n    Args:\n        seq (str): DNA sequence (A/C/G/T/U/N, case-insensitive).\n\n    Returns:\n        float: GC content (0.0 ~ 1.0). Returns 0.0 if sequence is empty.\n    \"\"\"\n    seq = seq.upper().replace(\"U\", \"T\").replace(\"N\", \"\")\n    try:\n        gc = (seq.count(\"G\") + seq.count(\"C\")) / len(seq)\n    except ZeroDivisionError as e:\n        print(e)\n        gc = 0.0\n    return gc\n</code></pre>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.check_sequence","title":"<code>check_sequence(seq, minl=1, maxl=500000000, gc=(0, 1), valid_chars='ACGTN')</code>","text":"<p>Check if a DNA sequence is valid based on length, GC content, and allowed characters.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>str</code> <p>DNA sequence.</p> required <code>minl</code> <code>int</code> <p>Minimum length. Defaults to 1.</p> <code>1</code> <code>maxl</code> <code>int</code> <p>Maximum length. Defaults to 500000000.</p> <code>500000000</code> <code>gc</code> <code>tuple</code> <p>GC content range (min, max). Defaults to (0, 1).</p> <code>(0, 1)</code> <code>valid_chars</code> <code>str</code> <p>Allowed characters. Defaults to \"ACGTN\".</p> <code>'ACGTN'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if valid, False otherwise.</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def check_sequence(\n    seq: str,\n    minl: int = 1,\n    maxl: int = 500000000,\n    gc: tuple = (0, 1),\n    valid_chars: str = \"ACGTN\"\n) -&gt; bool:\n    \"\"\"\n    Check if a DNA sequence is valid based on length, GC content, and allowed characters.\n\n    Args:\n        seq (str): DNA sequence.\n        minl (int, optional): Minimum length. Defaults to 1.\n        maxl (int, optional): Maximum length. Defaults to 500000000.\n        gc (tuple, optional): GC content range (min, max). Defaults to (0, 1).\n        valid_chars (str, optional): Allowed characters. Defaults to \"ACGTN\".\n\n    Returns:\n        bool: True if valid, False otherwise.\n    \"\"\"\n    if len(seq) &lt; minl or len(seq) &gt; maxl:\n        return False  # \u5e8f\u5217\u957f\u5ea6\u4e0d\u5728\u6709\u6548\u8303\u56f4\u5185\n    elif gc[0] &gt; calc_gc_content(seq) or gc[1] &lt; calc_gc_content(seq):\n        return False  # GC\u542b\u91cf\u4e0d\u5728\u6709\u6548\u8303\u56f4\u5185\n    elif set(seq.upper()) - set(valid_chars) != set():\n        return False  # \u5e8f\u5217\u5305\u542b\u4e0d\u652f\u6301\u7684\u5b57\u7b26\n    else:\n        return True  # \u5e8f\u5217\u6709\u6548\n</code></pre>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.random_generate_sequences","title":"<code>random_generate_sequences(minl, maxl=0, samples=1, gc=(0, 1), N_ratio=0.0, padding_size=0, seed=None)</code>","text":"<p>Randomly generate DNA sequences with specified length, GC content, and N ratio.</p> <p>Parameters:</p> Name Type Description Default <code>minl</code> <code>int</code> <p>Minimum sequence length.</p> required <code>maxl</code> <code>int</code> <p>Maximum sequence length. If 0, use minl as fixed length. Defaults to 0.</p> <code>0</code> <code>samples</code> <code>int</code> <p>Number of sequences to generate. Defaults to 1.</p> <code>1</code> <code>gc</code> <code>tuple</code> <p>GC content range (min, max). Defaults to (0, 1).</p> <code>(0, 1)</code> <code>N_ratio</code> <code>float</code> <p>Proportion of 'N' bases (0.0 ~ 1.0). Defaults to 0.0.</p> <code>0.0</code> <code>padding_size</code> <code>int</code> <p>Pad length to nearest multiple. Defaults to 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of generated DNA sequences.</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def random_generate_sequences(\n    minl: int,\n    maxl: int = 0,\n    samples: int = 1,\n    gc: tuple = (0, 1),\n    N_ratio: float = 0.0,\n    padding_size: int = 0,\n    seed: int = None\n) -&gt; list[str]:\n    \"\"\"\n    Randomly generate DNA sequences with specified length, GC content, and N ratio.\n\n    Args:\n        minl (int): Minimum sequence length.\n        maxl (int, optional): Maximum sequence length. If 0, use minl as fixed length. Defaults to 0.\n        samples (int, optional): Number of sequences to generate. Defaults to 1.\n        gc (tuple, optional): GC content range (min, max). Defaults to (0, 1).\n        N_ratio (float, optional): Proportion of 'N' bases (0.0 ~ 1.0). Defaults to 0.0.\n        padding_size (int, optional): Pad length to nearest multiple. Defaults to 0.\n        seed (int, optional): Random seed. Defaults to None.\n\n    Returns:\n        list[str]: List of generated DNA sequences.\n    \"\"\"\n    sequences = []\n    basemap = [\"A\", \"C\", \"G\", \"T\"]\n    if 0.0 &lt; N_ratio &lt;= 1.0:\n        basemap.append(\"N\")\n        weights = [(1 - N_ratio) / 4] * 4 + [N_ratio]\n    elif N_ratio &gt; 1.0:\n        basemap.append(\"N\")\n        weights = [(100 - N_ratio) / 4] * 4 + [N_ratio]\n    else:\n        weights = None\n    calc_gc = False if gc == (0, 1) else True # Guanqing Please check this line!\n    if seed:\n        random.seed(seed)\n    # progress bar\n    progress_bar = tqdm(total=samples, desc=\"Generating sequences\")\n    # generate sequences\n    if maxl:\n        # generate sequences with random length\n        while True:\n            if len(sequences) &gt;= samples:\n                break\n            length = random.randint(minl, maxl)\n            if padding_size:\n                length = (length // padding_size + 1) * padding_size if length % padding_size else length\n                if length &gt; maxl:\n                    length -= padding_size\n            seq = \"\".join(random.choices(basemap, weights=weights, k=length))\n            if calc_gc:\n                gc_content = calc_gc_content(seq)\n                if gc[0] &lt;= gc_content &lt;= gc[1]:\n                    sequences.append(seq)\n                    progress_bar.update(1)\n            else:\n                sequences.append(seq)\n                progress_bar.update(1)\n    # generate sequences with fixed length\n    else:\n        maxl = minl\n        length = minl\n        while True:\n            if len(sequences) &gt;= samples:\n                break\n            seq = \"\".join(random.choices(basemap, weights=weights, k=length))\n            # calculate GC content\n            if calc_gc:\n                gc_content = calc_gc_content(seq)\n                if gc[0] &lt;= gc_content &lt;= gc[1]:\n                    sequences.append(seq)\n                    progress_bar.update(1)\n            else:\n                sequences.append(seq)\n                progress_bar.update(1)\n    return sequences\n</code></pre>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.reverse_complement","title":"<code>reverse_complement(seq, reverse=True, complement=True)</code>","text":"<p>Compute the reverse complement of a DNA sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>str</code> <p>DNA sequence.</p> required <code>reverse</code> <code>bool</code> <p>Whether to reverse the sequence. Defaults to True.</p> <code>True</code> <code>complement</code> <code>bool</code> <p>Whether to complement the sequence. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The reverse complement (or as specified) of the input sequence.</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def reverse_complement(seq: str, reverse: bool = True, complement: bool = True) -&gt; str:\n    \"\"\"\n    Compute the reverse complement of a DNA sequence.\n\n    Args:\n        seq (str): DNA sequence.\n        reverse (bool, optional): Whether to reverse the sequence. Defaults to True.\n        complement (bool, optional): Whether to complement the sequence. Defaults to True.\n\n    Returns:\n        str: The reverse complement (or as specified) of the input sequence.\n    \"\"\"\n    mapping = {\n        \"A\": \"T\", \"T\": \"A\", \"C\": \"G\", \"G\": \"C\",\n        \"a\": \"t\", \"t\": \"a\", \"c\": \"g\", \"g\": \"c\",\n        \"N\": \"N\", \"n\": \"n\"\n    }\n    if reverse:\n        seq = seq[::-1]\n    if complement:\n        seq = \"\".join(mapping.get(base, base) for base in seq)\n    return seq\n</code></pre>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.seq2kmer","title":"<code>seq2kmer(seqs, k)</code>","text":"<p>Convert a list of DNA sequences to k-mers (overlapping k-mer tokenization).</p> <p>Parameters:</p> Name Type Description Default <code>seqs</code> <code>list[str]</code> <p>List of DNA sequences.</p> required <code>k</code> <code>int</code> <p>k-mer length.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of k-mer tokenized sequences (space-separated k-mers).</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def seq2kmer(seqs: list[str], k: int) -&gt; list[str]:\n    \"\"\"\n    Convert a list of DNA sequences to k-mers (overlapping k-mer tokenization).\n\n    Args:\n        seqs (list[str]): List of DNA sequences.\n        k (int): k-mer length.\n\n    Returns:\n        list[str]: List of k-mer tokenized sequences (space-separated k-mers).\n    \"\"\"\n    all_kmers = []\n    for seq in seqs:\n        kmer = [seq[x:x+k].upper() for x in range(len(seq)+1-k)]\n        kmers = \" \".join(kmer)\n        all_kmers.append(kmers)\n    return all_kmers\n</code></pre>"},{"location":"concepts/inference/","title":"Inference Concepts","text":"<p>Inference is the process of using a trained DNA language model to generate predictions, analyze sequences, or perform downstream tasks on new DNA data. This document covers the fundamental concepts and methods involved in inference with DNA language models.</p>"},{"location":"concepts/inference/#what-is-inference","title":"What is Inference?","text":"<p>Inference refers to the process of applying a trained model to new, unseen data to make predictions or generate outputs. In the context of DNA language models, inference involves:</p> <ul> <li>Sequence Analysis: Analyzing DNA sequences to understand their properties</li> <li>Prediction Generation: Generating predictions about sequence characteristics</li> <li>Feature Extraction: Extracting meaningful representations from DNA sequences</li> <li>Downstream Tasks: Performing specific biological tasks using the model's learned representations</li> </ul>"},{"location":"concepts/inference/#key-components-of-inference","title":"Key Components of Inference","text":""},{"location":"concepts/inference/#1-model-loading-and-initialization","title":"1. Model Loading and Initialization","text":"<p>Before inference can begin, the trained model must be loaded into memory. This involves:</p> <ul> <li>Model Restoration: Loading the trained model weights and architecture from storage</li> <li>Memory Allocation: Allocating sufficient memory for the model and input data</li> <li>Device Placement: Placing the model on appropriate computational devices (CPU, GPU, or distributed systems)</li> <li>State Configuration: Setting the model to evaluation mode to disable training-specific behaviors</li> </ul>"},{"location":"concepts/inference/#2-input-preprocessing","title":"2. Input Preprocessing","text":"<p>DNA sequences must be properly formatted and tokenized before feeding into the model:</p> <ul> <li>Sequence Cleaning: Removing invalid characters and normalizing sequences</li> <li>Tokenization: Converting DNA sequences into model-compatible tokens</li> <li>Padding/Truncation: Ensuring consistent input lengths for batch processing</li> <li>Batch Preparation: Organizing multiple sequences for efficient processing</li> </ul>"},{"location":"concepts/inference/#3-forward-pass","title":"3. Forward Pass","text":"<p>The core inference step where the model processes the input:</p> <ul> <li>Input Processing: Feeding preprocessed sequences through the model</li> <li>Computation: Performing matrix operations and neural network computations</li> <li>Output Generation: Producing raw model outputs (logits, probabilities, or embeddings)</li> <li>Memory Management: Efficiently managing computational resources during processing</li> </ul>"},{"location":"concepts/inference/#4-output-processing","title":"4. Output Processing","text":"<p>Transform raw model outputs into meaningful results:</p> <ul> <li>Logits Processing: Converting raw scores to probabilities using activation functions</li> <li>Post-processing: Applying task-specific transformations and filtering</li> <li>Result Formatting: Structuring outputs for downstream use and interpretation</li> <li>Confidence Scoring: Assessing the reliability of model predictions</li> </ul>"},{"location":"concepts/training/","title":"Training Concepts in Machine Learning","text":"<p>Training is the fundamental process in machine learning where a model learns patterns from data to make predictions or perform tasks. This document explains the core concepts of training and provides practical examples using DNALLM for DNA language models.</p>"},{"location":"concepts/training/#what-is-training-in-machine-learning","title":"What is Training in Machine Learning?","text":"<p>Training is the process of optimizing a model's parameters (weights and biases) to minimize a loss function, enabling the model to learn meaningful patterns from data. In the context of DNA language models, training involves:</p> <ul> <li>Parameter Optimization: Adjusting model weights to minimize prediction errors</li> <li>Pattern Learning: Discovering biological patterns and relationships in DNA sequences</li> <li>Representation Learning: Learning meaningful embeddings for DNA tokens and sequences</li> <li>Task Adaptation: Fine-tuning for specific biological tasks and applications</li> </ul>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>We recommend using Astral uv package management tool to install DNALLM's runtime environment and dependencies.</p> <p>What is uv? uv is a fast Python package management tool developed in Rust, which is 10-100 times faster than traditional tools like pip.</p>"},{"location":"getting_started/installation/#1-install-uv","title":"1. Install uv","text":"<pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"getting_started/installation/#2-install-dnallm","title":"2. Install DNALLM","text":"<pre><code># Clone the repository\ngit clone https://github.com/zhangtaolab/DNALLM.git\ncd DNALLM\n\n# Create virtual environment and install dependencies\nuv venv\n# Activate virtual environment\n## Linux &amp; MacOS users\nsource .venv/bin/activate\n## Windows users\n.venv\\Scripts\\activate\n\n# Install dependencies\nuv pip install -e '.[base]'\n\n# Install DNALLM\nuv pip install -e .\n</code></pre> <p>During the dependency installation process: - For users without GPU or using MacOS with Apple M-series chips, you can directly use the <code>base</code> dependencies - For Linux users with Nvidia graphics cards, you can usually use the <code>base</code> dependencies directly, as the package manager will automatically identify and install the corresponding CUDA version of torch - For Windows users, it's recommended to specify the CUDA version during dependency installation to prevent the package manager from failing to correctly identify your graphics card</p> <p>Considering that different users have graphics cards supporting different CUDA versions, if you need to specify a CUDA version for installation, you can use the following command: <pre><code># First install the specified version of torch (supports cpu, cuda121, cuda124, cuda126)\nuv pip install -e '.[cuda124]'\n# After installation is complete, install other dependencies\nuv pip install -e '.[base]'\n</code></pre></p> <p>For users with AMD graphics cards, please ensure you're using a Linux system and manually install the corresponding torch version: <pre><code>uv pip install 'torch&gt;=2.5' --index-url https://download.pytorch.org/whl/rocm6.2\n</code></pre></p> <p>For users with Intel graphics cards, if you want to use GPU acceleration, please refer to the official documentation to install the corresponding drivers and torch version.</p> <p>Some models require support from other dependencies. We will continue to add dependencies needed for different models.</p> <ol> <li> <p>Native <code>mamba</code> architecture runs significantly faster than transformer-compatible mamba architecture, but native mamba depends on Nvidia graphics cards. If you need native mamba architecture support, after installing DNALLM dependencies, use the following command to install:</p> <pre><code>uv pip install -e '.[mamba]' --no-cache-dir --no-build-isolation\n</code></pre> </li> <li> <p><code>EVO2</code> model fine-tuning and inference depends on its own software package or third-party Python library1/library2:</p> <pre><code># evo2 requires python version &gt;=3.11\ngit clone --recurse-submodules git@github.com:ArcInstitute/evo2.git\ncd evo2\n# install required dependency 'vortex'\ncd vortex\nuv pip install .\n# install evo2\ncd ..\nuv pip install .\n\n# add cudnn path to environment\nexport LD_LIBRARY_PATH=[path_to_DNALLM]/.venv/lib64/python3.11/site-packages/nvidia/cudnn/lib:${LD_LIBRARY_PATH}\n</code></pre> </li> <li> <p>Some models use their own developed model architectures that haven't been integrated into HuggingFace's transformers library yet. Therefore, fine-tuning and inference for these models require pre-installing the corresponding model dependency libraries:</p> <ul> <li>(1) GPN Project address: https://github.com/songlab-cal/gpn <pre><code>git clone https://github.com/songlab-cal/gpn\ncd gpn\nuv pip install .\n</code></pre></li> <li>(2) Omni-DNA Project address: https://huggingface.co/zehui127/Omni-DNA-20M <pre><code>uv pip install ai2-olmo\n</code></pre></li> <li>(3) megaDNA Project address: https://github.com/lingxusb/megaDNA <pre><code>git clone https://github.com/lingxusb/megaDNA\ncd megaDNA\nuv pip install .\n</code></pre></li> <li>(4) Enformer Project address: https://github.com/lucidrains/enformer-pytorch <pre><code>uv pip install enformer-pytorch\n</code></pre></li> <li>(5) Borzoi Project address: https://github.com/johahi/borzoi-pytorch <pre><code>uv pip install borzoi-pytorch\n</code></pre></li> </ul> </li> <li> <p>Some models support Flash Attention acceleration. If you need to install this dependency, you can refer to the project GitHub for installation. Note that <code>flash-attn</code> versions are tied to different Python versions, PyTorch versions, and CUDA versions. Please first check if there are matching version installation packages in GitHub Releases, otherwise you may encounter <code>HTTP Error 404: Not Found</code> errors.     <pre><code>uv pip install flash-attn --no-build-isolation\n</code></pre></p> </li> <li> <p>If compilation is required during installation and compilation errors occur, please first install the dependencies that may be needed. We recommend using <code>conda</code> to install dependencies.     <pre><code>conda install -c conda-forge gxx clang\n</code></pre></p> </li> </ol>"},{"location":"getting_started/installation/#3-check-if-installation-was-successful","title":"3. Check if installation was successful","text":"<pre><code>sh tests/test_all.sh\n</code></pre>"},{"location":"getting_started/installation_zh/","title":"\u5b89\u88c5","text":"<p>\u6211\u4eec\u63a8\u8350\u4f7f\u7528 Astral uv \u5305\u7ba1\u7406\u5de5\u5177\u6765\u5b89\u88c5DNALLM\u7684\u8fd0\u884c\u73af\u5883\u548c\u4f9d\u8d56</p> <p>\u3010\u4ec0\u4e48\u662fuv\uff1f\u3011 uv\u662f\u4e00\u4e2a\u57fa\u4e8eRust\u5f00\u53d1\u7684\u5feb\u901f\u7684Python\u5305\u7ba1\u7406\u5de5\u5177\uff0c\u6bd4pip\u7b49\u4f20\u7edf\u5de5\u5177\u5feb10-100\u500d\u3002</p>"},{"location":"getting_started/installation_zh/#1-uv","title":"1. \u5b89\u88c5uv","text":"<pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"getting_started/installation_zh/#2-dnallm","title":"2. \u5b89\u88c5DNALLM","text":"<pre><code># \u514b\u9686\u4ed3\u5e93\ngit clone https://github.com/zhangtaolab/DNALLM.git\ncd DNALLM\n\n# \u521b\u5efa\u865a\u62df\u73af\u5883\u5e76\u5b89\u88c5\u4f9d\u8d56\nuv venv\n# \u6fc0\u6d3b\u865a\u62df\u73af\u5883\n## Linux &amp; MacOS \u7528\u6237\nsource .venv/bin/activate\n## Windows \u7528\u6237\n.venv\\Scripts\\activate\n\n# \u5b89\u88c5\u4f9d\u8d56\nuv pip install -e '.[base]'\n\n# \u5b89\u88c5DNALLM\nuv pip install -e .\n</code></pre> <p>\u5728\u5b89\u88c5\u4f9d\u8d56\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5bf9\u4e8e\u6ca1\u6709GPU\u6216\u8005\u4f7f\u7528\u5e26\u82f9\u679cM\u7cfb\u5217\u82af\u7247\u7684MacOS\u7528\u6237\uff0c\u5b89\u88c5\u4f9d\u8d56\u65f6\u76f4\u63a5\u4f7f\u7528 <code>base</code> \u4f9d\u8d56\u5373\u53ef\uff1b\u5bf9\u4e8e\u4f7f\u7528\u5e26Nvidia\u663e\u5361\u7684Linux\u7528\u6237\uff0c\u901a\u5e38\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>base</code> \u4f9d\u8d56\uff0c\u5305\u7ba1\u7406\u8f6f\u4ef6\u4f1a\u81ea\u52a8\u8bc6\u522b\u5e76\u5b89\u88c5\u5bf9\u5e94cuda\u7248\u672c\u7684torch\u5305\uff1b\u5bf9\u4e8eWindows\u7528\u6237\uff0c\u5efa\u8bae\u5728\u5b89\u88c5\u4f9d\u8d56\u65f6\u6307\u5b9acuda\u7248\u672c\u4ee5\u9632\u6b62\u5305\u7ba1\u7406\u8f6f\u4ef6\u65e0\u6cd5\u6b63\u786e\u8bc6\u522b\u5230\u60a8\u7684\u663e\u5361\u3002</p> <p>\u8003\u8651\u5230\u4e0d\u540c\u7528\u6237\u4f7f\u7528\u7684\u663e\u5361\u652f\u6301\u7684CUDA\u7248\u672c\u4e0d\u540c\uff0c\u5982\u679c\u9700\u8981\u6307\u5b9aCUDA\u7248\u672c\u8fdb\u884c\u5b89\u88c5\uff0c\u53ef\u901a\u8fc7\u4e00\u4e0b\u547d\u4ee4\u8fdb\u884c\u5b89\u88c5 <pre><code># \u9996\u5148\u5b89\u88c5\u6307\u5b9a\u7248\u672c\u7684torch (\u652f\u6301cpu, cuda121, cuda124, cuda126)\nuv pip install -e '.[cuda124]'\n# \u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u5176\u4ed6\u4f9d\u8d56\nuv pip install -e '.[base]'\n</code></pre></p> <p>\u5bf9\u4e8e\u4f7f\u7528AMD\u663e\u5361\u7684\u7528\u6237\uff0c\u8bf7\u786e\u4fdd\u4f60\u662f\u7528\u7684\u7cfb\u7edf\u65f6Linux\uff0c\u5e76\u624b\u52a8\u5b89\u88c5\u5bf9\u5e94torch\u7248\u672c\u3002 <pre><code>uv pip install 'torch&gt;=2.5' --index-url https://download.pytorch.org/whl/rocm6.2\n</code></pre> \u5bf9\u4e8e\u4f7f\u7528Intel\u663e\u5361\u7684\u7528\u6237\uff0c\u5982\u679c\u60a8\u5e0c\u671b\u4f7f\u7528\u663e\u5361\u52a0\u901f\uff0c\u8bf7\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u5b89\u88c5\u597d\u5bf9\u5e94\u7684\u9a71\u52a8\u548ctorch\u7248\u672c\u3002</p> <p>\u6709\u4e9b\u6a21\u578b\u9700\u8981\u5176\u4ed6\u4f9d\u8d56\u7684\u652f\u6301\uff0c\u9488\u5bf9\u4e0d\u540c\u6a21\u578b\u9700\u8981\u7684\u4f9d\u8d56\uff0c\u6211\u4eec\u4f1a\u6301\u7eed\u6dfb\u52a0\u3002</p> <ol> <li> <p>\u539f\u751f <code>mamba</code> \u67b6\u6784\u7684\u8fd0\u884c\u901f\u5ea6\u663e\u8457\u4f18\u4e8etransformer\u517c\u5bb9\u7684mamba\u67b6\u6784\uff0c\u4e0d\u8fc7\u539f\u751f mamba \u4f9d\u8d56\u4e8eNvidia\u663e\u5361\uff0c\u5982\u679c\u9700\u8981\u539f\u751f mamba \u67b6\u6784\u652f\u6301\uff0c\u5b89\u88c5\u5b8cDNALLM\u4f9d\u8d56\u540e\uff0c\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a</p> <pre><code>uv pip install -e '.[mamba]' --no-cache-dir --no-build-isolation\n</code></pre> </li> <li> <p><code>EVO2</code> \u6a21\u578b\u7684\u5fae\u8c03\u548c\u63a8\u7406\u4f9d\u8d56\u5176\u81ea\u5df1\u7684\u8f6f\u4ef6\u5305 \u6216 \u7b2c\u4e09\u65b9python\u5e931/\u5e932</p> <pre><code># evo2 repuires python version &gt;=3.11\ngit clone --recurse-submodules git@github.com:ArcInstitute/evo2.git\ncd evo2\n# install required dependency 'vortex'\ncd vortex\nuv pip install .\n# install evo2\ncd ..\nuv pip install .\n\n# add cudnn path to environment\nexport LD_LIBRARY_PATH=[path_to_DNALLM]/.venv/lib64/python3.11/site-packages/nvidia/cudnn/lib:${LD_LIBRARY_PATH}\n</code></pre> </li> <li> <p>\u90e8\u5206\u6a21\u578b\u4f7f\u7528\u4e86\u81ea\u5df1\u5f00\u53d1\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5b83\u4eec\u5c1a\u672a\u88ab\u6574\u5408\u8fdbHuggingFace\u7684transformers\u5e93\u4e2d\uff0c\u56e0\u6b64\u5bf9\u8fd9\u7c7b\u6a21\u578b\u7684\u5fae\u8c03\u548c\u63a8\u7406\u9700\u8981\u63d0\u524d\u5b89\u88c5\u5bf9\u5e94\u7684\u6a21\u578b\u4f9d\u8d56\u5e93\u3002</p> <ul> <li>(1) GPN \u9879\u76ee\u5730\u5740\uff1ahttps://github.com/songlab-cal/gpn <pre><code>git clone https://github.com/songlab-cal/gpn\ncd gpn\nuv pip install .\n</code></pre></li> <li>(2) Omni-DNA \u9879\u76ee\u5730\u5740\uff1ahttps://huggingface.co/zehui127/Omni-DNA-20M <pre><code>uv pip install ai2-olmo\n</code></pre></li> <li>(3) megaDNA \u9879\u76ee\u5730\u5740\uff1ahttps://github.com/lingxusb/megaDNA <pre><code>git clone https://github.com/lingxusb/megaDNA\ncd megaDNA\nuv pip install .\n</code></pre></li> <li>(4) Enformer \u9879\u76ee\u5730\u5740\uff1ahttps://github.com/lucidrains/enformer-pytorch <pre><code>uv pip install enformer-pytorch\n</code></pre></li> <li>(5) Borzoi \u9879\u76ee\u5730\u5740\uff1ahttps://github.com/johahi/borzoi-pytorch <pre><code>uv pip install borzoi-pytorch\n</code></pre></li> </ul> </li> <li> <p>\u6709\u4e9b\u6a21\u578b\u652f\u6301Flash Attention\u52a0\u901f\uff0c\u5982\u679c\u9700\u8981\u5b89\u88c5\u8be5\u4f9d\u8d56\uff0c\u53ef\u4ee5\u53c2\u8003 \u9879\u76eeGitHub \u5b89\u88c5\u3002\u9700\u8981\u6ce8\u610f <code>flash-attn</code> \u7684\u7248\u672c\u548c\u4e0d\u540c\u7684python\u7248\u672c\u3001pytorch\u7248\u672c\u4ee5\u53cacuda\u7248\u672c\u6302\u94a9\uff0c\u8bf7\u5148\u68c0\u67e5 GitHub Releases \u4e2d\u662f\u5426\u6709\u5339\u914d\u7248\u672c\u7684\u5b89\u88c5\u5305\uff0c\u5426\u5219\u53ef\u80fd\u4f1a\u51fa\u73b0 <code>HTTP Error 404: Not Found</code> \u7684\u62a5\u9519\u3002     <pre><code>uv pip install flash-attn --no-build-isolation\n</code></pre></p> </li> <li> <p>\u5982\u679c\u51fa\u73b0\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u9700\u8981\u7f16\u8bd1\uff0c\u800c\u7f16\u8bd1\u8fc7\u7a0b\u62a5\u9519\uff0c\u8bf7\u524d\u5b89\u88c5\u597d\u53ef\u80fd\u9700\u8981\u7684\u4f9d\u8d56\uff0c\u63a8\u8350\u4f7f\u7528 <code>conda</code> \u5b89\u88c5\u4f9d\u8d56\u3002     <pre><code>conda install -c conda-forge gxx clang\n</code></pre></p> </li> </ol>"},{"location":"getting_started/installation_zh/#3","title":"3. \u68c0\u67e5\u662f\u5426\u5b89\u88c5\u6210\u529f","text":"<pre><code>sh tests/test_all.sh\n</code></pre>"},{"location":"getting_started/quick_start/","title":"Quick Start","text":"<p>This guide will help you get started with DNALLM quickly. DNALLM is a comprehensive toolkit for fine-tuning and inference with DNA Language Models.</p>"},{"location":"getting_started/quick_start/#1-installation","title":"1. Installation","text":""},{"location":"getting_started/quick_start/#install-dependencies-recommended-uv","title":"Install dependencies (recommended: uv)","text":"<pre><code># Install uv package manager\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Clone the repository\ngit clone https://github.com/zhangtaolab/DNALLM.git\n\ncd DNALLM\n\n# Create virtual environment\nuv venv\n\n# Activate virtual environment\nsource .venv/bin/activate  # Linux/MacOS\n# or\n.venv\\Scripts\\activate     # Windows\n\n# Install DNALLM with base dependencies\nuv pip install -e '.[base]'\n</code></pre>"},{"location":"getting_started/quick_start/#verify-installation","title":"Verify installation","text":"<pre><code># Test if everything is working\npython -c \"from dnallm import load_config, load_model_and_tokenizer; print('DNALLM installed successfully!')\"\n</code></pre>"},{"location":"getting_started/quick_start/#2-basic-usage","title":"2. Basic Usage","text":""},{"location":"getting_started/quick_start/#load-a-pre-trained-model","title":"Load a pre-trained model","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer\n\n# Load configuration\nconfigs = load_config(\"./example/notebooks/inference/inference_config.yaml\")\n\n# Load model and tokenizer from Hugging Face\nmodel_name = \"zhangtaolab/plant-dnagpt-BPE-promoter_strength_protoplast\"\nmodel, tokenizer = load_model_and_tokenizer(\n    model_name, \n    task_config=configs['task'], \n    source=\"huggingface\"\n)\n\n# Or load from ModelScope\n# model, tokenizer = load_model_and_tokenizer(\n#     model_name, \n#     task_config=configs['task'], \n#     source=\"modelscope\"\n# )\n</code></pre>"},{"location":"getting_started/quick_start/#make-predictions","title":"Make predictions","text":"<pre><code>from dnallm import Predictor\n\n# Initialize predictor\npredictor = Predictor(config=configs, model=model, tokenizer=tokenizer)\n\n# Input DNA sequence\nsequence = \"AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC\"\n\n# Make prediction\nprediction = predictor.predict(sequence)\nprint(f\"Prediction: {prediction}\")\n</code></pre>"},{"location":"getting_started/quick_start/#3-fine-tuning-dna-llm","title":"3. Fine-tuning DNA LLM","text":""},{"location":"getting_started/quick_start/#prepare-your-dataset","title":"Prepare your dataset","text":"<pre><code>from dnallm.datahandling import DatasetAuto\n\n# Load or create your dataset\ndataset = DatasetAuto(\n    data_path=\"path/to/your/data.csv\",\n    task_type=\"classification\",  # or \"regression\", \"generation\"\n    text_column=\"sequence\",\n    label_column=\"label\"\n)\n\n# Split dataset\ntrain_dataset, eval_dataset = dataset.split(train_ratio=0.8)\n</code></pre>"},{"location":"getting_started/quick_start/#fine-tune-the-model","title":"Fine-tune the model","text":"<pre><code>from dnallm.finetune import Trainer\n\n# Initialize trainer\ntrainer = Trainer(\n    config=configs,\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset\n)\n\n# Start training\ntrainer.train()\n\n# Save the fine-tuned model\ntrainer.save_model(\"path/to/save/finetuned_model\")\n</code></pre>"},{"location":"getting_started/quick_start/#4-advanced-features","title":"4. Advanced Features","text":""},{"location":"getting_started/quick_start/#in-silico-mutagenesis","title":"In-silico mutagenesis","text":"<pre><code>from dnallm import Mutagenesis\n\n# Initialize mutagenesis analyzer\nmutagenesis = Mutagenesis(config=configs, model=model, tokenizer=tokenizer)\n\n# Input sequence for saturation mutagenesis\nsequence = \"AATATATTTAATCGGTGTATAATTTCTGTGAAGATCCTCGATACTTCATATAAGAGATTTTGAGAGAGAGAGAGAACCAATTTTCGAATGGGTGAGTTGGCAAAGTATTCACTTTTCAGAACATAATTGGGAAACTAGTCACTTTACTATTCAAAATTTGCAAAGTAGTC\"\n\n# Generate all possible single-nucleotide mutations\nmutagenesis.mutate_sequence(sequence, replace_mut=True)\n\n# Evaluate mutation effects\npredictions = mutagenesis.evaluate(strategy=\"mean\")\n\n# Visualize results\nplot = mutagenesis.plot(predictions, save_path=\"mutation_effects.pdf\")\n</code></pre>"},{"location":"getting_started/quick_start/#benchmark-evaluation","title":"Benchmark evaluation","text":"<pre><code>from dnallm import Benchmark\n\n# Initialize benchmark\nbenchmark = Benchmark(config=configs, model=model, tokenizer=tokenizer)\n\n# Run benchmark on test dataset\nresults = benchmark.evaluate(test_dataset)\n\n# Print results\nprint(f\"Accuracy: {results['accuracy']:.4f}\")\nprint(f\"F1 Score: {results['f1']:.4f}\")\n</code></pre>"},{"location":"getting_started/quick_start/#5-command-line-interface","title":"5. Command Line Interface","text":"<p>DNALLM provides convenient CLI tools:</p> <pre><code># Training\ndnallm-train --config path/to/config.yaml\n\n# Prediction\ndnallm-predict --config path/to/config.yaml --input path/to/sequences.txt\n\n# MCP server\ndnallm-mcp-server --config path/to/config.yaml\n</code></pre>"},{"location":"getting_started/quick_start/#6-configuration","title":"6. Configuration","text":"<p>Create a configuration file (<code>config.yaml</code>) for your task:</p> <pre><code>task:\n  type: \"classification\"  # or \"regression\", \"generation\"\n  num_labels: 2\n\nmodel:\n  name: \"zhangtaolab/plant-dnagpt-BPE-promoter_strength_protoplast\"\n  source: \"huggingface\"  # or \"modelscope\"\n\ntraining:\n  learning_rate: 2e-5\n  batch_size: 8\n  num_epochs: 3\n\ndata:\n  train_file: \"path/to/train.csv\"\n  eval_file: \"path/to/eval.csv\"\n  text_column: \"sequence\"\n  label_column: \"label\"\n</code></pre>"},{"location":"getting_started/quick_start/#7-examples-and-tutorials","title":"7. Examples and Tutorials","text":"<p>Check out the example notebooks in the <code>example/notebooks/</code> directory:</p> <ul> <li>Fine-tuning: Multi-label classification, NER tasks</li> <li>Inference: Basic prediction and benchmarking</li> <li>Advanced: In-silico mutagenesis, embedding analysis</li> </ul>"},{"location":"getting_started/quick_start/#8-next-steps","title":"8. Next Steps","text":"<ul> <li>Explore the API documentation for detailed function references</li> <li>Check out tutorials for specific use cases</li> <li>Visit the FAQ for common questions</li> <li>Join the community discussions on GitHub</li> </ul>"},{"location":"getting_started/quick_start/#need-help","title":"Need Help?","text":"<ul> <li>Documentation: Browse the complete documentation</li> <li>Issues: Report bugs or request features on GitHub</li> <li>Examples: Check the example notebooks for working code</li> <li>Configuration: Refer to the configuration examples in the docs</li> </ul>"},{"location":"resources/datahandling/","title":"Datahandling","text":"<p>The datasets module is primarily used for reading online or local datasets, generating datasets, or processing already loaded datasets. The module defines a <code>DNADataset</code> class.</p>"},{"location":"resources/datahandling/#overview","title":"Overview","text":"<p>The <code>DNADataset</code> class provides comprehensive functionality for DNA sequence data management and processing, including:</p> <ul> <li> <p>Data Loading: Read data from online sources like <code>HuggingFace</code> or <code>ModelScope</code>, or from local files supporting multiple formats (.csv, .tsv, .json, .arrow, .parquet, .txt, dict, fasta, etc.)</p> </li> <li> <p>Data Validation: Filter sequences based on length, GC content, and valid base composition requirements</p> </li> <li> <p>Data Cleaning: Remove entries with missing sequence or label information</p> </li> <li> <p>Data Augmentation:</p> </li> <li>Random reverse complement generation</li> <li>Add reverse complement sequences (doubles the original dataset size)</li> <li> <p>Generate random synthetic data with configurable sequence length, count, GC content distribution, N-base inclusion, and padding requirements</p> </li> <li> <p>Data Shuffling: Randomize dataset order</p> </li> <li> <p>Data Splitting: Divide datasets into train/validation/test sets</p> </li> <li> <p>Sequence Tokenization: Convert DNA sequences to model-compatible tokens</p> </li> </ul>"},{"location":"resources/datahandling/#examples","title":"Examples","text":""},{"location":"resources/datahandling/#basic-setup","title":"Basic Setup","text":"<pre><code>from dnallm import DNADataset\nfrom transformers import AutoTokenizer\n</code></pre>"},{"location":"resources/datahandling/#data-loading","title":"Data Loading","text":"<pre><code># Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"zhangtaolab/plant-dnabert-BPE\")\n\n# Load data\n# 1. Load local data (specify sequence and label column headers)\n# 1.1 Single file\ndna_ds = DNADataset.load_local_data(\n    \"/path_to_your_datasets/data.csv\", \n    seq_col=\"sequence\", \n    label_col=\"labels\",\n    tokenizer=tokenizer, \n    max_length=512\n)\n\n# 1.2 Multiple files (e.g., pre-split datasets)\ndna_ds = DNADataset.load_local_data(\n    {\n        \"train\": \"train.csv\", \n        \"test\": \"test.csv\", \n        \"validation\": \"validation.csv\"\n    },\n    seq_col=\"sequence\", \n    label_col=\"labels\",\n    tokenizer=tokenizer, \n    max_length=512\n)\n\n# 2. Load online data\n# 2.1 From HuggingFace\ndna_ds = DNADataset.from_huggingface(\n    \"zhangtaolab/plant-multi-species-open-chromatin\", \n    seq_field=\"sequence\", \n    label_field=\"label\", \n    tokenizer=tokenizer, \n    max_length=512\n)\n\n# 2.2 From ModelScope\ndna_ds = DNADataset.from_modelscope(\n    \"zhangtaolab/plant-multi-species-open-chromatin\", \n    seq_field=\"sequence\", \n    label_field=\"label\", \n    tokenizer=tokenizer, \n    max_length=512\n)\n</code></pre>"},{"location":"resources/datahandling/#data-processing-and-augmentation","title":"Data Processing and Augmentation","text":"<pre><code># Common functionality demonstration\n\n# 1. Data validation (filter sequences by length, GC content, valid base composition)\ndna_ds.validate_sequences(minl=200, maxl=1000, valid_chars=\"ACGT\")\n\n# 2. Data cleaning (remove entries with missing sequence or label information)\ndna_ds.process_missing_data()\n\n# 3. Data splitting\ndna_ds.split_data(test_size=0.2, val_size=0.1)\n\n# 4. Data shuffling\ndna_ds.shuffle(seed=42)\n\n# 5. Data augmentation\n# 5.1 Random reverse complement\ndna_ds.raw_reverse_complement(ratio=0.5)  # Apply reverse complement to 50% of sequences\n\n# 5.2 Add reverse complement sequences (doubles the original dataset size)\ndna_ds.augment_reverse_complement()\n\n# 5.3 Generate random synthetic data\ndna_ds.random_generate(\n    minl=200,           # Minimum sequence length\n    maxl=2000,          # Maximum sequence length\n    samples=3000,       # Number of sequences to generate\n    gc=(0.1, 0.9),     # GC content range\n    N_ratio=0.0,        # Ratio of N bases to include\n    padding_size=1,     # Ensure sequences are multiples of this value\n    append=True,         # Append to existing dataset\n    label_func=None     # Custom function for generating labels\n)\n\n# Note: label_func is a custom function for generating data labels\n# If append=True is not specified, generated data will replace the original dataset\n# (useful for random dataset initialization)\n\n# 6. Data downsampling\nnew_ds = dna_ds.sampling(ratio=0.1)\n\n# 7. Data inspection\ndna_ds.show(head=20)          # Display first N formatted data entries\ntmp_ds = dna_ds.head(head=5)  # Extract first N data entries\n\n# 8. Sequence tokenization (requires DNADataset.tokenizer to be defined)\ndna_ds.encode_sequences()\n</code></pre>"},{"location":"resources/datahandling/#api-reference","title":"API Reference","text":"<p>For detailed function documentation and parameter descriptions, please refer to the API Reference.</p>"},{"location":"resources/datahandling/#key-features-summary","title":"Key Features Summary","text":"Feature Description Method Data Loading Load from HuggingFace, ModelScope, or local files <code>load_local_data()</code>, <code>from_huggingface()</code>, <code>from_modelscope()</code> Validation Filter by length, GC content, base composition <code>validate_sequences()</code> Cleaning Remove missing data entries <code>process_missing_data()</code> Augmentation Generate synthetic sequences and variants <code>raw_reverse_complement()</code>, <code>augment_reverse_complement()</code>, <code>random_generate()</code> Processing Split, shuffle, and sample datasets <code>split_data()</code>, <code>shuffle()</code>, <code>sampling()</code> Tokenization Convert sequences to model tokens <code>encode_sequences()</code>"},{"location":"resources/datahandling/#tips","title":"Tips","text":"<ul> <li>Always validate your data before processing to ensure quality</li> <li>Use data augmentation to increase dataset size and improve model robustness</li> <li>Consider GC content distribution when generating synthetic sequences</li> <li>Set appropriate sequence length limits based on your model's requirements</li> </ul>"},{"location":"resources/datahandling_zh/","title":"datahandling","text":"<p>\u8be5\u6a21\u5757\u4e3b\u8981\u7528\u4e8e\u8bfb\u53d6\u5728\u7ebf\u6216\u8005\u672c\u5730\u7684\u6570\u636e\u96c6\u3001\u751f\u6210\u6570\u636e\u96c6\u6216\u5904\u7406\u5df2\u8bfb\u53d6\u7684\u6570\u636e\u96c6\u3002\u6a21\u5757\u5b9a\u4e49\u4e86\u4e00\u4e2a <code>DNADataset</code> \u7684\u7c7b\u3002 \u5e38\u7528\u529f\u80fd\u4e3b\u8981\u5305\u542b\u4ee5\u4e0b\u51e0\u79cd\uff1a</p> <ul> <li> <p>\u8bfb\u53d6\u6570\u636e   \u5728\u7ebf\u6570\u636e\u96c6\u53ef\u4ee5\u901a\u8fc7\u4ece <code>HuggingFace</code> \u6216 <code>Modelscope</code> \u4e24\u4e2a\u6570\u636e\u5e93\u8fdb\u884c\u4e0b\u8f7d\u548c\u8bfb\u53d6\u3002\u672c\u5730\u6570\u636e\u96c6\u652f\u6301\u8bfb\u53d6\u591a\u79cd\u7c7b\u578b\u4fdd\u5b58\u7684\u5e8f\u5217\u6570\u636e\uff08.csv, .tsv, .json, .arrow, .parquet, .txt, dict, fasta\u7b49\uff09\u3002  </p> </li> <li> <p>\u6570\u636e\u68c0\u67e5\uff08\u652f\u6301\u5e8f\u5217\u957f\u5ea6\u8fc7\u6ee4\u3001GC\u542b\u91cf\u8fc7\u6ee4\u3001\u5305\u542b\u7684\u78b1\u57fa\u68c0\u67e5\uff0c\u4e0d\u7b26\u5408\u8981\u6c42\u7684\u5e8f\u5217\u5c06\u88ab\u8fc7\u6ee4\uff09</p> </li> <li> <p>\u6570\u636e\u6e05\u6d17\uff08\u8fc7\u6ee4\u6389\u5e8f\u5217\u6216\u6807\u7b7e\u4fe1\u606f\u7f3a\u5931\u7684\u6570\u636e\uff09</p> </li> <li> <p>\u6570\u636e\u589e\u5f3a</p> </li> <li>\u968f\u673a\u53cd\u5411\u4e92\u8865</li> <li>\u6dfb\u52a0\u53cd\u5411\u4e92\u8865\u5e8f\u5217\uff08\u539f\u6709\u6570\u636e\u96c6\u6269\u59271\u500d\uff09</li> <li> <p>\u6dfb\u52a0\u968f\u673a\u6570\u636e\uff08\u53ef\u8c03\u6574\u5e8f\u5217\u957f\u5ea6\u3001\u5e8f\u5217\u6570\u3001GC\u542b\u91cf\u5206\u5e03\u3001\u662f\u5426\u5305\u542b\u78b1\u57faN\uff0c\u5e8f\u5217\u662f\u5426\u9700\u8981\u4e3a\u6307\u5b9a\u500d\u6570\u7684\u957f\u5ea6\uff09</p> </li> <li> <p>\u6570\u636e\u6253\u4e71</p> </li> <li> <p>\u6570\u636e\u62c6\u5206</p> </li> <li> <p>\u5e8f\u5217\u5206\u8bcd\uff08tokenization\uff09</p> </li> </ul>"},{"location":"resources/datahandling_zh/#_1","title":"\u793a\u4f8b\uff1a","text":"<pre><code>from dnallm import DNADataset\nfrom transformers import AutoTokenizer\n</code></pre> <pre><code># \u8bfb\u53d6tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"zhangtaolab/plant-dnabert-BPE\")\n\n# \u8bfb\u53d6\u6570\u636e\n# 1. \u8bfb\u53d6\u672c\u5730\u6570\u636e\uff08\u9700\u8981\u5236\u5b9a\u5e8f\u5217\u548c\u6807\u7b7e\u5217\u7684\u5934\u540d\uff09\uff0c\u6307\u5b9atokenizer\n# 1.1 \u5355\u4e2a\u6587\u4ef6\ndna_ds = DNADataset.load_local_data(\n    \"/path_to_your_datasets/data.csv\", seq_col=\"sequence\", label_col=\"labels\",\n    tokenizer=tokenizer, max_length=512\n)\n# 1.2 \u591a\u4e2a\u6587\u4ef6\uff08\u5982\u6570\u636e\u96c6\u5df2\u62c6\u5206\uff09\ndna_ds = DNADataset.load_local_data(\n    {\"train\": \"train.csv\", \"test\": \"test.csv\", \"validation\": \"validation.csv\"},\n    seq_col=\"sequence\", label_col=\"labels\",\n    tokenizer=tokenizer, max_length=512\n)\n\n# 2. \u8bfb\u53d6\u5728\u7ebf\u6570\u636e\n# 2.1 HuggingFace\ndna_ds = DNADataset.from_huggingface(\"zhangtaolab/plant-multi-species-open-chromatin\", seq_field=\"sequence\", label_field=\"label\", tokenizer=tokenizer, max_length=512)\n# 2.2 ModelScope\ndna_ds = DNADataset.from_modelscope(\"zhangtaolab/plant-multi-species-open-chromatin\", seq_field=\"sequence\", label_field=\"label\", tokenizer=tokenizer, max_length=512)\n</code></pre> <pre><code># \u5e38\u7528\u529f\u80fd\u4ecb\u7ecd\n# 1. \u6570\u636e\u68c0\u67e5\uff08\u652f\u6301\u5e8f\u5217\u957f\u5ea6\u8fc7\u6ee4\u3001GC\u542b\u91cf\u8fc7\u6ee4\u3001\u5305\u542b\u7684\u78b1\u57fa\u68c0\u67e5\uff0c\u4e0d\u7b26\u5408\u8981\u6c42\u7684\u5e8f\u5217\u5c06\u88ab\u8fc7\u6ee4\uff09\ndna_ds.validate_sequences(minl=200, maxl=1000, valid_chars=\"ACGT\")\n\n# 2. \u6570\u636e\u6e05\u6d17\uff08\u8fc7\u6ee4\u6389\u7f3a\u4e4f\u5e8f\u5217\u6216\u6807\u7b7e\u4fe1\u606f\u7684\u6570\u636e\uff09\ndna_ds.process_missing_data()\n\n# 3. \u62c6\u5206\u6570\u636e\ndna_ds.split_data(test_size=0.2, val_size=0.1)\n\n# 4. \u6570\u636e\u6253\u4e71\ndna_ds.shuffle(seed=42)\n\n# 5. \u6570\u636e\u589e\u5f3a\n# 5.1 \u968f\u673a\u53cd\u5411\u4e92\u8865\ndna_ds.raw_reverse_complement(ratio=0.5) # 50%\u7684\u5e8f\u5217\u8fdb\u884c\u53cd\u5411\u4e92\u8865\n\n# 5.2 \u6dfb\u52a0\u53cd\u5411\u4e92\u8865\u5e8f\u5217\uff08\u539f\u6709\u6570\u636e\u96c6\u6269\u59271\u500d\uff09\ndna_ds.augment_reverse_complement()\n\n# 5.3 \u52a0\u5165\u968f\u673a\u6570\u636e\uff08\u53ef\u8c03\u6574\u5e8f\u5217\u957f\u5ea6\u3001\u5e8f\u5217\u6570\u3001GC\u542b\u91cf\u5206\u5e03\u3001\u662f\u5426\u5305\u542b\u78b1\u57faN\uff0c\u5e8f\u5217\u662f\u5426\u9700\u8981\u4e3a\u6307\u5b9a\u500d\u6570\u7684\u957f\u5ea6\uff09\ndna_ds.random_generate(minl=200, maxl=2000, samples=3000, gc=(0.1, 0.9), N_ratio=0.0, padding_size=1, append=True, label_func=None)\n## `label_func`\u662f\u7528\u6765\u81ea\u5b9a\u4e49\u6570\u636e\u6807\u7b7e\u7684\u51fd\u6570\n## \u5982\u679c\u4e0d\u52a0`append=True`\u7684\u53c2\u6570\uff0c\u5373\u4f7f\u7528\u751f\u6210\u7684\u6570\u636e\u96c6\u8986\u76d6\u539f\u59cb\u6570\u636e\u96c6\uff08\u53ef\u7528\u4e8e\u968f\u673a\u521d\u59cb\u5316\u6570\u636e\u96c6\uff09\n\n# 6. \u6570\u636e\u964d\u91c7\u6837\nnew_ds = dna_ds.sampling(ratio=0.1)\n\n# 7. \u6570\u636e\u5c55\u793a\ndna_ds.show(head=20)          # \u663e\u793a\u683c\u5f0f\u5316\u540e\u7684\u524dN\u4e2a\u6570\u636e\ntmp_ds = dna_ds.head(head=5)  # \u63d0\u53d6\u524dN\u4e2a\u6570\u636e\n\n# 8. \u5e8f\u5217tokenization\uff08\u8981\u6c42\u63d0\u524d\u5b9a\u4e49\u597dDNADataset.tokenizer\uff09\ndna_ds.encode_sequences()\n</code></pre>"},{"location":"resources/datahandling_zh/#_2","title":"\u51fd\u6570\u53ca\u53c2\u6570\u8bf4\u660e","text":"<p>\u8bf7\u53c2\u8003 API \u90e8\u5206\u8bf4\u660e</p>"},{"location":"resources/model_zoo%20copy/","title":"\u6a21\u578b\u5e93","text":"<p>DNALLM\u4e2d\u6536\u5f55\u4e86\u51e0\u4e4e\u6240\u6709\u53ef\u4ee5\u5728\u7ebf\u83b7\u53d6\u7684 DNA\u5927\u8bed\u8a00\u6a21\u578b \u548c\u90e8\u5206\u57fa\u4e8eDNA\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u9488\u5bf9\u4e0d\u540c\u7684\u6a21\u578b\uff0c\u6211\u4eec\u5bf9\u5176\u8fdb\u884c\u4e86\u9002\u914d\uff0c\u4f7f\u5176\u53ef\u4ee5\u901a\u8fc7 DNALLM\u5305 \u8fdb\u884c\u6a21\u578b\u7684\u5fae\u8c03\u548c\u63a8\u7406\u3002</p>"},{"location":"resources/model_zoo%20copy/#_2","title":"\u6a21\u578b\u5217\u8868","text":"<p>\u76ee\u524d\u5df2\u6536\u5f55\u7684\u6a21\u578b\u548c\u6a21\u578b\u7684\u5fae\u8c03/\u63a8\u7406\u652f\u6301\u60c5\u51b5\u5982\u4e0b\uff1a</p> \u6a21\u578b\u540d\u79f0 \u6a21\u578b\u4f5c\u8005 \u6a21\u578b\u7c7b\u578b \u6a21\u578b\u67b6\u6784 \u6a21\u578b\u5927\u5c0f \u6a21\u578b\u6570\u91cf \u6a21\u578b\u6765\u6e90 \u662f\u5426\u652f\u6301\u5fae\u8c03 Nucleotide Transformer InstaDeepAI MaskedLM ESM 50M / 100M / 250M / 500M / 2.5B 8 Nature Methods \u662f AgroNT InstaDeepAI MaskedLM ESM 1B 1 Current Biology \u662f Caduceus-Ph Kuleshov-Group MaskedLM Caduceus 0.5M / 2M / 8M 3 arXiv \u662f Caduceus-Ps Kuleshov-Group MaskedLM Caduceus 0.5M / 2M / 8M 3 arXiv \u662f PlantCaduceus Kuleshov-Group MaskedLM Caduceus 20M / 40M / 112M / 225M 4 bioRxiv \u662f DNABERT Zhihan1996 MaskedLM BERT 100M 4 Bioinformatics \u662f DNABERT-2 Zhihan1996 MaskedLM BERT 117M 1 arXiv \u662f DNABERT-S Zhihan1996 MaskedLM BERT 117M 1 arXiv \u662f GENA-LM AIRI-Institute MaskedLM BERT 150M / 500M 7 Nucleic Acids Research \u662f GENA-LM-BigBird AIRI-Institute MaskedLM BigBird 150M 3 Nucleic Acids Research \u662f GENERator GenerTeam CausalLM Llama 0.5B / 1.2B / 3B 4 arXiv \u662f GenomeOcean pGenomeOcean CausalLM Mistral 100M / 500M / 4B 3 bioRxiv \u662f GPN songlab MaskedLM ConvNet 60M 1 PNAS \u5426 GROVER PoetschLab MaskedLM BERT 100M 1 Nature Machine Intelligence \u662f HyenaDNA LongSafari CausalLM HyenaDNA 0.5M / 0.7M / 2M / 4M / 15M / 30M / 55M 7 arXiv \u662f Jamba-DNA RaphaelMourad CausalLM Jamba 114M 1 GitHub \u662f Mistral-DNA RaphaelMourad CausalLM Mistral 1M / 17M / 138M / 417M / 422M 10 GitHub \u662f ModernBert-DNA RaphaelMourad MaskedLM ModernBert 37M 3 GitHub \u662f MutBERT JadenLong MaskedLM RoPEBert 86M 3 bioRxiv \u662f OmniNA XLS CausalLM Llama 66M / 220M 2 bioRxiv \u662f Omni-DNA zehui127 CausalLM OLMoModel 20M / 60M / 116M / 300M / 700M / 1B 6 arXiv \u5426 EVO-1 togethercomputer CausalLM StripedHyena 6.5B 2 GitHub \u662f EVO-2 arcinstitute CausalLM StripedHyena2 1B / 7B / 40B 3 GitHub \u5426 ProkBERT neuralbioinfo MaskedLM MegatronBert 21M / 25M / 27M 3 Frontiers in Microbiology \u662f Plant DNABERT zhangtaolab MaskedLM BERT 100M 1 Molecular Plant \u662f Plant DNAGPT zhangtaolab CausalLM GPT2 100M 1 Molecular Plant \u662f Plant Nucleotide Transformer zhangtaolab MaskedLM ESM 100M 1 Molecular Plant \u662f Plant DNAGemma zhangtaolab CausalLM Gemma 150M 1 Molecular Plant \u662f Plant DNAMamba zhangtaolab CausalLM Mamba 100M 1 Molecular Plant \u662f Plant DNAModernBert zhangtaolab MaskedLM ModernBert 100M 1 Molecular Plant \u662f"},{"location":"resources/model_zoo/","title":"Model Zoo","text":"<p>DNALLM includes almost all publicly available DNA Large Language Models and some DNA-based deep learning models. We have adapted these models to work seamlessly with the DNALLM package for fine-tuning and inference.</p>"},{"location":"resources/model_zoo/#model-collection","title":"Model Collection","text":"<p>The following table shows all currently supported models and their fine-tuning/inference capabilities:</p> Model Name Author Model Type Architecture Model Size Count Source Fine-tuning Support Nucleotide Transformer InstaDeepAI MaskedLM ESM 50M / 100M / 250M / 500M / 2.5B 8 Nature Methods \u2705 AgroNT InstaDeepAI MaskedLM ESM 1B 1 Current Biology \u2705 Caduceus-Ph Kuleshov-Group MaskedLM Caduceus 0.5M / 2M / 8M 3 arXiv \u2705 Caduceus-Ps Kuleshov-Group MaskedLM Caduceus 0.5M / 2M / 8M 3 arXiv \u2705 PlantCaduceus Kuleshov-Group MaskedLM Caduceus 20M / 40M / 112M / 225M 4 bioRxiv \u2705 DNABERT Zhihan1996 MaskedLM BERT 100M 4 Bioinformatics \u2705 DNABERT-2 Zhihan1996 MaskedLM BERT 117M 1 arXiv \u2705 DNABERT-S Zhihan1996 MaskedLM BERT 117M 1 arXiv \u2705 GENA-LM AIRI-Institute MaskedLM BERT 150M / 500M 7 Nucleic Acids Research \u2705 GENA-LM-BigBird AIRI-Institute MaskedLM BigBird 150M 3 Nucleic Acids Research \u2705 GENERator GenerTeam CausalLM Llama 0.5B / 1.2B / 3B 4 arXiv \u2705 GenomeOcean pGenomeOcean CausalLM Mistral 100M / 500M / 4B 3 bioRxiv \u2705 GPN songlab MaskedLM ConvNet 60M 1 PNAS \u274c GROVER PoetschLab MaskedLM BERT 100M 1 Nature Machine Intelligence \u2705 HyenaDNA LongSafari CausalLM HyenaDNA 0.5M / 0.7M / 2M / 4M / 15M / 30M / 55M 7 arXiv \u2705 Jamba-DNA RaphaelMourad CausalLM Jamba 114M 1 GitHub \u2705 Mistral-DNA RaphaelMourad CausalLM Mistral 1M / 17M / 138M / 417M / 422M 10 GitHub \u2705 ModernBert-DNA RaphaelMourad MaskedLM ModernBert 37M 3 GitHub \u2705 MutBERT JadenLong MaskedLM RoPEBert 86M 3 bioRxiv \u2705 OmniNA XLS CausalLM Llama 66M / 220M 2 bioRxiv \u2705 Omni-DNA zehui127 CausalLM OLMoModel 20M / 60M / 116M / 300M / 700M / 1B 6 arXiv \u274c EVO-1 togethercomputer CausalLM StripedHyena 6.5B 2 GitHub \u274c EVO-2 arcinstitute CausalLM StripedHyena2 1B / 7B / 40B 3 GitHub \u274c ProkBERT neuralbioinfo MaskedLM MegatronBert 21M / 25M / 27M 3 Frontiers in Microbiology \u2705 Plant DNABERT zhangtaolab MaskedLM BERT 100M 1 Molecular Plant \u2705 Plant DNAGPT zhangtaolab CausalLM GPT2 100M 1 Molecular Plant \u2705 Plant Nucleotide Transformer zhangtaolab MaskedLM ESM 100M 1 Molecular Plant \u2705 Plant DNAGemma zhangtaolab CausalLM Gemma 150M 1 Molecular Plant \u2705 Plant DNAMamba zhangtaolab CausalLM Mamba 100M 1 Molecular Plant \u2705 Plant DNAModernBert zhangtaolab MaskedLM ModernBert 100M 1 Molecular Plant \u2705"},{"location":"resources/model_zoo/#model-categories","title":"Model Categories","text":""},{"location":"resources/model_zoo/#by-architecture-type","title":"By Architecture Type","text":""},{"location":"resources/model_zoo/#masked-language-models-mlm","title":"Masked Language Models (MLM)","text":"<ul> <li>BERT-based: DNABERT, DNABERT-2, DNABERT-S, Plant DNABERT, GENA-LM, GROVER, MutBERT, ProkBERT, Plant DNAModernBert</li> <li>ESM-based: Nucleotide Transformer, AgroNT, Plant Nucleotide Transformer</li> <li>Caduceus-based: Caduceus-Ph, Caduceus-Ps, PlantCaduceus</li> <li>Other: GENA-LM-BigBird, GPN</li> </ul>"},{"location":"resources/model_zoo/#causal-language-models-clm","title":"Causal Language Models (CLM)","text":"<ul> <li>Llama-based: GENERator, OmniNA</li> <li>Mistral-based: GenomeOcean, Mistral-DNA</li> <li>Hyena-based: HyenaDNA, EVO-1, EVO-2</li> <li>Other: Jamba-DNA, Plant DNAGPT, Plant DNAGemma, Plant DNAMamba, Omni-DNA</li> </ul>"},{"location":"resources/model_zoo/#by-model-size","title":"By Model Size","text":"Size Category Model Count Examples Small (&lt;100M) 15 Caduceus-Ph, HyenaDNA variants, ModernBert-DNA Medium (100M-1B) 18 DNABERT series, Plant models, GENA-LM Large (1B-10B) 8 Nucleotide Transformer, EVO-1, GENERator Extra Large (&gt;10B) 3 EVO-2 (40B)"},{"location":"resources/model_zoo/#by-source-platform","title":"By Source Platform","text":"Platform Model Count Examples Hugging Face Hub 25+ Most models with direct integration ModelScope 10+ Alternative source for some models GitHub 8 Community-contributed models Academic Journals 15+ Peer-reviewed publications"},{"location":"resources/model_zoo/#usage-guidelines","title":"Usage Guidelines","text":""},{"location":"resources/model_zoo/#fine-tuning-support","title":"Fine-tuning Support","text":"<ul> <li>\u2705 Supported: 35 models with full fine-tuning capabilities</li> <li>\u274c Not Supported: 3 models (GPN, Omni-DNA, EVO-2) - inference only</li> </ul>"},{"location":"resources/model_zoo/#model-selection-tips","title":"Model Selection Tips","text":"<ol> <li>For Classification Tasks: Choose BERT-based models (DNABERT, Plant DNABERT)</li> <li>For Generation Tasks: Use CausalLM models (Plant DNAGPT, GenomeOcean)</li> <li>For Large-scale Analysis: Consider Nucleotide Transformer or EVO models</li> <li>For Plant-specific Tasks: Prefer Plant-prefixed models</li> </ol>"},{"location":"resources/model_zoo/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Small Models (&lt;100M): Fast inference, suitable for real-time applications</li> <li>Medium Models (100M-1B): Good balance of performance and speed</li> <li>Large Models (&gt;1B): Best performance but slower inference</li> </ul>"},{"location":"resources/model_zoo/#getting-started","title":"Getting Started","text":"<p>To use any of these models with DNALLM:</p> <pre><code>from dnallm import load_model_and_tokenizer\n\n# Load a supported model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    source=\"huggingface\"\n)\n\n# For fine-tuning\nfrom dnallm.finetune import DNATrainer\ntrainer = DNATrainer(model=model, tokenizer=tokenizer)\n</code></pre>"},{"location":"resources/model_zoo/#contributing-new-models","title":"Contributing New Models","text":"<p>To add support for new DNA language models:</p> <ol> <li>Ensure the model is publicly available</li> <li>Test compatibility with DNALLM's architecture</li> <li>Submit a pull request with integration code</li> <li>Include proper documentation and examples</li> </ol> <p>For detailed integration instructions, see the Development Guide.</p>"},{"location":"tutorials/benchmark/","title":"Model Benchmarking","text":"<p>This section provides comprehensive tutorials and guides for benchmarking DNA language models using DNALLM. Benchmarking allows you to compare model performance across different tasks, datasets, and evaluation metrics.</p>"},{"location":"tutorials/benchmark/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Basic Benchmarking: Get started with simple model comparisons</li> <li>Advanced Techniques: Cross-validation, custom metrics, and performance profiling</li> <li>Real-world Examples: Practical applications and use cases</li> <li>Best Practices: Optimization strategies and troubleshooting</li> </ul>"},{"location":"tutorials/benchmark/#quick-navigation","title":"Quick Navigation","text":"Topic Description Difficulty Getting Started Basic benchmarking setup and configuration Beginner Advanced Techniques Cross-validation, custom metrics, profiling Intermediate Configuration Guide Detailed configuration options and examples Intermediate Examples and Use Cases Real-world benchmarking scenarios All Levels Troubleshooting Common issues and solutions All Levels"},{"location":"tutorials/benchmark/#prerequisites","title":"Prerequisites","text":"<p>Before diving into benchmarking, ensure you have:</p> <ul> <li>\u2705 DNALLM installed and configured</li> <li>\u2705 Access to DNA language models</li> <li>\u2705 Test datasets in appropriate formats</li> <li>\u2705 Sufficient computational resources</li> </ul>"},{"location":"tutorials/benchmark/#quick-start","title":"Quick Start","text":"<pre><code>from dnallm import load_config, Benchmark\n\n# Load configuration\nconfig = load_config(\"benchmark_config.yaml\")\n\n# Initialize and run benchmark\nbenchmark = Benchmark(config=config)\nresults = benchmark.run()\n</code></pre>"},{"location":"tutorials/benchmark/#key-features","title":"Key Features","text":"<ul> <li>Multi-Model Comparison: Evaluate multiple models simultaneously</li> <li>Comprehensive Metrics: Accuracy, F1, precision, recall, ROC-AUC, and more</li> <li>Performance Profiling: Memory usage, inference time, and resource monitoring</li> <li>Flexible Output: HTML reports, CSV exports, and interactive visualizations</li> <li>Cross-Validation: Robust evaluation with k-fold validation</li> </ul>"},{"location":"tutorials/benchmark/#next-steps","title":"Next Steps","text":"<p>Choose your path:</p> <ul> <li>New to benchmarking? Start with Getting Started</li> <li>Want advanced features? Jump to Advanced Techniques</li> <li>Need configuration help? Check Configuration Guide</li> <li>Looking for examples? Explore Examples and Use Cases</li> </ul> <p>Need Help? Check our FAQ or open an issue on GitHub.</p>"},{"location":"tutorials/benchmark/advanced_techniques/","title":"Advanced Benchmarking Techniques","text":"<p>This guide covers advanced benchmarking techniques including cross-validation, custom metrics, performance profiling, and optimization strategies.</p>"},{"location":"tutorials/benchmark/advanced_techniques/#overview","title":"Overview","text":"<p>Advanced benchmarking techniques help you: - Ensure robust and reliable model evaluation - Implement custom evaluation metrics - Profile and optimize model performance - Handle complex benchmarking scenarios</p>"},{"location":"tutorials/benchmark/advanced_techniques/#cross-validation-benchmarking","title":"Cross-Validation Benchmarking","text":"<p>Cross-validation provides more robust performance estimates by testing models on multiple data splits.</p>"},{"location":"tutorials/benchmark/advanced_techniques/#k-fold-cross-validation","title":"K-Fold Cross-Validation","text":"<pre><code>from sklearn.model_selection import KFold\nimport numpy as np\nfrom dnallm import Benchmark\n\ndef run_cross_validation_benchmark(models, datasets, k_folds=5):\n    \"\"\"Run k-fold cross-validation benchmark.\"\"\"\n\n    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n    cv_results = {}\n\n    for model_name, model_info in models.items():\n        cv_results[model_name] = {}\n\n        for dataset_name, dataset in datasets.items():\n            fold_scores = []\n\n            # Split dataset into k folds\n            for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n                print(f\"Running fold {fold + 1}/{k_folds} for {model_name} on {dataset_name}\")\n\n                # Split data for this fold\n                train_data = dataset.select(train_idx)\n                val_data = dataset.select(val_idx)\n\n                # Evaluate on validation fold\n                fold_result = benchmark.evaluate_single_model(\n                    model_info[\"model\"],\n                    model_info[\"tokenizer\"],\n                    val_data,\n                    metrics=[\"accuracy\", \"f1_score\", \"precision\", \"recall\"]\n                )\n\n                fold_scores.append(fold_result)\n\n            # Aggregate fold results\n            cv_results[model_name][dataset_name] = {\n                \"mean_accuracy\": np.mean([s[\"accuracy\"] for s in fold_scores]),\n                \"std_accuracy\": np.mean([s[\"accuracy\"] for s in fold_scores]),\n                \"mean_f1\": np.mean([s[\"f1_score\"] for s in fold_scores]),\n                \"std_f1\": np.std([s[\"f1_score\"] for s in fold_scores]),\n                \"fold_results\": fold_scores\n            }\n\n    return cv_results\n\n# Usage\ncv_results = run_cross_validation_benchmark(loaded_models, datasets, k_folds=5)\n\n# Display results\nfor model_name, results in cv_results.items():\n    print(f\"\\n{model_name} Cross-Validation Results:\")\n    for dataset_name, metrics in results.items():\n        print(f\"  {dataset_name}:\")\n        print(f\"    Accuracy: {metrics['mean_accuracy']:.4f} \u00b1 {metrics['std_accuracy']:.4f}\")\n        print(f\"    F1 Score: {metrics['mean_f1']:.4f} \u00b1 {metrics['std_f1']:.4f}\")\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#stratified-k-fold-for-imbalanced-data","title":"Stratified K-Fold for Imbalanced Data","text":"<pre><code>from sklearn.model_selection import StratifiedKFold\n\ndef run_stratified_cv_benchmark(models, datasets, k_folds=5):\n    \"\"\"Run stratified k-fold cross-validation for imbalanced datasets.\"\"\"\n\n    stratified_kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n    cv_results = {}\n\n    for model_name, model_info in models.items():\n        cv_results[model_name] = {}\n\n        for dataset_name, dataset in datasets.items():\n            # Get labels for stratification\n            labels = [item[\"label\"] for item in dataset]\n\n            fold_scores = []\n            for fold, (train_idx, val_idx) in enumerate(stratified_kfold.split(dataset, labels)):\n                # ... rest of the implementation similar to above\n                pass\n\n    return cv_results\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#custom-evaluation-metrics","title":"Custom Evaluation Metrics","text":"<p>DNALLM allows you to implement custom evaluation metrics for specific use cases.</p>"},{"location":"tutorials/benchmark/advanced_techniques/#basic-custom-metric","title":"Basic Custom Metric","text":"<pre><code>from dnallm.tasks.metrics import CustomMetric\nimport numpy as np\n\nclass GCContentMetric(CustomMetric):\n    \"\"\"Custom metric to evaluate GC content prediction accuracy.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.name = \"gc_content_accuracy\"\n\n    def compute(self, predictions, targets, sequences=None):\n        \"\"\"Compute GC content prediction accuracy.\"\"\"\n        if sequences is None:\n            return {\"gc_content_accuracy\": 0.0}\n\n        gc_accuracy = []\n        for pred, target, seq in zip(predictions, targets, sequences):\n            # Calculate predicted GC content\n            pred_gc = self._calculate_gc_content(seq, pred)\n            # Calculate actual GC content\n            actual_gc = self._calculate_gc_content(seq, target)\n\n            # Compute accuracy\n            accuracy = 1.0 - abs(pred_gc - actual_gc) / max(actual_gc, 0.01)\n            gc_accuracy.append(max(0.0, accuracy))\n\n        return {\"gc_content_accuracy\": np.mean(gc_accuracy)}\n\n    def _calculate_gc_content(self, sequence, mask):\n        \"\"\"Calculate GC content based on sequence and mask.\"\"\"\n        gc_count = 0\n        total_count = 0\n\n        for i, char in enumerate(sequence):\n            if mask[i] == 1:  # If position is masked\n                if char in ['G', 'C']:\n                    gc_count += 1\n                total_count += 1\n\n        return gc_count / max(total_count, 1)\n\n# Usage in benchmark\nbenchmark = Benchmark(\n    models=loaded_models,\n    datasets=datasets,\n    metrics=[\"accuracy\", \"f1_score\", GCContentMetric()],\n    batch_size=32\n)\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#advanced-custom-metric-with-multiple-outputs","title":"Advanced Custom Metric with Multiple Outputs","text":"<pre><code>class ComprehensiveDNAMetric(CustomMetric):\n    \"\"\"Comprehensive DNA sequence evaluation metric.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.name = \"comprehensive_dna_score\"\n\n    def compute(self, predictions, targets, sequences=None, **kwargs):\n        \"\"\"Compute comprehensive DNA evaluation score.\"\"\"\n        results = {}\n\n        # Base accuracy\n        results[\"base_accuracy\"] = self._compute_accuracy(predictions, targets)\n\n        # Sequence-specific metrics\n        if sequences is not None:\n            results[\"gc_content_score\"] = self._compute_gc_content_score(predictions, targets, sequences)\n            results[\"conservation_score\"] = self._compute_conservation_score(predictions, targets, sequences)\n            results[\"motif_score\"] = self._compute_motif_score(predictions, targets, sequences)\n\n        # Overall score (weighted average)\n        weights = [0.4, 0.2, 0.2, 0.2]  # Adjust weights as needed\n        scores = [results[\"base_accuracy\"], results[\"gc_content_score\"], \n                 results[\"conservation_score\"], results[\"motif_score\"]]\n\n        results[\"overall_score\"] = np.average(scores, weights=weights)\n\n        return results\n\n    def _compute_accuracy(self, predictions, targets):\n        \"\"\"Compute basic accuracy.\"\"\"\n        return np.mean(np.array(predictions) == np.array(targets))\n\n    def _compute_gc_content_score(self, predictions, targets, sequences):\n        \"\"\"Compute GC content prediction score.\"\"\"\n        # Implementation details...\n        return 0.85\n\n    def _compute_conservation_score(self, predictions, targets, sequences):\n        \"\"\"Compute conservation prediction score.\"\"\"\n        # Implementation details...\n        return 0.78\n\n    def _compute_motif_score(self, predictions, targets, sequences):\n        \"\"\"Compute motif prediction score.\"\"\"\n        # Implementation details...\n        return 0.92\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#performance-profiling","title":"Performance Profiling","text":"<p>Performance profiling helps you understand model efficiency and identify bottlenecks.</p>"},{"location":"tutorials/benchmark/advanced_techniques/#basic-performance-profiling","title":"Basic Performance Profiling","text":"<pre><code>import time\nimport psutil\nimport torch\nfrom memory_profiler import profile\n\ndef profile_model_performance(model, tokenizer, dataset, num_samples=100):\n    \"\"\"Profile model performance including time and memory usage.\"\"\"\n\n    # Select subset for profiling\n    profile_data = dataset.select(range(min(num_samples, len(dataset))))\n\n    # Warm up (important for accurate timing)\n    print(\"Warming up model...\")\n    for _ in range(10):\n        _ = model(torch.randn(1, 512).to(model.device))\n\n    # Memory profiling\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        memory_before = torch.cuda.memory_allocated()\n        memory_reserved_before = torch.cuda.memory_reserved()\n\n    # Time profiling\n    print(\"Running performance profiling...\")\n    start_time = time.time()\n\n    predictions = []\n    batch_times = []\n\n    for i, batch in enumerate(profile_data.get_dataloader(batch_size=1)):\n        batch_start = time.time()\n\n        with torch.no_grad():\n            outputs = model(batch[\"input_ids\"].to(model.device))\n            predictions.append(outputs.logits.argmax(-1).cpu())\n\n        batch_time = time.time() - batch_start\n        batch_times.append(batch_time)\n\n        if i % 20 == 0:\n            print(f\"Processed {i+1}/{num_samples} samples...\")\n\n    total_time = time.time() - start_time\n\n    # Memory after\n    if torch.cuda.is_available():\n        memory_after = torch.cuda.memory_allocated()\n        memory_reserved_after = torch.cuda.memory_reserved()\n        memory_used = memory_after - memory_before\n        memory_reserved = memory_reserved_after - memory_reserved_before\n\n    # CPU profiling\n    cpu_percent = psutil.cpu_percent(interval=1)\n\n    # Calculate statistics\n    avg_batch_time = np.mean(batch_times)\n    std_batch_time = np.std(batch_times)\n\n    return {\n        \"total_inference_time\": total_time,\n        \"avg_batch_time\": avg_batch_time,\n        \"std_batch_time\": std_batch_time,\n        \"samples_per_second\": num_samples / total_time,\n        \"memory_used_mb\": memory_used / 1024 / 1024 if torch.cuda.is_available() else 0,\n        \"memory_reserved_mb\": memory_reserved / 1024 / 1024 if torch.cuda.is_available() else 0,\n        \"cpu_usage_percent\": cpu_percent,\n        \"throughput\": num_samples / total_time\n    }\n\n# Profile all models\nperformance_profiles = {}\nfor model_name, model_info in loaded_models.items():\n    print(f\"\\nProfiling {model_name}...\")\n    performance_profiles[model_name] = profile_model_performance(\n        model_info[\"model\"],\n        model_info[\"tokenizer\"],\n        datasets[\"promoter_strength\"],\n        num_samples=200\n    )\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#advanced-memory-profiling","title":"Advanced Memory Profiling","text":"<pre><code>import tracemalloc\nfrom contextlib import contextmanager\n\n@contextmanager\ndef memory_profiler():\n    \"\"\"Context manager for detailed memory profiling.\"\"\"\n    tracemalloc.start()\n    try:\n        yield\n    finally:\n        current, peak = tracemalloc.get_traced_memory()\n        print(f\"Current memory usage: {current / 1024 / 1024:.2f} MB\")\n        print(f\"Peak memory usage: {peak / 1024 / 1024:.2f} MB\")\n        tracemalloc.stop()\n\ndef detailed_memory_profile(model, dataset, batch_size=32):\n    \"\"\"Detailed memory profiling with tracemalloc.\"\"\"\n\n    with memory_profiler():\n        # Load data\n        dataloader = dataset.get_dataloader(batch_size=batch_size)\n\n        # Run inference\n        for batch in dataloader:\n            with torch.no_grad():\n                outputs = model(batch[\"input_ids\"].to(model.device))\n\n            # Force garbage collection\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"tutorials/benchmark/advanced_techniques/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>from torch.cuda.amp import autocast, GradScaler\n\ndef benchmark_with_mixed_precision(model, tokenizer, dataset):\n    \"\"\"Benchmark model with mixed precision for improved performance.\"\"\"\n\n    # Enable mixed precision\n    scaler = GradScaler()\n\n    start_time = time.time()\n    predictions = []\n\n    for batch in dataset.get_dataloader(batch_size=32):\n        with autocast():\n            outputs = model(batch[\"input_ids\"].to(model.device))\n            predictions.append(outputs.logits.argmax(-1).cpu())\n\n    mixed_precision_time = time.time() - start_time\n\n    # Compare with full precision\n    start_time = time.time()\n    predictions_fp32 = []\n\n    for batch in dataset.get_dataloader(batch_size=32):\n        outputs = model(batch[\"input_ids\"].to(model.device))\n        predictions_fp32.append(outputs.logits.argmax(-1).cpu())\n\n    fp32_time = time.time() - start_time\n\n    return {\n        \"mixed_precision_time\": mixed_precision_time,\n        \"fp32_time\": fp32_time,\n        \"speedup\": fp32_time / mixed_precision_time,\n        \"memory_savings\": \"~50% (estimated)\"\n    }\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#batch-size-optimization","title":"Batch Size Optimization","text":"<pre><code>def find_optimal_batch_size(model, dataset, max_batch_size=128):\n    \"\"\"Find optimal batch size for given model and hardware.\"\"\"\n\n    batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128]\n    results = {}\n\n    for batch_size in batch_sizes:\n        if batch_size &gt; max_batch_size:\n            continue\n\n        try:\n            # Test batch size\n            start_time = time.time()\n            memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n\n            # Run inference\n            dataloader = dataset.get_dataloader(batch_size=batch_size)\n            for batch in dataloader:\n                with torch.no_grad():\n                    outputs = model(batch[\"input_ids\"].to(model.device))\n                break  # Just test one batch\n\n            inference_time = time.time() - start_time\n            memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n\n            results[batch_size] = {\n                \"inference_time\": inference_time,\n                \"memory_used\": (memory_after - memory_before) / 1024 / 1024,\n                \"throughput\": batch_size / inference_time\n            }\n\n            print(f\"Batch size {batch_size}: {results[batch_size]}\")\n\n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"Batch size {batch_size} failed: OOM\")\n                break\n            else:\n                print(f\"Batch size {batch_size} failed: {e}\")\n\n    # Find optimal batch size\n    optimal_batch_size = max(results.keys(), key=lambda x: results[x][\"throughput\"])\n\n    return optimal_batch_size, results\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#advanced-benchmarking-scenarios","title":"Advanced Benchmarking Scenarios","text":""},{"location":"tutorials/benchmark/advanced_techniques/#multi-dataset-benchmarking","title":"Multi-Dataset Benchmarking","text":"<pre><code>def run_multi_dataset_benchmark(models, datasets, metrics):\n    \"\"\"Run benchmark across multiple datasets with aggregated results.\"\"\"\n\n    all_results = {}\n\n    for model_name, model_info in models.items():\n        all_results[model_name] = {\n            \"dataset_results\": {},\n            \"aggregated_metrics\": {}\n        }\n\n        # Run on each dataset\n        for dataset_name, dataset in datasets.items():\n            dataset_result = benchmark.evaluate_single_model(\n                model_info[\"model\"],\n                model_info[\"tokenizer\"],\n                dataset,\n                metrics=metrics\n            )\n\n            all_results[model_name][\"dataset_results\"][dataset_name] = dataset_result\n\n        # Aggregate across datasets\n        all_results[model_name][\"aggregated_metrics\"] = aggregate_metrics(\n            all_results[model_name][\"dataset_results\"]\n        )\n\n    return all_results\n\ndef aggregate_metrics(dataset_results):\n    \"\"\"Aggregate metrics across multiple datasets.\"\"\"\n    aggregated = {}\n\n    for metric in dataset_results[list(dataset_results.keys())[0]].keys():\n        values = [dataset_results[ds][metric] for ds in dataset_results.keys()]\n        aggregated[f\"{metric}_mean\"] = np.mean(values)\n        aggregated[f\"{metric}_std\"] = np.std(values)\n        aggregated[f\"{metric}_min\"] = np.min(values)\n        aggregated[f\"{metric}_max\"] = np.max(values)\n\n    return aggregated\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#time-series-benchmarking","title":"Time-Series Benchmarking","text":"<pre><code>def run_time_series_benchmark(model, dataset, time_column, interval_days=30):\n    \"\"\"Run benchmark on time-series data with temporal splits.\"\"\"\n\n    # Sort by time\n    sorted_data = sorted(dataset, key=lambda x: x[time_column])\n\n    # Create temporal splits\n    total_days = (sorted_data[-1][time_column] - sorted_data[0][time_column]).days\n    num_splits = total_days // interval_days\n\n    temporal_results = []\n\n    for i in range(num_splits):\n        start_idx = i * len(sorted_data) // num_splits\n        end_idx = (i + 1) * len(sorted_data) // num_splits\n\n        # Test on future data\n        test_data = sorted_data[end_idx:]\n        if len(test_data) == 0:\n            continue\n\n        # Evaluate performance\n        result = benchmark.evaluate_single_model(\n            model, tokenizer, test_data, metrics=[\"accuracy\", \"f1_score\"]\n        )\n\n        temporal_results.append({\n            \"time_period\": i,\n            \"start_date\": sorted_data[start_idx][time_column],\n            \"end_date\": sorted_data[end_idx][time_column],\n            \"test_size\": len(test_data),\n            **result\n        })\n\n    return temporal_results\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/benchmark/advanced_techniques/#1-reproducibility","title":"1. Reproducibility","text":"<pre><code># Set random seeds\nimport random\nimport numpy as np\nimport torch\n\ndef set_reproducibility(seed=42):\n    \"\"\"Set all random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Use in benchmark\nset_reproducibility(42)\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#2-resource-management","title":"2. Resource Management","text":"<pre><code>def cleanup_resources():\n    \"\"\"Clean up GPU memory and other resources.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n    import gc\n    gc.collect()\n\n# Call between model evaluations\nfor model_name, model_info in loaded_models.items():\n    # Run benchmark\n    result = benchmark.evaluate_single_model(...)\n\n    # Clean up\n    cleanup_resources()\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#3-progress-monitoring","title":"3. Progress Monitoring","text":"<pre><code>from tqdm import tqdm\nimport logging\n\ndef setup_logging():\n    \"\"\"Setup logging for benchmark progress.\"\"\"\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler('benchmark.log'),\n            logging.StreamHandler()\n        ]\n    )\n\n# Use in benchmark\nsetup_logging()\nlogger = logging.getLogger(__name__)\n\nfor model_name in tqdm(loaded_models.keys(), desc=\"Benchmarking models\"):\n    logger.info(f\"Starting benchmark for {model_name}\")\n    # ... benchmark code\n    logger.info(f\"Completed benchmark for {model_name}\")\n</code></pre>"},{"location":"tutorials/benchmark/advanced_techniques/#next-steps","title":"Next Steps","text":"<p>After mastering these advanced techniques:</p> <ol> <li>Explore Real-world Examples: See Examples and Use Cases</li> <li>Learn Configuration Options: Check Configuration Guide</li> <li>Troubleshoot Issues: Visit Troubleshooting</li> <li>Contribute: Help improve DNALLM's benchmarking capabilities</li> </ol> <p>Ready for real-world examples? Continue to Examples and Use Cases to see these techniques in action.</p>"},{"location":"tutorials/benchmark/configuration/","title":"Configuration Guide","text":"<p>This guide provides detailed information about all configuration options available for DNALLM benchmarking, including examples and best practices.</p>"},{"location":"tutorials/benchmark/configuration/#overview","title":"Overview","text":"<p>DNALLM benchmarking configuration is defined in YAML format and supports: - Model Configuration: Multiple models from different sources - Dataset Configuration: Various data formats and preprocessing options - Evaluation Settings: Metrics, batch sizes, and hardware options - Output Options: Report formats and visualization settings</p>"},{"location":"tutorials/benchmark/configuration/#configuration-structure","title":"Configuration Structure","text":""},{"location":"tutorials/benchmark/configuration/#basic-configuration-schema","title":"Basic Configuration Schema","text":"<pre><code>benchmark:\n  # Basic information\n  name: \"string\"\n  description: \"string\"\n\n  # Model definitions\n  models: []\n\n  # Dataset definitions\n  datasets: []\n\n  # Evaluation settings\n  evaluation: {}\n\n  # Output configuration\n  output: {}\n\n  # Advanced options\n  advanced: {}\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#model-configuration","title":"Model Configuration","text":""},{"location":"tutorials/benchmark/configuration/#basic-model-definition","title":"Basic Model Definition","text":"<pre><code>models:\n  - name: \"Plant DNABERT\"\n    path: \"zhangtaolab/plant-dnabert-BPE\"\n    source: \"huggingface\"\n    task_type: \"classification\"\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#advanced-model-configuration","title":"Advanced Model Configuration","text":"<pre><code>models:\n  - name: \"Plant DNABERT\"\n    path: \"zhangtaolab/plant-dnabert-BPE\"\n    source: \"huggingface\"\n    task_type: \"classification\"\n    revision: \"main\"  # Git branch/tag\n    trust_remote_code: true\n    torch_dtype: \"float16\"  # or \"float32\", \"bfloat16\"\n    device_map: \"auto\"\n    load_in_8bit: false\n    load_in_4bit: false\n\n  - name: \"Custom Model\"\n    path: \"/path/to/local/model\"\n    source: \"local\"\n    task_type: \"generation\"\n    model_class: \"CustomModelClass\"\n    tokenizer_class: \"CustomTokenizerClass\"\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#model-source-types","title":"Model Source Types","text":"Source Description Example <code>huggingface</code> Hugging Face Hub <code>\"zhangtaolab/plant-dnabert-BPE\"</code> <code>modelscope</code> ModelScope repository <code>\"zhangtaolab/plant-dnabert-BPE\"</code> <code>local</code> Local file system <code>\"/path/to/model\"</code> <code>s3</code> AWS S3 bucket <code>\"s3://bucket/model\"</code>"},{"location":"tutorials/benchmark/configuration/#task-types","title":"Task Types","text":"Task Type Description Use Case <code>classification</code> Binary/multi-class classification Promoter prediction, motif detection <code>generation</code> Sequence generation DNA synthesis, sequence design <code>masked</code> Masked language modeling Sequence completion, mutation analysis <code>embedding</code> Feature extraction Sequence representation, similarity <code>regression</code> Continuous value prediction Expression level, binding affinity"},{"location":"tutorials/benchmark/configuration/#dataset-configuration","title":"Dataset Configuration","text":""},{"location":"tutorials/benchmark/configuration/#basic-dataset-definition","title":"Basic Dataset Definition","text":"<pre><code>datasets:\n  - name: \"promoter_data\"\n    path: \"path/to/promoter_data.csv\"\n    task: \"binary_classification\"\n    text_column: \"sequence\"\n    label_column: \"label\"\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#advanced-dataset-configuration","title":"Advanced Dataset Configuration","text":"<pre><code>datasets:\n  - name: \"promoter_data\"\n    path: \"path/to/promoter_data.csv\"\n    task: \"binary_classification\"\n    text_column: \"sequence\"\n    label_column: \"label\"\n\n    # Preprocessing options\n    max_length: 512\n    truncation: true\n    padding: \"max_length\"\n\n    # Data splitting\n    test_size: 0.2\n    val_size: 0.1\n    random_state: 42\n\n    # Data filtering\n    min_length: 100\n    max_length: 1000\n    valid_chars: \"ACGT\"\n\n    # Data augmentation\n    augment: true\n    reverse_complement_ratio: 0.5\n    random_mutation_ratio: 0.1\n\n    # Custom preprocessing\n    preprocessors:\n      - \"remove_n_bases\"\n      - \"normalize_case\"\n      - \"add_padding\"\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#dataset-formats","title":"Dataset Formats","text":""},{"location":"tutorials/benchmark/configuration/#csvtsv-format","title":"CSV/TSV Format","text":"<pre><code>datasets:\n  - name: \"csv_dataset\"\n    path: \"data.csv\"\n    format: \"csv\"\n    separator: \",\"  # or \"\\t\" for TSV\n    encoding: \"utf-8\"\n    text_column: \"sequence\"\n    label_column: \"label\"\n    additional_columns: [\"metadata\", \"source\"]\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#json-format","title":"JSON Format","text":"<pre><code>datasets:\n  - name: \"json_dataset\"\n    path: \"data.json\"\n    format: \"json\"\n    text_key: \"sequence\"\n    label_key: \"label\"\n    nested_path: \"data.items\"  # For nested JSON structures\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#fasta-format","title":"FASTA Format","text":"<pre><code>datasets:\n  - name: \"fasta_dataset\"\n    path: \"sequences.fasta\"\n    format: \"fasta\"\n    label_parser: \"header\"  # Extract label from header\n    header_format: \"sequence_id|label:value\"  # Custom header format\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#arrowparquet-format","title":"Arrow/Parquet Format","text":"<pre><code>datasets:\n  - name: \"arrow_dataset\"\n    path: \"data.arrow\"\n    format: \"arrow\"\n    text_column: \"sequence\"\n    label_column: \"label\"\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#data-preprocessing-options","title":"Data Preprocessing Options","text":"<pre><code>datasets:\n  - name: \"processed_data\"\n    path: \"raw_data.csv\"\n\n    # Sequence processing\n    preprocessing:\n      remove_n_bases: true\n      normalize_case: true\n      add_padding: true\n      padding_size: 512\n\n    # Quality filtering\n    filtering:\n      min_length: 200\n      max_length: 1000\n      min_gc_content: 0.2\n      max_gc_content: 0.8\n      valid_chars: \"ACGT\"\n\n    # Data augmentation\n    augmentation:\n      reverse_complement: true\n      random_mutations: true\n      mutation_rate: 0.01\n      synthetic_samples: 1000\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#evaluation-configuration","title":"Evaluation Configuration","text":""},{"location":"tutorials/benchmark/configuration/#basic-evaluation-settings","title":"Basic Evaluation Settings","text":"<pre><code>evaluation:\n  batch_size: 32\n  max_length: 512\n  device: \"cuda\"\n  num_workers: 4\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#advanced-evaluation-options","title":"Advanced Evaluation Options","text":"<pre><code>evaluation:\n  # Batch processing\n  batch_size: 32\n  gradient_accumulation_steps: 1\n\n  # Sequence processing\n  max_length: 512\n  truncation: true\n  padding: \"max_length\"\n\n  # Hardware settings\n  device: \"cuda\"  # or \"cpu\", \"auto\"\n  num_workers: 4\n  pin_memory: true\n\n  # Performance optimization\n  use_fp16: true\n  use_bf16: false\n  mixed_precision: true\n\n  # Memory management\n  max_memory: \"16GB\"\n  memory_efficient_attention: true\n\n  # Reproducibility\n  seed: 42\n  deterministic: true\n\n  # Evaluation strategy\n  eval_strategy: \"steps\"  # or \"epoch\"\n  eval_steps: 100\n  eval_accumulation_steps: 1\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#device-configuration","title":"Device Configuration","text":"<pre><code>evaluation:\n  # Single GPU\n  device: \"cuda:0\"\n\n  # Multiple GPUs\n  device: \"cuda\"\n  parallel_strategy: \"data_parallel\"\n\n  # CPU only\n  device: \"cpu\"\n  num_threads: 8\n\n  # Auto device selection\n  device: \"auto\"\n  device_map: \"auto\"\n\n  # Mixed precision\n  use_fp16: true\n  use_bf16: false\n  mixed_precision: true\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#metrics-configuration","title":"Metrics Configuration","text":""},{"location":"tutorials/benchmark/configuration/#basic-metrics","title":"Basic Metrics","text":"<pre><code>metrics:\n  - \"accuracy\"\n  - \"f1_score\"\n  - \"precision\"\n  - \"recall\"\n  - \"roc_auc\"\n  - \"mse\"\n  - \"mae\"\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#advanced-metrics","title":"Advanced Metrics","text":"<pre><code>metrics:\n  # Classification metrics\n  - \"accuracy\"\n  - \"f1_score\"\n  - \"precision\"\n  - \"recall\"\n  - \"roc_auc\"\n  - \"pr_auc\"\n  - \"matthews_correlation\"\n\n  # Regression metrics\n  - \"mse\"\n  - \"mae\"\n  - \"rmse\"\n  - \"r2_score\"\n  - \"pearson_correlation\"\n  - \"spearman_correlation\"\n\n  # Custom metrics\n  - name: \"gc_content_accuracy\"\n    class: \"GCContentMetric\"\n    parameters:\n      threshold: 0.1\n\n  - name: \"conservation_score\"\n    class: \"ConservationMetric\"\n    parameters:\n      window_size: 10\n      similarity_threshold: 0.8\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#custom-metric-configuration","title":"Custom Metric Configuration","text":"<pre><code>metrics:\n  - name: \"custom_dna_metric\"\n    class: \"CustomDNAMetric\"\n    parameters:\n      gc_weight: 0.3\n      conservation_weight: 0.4\n      motif_weight: 0.3\n      threshold: 0.5\n    file_path: \"path/to/custom_metric.py\"\n    class_name: \"CustomDNAMetric\"\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#output-configuration","title":"Output Configuration","text":""},{"location":"tutorials/benchmark/configuration/#basic-output-settings","title":"Basic Output Settings","text":"<pre><code>output:\n  format: \"html\"\n  path: \"benchmark_results\"\n  save_predictions: true\n  generate_plots: true\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#advanced-output-options","title":"Advanced Output Options","text":"<pre><code>output:\n  # Output formats\n  formats: [\"html\", \"csv\", \"json\", \"pdf\"]\n\n  # File paths\n  path: \"benchmark_results\"\n  predictions_file: \"predictions.csv\"\n  metrics_file: \"metrics.json\"\n  plots_dir: \"plots\"\n\n  # Content options\n  save_predictions: true\n  save_embeddings: false\n  save_attention_maps: false\n  save_token_probabilities: false\n\n  # Visualization\n  generate_plots: true\n  plot_types: [\"bar\", \"line\", \"heatmap\", \"scatter\"]\n  plot_style: \"seaborn\"\n  plot_colors: [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n\n  # Report customization\n  report_title: \"DNA Model Benchmark Report\"\n  report_description: \"Comprehensive comparison of DNA language models\"\n  include_summary: true\n  include_details: true\n  include_recommendations: true\n\n  # Export options\n  export_predictions: true\n  export_metrics: true\n  export_config: true\n  export_logs: true\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#report-customization","title":"Report Customization","text":"<pre><code>output:\n  report:\n    title: \"DNA Model Benchmark Report\"\n    subtitle: \"Performance Comparison on Promoter Prediction\"\n    author: \"Your Name\"\n    date: \"auto\"\n\n    # Sections to include\n    sections:\n      - \"executive_summary\"\n      - \"model_overview\"\n      - \"dataset_description\"\n      - \"results_summary\"\n      - \"detailed_results\"\n      - \"performance_analysis\"\n      - \"recommendations\"\n      - \"appendix\"\n\n    # Custom styling\n    styling:\n      theme: \"modern\"\n      color_scheme: \"blue\"\n      font_family: \"Arial\"\n      font_size: 12\n\n    # Interactive elements\n    interactive:\n      enable_zoom: true\n      enable_hover: true\n      enable_selection: true\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"tutorials/benchmark/configuration/#cross-validation-settings","title":"Cross-Validation Settings","text":"<pre><code>advanced:\n  cross_validation:\n    enabled: true\n    method: \"k_fold\"  # or \"stratified_k_fold\", \"time_series_split\"\n    n_splits: 5\n    shuffle: true\n    random_state: 42\n\n    # Stratified options\n    stratification:\n      enabled: true\n      column: \"label\"\n      bins: 10\n\n    # Time series options\n    time_series:\n      column: \"date\"\n      test_size: 0.2\n      gap: 0\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#performance-profiling","title":"Performance Profiling","text":"<pre><code>advanced:\n  performance_profiling:\n    enabled: true\n\n    # Memory profiling\n    memory:\n      track_gpu: true\n      track_cpu: true\n      track_peak: true\n      profile_allocations: true\n\n    # Time profiling\n    timing:\n      track_inference: true\n      track_preprocessing: true\n      track_postprocessing: true\n      warmup_runs: 10\n\n    # Resource monitoring\n    resources:\n      track_cpu_usage: true\n      track_gpu_usage: true\n      track_io: true\n      sampling_interval: 0.1\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#custom-evaluation-pipeline","title":"Custom Evaluation Pipeline","text":"<pre><code>advanced:\n  custom_pipeline:\n    enabled: true\n    pipeline_file: \"path/to/custom_pipeline.py\"\n\n    # Pipeline steps\n    steps:\n      - name: \"data_preprocessing\"\n        function: \"custom_preprocess\"\n        parameters:\n          normalize: true\n          augment: false\n\n      - name: \"model_evaluation\"\n        function: \"custom_evaluate\"\n        parameters:\n          metric: \"custom_metric\"\n          threshold: 0.5\n\n      - name: \"result_aggregation\"\n        function: \"custom_aggregate\"\n        parameters:\n          method: \"weighted_average\"\n          weights: [0.4, 0.3, 0.3]\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"tutorials/benchmark/configuration/#complete-example-promoter-prediction","title":"Complete Example: Promoter Prediction","text":"<pre><code>benchmark:\n  name: \"Promoter Prediction Benchmark\"\n  description: \"Comparing DNA language models on promoter prediction tasks\"\n\n  models:\n    - name: \"Plant DNABERT\"\n      path: \"zhangtaolab/plant-dnabert-BPE\"\n      source: \"huggingface\"\n      task_type: \"classification\"\n\n    - name: \"Plant DNAGPT\"\n      path: \"zhangtaolab/plant-dnagpt-BPE\"\n      source: \"huggingface\"\n      task_type: \"generation\"\n\n    - name: \"Nucleotide Transformer\"\n      path: \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n      source: \"huggingface\"\n      task_type: \"classification\"\n\n  datasets:\n    - name: \"promoter_strength\"\n      path: \"data/promoter_strength.csv\"\n      task: \"binary_classification\"\n      text_column: \"sequence\"\n      label_column: \"label\"\n      max_length: 512\n      test_size: 0.2\n      val_size: 0.1\n\n    - name: \"open_chromatin\"\n      path: \"data/open_chromatin.csv\"\n      task: \"binary_classification\"\n      text_column: \"sequence\"\n      label_column: \"label\"\n      max_length: 512\n\n  metrics:\n    - \"accuracy\"\n    - \"f1_score\"\n    - \"precision\"\n    - \"recall\"\n    - \"roc_auc\"\n    - name: \"gc_content_accuracy\"\n      class: \"GCContentMetric\"\n\n  evaluation:\n    batch_size: 32\n    max_length: 512\n    device: \"cuda\"\n    num_workers: 4\n    use_fp16: true\n    seed: 42\n\n  output:\n    format: \"html\"\n    path: \"promoter_benchmark_results\"\n    save_predictions: true\n    generate_plots: true\n    report_title: \"Promoter Prediction Model Comparison\"\n\n  advanced:\n    cross_validation:\n      enabled: true\n      method: \"stratified_k_fold\"\n      n_splits: 5\n\n    performance_profiling:\n      enabled: true\n      memory:\n        track_gpu: true\n        track_peak: true\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#minimal-example","title":"Minimal Example","text":"<pre><code>benchmark:\n  name: \"Quick Model Test\"\n\n  models:\n    - name: \"Test Model\"\n      path: \"zhangtaolab/plant-dnabert-BPE\"\n      source: \"huggingface\"\n      task_type: \"classification\"\n\n  datasets:\n    - name: \"test_data\"\n      path: \"test.csv\"\n      task: \"binary_classification\"\n      text_column: \"sequence\"\n      label_column: \"label\"\n\n  metrics:\n    - \"accuracy\"\n    - \"f1_score\"\n\n  evaluation:\n    batch_size: 16\n    device: \"cuda\"\n\n  output:\n    format: \"csv\"\n    path: \"quick_test_results\"\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#configuration-validation","title":"Configuration Validation","text":""},{"location":"tutorials/benchmark/configuration/#schema-validation","title":"Schema Validation","text":"<p>DNALLM automatically validates your configuration:</p> <pre><code>from dnallm import validate_config\n\n# Validate configuration\ntry:\n    validate_config(\"benchmark_config.yaml\")\n    print(\"Configuration is valid!\")\nexcept ValidationError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#common-validation-errors","title":"Common Validation Errors","text":"Error Cause Solution <code>Model not found</code> Invalid model path Check model exists on specified source <code>Invalid task type</code> Unsupported task Use supported task types <code>Missing required field</code> Incomplete configuration Add missing required fields <code>Invalid metric name</code> Unknown metric Use supported metric names <code>Path not found</code> Invalid file path Check file exists and is accessible"},{"location":"tutorials/benchmark/configuration/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/benchmark/configuration/#1-configuration-organization","title":"1. Configuration Organization","text":"<pre><code># Use descriptive names\nbenchmark:\n  name: \"Comprehensive DNA Model Evaluation 2024\"\n\n# Group related settings\nevaluation:\n  # Hardware settings\n  device: \"cuda\"\n  num_workers: 4\n\n  # Performance settings\n  batch_size: 32\n  use_fp16: true\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#2-environment-specific-configs","title":"2. Environment-Specific Configs","text":"<pre><code># Development config\nevaluation:\n  batch_size: 8\n  device: \"cpu\"\n\n# Production config  \nevaluation:\n  batch_size: 64\n  device: \"cuda\"\n  use_fp16: true\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#3-version-control","title":"3. Version Control","text":"<pre><code># Include version information\nbenchmark:\n  version: \"1.0.0\"\n  config_version: \"2024.1\"\n  created_by: \"Your Name\"\n  created_date: \"2024-01-15\"\n</code></pre>"},{"location":"tutorials/benchmark/configuration/#next-steps","title":"Next Steps","text":"<p>After configuring your benchmark:</p> <ol> <li>Run Your Benchmark: Follow the Getting Started guide</li> <li>Explore Advanced Features: Learn about Advanced Techniques</li> <li>See Real Examples: Check Examples and Use Cases</li> <li>Troubleshoot Issues: Visit Troubleshooting</li> </ol> <p>Need help with configuration? Check our FAQ or open an issue on GitHub.</p>"},{"location":"tutorials/benchmark/examples/","title":"Examples and Use Cases","text":"<p>This guide provides real-world examples and practical use cases for DNALLM benchmarking, demonstrating how to apply the concepts learned in previous sections.</p>"},{"location":"tutorials/benchmark/examples/#overview","title":"Overview","text":"<p>The examples in this guide cover: - Research Applications: Academic model comparison and evaluation - Production Use Cases: Model selection for deployment - Performance Analysis: Optimization and profiling scenarios - Custom Scenarios: Specialized benchmarking requirements</p>"},{"location":"tutorials/benchmark/examples/#research-applications","title":"Research Applications","text":""},{"location":"tutorials/benchmark/examples/#example-1-academic-model-comparison","title":"Example 1: Academic Model Comparison","text":"<p>Scenario: Comparing multiple DNA language models for publication in a research paper.</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom dnallm import Benchmark\n\n# Define research models\nresearch_models = [\n    {\n        \"name\": \"Plant DNABERT\",\n        \"path\": \"zhangtaolab/plant-dnabert-BPE\",\n        \"source\": \"huggingface\",\n        \"task_type\": \"classification\"\n    },\n    {\n        \"name\": \"Nucleotide Transformer\",\n        \"path\": \"InstaDeepAI/nucleotide-transformer-500m-human-ref\",\n        \"source\": \"huggingface\",\n        \"task_type\": \"classification\"\n    },\n    {\n        \"name\": \"DNABERT-2\",\n        \"path\": \"zhangtaolab/DNABERT-2\",\n        \"source\": \"huggingface\",\n        \"task_type\": \"classification\"\n    }\n]\n\n# Load research datasets\ndatasets = {\n    \"promoter_prediction\": DNADataset.load_local_data(\n        \"data/promoter_prediction.csv\",\n        seq_col=\"sequence\",\n        label_col=\"label\",\n        max_length=512\n    ),\n    \"motif_detection\": DNADataset.load_local_data(\n        \"data/motif_detection.csv\",\n        seq_col=\"sequence\",\n        label_col=\"label\",\n        max_length=512\n    )\n}\n\n# Run comprehensive benchmark with cross-validation\nbenchmark = Benchmark(\n    models=research_models,\n    datasets=datasets,\n    metrics=[\"accuracy\", \"f1_score\", \"precision\", \"recall\", \"roc_auc\"],\n    batch_size=32,\n    device=\"cuda\"\n)\n\n# Execute with cross-validation\nresults = benchmark.run_with_cross_validation(k_folds=5)\n\n# Generate publication-ready results\npublication_results = generate_publication_results(results)\ncreate_publication_plots(publication_results)\nexport_research_results(publication_results, \"research_paper_results\")\n</code></pre>"},{"location":"tutorials/benchmark/examples/#example-2-cross-species-model-evaluation","title":"Example 2: Cross-Species Model Evaluation","text":"<p>Scenario: Evaluating how well models trained on one species perform on related species.</p> <pre><code>def run_cross_species_benchmark():\n    \"\"\"Evaluate model performance across different species.\"\"\"\n\n    # Define species-specific datasets\n    species_datasets = {\n        \"Arabidopsis_thaliana\": \"data/arabidopsis_promoters.csv\",\n        \"Oryza_sativa\": \"data/rice_promoters.csv\",\n        \"Zea_mays\": \"data/maize_promoters.csv\"\n    }\n\n    # Load datasets\n    datasets = {}\n    for species, path in species_datasets.items():\n        datasets[species] = DNADataset.load_local_data(\n            path,\n            seq_col=\"sequence\",\n            label_col=\"label\",\n            max_length=512\n        )\n\n    # Run cross-species evaluation\n    cross_species_results = {}\n\n    for model_name, model_info in research_models.items():\n        cross_species_results[model_name] = {}\n\n        for source_species in species_datasets.keys():\n            for target_species in species_datasets.keys():\n                if source_species == target_species:\n                    continue\n\n                # Evaluate model trained on source species on target species\n                result = benchmark.evaluate_single_model(\n                    model_info[\"model\"],\n                    model_info[\"tokenizer\"],\n                    datasets[target_species],\n                    metrics=[\"accuracy\", \"f1_score\"]\n                )\n\n                cross_species_results[model_name][f\"{source_species}_to_{target_species}\"] = result\n\n    return cross_species_results\n\n# Run cross-species benchmark\ncross_species_results = run_cross_species_benchmark()\nanalyze_transfer_learning(cross_species_results)\n</code></pre>"},{"location":"tutorials/benchmark/examples/#production-use-cases","title":"Production Use Cases","text":""},{"location":"tutorials/benchmark/examples/#example-3-model-selection-for-production","title":"Example 3: Model Selection for Production","text":"<p>Scenario: Choosing the best model for deployment in a production environment.</p> <pre><code>def production_model_selection():\n    \"\"\"Select the best model for production deployment.\"\"\"\n\n    # Define production criteria\n    production_criteria = {\n        \"accuracy_threshold\": 0.85,\n        \"inference_time_threshold\": 0.1,  # seconds per sequence\n        \"memory_threshold\": 8,  # GB\n        \"cost_threshold\": 100  # dollars per month\n    }\n\n    # Production datasets (representative of real-world usage)\n    production_datasets = {\n        \"high_throughput\": load_production_dataset(\"data/high_throughput.csv\"),\n        \"edge_device\": load_production_dataset(\"data/edge_device.csv\"),\n        \"real_time\": load_production_dataset(\"data/real_time.csv\")\n    }\n\n    # Run production benchmark\n    production_benchmark = Benchmark(\n        models=candidate_models,\n        datasets=production_datasets,\n        metrics=[\"accuracy\", \"inference_time\", \"memory_usage\", \"throughput\"],\n        batch_size=64,  # Production batch size\n        device=\"cuda\"\n    )\n\n    # Execute with production settings\n    production_results = production_benchmark.run()\n\n    # Apply production criteria\n    qualified_models = filter_by_production_criteria(\n        production_results, \n        production_criteria\n    )\n\n    # Rank by production suitability\n    ranked_models = rank_production_models(qualified_models)\n\n    return ranked_models\n\n# Run production model selection\nproduction_rankings = production_model_selection()\n\n# Display results\nprint(\"Production Model Rankings:\")\nfor i, (model_name, score) in enumerate(production_rankings, 1):\n    print(f\"{i}. {model_name}: {score:.3f}\")\n\n# Generate production report\ngenerate_production_report(production_rankings, \"production_selection_report\")\n</code></pre>"},{"location":"tutorials/benchmark/examples/#example-4-performance-monitoring-and-alerting","title":"Example 4: Performance Monitoring and Alerting","text":"<p>Scenario: Continuous monitoring of model performance in production with automated alerting.</p> <pre><code>class ProductionModelMonitor:\n    \"\"\"Monitor model performance in production.\"\"\"\n\n    def __init__(self, model, dataset, alert_thresholds):\n        self.model = model\n        self.dataset = dataset\n        self.alert_thresholds = alert_thresholds\n        self.performance_history = []\n        self.setup_logging()\n\n    def monitor_performance(self, interval_minutes=60):\n        \"\"\"Monitor model performance continuously.\"\"\"\n        self.logger.info(\"Starting production performance monitoring...\")\n\n        while True:\n            try:\n                # Run performance evaluation\n                current_performance = self.evaluate_current_performance()\n\n                # Store in history\n                self.performance_history.append({\n                    'timestamp': datetime.now(),\n                    'performance': current_performance\n                })\n\n                # Check for performance degradation\n                if self.check_performance_degradation(current_performance):\n                    self.send_alert(current_performance)\n\n                # Log performance\n                self.logger.info(f\"Performance: {current_performance}\")\n\n                # Wait for next evaluation\n                time.sleep(interval_minutes * 60)\n\n            except Exception as e:\n                self.logger.error(f\"Monitoring error: {e}\")\n                self.send_error_alert(str(e))\n                time.sleep(interval_minutes * 60)\n\n    def evaluate_current_performance(self):\n        \"\"\"Evaluate current model performance.\"\"\"\n        benchmark = Benchmark(\n            models=[self.model],\n            datasets=[self.dataset],\n            metrics=[\"accuracy\", \"f1_score\", \"inference_time\"],\n            batch_size=32,\n            device=\"cuda\"\n        )\n\n        results = benchmark.run()\n        return results[list(results.keys())[0]]\n\n# Usage example\nalert_thresholds = {\n    \"accuracy\": 0.80,\n    \"f1_score\": 0.75,\n    \"inference_time\": 0.2\n}\n\nmonitor = ProductionModelMonitor(\n    model=selected_model,\n    dataset=production_dataset,\n    alert_thresholds=alert_thresholds\n)\n\n# Start monitoring (in production, run this as a service)\n# monitor.monitor_performance(interval_minutes=60)\n</code></pre>"},{"location":"tutorials/benchmark/examples/#performance-analysis-scenarios","title":"Performance Analysis Scenarios","text":""},{"location":"tutorials/benchmark/examples/#example-5-model-optimization-analysis","title":"Example 5: Model Optimization Analysis","text":"<p>Scenario: Analyzing model performance to identify optimization opportunities.</p> <pre><code>def analyze_model_optimization():\n    \"\"\"Analyze model performance for optimization opportunities.\"\"\"\n\n    # Test different optimization strategies\n    optimization_results = {}\n\n    # 1. Batch size optimization\n    batch_size_results = optimize_batch_size(selected_model, test_dataset)\n    optimization_results[\"batch_size\"] = batch_size_results\n\n    # 2. Mixed precision analysis\n    precision_results = analyze_mixed_precision(selected_model, test_dataset)\n    optimization_results[\"mixed_precision\"] = precision_results\n\n    # 3. Memory optimization\n    memory_results = analyze_memory_optimization(selected_model, test_dataset)\n    optimization_results[\"memory\"] = memory_results\n\n    return optimization_results\n\ndef optimize_batch_size(model, dataset):\n    \"\"\"Find optimal batch size for the model.\"\"\"\n    batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128]\n    results = {}\n\n    for batch_size in batch_sizes:\n        try:\n            # Test batch size\n            start_time = time.time()\n            memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n\n            # Run inference\n            dataloader = dataset.get_dataloader(batch_size=batch_size)\n            for batch in dataloader:\n                with torch.no_grad():\n                    outputs = model(batch[\"input_ids\"].to(model.device))\n                break  # Just test one batch\n\n            inference_time = time.time() - start_time\n            memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n\n            results[batch_size] = {\n                \"inference_time\": inference_time,\n                \"memory_used\": (memory_after - memory_before) / 1024 / 1024,\n                \"throughput\": batch_size / inference_time\n            }\n\n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"Batch size {batch_size} failed: OOM\")\n                break\n\n    return results\n\n# Run optimization analysis\noptimization_results = analyze_model_optimization()\n\n# Generate optimization report\ngenerate_optimization_report(optimization_results, \"model_optimization_report\")\n</code></pre>"},{"location":"tutorials/benchmark/examples/#custom-benchmarking-scenarios","title":"Custom Benchmarking Scenarios","text":""},{"location":"tutorials/benchmark/examples/#example-6-multi-task-model-evaluation","title":"Example 6: Multi-Task Model Evaluation","text":"<p>Scenario: Evaluating models that can perform multiple tasks simultaneously.</p> <pre><code>def multi_task_benchmark():\n    \"\"\"Benchmark models on multiple tasks simultaneously.\"\"\"\n\n    # Define multiple tasks\n    tasks = {\n        \"promoter_prediction\": {\n            \"dataset\": \"data/promoter_data.csv\",\n            \"task_type\": \"binary_classification\",\n            \"metrics\": [\"accuracy\", \"f1_score\", \"roc_auc\"]\n        },\n        \"motif_detection\": {\n            \"dataset\": \"data/motif_data.csv\",\n            \"task_type\": \"binary_classification\",\n            \"metrics\": [\"accuracy\", \"f1_score\", \"precision\"]\n        },\n        \"sequence_generation\": {\n            \"dataset\": \"data/generation_data.csv\",\n            \"task_type\": \"generation\",\n            \"metrics\": [\"perplexity\", \"bleu_score\", \"diversity\"]\n        }\n    }\n\n    # Multi-task models\n    multi_task_models = [\n        {\n            \"name\": \"Unified DNA Model\",\n            \"path\": \"path/to/unified_model\",\n            \"source\": \"local\",\n            \"task_type\": \"multi_task\"\n        }\n    ]\n\n    # Run multi-task benchmark\n    multi_task_results = {}\n\n    for model_info in multi_task_models:\n        model_name = model_info[\"name\"]\n        multi_task_results[model_name] = {}\n\n        for task_name, task_config in tasks.items():\n            # Load task-specific dataset\n            dataset = DNADataset.load_local_data(\n                task_config[\"dataset\"],\n                seq_col=\"sequence\",\n                label_col=\"label\",\n                max_length=512\n            )\n\n            # Evaluate on this task\n            task_result = benchmark.evaluate_single_model(\n                model_info[\"model\"],\n                model_info[\"tokenizer\"],\n                dataset,\n                metrics=task_config[\"metrics\"]\n            )\n\n            multi_task_results[model_name][task_name] = task_result\n\n    return multi_task_results\n\n# Run multi-task benchmark\nmulti_task_results = multi_task_benchmark()\n\n# Analyze multi-task performance\nanalyze_multi_task_performance(multi_task_results)\n</code></pre>"},{"location":"tutorials/benchmark/examples/#example-7-time-series-model-evaluation","title":"Example 7: Time-Series Model Evaluation","text":"<p>Scenario: Evaluating model performance over time with temporal data.</p> <pre><code>def time_series_benchmark():\n    \"\"\"Benchmark model performance over time.\"\"\"\n\n    # Load time-series dataset\n    time_series_data = pd.read_csv(\"data/time_series_data.csv\")\n    time_series_data['date'] = pd.to_datetime(time_series_data['date'])\n\n    # Sort by time\n    time_series_data = time_series_data.sort_values('date')\n\n    # Create temporal splits\n    total_days = (time_series_data['date'].max() - time_series_data['date'].min()).days\n    interval_days = 30\n    num_splits = total_days // interval_days\n\n    temporal_results = []\n\n    for i in range(num_splits):\n        start_idx = i * len(time_series_data) // num_splits\n        end_idx = (i + 1) * len(time_series_data) // num_splits\n\n        # Test on future data\n        test_data = time_series_data.iloc[end_idx:]\n        if len(test_data) == 0:\n            continue\n\n        # Convert to DNALLM dataset format\n        test_dataset = DNADataset.from_dataframe(\n            test_data,\n            seq_col=\"sequence\",\n            label_col=\"label\",\n            max_length=512\n        )\n\n        # Evaluate performance\n        result = benchmark.evaluate_single_model(\n            selected_model,\n            selected_tokenizer,\n            test_dataset,\n            metrics=[\"accuracy\", \"f1_score\"]\n        )\n\n        temporal_results.append({\n            \"time_period\": i,\n            \"start_date\": time_series_data.iloc[start_idx]['date'],\n            \"end_date\": time_series_data.iloc[end_idx]['date'],\n            \"test_size\": len(test_data),\n            **result\n        })\n\n    return temporal_results\n\n# Run time-series benchmark\ntemporal_results = time_series_benchmark()\n\n# Analyze temporal performance\nanalyze_temporal_performance(temporal_results)\n</code></pre>"},{"location":"tutorials/benchmark/examples/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"tutorials/benchmark/examples/#1-research-applications","title":"1. Research Applications","text":"<ul> <li>Use cross-validation for robust evaluation</li> <li>Generate publication-ready plots and tables</li> <li>Include statistical significance testing</li> <li>Document all experimental conditions</li> </ul>"},{"location":"tutorials/benchmark/examples/#2-production-use-cases","title":"2. Production Use Cases","text":"<ul> <li>Define clear performance criteria</li> <li>Test on representative production data</li> <li>Monitor performance continuously</li> <li>Implement automated alerting</li> </ul>"},{"location":"tutorials/benchmark/examples/#3-performance-analysis","title":"3. Performance Analysis","text":"<ul> <li>Profile memory and time usage</li> <li>Test optimization strategies</li> <li>Document performance baselines</li> <li>Track performance over time</li> </ul>"},{"location":"tutorials/benchmark/examples/#4-custom-scenarios","title":"4. Custom Scenarios","text":"<ul> <li>Adapt benchmarking to specific requirements</li> <li>Implement custom evaluation metrics</li> <li>Handle multi-task and time-series data</li> <li>Consider domain-specific constraints</li> </ul>"},{"location":"tutorials/benchmark/examples/#next-steps","title":"Next Steps","text":"<p>After exploring these examples:</p> <ol> <li>Adapt to Your Use Case: Modify examples for your specific requirements</li> <li>Combine Techniques: Use multiple approaches together</li> <li>Scale Up: Apply to larger datasets and model collections</li> <li>Automate: Build automated benchmarking pipelines</li> </ol> <p>Ready to implement? Start with the examples that match your use case, or combine multiple approaches for comprehensive evaluation.</p>"},{"location":"tutorials/benchmark/getting_started/","title":"Getting Started with Benchmarking","text":"<p>This guide will walk you through the basics of benchmarking DNA language models using DNALLM. You'll learn how to set up your first benchmark, configure models and datasets, and interpret results.</p>"},{"location":"tutorials/benchmark/getting_started/#overview","title":"Overview","text":"<p>Benchmarking in DNALLM allows you to: - Compare multiple DNA language models on the same tasks - Evaluate performance across different datasets - Measure accuracy, speed, and resource usage - Generate comprehensive performance reports</p>"},{"location":"tutorials/benchmark/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed and configured:</p> <pre><code># Install DNALLM\npip install dnallm\n\n# Or with uv (recommended)\nuv pip install dnallm\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#basic-setup","title":"Basic Setup","text":""},{"location":"tutorials/benchmark/getting_started/#1-import-required-modules","title":"1. Import Required Modules","text":"<pre><code>from dnallm import load_config, Benchmark\nfrom dnallm.inference import load_model_and_tokenizer\nfrom dnallm.datahandling import DNADataset\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#2-create-a-simple-configuration","title":"2. Create a Simple Configuration","text":"<p>Create a <code>benchmark_config.yaml</code> file:</p> <pre><code># benchmark_config.yaml\nbenchmark:\n  name: \"My First Benchmark\"\n  description: \"Comparing DNA models on promoter prediction\"\n\n  models:\n    - name: \"Plant DNABERT\"\n      path: \"zhangtaolab/plant-dnabert-BPE\"\n      source: \"huggingface\"\n      task_type: \"classification\"\n\n    - name: \"Plant DNAGPT\"\n      path: \"zhangtaolab/plant-dnagpt-BPE\"\n      source: \"huggingface\"\n      task_type: \"generation\"\n\n  datasets:\n    - name: \"promoter_data\"\n      path: \"path/to/your/data.csv\"\n      task: \"binary_classification\"\n      text_column: \"sequence\"\n      label_column: \"label\"\n\n  metrics:\n    - \"accuracy\"\n    - \"f1_score\"\n    - \"precision\"\n    - \"recall\"\n\n  evaluation:\n    batch_size: 16\n    max_length: 512\n    device: \"cuda\"  # or \"cpu\"\n\n  output:\n    format: \"html\"\n    path: \"benchmark_results\"\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#3-load-your-data","title":"3. Load Your Data","text":"<pre><code># Load your dataset\ndataset = DNADataset.load_local_data(\n    \"path/to/your/data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    max_length=512\n)\n\n# Split if needed\nif not dataset.is_split:\n    dataset.split_data(test_size=0.2, val_size=0.1)\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#4-run-the-benchmark","title":"4. Run the Benchmark","text":"<pre><code># Load configuration\nconfig = load_config(\"benchmark_config.yaml\")\n\n# Initialize benchmark\nbenchmark = Benchmark(config=config)\n\n# Run benchmark\nresults = benchmark.run()\n\n# Display results\nprint(\"Benchmark Results:\")\nprint(\"=\" * 50)\nfor model_name, model_results in results.items():\n    print(f\"\\n{model_name}:\")\n    for dataset_name, metrics in model_results.items():\n        print(f\"  {dataset_name}:\")\n        for metric, value in metrics.items():\n            print(f\"    {metric}: {value:.4f}\")\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#command-line-interface","title":"Command Line Interface","text":"<p>DNALLM also provides a convenient command-line interface:</p> <pre><code># Basic benchmark run\ndnallm-benchmark --config benchmark_config.yaml\n\n# Generate detailed report\ndnallm-benchmark --config config.yaml --output report.html\n\n# Run with custom parameters\ndnallm-benchmark --config config.yaml --batch-size 32 --device cuda\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#understanding-results","title":"Understanding Results","text":""},{"location":"tutorials/benchmark/getting_started/#basic-metrics","title":"Basic Metrics","text":"Metric Description Range Best Value Accuracy Correct predictions / Total predictions 0.0 - 1.0 1.0 F1 Score Harmonic mean of precision and recall 0.0 - 1.0 1.0 Precision True positives / (True positives + False positives) 0.0 - 1.0 1.0 Recall True positives / (True positives + False negatives) 0.0 - 1.0 1.0"},{"location":"tutorials/benchmark/getting_started/#performance-metrics","title":"Performance Metrics","text":"Metric Description Unit Inference Time Time to process one batch seconds Memory Usage GPU/RAM memory consumption MB/GB Throughput Samples processed per second samples/sec"},{"location":"tutorials/benchmark/getting_started/#example-complete-benchmark","title":"Example: Complete Benchmark","text":"<p>Here's a complete working example:</p> <pre><code>import os\nfrom dnallm import load_config, Benchmark\nfrom dnallm.datahandling import DNADataset\n\n# 1. Prepare your data\ndata_path = \"path/to/your/dna_sequences.csv\"\nif not os.path.exists(data_path):\n    print(\"Please provide a valid data path\")\n    exit()\n\n# 2. Load and prepare dataset\ndataset = DNADataset.load_local_data(\n    data_path,\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    max_length=512\n)\n\n# 3. Create configuration\nconfig = {\n    \"benchmark\": {\n        \"name\": \"DNA Model Comparison\",\n        \"models\": [\n            {\n                \"name\": \"Plant DNABERT\",\n                \"path\": \"zhangtaolab/plant-dnabert-BPE\",\n                \"source\": \"huggingface\",\n                \"task_type\": \"classification\"\n            },\n            {\n                \"name\": \"Plant DNAGPT\",\n                \"path\": \"zhangtaolab/plant-dnagpt-BPE\", \n                \"source\": \"huggingface\",\n                \"task_type\": \"generation\"\n            }\n        ],\n        \"datasets\": [dataset],\n        \"metrics\": [\"accuracy\", \"f1_score\", \"precision\", \"recall\"],\n        \"evaluation\": {\n            \"batch_size\": 16,\n            \"max_length\": 512,\n            \"device\": \"cuda\"\n        },\n        \"output\": {\n            \"format\": \"html\",\n            \"path\": \"my_benchmark_results\"\n        }\n    }\n}\n\n# 4. Run benchmark\nbenchmark = Benchmark(config=config)\nresults = benchmark.run()\n\n# 5. Generate report\nbenchmark.generate_report(\n    output_path=\"my_benchmark_results\",\n    format=\"html\",\n    include_predictions=True\n)\n\nprint(\"Benchmark completed! Check 'my_benchmark_results' folder for results.\")\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#data-format-requirements","title":"Data Format Requirements","text":"<p>Your dataset should be in one of these formats:</p>"},{"location":"tutorials/benchmark/getting_started/#csvtsv-format","title":"CSV/TSV Format","text":"<pre><code>sequence,label\nATCGATCGATCG,1\nGCTAGCTAGCTA,0\nTATATATATATA,1\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#json-format","title":"JSON Format","text":"<pre><code>[\n  {\"sequence\": \"ATCGATCGATCG\", \"label\": 1},\n  {\"sequence\": \"GCTAGCTAGCTA\", \"label\": 0}\n]\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#fasta-format","title":"FASTA Format","text":"<pre><code>&gt;sequence1|label:1\nATCGATCGATCG\n&gt;sequence2|label:0\nGCTAGCTAGCTA\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#common-tasks","title":"Common Tasks","text":""},{"location":"tutorials/benchmark/getting_started/#binary-classification","title":"Binary Classification","text":"<pre><code>task: \"binary_classification\"\nnum_labels: 2\nlabel_names: [\"Negative\", \"Positive\"]\nthreshold: 0.5\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#multi-class-classification","title":"Multi-class Classification","text":"<pre><code>task: \"multiclass\"\nnum_labels: 4\nlabel_names: [\"Class_A\", \"Class_B\", \"Class_C\", \"Class_D\"]\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#regression","title":"Regression","text":"<pre><code>task: \"regression\"\nnum_labels: 1\n</code></pre>"},{"location":"tutorials/benchmark/getting_started/#next-steps","title":"Next Steps","text":"<p>After completing this basic tutorial:</p> <ol> <li>Explore Advanced Features: Learn about cross-validation and custom metrics</li> <li>Optimize Performance: Discover performance profiling techniques</li> <li>Customize Output: Learn about advanced configuration options</li> <li>Real-world Examples: See practical use cases</li> </ol>"},{"location":"tutorials/benchmark/getting_started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/benchmark/getting_started/#common-issues","title":"Common Issues","text":"<p>\"Model not found\" error <pre><code># Check if model exists on Hugging Face\n# Visit: https://huggingface.co/models?search=dna\n</code></pre></p> <p>Memory errors <pre><code># Reduce batch size in config\nevaluation:\n  batch_size: 8  # Reduced from 16\n</code></pre></p> <p>Slow performance <pre><code># Enable mixed precision\nevaluation:\n  use_fp16: true\n</code></pre></p>"},{"location":"tutorials/benchmark/getting_started/#additional-resources","title":"Additional Resources","text":"<ul> <li>Configuration Guide - Detailed configuration options</li> <li>Advanced Techniques - Cross-validation and custom metrics</li> <li>Examples and Use Cases - Real-world scenarios</li> <li>Troubleshooting - Common problems and solutions</li> </ul> <p>Ready for more? Continue to Advanced Techniques to learn about cross-validation, custom metrics, and performance profiling.</p>"},{"location":"tutorials/fine_tuning/","title":"Fine-tuning DNA Language Models","text":"<p>This section provides comprehensive tutorials and guides for fine-tuning DNA language models using DNALLM. Fine-tuning allows you to adapt pre-trained models to your specific DNA analysis tasks and datasets.</p>"},{"location":"tutorials/fine_tuning/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Basic Fine-tuning: Get started with simple model adaptation</li> <li>Advanced Techniques: Custom loss functions, data augmentation, and optimization</li> <li>Task-Specific Guides: Classification, generation, and specialized tasks</li> <li>Best Practices: Hyperparameter tuning, monitoring, and deployment</li> </ul>"},{"location":"tutorials/fine_tuning/#quick-navigation","title":"Quick Navigation","text":"Topic Description Difficulty Getting Started Basic fine-tuning setup and configuration Beginner Task-Specific Guides Fine-tuning for different task types Intermediate Advanced Techniques Custom training, optimization, and monitoring Advanced Configuration Guide Detailed configuration options and examples Intermediate Examples and Use Cases Real-world fine-tuning scenarios All Levels Troubleshooting Common issues and solutions All Levels"},{"location":"tutorials/fine_tuning/#prerequisites","title":"Prerequisites","text":"<p>Before diving into fine-tuning, ensure you have:</p> <ul> <li>\u2705 DNALLM installed and configured</li> <li>\u2705 Access to pre-trained DNA language models</li> <li>\u2705 Training datasets in appropriate formats</li> <li>\u2705 Sufficient computational resources (GPU recommended)</li> <li>\u2705 Understanding of your target task and data</li> </ul>"},{"location":"tutorials/fine_tuning/#quick-start","title":"Quick Start","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n\n# Load configuration\nconfig = load_config(\"finetune_config.yaml\")\n\n# Load pre-trained model and tokenizer\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load and prepare dataset\ndataset = DNADataset.load_local_data(\n    \"path/to/your/data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Initialize trainer and start fine-tuning\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    config=config\n)\n\ntrainer.train()\n</code></pre>"},{"location":"tutorials/fine_tuning/#supported-task-types","title":"Supported Task Types","text":"Task Type Description Use Cases Classification Binary, multi-class, and multi-label classification Promoter prediction, motif detection, functional annotation Generation Sequence generation and completion DNA synthesis, sequence design, mutation analysis Masked Language Modeling Sequence completion and prediction Sequence analysis, mutation prediction Token Classification Named entity recognition and tagging Gene identification, regulatory element detection Regression Continuous value prediction Expression level prediction, binding affinity"},{"location":"tutorials/fine_tuning/#key-features","title":"Key Features","text":"<ul> <li>Flexible Architecture: Support for various model architectures (BERT, GPT, Transformer variants)</li> <li>Task-Specific Heads: Automatic head selection based on task type</li> <li>Data Processing: Built-in DNA sequence preprocessing and augmentation</li> <li>Training Optimization: Mixed precision, gradient accumulation, and scheduling</li> <li>Monitoring: TensorBoard integration and comprehensive logging</li> <li>Checkpointing: Automatic model saving and resumption</li> </ul>"},{"location":"tutorials/fine_tuning/#model-sources","title":"Model Sources","text":"<ul> <li>Hugging Face Hub: Access to thousands of pre-trained models</li> <li>ModelScope: Alternative model repository with specialized models</li> <li>Local Models: Use your own pre-trained models</li> <li>Custom Architectures: Implement and fine-tune custom model designs</li> </ul>"},{"location":"tutorials/fine_tuning/#next-steps","title":"Next Steps","text":"<p>Choose your path:</p> <ul> <li>New to fine-tuning? Start with Getting Started</li> <li>Want task-specific guidance? Check Task-Specific Guides</li> <li>Need advanced features? Explore Advanced Techniques</li> <li>Looking for examples? See Examples and Use Cases</li> </ul> <p>Need Help? Check our FAQ or open an issue on GitHub.</p>"},{"location":"tutorials/fine_tuning/advanced_techniques/","title":"Advanced Fine-tuning Techniques","text":"<p>This guide covers advanced fine-tuning techniques including custom training strategies, optimization methods, monitoring, and deployment considerations.</p>"},{"location":"tutorials/fine_tuning/advanced_techniques/#overview","title":"Overview","text":"<p>Advanced fine-tuning techniques help you: - Implement custom training loops and loss functions - Optimize training performance and memory usage - Monitor and debug training progress effectively - Deploy fine-tuned models in production environments</p>"},{"location":"tutorials/fine_tuning/advanced_techniques/#custom-training-strategies","title":"Custom Training Strategies","text":""},{"location":"tutorials/fine_tuning/advanced_techniques/#custom-loss-functions","title":"Custom Loss Functions","text":"<p>DNALLM allows you to implement custom loss functions for specific use cases.</p>"},{"location":"tutorials/fine_tuning/advanced_techniques/#weighted-loss-for-imbalanced-data","title":"Weighted Loss for Imbalanced Data","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass WeightedCrossEntropyLoss(nn.Module):\n    \"\"\"Weighted cross-entropy loss for imbalanced datasets.\"\"\"\n\n    def __init__(self, class_weights=None, label_smoothing=0.0):\n        super().__init__()\n        self.class_weights = class_weights\n        self.label_smoothing = label_smoothing\n\n    def forward(self, logits, targets):\n        if self.class_weights is not None:\n            loss = F.cross_entropy(\n                logits, \n                targets, \n                weight=self.class_weights,\n                label_smoothing=self.label_smoothing\n            )\n        else:\n            loss = F.cross_entropy(logits, targets, label_smoothing=self.label_smoothing)\n\n        return loss\n\n# Usage in trainer\nclass CustomDNATrainer(DNATrainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # Calculate class weights\n        labels = [item['label'] for item in self.train_dataset]\n        class_counts = torch.bincount(torch.tensor(labels))\n        class_weights = 1.0 / class_counts\n        class_weights = class_weights / class_weights.sum()\n\n        # Set custom loss\n        self.criterion = WeightedCrossEntropyLoss(class_weights=class_weights)\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        outputs = model(**inputs)\n        logits = outputs.logits\n        labels = inputs[\"labels\"]\n\n        loss = self.criterion(logits, labels)\n\n        if return_outputs:\n            return loss, outputs\n        return loss\n</code></pre>"},{"location":"tutorials/fine_tuning/advanced_techniques/#focal-loss-for-hard-examples","title":"Focal Loss for Hard Examples","text":"<pre><code>class FocalLoss(nn.Module):\n    \"\"\"Focal loss for handling hard examples.\"\"\"\n\n    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, logits, targets):\n        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\n# Usage\ntrainer = CustomDNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\ntrainer.criterion = FocalLoss(alpha=1, gamma=2)\n</code></pre>"},{"location":"tutorials/fine_tuning/advanced_techniques/#custom-training-loops","title":"Custom Training Loops","text":"<p>Implement custom training loops for advanced control over the training process.</p>"},{"location":"tutorials/fine_tuning/advanced_techniques/#custom-training-with-gradient-accumulation","title":"Custom Training with Gradient Accumulation","text":"<pre><code>class CustomTrainer:\n    \"\"\"Custom trainer with advanced features.\"\"\"\n\n    def __init__(self, model, tokenizer, train_dataset, config):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.train_dataset = train_dataset\n        self.config = config\n\n        # Setup optimizer and scheduler\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=config.finetune.learning_rate,\n            weight_decay=config.finetune.weight_decay\n        )\n\n        self.scheduler = self._get_scheduler()\n        self.scaler = torch.cuda.amp.GradScaler() if config.finetune.bf16 else None\n\n    def _get_scheduler(self):\n        \"\"\"Get learning rate scheduler.\"\"\"\n        num_training_steps = len(self.train_dataset) // self.config.finetune.per_device_train_batch_size\n        num_warmup_steps = int(num_training_steps * self.config.finetune.warmup_ratio)\n\n        return torch.optim.lr_scheduler.get_scheduler(\n            name=self.config.finetune.lr_scheduler_type,\n            optimizer=self.optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n\n    def train_epoch(self, epoch):\n        \"\"\"Train for one epoch.\"\"\"\n        self.model.train()\n        total_loss = 0\n\n        # Create data loader\n        dataloader = torch.utils.data.DataLoader(\n            self.train_dataset,\n            batch_size=self.config.finetune.per_device_train_batch_size,\n            shuffle=True,\n            collate_fn=self._collate_fn\n        )\n\n        for step, batch in enumerate(dataloader):\n            # Move to device\n            batch = {k: v.to(self.model.device) for k, v in batch.items()}\n\n            # Forward pass with mixed precision\n            if self.scaler:\n                with torch.cuda.amp.autocast():\n                    outputs = self.model(**batch)\n                    loss = outputs.loss\n            else:\n                outputs = self.model(**batch)\n                loss = outputs.loss\n\n            # Scale loss for gradient accumulation\n            loss = loss / self.config.finetune.gradient_accumulation_steps\n\n            # Backward pass\n            if self.scaler:\n                self.scaler.scale(loss).backward()\n            else:\n                loss.backward()\n\n            # Gradient accumulation\n            if (step + 1) % self.config.finetune.gradient_accumulation_steps == 0:\n                if self.scaler:\n                    self.scaler.unscale_(self.optimizer)\n                    torch.nn.utils.clip_grad_norm_(\n                        self.model.parameters(), \n                        self.config.finetune.max_grad_norm\n                    )\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n                else:\n                    torch.nn.utils.clip_grad_norm_(\n                        self.model.parameters(), \n                        self.config.finetune.max_grad_norm\n                    )\n                    self.optimizer.step()\n\n                self.scheduler.step()\n                self.optimizer.zero_grad()\n\n            total_loss += loss.item()\n\n            # Logging\n            if step % self.config.finetune.logging_steps == 0:\n                print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item():.4f}\")\n\n        return total_loss / len(dataloader)\n\n    def train(self, num_epochs):\n        \"\"\"Main training loop.\"\"\"\n        for epoch in range(num_epochs):\n            print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n            loss = self.train_epoch(epoch)\n            print(f\"Epoch {epoch + 1} completed. Average loss: {loss:.4f}\")\n\n# Usage\ncustom_trainer = CustomTrainer(model, tokenizer, dataset.train_data, config)\ncustom_trainer.train(num_epochs=config.finetune.num_train_epochs)\n</code></pre>"},{"location":"tutorials/fine_tuning/advanced_techniques/#advanced-optimization-techniques","title":"Advanced Optimization Techniques","text":""},{"location":"tutorials/fine_tuning/advanced_techniques/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>Implement advanced learning rate scheduling strategies.</p>"},{"location":"tutorials/fine_tuning/advanced_techniques/#cosine-annealing-with-warm-restarts","title":"Cosine Annealing with Warm Restarts","text":"<pre><code>class CosineAnnealingWarmRestarts:\n    \"\"\"Cosine annealing with warm restarts.\"\"\"\n\n    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0):\n        self.optimizer = optimizer\n        self.T_0 = T_0\n        self.T_mult = T_mult\n        self.eta_min = eta_min\n        self.T_cur = 0\n        self.base_lr = optimizer.param_groups[0]['lr']\n\n    def step(self):\n        if self.T_cur &gt;= self.T_0:\n            self.T_0 *= self.T_mult\n            self.T_cur = 0\n\n        lr = self.eta_min + (self.base_lr - self.eta_min) * \\\n             (1 + math.cos(math.pi * self.T_cur / self.T_0)) / 2\n\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n\n        self.T_cur += 1\n\n# Usage\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1000, T_mult=2)\n</code></pre>"},{"location":"tutorials/fine_tuning/advanced_techniques/#one-cycle-policy","title":"One Cycle Policy","text":"<pre><code>class OneCycleScheduler:\n    \"\"\"One cycle learning rate scheduler.\"\"\"\n\n    def __init__(self, optimizer, max_lr, total_steps, pct_start=0.3):\n        self.optimizer = optimizer\n        self.max_lr = max_lr\n        self.total_steps = total_steps\n        self.pct_start = pct_start\n        self.step_count = 0\n\n        # Calculate step counts\n        self.warmup_steps = int(total_steps * pct_start)\n        self.decay_steps = total_steps - self.warmup_steps\n\n    def step(self):\n        if self.step_count &lt; self.warmup_steps:\n            # Warmup phase\n            lr = self.max_lr * (self.step_count / self.warmup_steps)\n        else:\n            # Decay phase\n            decay_step = self.step_count - self.warmup_steps\n            lr = self.max_lr * (1 - decay_step / self.decay_steps)\n\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n\n        self.step_count += 1\n\n# Usage\nscheduler = OneCycleScheduler(optimizer, max_lr=1e-3, total_steps=10000)\n</code></pre>"},{"location":"tutorials/fine_tuning/advanced_techniques/#memory-optimization-techniques","title":"Memory Optimization Techniques","text":""},{"location":"tutorials/fine_tuning/advanced_techniques/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<p>Implement gradient checkpointing to reduce memory usage.</p> <pre><code># Enable gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# Or in configuration\nfinetune:\n  gradient_checkpointing: true\n</code></pre>"},{"location":"tutorials/fine_tuning/advanced_techniques/#memory-efficient-attention","title":"Memory Efficient Attention","text":"<p>Use memory-efficient attention mechanisms.</p> <pre><code># Enable memory efficient attention\nfrom transformers import AutoConfig\n\nconfig = AutoConfig.from_pretrained(\"zhangtaolab/plant-dnabert-BPE\")\nconfig.use_memory_efficient_attention = True\n\nmodel = AutoModel.from_pretrained(\"zhangtaolab/plant-dnabert-BPE\", config=config)\n</code></pre>"},{"location":"tutorials/fine_tuning/advanced_techniques/#dynamic-batching","title":"Dynamic Batching","text":"<p>Implement dynamic batching for variable-length sequences.</p> <pre><code>class DynamicBatchSampler:\n    \"\"\"Dynamic batch sampler for variable-length sequences.\"\"\"\n\n    def __init__(self, dataset, max_tokens_per_batch=4096, max_batch_size=32):\n        self.dataset = dataset\n        self.max_tokens_per_batch = max_tokens_per_batch\n        self.max_batch_size = max_batch_size\n\n        # Sort by length for efficient batching\n        self.lengths = [len(item['sequence']) for item in dataset]\n        self.indices = sorted(range(len(dataset)), key=lambda i: self.lengths[i])\n\n    def __iter__(self):\n        batch = []\n        current_tokens = 0\n\n        for idx in self.indices:\n            sequence_length = self.lengths[idx]\n\n            # Check if adding this sample would exceed limits\n            if (len(batch) &gt;= self.max_batch_size or \n                current_tokens + sequence_length &gt; self.max_tokens_per_batch):\n                if batch:\n                    yield batch\n                    batch = []\n                    current_tokens = 0\n\n            batch.append(idx)\n            current_tokens += sequence_length\n\n        if batch:\n            yield batch\n\n# Usage\nsampler = DynamicBatchSampler(dataset.train_data, max_tokens_per_batch=4096)\ndataloader = torch.utils.data.DataLoader(\n    dataset.train_data,\n    batch_sampler=sampler,\n    collate_fn=custom_collate_fn\n)\n</code></pre>"},{"location":"tutorials/fine_tuning/advanced_techniques/#advanced-monitoring-and-debugging","title":"Advanced Monitoring and Debugging","text":""},{"location":"tutorials/fine_tuning/advanced_techniques/#custom-callbacks","title":"Custom Callbacks","text":"<p>Implement custom callbacks for advanced monitoring.</p> <pre><code>class CustomCallback:\n    \"\"\"Custom callback for advanced monitoring.\"\"\"\n\n    def __init__(self, model, tokenizer, eval_dataset):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.eval_dataset = eval_dataset\n        self.best_metric = float('inf')\n        self.patience_counter = 0\n\n    def on_step_end(self, step, logs=None):\n        \"\"\"Called at the end of each step.\"\"\"\n        if step % 100 == 0:\n            # Log learning rate\n            lr = self.model.optimizer.param_groups[0]['lr']\n            print(f\"Step {step}, Learning Rate: {lr:.2e}\")\n\n            # Log gradient norm\n            total_norm = 0\n            for p in self.model.parameters():\n                if p.grad is not None:\n                    param_norm = p.grad.data.norm(2)\n                    total_norm += param_norm.item() ** 2\n            total_norm = total_norm ** (1. / 2)\n            print(f\"Step {step}, Gradient Norm: {total_norm:.4f}\")\n\n    def on_epoch_end(self, epoch, logs=None):\n        \"\"\"Called at the end of each epoch.\"\"\"\n        # Evaluate on validation set\n        self.model.eval()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for batch in self.eval_dataset:\n                outputs = self.model(**batch)\n                loss = outputs.loss\n                total_loss += loss.item()\n\n                # Calculate accuracy\n                predictions = outputs.logits.argmax(-1)\n                correct += (predictions == batch['labels']).sum().item()\n                total += batch['labels'].size(0)\n\n        avg_loss = total_loss / len(self.eval_dataset)\n        accuracy = correct / total\n\n        print(f\"Epoch {epoch}, Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n        # Early stopping logic\n        if avg_loss &lt; self.best_metric:\n            self.best_metric = avg_loss\n            self.patience_counter = 0\n            # Save best model\n            self.model.save_pretrained(f\"best_model_epoch_{epoch}\")\n        else:\n            self.patience_counter += 1\n            if self.patience_counter &gt;= 3:\n                print(\"Early stopping triggered!\")\n                return True\n\n        return False\n\n# Usage\ncallback = CustomCallback(model, tokenizer, dataset.val_data)\n</code></pre>"},{"location":"tutorials/fine_tuning/advanced_techniques/#advanced-logging","title":"Advanced Logging","text":"<p>Implement comprehensive logging for debugging.</p> <pre><code>import logging\nimport json\nfrom datetime import datetime\n\nclass AdvancedLogger:\n    \"\"\"Advanced logging for fine-tuning experiments.\"\"\"\n\n    def __init__(self, log_dir, experiment_name):\n        self.log_dir = log_dir\n        self.experiment_name = experiment_name\n        self.setup_logging()\n\n        # Track metrics\n        self.metrics = {\n            'train_loss': [],\n            'val_loss': [],\n            'learning_rate': [],\n            'gradient_norm': [],\n            'memory_usage': []\n        }\n\n    def setup_logging(self):\n        \"\"\"Setup logging configuration.\"\"\"\n        log_file = f\"{self.log_dir}/{self.experiment_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.FileHandler(log_file),\n                logging.StreamHandler()\n            ]\n        )\n\n        self.logger = logging.getLogger(__name__)\n\n    def log_metrics(self, step, metrics):\n        \"\"\"Log training metrics.\"\"\"\n        for key, value in metrics.items():\n            if key in self.metrics:\n                self.metrics[key].append(value)\n\n        # Log to file\n        self.logger.info(f\"Step {step}: {json.dumps(metrics, indent=2)}\")\n\n        # Save metrics to JSON\n        with open(f\"{self.log_dir}/metrics.json\", 'w') as f:\n            json.dump(self.metrics, f, indent=2)\n\n    def log_model_info(self, model):\n        \"\"\"Log model architecture information.\"\"\"\n        total_params = sum(p.numel() for p in model.parameters())\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n        self.logger.info(f\"Total parameters: {total_params:,}\")\n        self.logger.info(f\"Trainable parameters: {trainable_params:,}\")\n        self.logger.info(f\"Model size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n\n# Usage\nlogger = AdvancedLogger(\"./logs\", \"promoter_classification\")\nlogger.log_model_info(model)\n</code></pre>"},{"location":"tutorials/fine_tuning/advanced_techniques/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":""},{"location":"tutorials/fine_tuning/advanced_techniques/#grid-search","title":"Grid Search","text":"<p>Implement grid search for hyperparameter optimization.</p> <pre><code>def grid_search_hyperparameters():\n    \"\"\"Grid search for hyperparameter optimization.\"\"\"\n\n    # Define hyperparameter grid\n    hyperparams = {\n        'learning_rate': [1e-5, 2e-5, 5e-5, 1e-4],\n        'batch_size': [8, 16, 32],\n        'weight_decay': [0.01, 0.05, 0.1],\n        'warmup_ratio': [0.1, 0.2, 0.3]\n    }\n\n    best_config = None\n    best_score = float('inf')\n\n    # Generate all combinations\n    from itertools import product\n    keys = hyperparams.keys()\n    values = hyperparams.values()\n\n    for combination in product(*values):\n        config_dict = dict(zip(keys, combination))\n\n        # Update configuration\n        config.finetune.learning_rate = config_dict['learning_rate']\n        config.finetune.per_device_train_batch_size = config_dict['batch_size']\n        config.finetune.weight_decay = config_dict['weight_decay']\n        config.finetune.warmup_ratio = config_dict['warmup_ratio']\n\n        # Train and evaluate\n        score = train_and_evaluate(config)\n\n        print(f\"Config: {config_dict}, Score: {score}\")\n\n        if score &lt; best_score:\n            best_score = score\n            best_config = config_dict.copy()\n\n    print(f\"Best config: {best_config}\")\n    print(f\"Best score: {best_score}\")\n\n    return best_config\n\ndef train_and_evaluate(config):\n    \"\"\"Train model and return validation score.\"\"\"\n    # Implementation of training and evaluation\n    # Return validation loss or other metric\n    pass\n</code></pre>"},{"location":"tutorials/fine_tuning/advanced_techniques/#bayesian-optimization","title":"Bayesian Optimization","text":"<p>Use Bayesian optimization for more efficient hyperparameter search.</p> <pre><code>from skopt import gp_minimize\nfrom skopt.space import Real, Integer\n\ndef objective(params):\n    \"\"\"Objective function for Bayesian optimization.\"\"\"\n    lr, batch_size, weight_decay, warmup_ratio = params\n\n    # Update configuration\n    config.finetune.learning_rate = lr\n    config.finetune.per_device_train_batch_size = int(batch_size)\n    config.finetune.weight_decay = weight_decay\n    config.finetune.warmup_ratio = warmup_ratio\n\n    # Train and evaluate\n    score = train_and_evaluate(config)\n    return score\n\ndef bayesian_optimization():\n    \"\"\"Bayesian optimization for hyperparameters.\"\"\"\n\n    # Define search space\n    space = [\n        Real(1e-5, 1e-4, name='learning_rate', prior='log-uniform'),\n        Integer(4, 64, name='batch_size'),\n        Real(0.001, 0.1, name='weight_decay'),\n        Real(0.05, 0.5, name='warmup_ratio')\n    ]\n\n    # Run optimization\n    result = gp_minimize(\n        objective,\n        space,\n        n_calls=20,\n        random_state=42,\n        n_initial_points=5\n    )\n\n    print(f\"Best parameters: {result.x}\")\n    print(f\"Best score: {result.fun}\")\n\n    return result.x\n</code></pre>"},{"location":"tutorials/fine_tuning/advanced_techniques/#next-steps","title":"Next Steps","text":"<p>After mastering these advanced techniques:</p> <ol> <li>Explore Real-world Examples: See Examples and Use Cases</li> <li>Configuration Options: Check detailed configuration options</li> <li>Troubleshooting: Visit common issues and solutions</li> <li>Deployment: Learn about model deployment and serving</li> </ol> <p>Ready for real-world examples? Continue to Examples and Use Cases to see these advanced techniques in action.</p>"},{"location":"tutorials/fine_tuning/configuration/","title":"Configuration Guide","text":"<p>This guide provides detailed information about all configuration options available for DNALLM fine-tuning, including examples and best practices.</p>"},{"location":"tutorials/fine_tuning/configuration/#overview","title":"Overview","text":"<p>DNALLM fine-tuning configuration is defined in YAML format and supports: - Task Configuration: Task type, labels, and thresholds - Training Configuration: Learning rates, batch sizes, and optimization - Model Configuration: Architecture, tokenizer, and source settings - Advanced Options: Custom training, monitoring, and deployment</p>"},{"location":"tutorials/fine_tuning/configuration/#configuration-structure","title":"Configuration Structure","text":""},{"location":"tutorials/fine_tuning/configuration/#basic-configuration-schema","title":"Basic Configuration Schema","text":"<pre><code># finetune_config.yaml\ntask:\n  # Task-specific settings\n  task_type: \"string\"\n  num_labels: integer\n  label_names: []\n  threshold: float\n\nfinetune:\n  # Training parameters\n  output_dir: \"string\"\n  num_train_epochs: integer\n  per_device_train_batch_size: integer\n  learning_rate: float\n\n  # Optimization settings\n  weight_decay: float\n  warmup_ratio: float\n  gradient_accumulation_steps: integer\n\n  # Monitoring and saving\n  logging_strategy: \"string\"\n  eval_strategy: \"string\"\n  save_strategy: \"string\"\n\n  # Advanced training options\n  bf16: boolean\n  fp16: boolean\n  load_best_model_at_end: boolean\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#task-configuration","title":"Task Configuration","text":""},{"location":"tutorials/fine_tuning/configuration/#basic-task-settings","title":"Basic Task Settings","text":"<pre><code>task:\n  task_type: \"binary\"           # Required: task type\n  num_labels: 2                 # Required: number of output classes\n  label_names: [\"neg\", \"pos\"]   # Optional: human-readable labels\n  threshold: 0.5                # Optional: classification threshold\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#task-types-and-requirements","title":"Task Types and Requirements","text":"Task Type Required Fields Optional Fields Description <code>binary</code> <code>num_labels: 2</code> <code>label_names</code>, <code>threshold</code> Binary classification <code>multiclass</code> <code>num_labels: &gt;2</code> <code>label_names</code> Multi-class classification <code>multilabel</code> <code>num_labels: &gt;1</code> <code>label_names</code>, <code>threshold</code> Multi-label classification <code>regression</code> <code>num_labels: 1</code> None Continuous value prediction <code>generation</code> None None Sequence generation <code>mask</code> None None Masked language modeling <code>token</code> <code>num_labels: &gt;1</code> <code>label_names</code> Token classification"},{"location":"tutorials/fine_tuning/configuration/#task-configuration-examples","title":"Task Configuration Examples","text":""},{"location":"tutorials/fine_tuning/configuration/#binary-classification","title":"Binary Classification","text":"<pre><code>task:\n  task_type: \"binary\"\n  num_labels: 2\n  label_names: [\"negative\", \"positive\"]\n  threshold: 0.5\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#multi-class-classification","title":"Multi-class Classification","text":"<pre><code>task:\n  task_type: \"multiclass\"\n  num_labels: 4\n  label_names: [\"enzyme\", \"receptor\", \"structural\", \"regulatory\"]\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#multi-label-classification","title":"Multi-label Classification","text":"<pre><code>task:\n  task_type: \"multilabel\"\n  num_labels: 5\n  label_names: [\"promoter\", \"enhancer\", \"silencer\", \"insulator\", \"locus_control\"]\n  threshold: 0.5\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#regression","title":"Regression","text":"<pre><code>task:\n  task_type: \"regression\"\n  num_labels: 1\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#generation","title":"Generation","text":"<pre><code>task:\n  task_type: \"generation\"\n  # No additional fields needed\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#training-configuration","title":"Training Configuration","text":""},{"location":"tutorials/fine_tuning/configuration/#basic-training-settings","title":"Basic Training Settings","text":"<pre><code>finetune:\n  # Output and logging\n  output_dir: \"./outputs\"\n  report_to: \"tensorboard\"\n\n  # Training duration\n  num_train_epochs: 3\n  max_steps: -1  # -1 means use epochs\n\n  # Batch sizes\n  per_device_train_batch_size: 8\n  per_device_eval_batch_size: 16\n  gradient_accumulation_steps: 1\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#optimization-settings","title":"Optimization Settings","text":"<pre><code>finetune:\n  # Learning rate and scheduling\n  learning_rate: 2e-5\n  lr_scheduler_type: \"linear\"  # linear, cosine, cosine_with_restarts, polynomial\n  warmup_ratio: 0.1\n  warmup_steps: 0  # Alternative to warmup_ratio\n\n  # Optimizer settings\n  weight_decay: 0.01\n  adam_beta1: 0.9\n  adam_beta2: 0.999\n  adam_epsilon: 1e-8\n\n  # Gradient handling\n  max_grad_norm: 1.0\n  gradient_accumulation_steps: 1\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":""},{"location":"tutorials/fine_tuning/configuration/#linear-scheduler","title":"Linear Scheduler","text":"<pre><code>finetune:\n  lr_scheduler_type: \"linear\"\n  warmup_ratio: 0.1\n  # Learning rate decreases linearly from warmup to 0\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#cosine-scheduler","title":"Cosine Scheduler","text":"<pre><code>finetune:\n  lr_scheduler_type: \"cosine\"\n  warmup_ratio: 0.1\n  # Learning rate follows cosine curve\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#cosine-with-restarts","title":"Cosine with Restarts","text":"<pre><code>finetune:\n  lr_scheduler_type: \"cosine_with_restarts\"\n  warmup_ratio: 0.1\n  num_train_epochs: 6\n  # Learning rate restarts every 2 epochs\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#polynomial-scheduler","title":"Polynomial Scheduler","text":"<pre><code>finetune:\n  lr_scheduler_type: \"polynomial\"\n  warmup_ratio: 0.1\n  power: 1.0  # Polynomial power\n  # Learning rate decreases polynomially\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#monitoring-and-evaluation","title":"Monitoring and Evaluation","text":"<pre><code>finetune:\n  # Logging\n  logging_strategy: \"steps\"  # steps, epoch, no\n  logging_steps: 100\n  logging_first_step: true\n\n  # Evaluation\n  eval_strategy: \"steps\"  # steps, epoch, no\n  eval_steps: 100\n  eval_delay: 0\n\n  # Saving\n  save_strategy: \"steps\"  # steps, epoch, no\n  save_steps: 500\n  save_total_limit: 3\n  save_safetensors: true\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#model-selection-and-checkpointing","title":"Model Selection and Checkpointing","text":"<pre><code>finetune:\n  # Model selection\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_loss\"  # or \"eval_accuracy\", \"eval_f1\"\n  greater_is_better: false  # false for loss, true for accuracy/f1\n\n  # Checkpointing\n  save_total_limit: 3\n  save_safetensors: true\n  resume_from_checkpoint: null  # Path to resume from\n\n  # Early stopping\n  early_stopping_patience: 3\n  early_stopping_threshold: 0.001\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#advanced-training-options","title":"Advanced Training Options","text":""},{"location":"tutorials/fine_tuning/configuration/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>finetune:\n  # Mixed precision options\n  fp16: false\n  bf16: false\n\n  # FP16 specific settings\n  fp16_full_eval: false\n  fp16_eval: false\n\n  # BF16 specific settings\n  bf16_full_eval: false\n  bf16_eval: false\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#memory-optimization","title":"Memory Optimization","text":"<pre><code>finetune:\n  # Memory optimization\n  dataloader_pin_memory: true\n  dataloader_num_workers: 4\n\n  # Gradient checkpointing\n  gradient_checkpointing: false\n\n  # Memory efficient attention\n  memory_efficient_attention: false\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#reproducibility","title":"Reproducibility","text":"<pre><code>finetune:\n  # Reproducibility\n  seed: 42\n  deterministic: true\n\n  # Data loading\n  dataloader_drop_last: false\n  remove_unused_columns: true\n\n  # Training\n  group_by_length: false\n  length_column_name: \"length\"\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#model-configuration","title":"Model Configuration","text":""},{"location":"tutorials/fine_tuning/configuration/#model-loading","title":"Model Loading","text":"<pre><code>model:\n  # Model source\n  source: \"huggingface\"  # huggingface, modelscope, local\n\n  # Model path\n  path: \"zhangtaolab/plant-dnabert-BPE\"\n\n  # Model options\n  revision: \"main\"\n  trust_remote_code: true\n  torch_dtype: \"float32\"  # float32, float16, bfloat16\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#tokenizer-configuration","title":"Tokenizer Configuration","text":"<pre><code>tokenizer:\n  # Tokenizer options\n  use_fast: true\n  model_max_length: 512\n\n  # Special tokens\n  pad_token: \"[PAD]\"\n  unk_token: \"[UNK]\"\n  mask_token: \"[MASK]\"\n  sep_token: \"[SEP]\"\n  cls_token: \"[CLS]\"\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#data-configuration","title":"Data Configuration","text":""},{"location":"tutorials/fine_tuning/configuration/#dataset-settings","title":"Dataset Settings","text":"<pre><code>dataset:\n  # Data loading\n  max_length: 512\n  truncation: true\n  padding: \"max_length\"\n\n  # Data splitting\n  test_size: 0.2\n  val_size: 0.1\n  random_state: 42\n\n  # Data augmentation\n  augment: true\n  reverse_complement_ratio: 0.5\n  random_mutation_ratio: 0.1\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#data-preprocessing","title":"Data Preprocessing","text":"<pre><code>dataset:\n  preprocessing:\n    # Sequence processing\n    remove_n_bases: true\n    normalize_case: true\n    add_padding: true\n    padding_size: 512\n\n    # Quality filtering\n    min_length: 100\n    max_length: 1000\n    valid_chars: \"ACGT\"\n\n    # Data augmentation\n    reverse_complement: true\n    random_mutations: true\n    mutation_rate: 0.01\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#complete-configuration-examples","title":"Complete Configuration Examples","text":""},{"location":"tutorials/fine_tuning/configuration/#binary-classification-example","title":"Binary Classification Example","text":"<pre><code>task:\n  task_type: \"binary\"\n  num_labels: 2\n  label_names: [\"negative\", \"positive\"]\n  threshold: 0.5\n\nfinetune:\n  output_dir: \"./promoter_classification\"\n  num_train_epochs: 5\n  per_device_train_batch_size: 16\n  per_device_eval_batch_size: 32\n  gradient_accumulation_steps: 1\n\n  # Optimization\n  learning_rate: 2e-5\n  weight_decay: 0.01\n  warmup_ratio: 0.1\n  lr_scheduler_type: \"linear\"\n\n  # Monitoring\n  logging_strategy: \"steps\"\n  logging_steps: 100\n  eval_strategy: \"steps\"\n  eval_steps: 100\n  save_strategy: \"steps\"\n  save_steps: 500\n  save_total_limit: 3\n\n  # Model selection\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_f1\"\n  greater_is_better: true\n\n  # Mixed precision\n  bf16: true\n\n  # Reproducibility\n  seed: 42\n  deterministic: true\n\n  # Reporting\n  report_to: \"tensorboard\"\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#multi-class-classification-example","title":"Multi-class Classification Example","text":"<pre><code>task:\n  task_type: \"multiclass\"\n  num_labels: 4\n  label_names: [\"enzyme\", \"receptor\", \"structural\", \"regulatory\"]\n\nfinetune:\n  output_dir: \"./functional_annotation\"\n  num_train_epochs: 8\n  per_device_train_batch_size: 16\n  per_device_eval_batch_size: 32\n  gradient_accumulation_steps: 2\n\n  # Higher learning rate for multi-class\n  learning_rate: 3e-5\n  weight_decay: 0.02\n  warmup_ratio: 0.15\n  lr_scheduler_type: \"cosine\"\n\n  # Monitoring\n  logging_strategy: \"steps\"\n  logging_steps: 200\n  eval_strategy: \"steps\"\n  eval_steps: 200\n  save_strategy: \"steps\"\n  save_steps: 1000\n  save_total_limit: 5\n\n  # Model selection\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_accuracy\"\n  greater_is_better: true\n\n  # Mixed precision\n  fp16: true\n\n  # Reproducibility\n  seed: 42\n  deterministic: true\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#generation-task-example","title":"Generation Task Example","text":"<pre><code>task:\n  task_type: \"generation\"\n\nfinetune:\n  output_dir: \"./sequence_generation\"\n  num_train_epochs: 15\n  per_device_train_batch_size: 8\n  per_device_eval_batch_size: 16\n  gradient_accumulation_steps: 2\n\n  # Higher learning rate for generation\n  learning_rate: 5e-5\n  weight_decay: 0.01\n  warmup_ratio: 0.2\n  lr_scheduler_type: \"cosine_with_restarts\"\n\n  # Monitoring\n  logging_strategy: \"steps\"\n  logging_steps: 500\n  eval_strategy: \"steps\"\n  eval_steps: 500\n  save_strategy: \"steps\"\n  save_steps: 2000\n  save_total_limit: 3\n\n  # Model selection\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_loss\"\n  greater_is_better: false\n\n  # Generation settings\n  generation_max_length: 512\n  generation_num_beams: 4\n  generation_early_stopping: true\n\n  # Mixed precision\n  bf16: true\n\n  # Reproducibility\n  seed: 42\n  deterministic: true\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#regression-task-example","title":"Regression Task Example","text":"<pre><code>task:\n  task_type: \"regression\"\n  num_labels: 1\n\nfinetune:\n  output_dir: \"./expression_prediction\"\n  num_train_epochs: 10\n  per_device_train_batch_size: 16\n  per_device_eval_batch_size: 32\n  gradient_accumulation_steps: 1\n\n  # Higher learning rate for regression\n  learning_rate: 1e-4\n  weight_decay: 0.05\n  warmup_ratio: 0.1\n  lr_scheduler_type: \"polynomial\"\n\n  # Monitoring\n  logging_strategy: \"steps\"\n  logging_steps: 100\n  eval_strategy: \"steps\"\n  eval_steps: 100\n  save_strategy: \"steps\"\n  save_steps: 500\n  save_total_limit: 3\n\n  # Model selection\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_rmse\"\n  greater_is_better: false\n\n  # Mixed precision\n  fp16: true\n\n  # Reproducibility\n  seed: 42\n  deterministic: true\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"tutorials/fine_tuning/configuration/#development-configuration","title":"Development Configuration","text":"<pre><code>finetune:\n  # Development settings\n  num_train_epochs: 1\n  per_device_train_batch_size: 4\n  logging_steps: 10\n  eval_steps: 50\n  save_steps: 100\n\n  # Quick testing\n  max_steps: 100\n  eval_delay: 0\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#production-configuration","title":"Production Configuration","text":"<pre><code>finetune:\n  # Production settings\n  num_train_epochs: 10\n  per_device_train_batch_size: 32\n  gradient_accumulation_steps: 2\n\n  # Robust training\n  early_stopping_patience: 5\n  save_total_limit: 10\n\n  # Monitoring\n  logging_steps: 500\n  eval_steps: 500\n  save_steps: 2000\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#gpu-memory-optimization","title":"GPU Memory Optimization","text":"<pre><code>finetune:\n  # Memory optimization\n  per_device_train_batch_size: 8\n  gradient_accumulation_steps: 4\n  gradient_checkpointing: true\n  memory_efficient_attention: true\n\n  # Mixed precision\n  bf16: true\n\n  # Data loading\n  dataloader_num_workers: 2\n  dataloader_pin_memory: false\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#configuration-validation","title":"Configuration Validation","text":""},{"location":"tutorials/fine_tuning/configuration/#schema-validation","title":"Schema Validation","text":"<p>DNALLM automatically validates your configuration:</p> <pre><code>from dnallm import validate_config\n\n# Validate configuration\ntry:\n    validate_config(\"finetune_config.yaml\")\n    print(\"Configuration is valid!\")\nexcept ValidationError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#common-validation-errors","title":"Common Validation Errors","text":"Error Cause Solution <code>Invalid task type</code> Unsupported task type Use supported task types <code>Missing required field</code> Incomplete configuration Add missing required fields <code>Invalid learning rate</code> Learning rate too high/low Use reasonable values (1e-6 to 1e-3) <code>Invalid batch size</code> Batch size too large Reduce batch size or use gradient accumulation"},{"location":"tutorials/fine_tuning/configuration/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/fine_tuning/configuration/#1-configuration-organization","title":"1. Configuration Organization","text":"<pre><code># Use descriptive names\nfinetune:\n  output_dir: \"./promoter_classification_2024\"\n\n# Group related settings\nfinetune:\n  # Training duration\n  num_train_epochs: 5\n  max_steps: -1\n\n  # Batch processing\n  per_device_train_batch_size: 16\n  gradient_accumulation_steps: 1\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#2-environment-specific-configs","title":"2. Environment-Specific Configs","text":"<pre><code># Development config\nfinetune:\n  num_train_epochs: 1\n  per_device_train_batch_size: 4\n\n# Production config\nfinetune:\n  num_train_epochs: 10\n  per_device_train_batch_size: 32\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#3-version-control","title":"3. Version Control","text":"<pre><code># Include version information\nconfig_version: \"1.0.0\"\ncreated_by: \"Your Name\"\ncreated_date: \"2024-01-15\"\nexperiment_name: \"promoter_classification_v1\"\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#4-hyperparameter-tuning","title":"4. Hyperparameter Tuning","text":"<pre><code># Use consistent naming for experiments\nfinetune:\n  output_dir: \"./experiments/lr_{learning_rate}_bs_{per_device_train_batch_size}\"\n\n# Document hyperparameter choices\nnotes: \"Testing different learning rates for promoter classification\"\nhyperparameters:\n  learning_rate: \"2e-5\"\n  batch_size: \"16\"\n  scheduler: \"linear\"\n</code></pre>"},{"location":"tutorials/fine_tuning/configuration/#next-steps","title":"Next Steps","text":"<p>After configuring your fine-tuning:</p> <ol> <li>Run Your Training: Follow the Getting Started guide</li> <li>Explore Task-Specific Guides: Check Task-Specific Guides</li> <li>Advanced Techniques: Learn about Advanced Techniques</li> <li>Real-world Examples: See Examples and Use Cases</li> </ol> <p>Need help with configuration? Check our FAQ or open an issue on GitHub.</p>"},{"location":"tutorials/fine_tuning/getting_started/","title":"Getting Started with Fine-tuning","text":"<p>This guide will walk you through the basics of fine-tuning DNA language models using DNALLM. You'll learn how to set up your first fine-tuning experiment, configure models and datasets, and monitor training progress.</p>"},{"location":"tutorials/fine_tuning/getting_started/#overview","title":"Overview","text":"<p>Fine-tuning in DNALLM allows you to: - Adapt pre-trained DNA language models to your specific tasks - Leverage transfer learning for better performance on small datasets - Customize models for domain-specific DNA analysis - Achieve state-of-the-art results with minimal data</p>"},{"location":"tutorials/fine_tuning/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed and configured:</p> <pre><code># Install DNALLM\npip install dnallm\n\n# Or with uv (recommended)\nuv pip install dnallm\n\n# Install additional dependencies for fine-tuning\npip install torch transformers datasets accelerate\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#basic-setup","title":"Basic Setup","text":""},{"location":"tutorials/fine_tuning/getting_started/#1-import-required-modules","title":"1. Import Required Modules","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\nfrom transformers import TrainingArguments\nimport torch\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#2-create-a-simple-configuration","title":"2. Create a Simple Configuration","text":"<p>Create a <code>finetune_config.yaml</code> file:</p> <pre><code># finetune_config.yaml\ntask:\n  task_type: \"binary\"  # binary, multiclass, multilabel, regression, generation, mask, token\n  num_labels: 2\n  label_names: [\"negative\", \"positive\"]\n  threshold: 0.5\n\nfinetune:\n  output_dir: \"./outputs\"\n  num_train_epochs: 3\n  per_device_train_batch_size: 8\n  per_device_eval_batch_size: 16\n  gradient_accumulation_steps: 1\n  learning_rate: 2e-5\n  weight_decay: 0.01\n  warmup_ratio: 0.1\n  logging_strategy: \"steps\"\n  logging_steps: 100\n  eval_strategy: \"steps\"\n  eval_steps: 100\n  save_strategy: \"steps\"\n  save_steps: 500\n  save_total_limit: 3\n  load_best_model_at_end: true\n  metric_for_best_model: \"eval_loss\"\n  report_to: \"tensorboard\"\n  seed: 42\n  bf16: false\n  fp16: false\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#3-load-your-data","title":"3. Load Your Data","text":"<pre><code># Load your dataset\ndataset = DNADataset.load_local_data(\n    \"path/to/your/data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    max_length=512\n)\n\n# Split data into train/validation sets\nif not dataset.is_split:\n    dataset.split_data(test_size=0.2, val_size=0.1)\n\nprint(f\"Training samples: {len(dataset.train_data)}\")\nprint(f\"Validation samples: {len(dataset.val_data)}\")\nprint(f\"Test samples: {len(dataset.test_data)}\")\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#4-load-pre-trained-model","title":"4. Load Pre-trained Model","text":"<pre><code># Load configuration\nconfig = load_config(\"finetune_config.yaml\")\n\n# Load pre-trained model and tokenizer\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(f\"Model loaded on device: {device}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#5-initialize-trainer-and-start-training","title":"5. Initialize Trainer and Start Training","text":"<pre><code># Initialize trainer\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\n# Start training\nprint(\"Starting fine-tuning...\")\ntrainer.train()\n\n# Save the final model\ntrainer.save_model(\"./final_model\")\ntokenizer.save_pretrained(\"./final_model\")\nprint(\"Training completed! Model saved to ./final_model\")\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#command-line-interface","title":"Command Line Interface","text":"<p>DNALLM also provides a convenient command-line interface:</p> <pre><code># Basic fine-tuning run\ndnallm-finetune --config finetune_config.yaml --model zhangtaolab/plant-dnabert-BPE --dataset path/to/data.csv\n\n# Fine-tune with custom parameters\ndnallm-finetune --config config.yaml --epochs 5 --batch-size 16 --learning-rate 1e-4\n\n# Resume from checkpoint\ndnallm-finetune --config config.yaml --resume-from-checkpoint ./checkpoint-1000\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#understanding-the-configuration","title":"Understanding the Configuration","text":""},{"location":"tutorials/fine_tuning/getting_started/#task-configuration","title":"Task Configuration","text":"<p>The <code>task</code> section defines what type of task you're fine-tuning for:</p> <pre><code>task:\n  task_type: \"binary\"           # Task type (see table below)\n  num_labels: 2                 # Number of output classes\n  label_names: [\"neg\", \"pos\"]   # Human-readable label names\n  threshold: 0.5                # Classification threshold\n</code></pre> Task Type Description Output <code>binary</code> Binary classification Single probability (0-1) <code>multiclass</code> Multi-class classification Probability distribution <code>multilabel</code> Multi-label classification Multiple binary outputs <code>regression</code> Continuous value prediction Single real number <code>generation</code> Sequence generation Generated text <code>mask</code> Masked language modeling Predicted tokens <code>token</code> Token classification Labels per token"},{"location":"tutorials/fine_tuning/getting_started/#training-configuration","title":"Training Configuration","text":"<p>The <code>finetune</code> section controls training parameters:</p> <pre><code>finetune:\n  # Basic training settings\n  num_train_epochs: 3                    # Total training epochs\n  per_device_train_batch_size: 8         # Batch size per device\n  per_device_eval_batch_size: 16         # Evaluation batch size\n\n  # Optimization\n  learning_rate: 2e-5                    # Learning rate\n  weight_decay: 0.01                     # Weight decay\n  warmup_ratio: 0.1                      # Warmup proportion\n\n  # Training strategy\n  gradient_accumulation_steps: 1         # Gradient accumulation\n  max_grad_norm: 1.0                    # Gradient clipping\n\n  # Monitoring and saving\n  logging_strategy: \"steps\"              # When to log\n  logging_steps: 100                     # Log every N steps\n  eval_strategy: \"steps\"                 # When to evaluate\n  eval_steps: 100                        # Evaluate every N steps\n  save_strategy: \"steps\"                 # When to save\n  save_steps: 500                        # Save every N steps\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#data-format-requirements","title":"Data Format Requirements","text":"<p>Your dataset should be in one of these formats:</p>"},{"location":"tutorials/fine_tuning/getting_started/#csvtsv-format","title":"CSV/TSV Format","text":"<pre><code>sequence,label\nATCGATCGATCG,1\nGCTAGCTAGCTA,0\nTATATATATATA,1\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#json-format","title":"JSON Format","text":"<pre><code>[\n  {\"sequence\": \"ATCGATCGATCG\", \"label\": 1},\n  {\"sequence\": \"GCTAGCTAGCTA\", \"label\": 0}\n]\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#fasta-format","title":"FASTA Format","text":"<pre><code>&gt;sequence1|label:1\nATCGATCGATCG\n&gt;sequence2|label:0\nGCTAGCTAGCTA\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#example-complete-fine-tuning-workflow","title":"Example: Complete Fine-tuning Workflow","text":"<p>Here's a complete working example:</p> <pre><code>import os\nfrom dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n\ndef run_finetuning():\n    # 1. Check data availability\n    data_path = \"path/to/your/dna_sequences.csv\"\n    if not os.path.exists(data_path):\n        print(\"Please provide a valid data path\")\n        return\n\n    # 2. Load configuration\n    config = load_config(\"finetune_config.yaml\")\n\n    # 3. Load and prepare dataset\n    dataset = DNADataset.load_local_data(\n        data_path,\n        seq_col=\"sequence\",\n        label_col=\"label\",\n        max_length=512\n    )\n\n    # Split data\n    if not dataset.is_split:\n        dataset.split_data(test_size=0.2, val_size=0.1)\n\n    print(f\"Dataset loaded: {len(dataset.train_data)} train, {len(dataset.val_data)} val\")\n\n    # 4. Load pre-trained model\n    model, tokenizer = load_model_and_tokenizer(\n        \"zhangtaolab/plant-dnabert-BPE\",\n        task_config=config['task'],\n        source=\"huggingface\"\n    )\n\n    # 5. Initialize trainer\n    trainer = DNATrainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=dataset.train_data,\n        eval_dataset=dataset.val_data,\n        config=config\n    )\n\n    # 6. Start training\n    print(\"Starting fine-tuning...\")\n    trainer.train()\n\n    # 7. Evaluate on test set\n    test_results = trainer.evaluate(dataset.test_data)\n    print(f\"Test results: {test_results}\")\n\n    # 8. Save model\n    output_dir = \"./finetuned_model\"\n    trainer.save_model(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n    print(f\"Fine-tuning completed! Model saved to {output_dir}\")\n    return output_dir\n\n# Run the complete workflow\nif __name__ == \"__main__\":\n    model_path = run_finetuning()\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#monitoring-training-progress","title":"Monitoring Training Progress","text":""},{"location":"tutorials/fine_tuning/getting_started/#tensorboard-integration","title":"TensorBoard Integration","text":"<p>DNALLM automatically logs training metrics to TensorBoard:</p> <pre><code># Start TensorBoard\ntensorboard --logdir ./outputs\n\n# Open in browser: http://localhost:6006\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<ul> <li>Training Loss: Should decrease over time</li> <li>Validation Loss: Should decrease but not overfit</li> <li>Learning Rate: Should follow the scheduled curve</li> <li>Gradient Norm: Should be stable (around 1.0)</li> <li>Memory Usage: Monitor GPU memory consumption</li> </ul>"},{"location":"tutorials/fine_tuning/getting_started/#early-stopping","title":"Early Stopping","text":"<p>Configure early stopping to prevent overfitting:</p> <pre><code>finetune:\n  # ... other settings ...\n  early_stopping_patience: 3\n  early_stopping_threshold: 0.001\n  metric_for_best_model: \"eval_loss\"\n  greater_is_better: false\n</code></pre>"},{"location":"tutorials/fine_tuning/getting_started/#common-hyperparameters","title":"Common Hyperparameters","text":""},{"location":"tutorials/fine_tuning/getting_started/#learning-rate","title":"Learning Rate","text":"<ul> <li>Conservative: 1e-5 to 5e-5 (good for most cases)</li> <li>Aggressive: 5e-5 to 1e-4 (when you have more data)</li> <li>Very Small: 1e-6 to 1e-5 (when fine-tuning on very similar data)</li> </ul>"},{"location":"tutorials/fine_tuning/getting_started/#batch-size","title":"Batch Size","text":"<ul> <li>Small: 4-8 (when memory is limited)</li> <li>Medium: 8-16 (good balance)</li> <li>Large: 16-32 (when you have sufficient memory)</li> </ul>"},{"location":"tutorials/fine_tuning/getting_started/#training-epochs","title":"Training Epochs","text":"<ul> <li>Short: 1-3 epochs (when data is similar to pre-training)</li> <li>Medium: 3-10 epochs (typical fine-tuning)</li> <li>Long: 10+ epochs (when data is very different)</li> </ul>"},{"location":"tutorials/fine_tuning/getting_started/#next-steps","title":"Next Steps","text":"<p>After completing this basic tutorial:</p> <ol> <li>Explore Task-Specific Guides: Learn about different task types</li> <li>Advanced Techniques: Discover custom training strategies</li> <li>Configuration Options: Check detailed configuration options</li> <li>Real-world Examples: See practical use cases</li> </ol>"},{"location":"tutorials/fine_tuning/getting_started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/fine_tuning/getting_started/#common-issues","title":"Common Issues","text":"<p>\"CUDA out of memory\" error <pre><code># Reduce batch size\nfinetune:\n  per_device_train_batch_size: 4  # Reduced from 8\n  gradient_accumulation_steps: 2   # Compensate for smaller batch\n</code></pre></p> <p>Training loss not decreasing <pre><code># Adjust learning rate\nfinetune:\n  learning_rate: 5e-5  # Increased from 2e-5\n  warmup_ratio: 0.2    # Increased warmup\n</code></pre></p> <p>Overfitting (validation loss increases) <pre><code># Add regularization\nfinetune:\n  weight_decay: 0.1    # Increased from 0.01\n  dropout: 0.2         # Add dropout\n</code></pre></p>"},{"location":"tutorials/fine_tuning/getting_started/#additional-resources","title":"Additional Resources","text":"<ul> <li>Task-Specific Guides - Fine-tuning for different tasks</li> <li>Advanced Techniques - Custom training and optimization</li> <li>Configuration Guide - Detailed configuration options</li> <li>Examples and Use Cases - Real-world scenarios</li> <li>Troubleshooting - Common problems and solutions</li> </ul> <p>Ready for more? Continue to Task-Specific Guides to learn about fine-tuning for different types of DNA analysis tasks.</p>"},{"location":"tutorials/fine_tuning/task_guides/","title":"Task-Specific Fine-tuning Guides","text":"<p>This guide provides detailed instructions for fine-tuning DNA language models on different types of tasks. Each task type has specific requirements, configurations, and best practices.</p>"},{"location":"tutorials/fine_tuning/task_guides/#overview","title":"Overview","text":"<p>DNALLM supports various task types, each requiring different model architectures, loss functions, and evaluation metrics:</p> <ul> <li>Classification Tasks: Binary, multi-class, and multi-label classification</li> <li>Generation Tasks: Sequence generation and completion</li> <li>Masked Language Modeling: Sequence prediction and analysis</li> <li>Token Classification: Named entity recognition and tagging</li> <li>Regression Tasks: Continuous value prediction</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#binary-classification","title":"Binary Classification","text":""},{"location":"tutorials/fine_tuning/task_guides/#use-cases","title":"Use Cases","text":"<ul> <li>Promoter prediction (promoter vs. non-promoter)</li> <li>Motif detection (contains motif vs. doesn't contain)</li> <li>Functional annotation (functional vs. non-functional)</li> <li>Disease association (disease-related vs. normal)</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#configuration","title":"Configuration","text":"<pre><code>task:\n  task_type: \"binary\"\n  num_labels: 2\n  label_names: [\"negative\", \"positive\"]\n  threshold: 0.5  # Classification threshold\n\nfinetune:\n  learning_rate: 2e-5\n  num_train_epochs: 5\n  per_device_train_batch_size: 16\n  metric_for_best_model: \"eval_f1\"  # or \"eval_accuracy\"\n  greater_is_better: true\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#data-format","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,1\nGCTAGCTAGCTA,0\nTATATATATATA,1\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#example-implementation","title":"Example Implementation","text":"<pre><code>from dnallm import load_config, load_model_and_tokenizer, DNADataset, DNATrainer\n\n# Load configuration\nconfig = load_config(\"binary_classification_config.yaml\")\n\n# Load pre-trained model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load dataset\ndataset = DNADataset.load_local_data(\n    \"promoter_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Split data\ndataset.split_data(test_size=0.2, val_size=0.1)\n\n# Initialize trainer\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\n# Train\ntrainer.train()\n\n# Evaluate\ntest_results = trainer.evaluate(dataset.test_data)\nprint(f\"Test F1: {test_results['eval_f1']:.4f}\")\nprint(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#best-practices","title":"Best Practices","text":"<ul> <li>Data Balance: Ensure balanced positive/negative samples</li> <li>Threshold Tuning: Adjust classification threshold based on your needs</li> <li>Evaluation Metrics: Use F1-score for imbalanced datasets</li> <li>Data Augmentation: Apply reverse complement and random mutations</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#multi-class-classification","title":"Multi-class Classification","text":""},{"location":"tutorials/fine_tuning/task_guides/#use-cases_1","title":"Use Cases","text":"<ul> <li>Functional category classification (enzyme, receptor, structural, etc.)</li> <li>Tissue-specific expression classification</li> <li>Evolutionary conservation level classification</li> <li>Regulatory element type classification</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#configuration_1","title":"Configuration","text":"<pre><code>task:\n  task_type: \"multiclass\"\n  num_labels: 4\n  label_names: [\"enzyme\", \"receptor\", \"structural\", \"regulatory\"]\n  # No threshold needed for multi-class\n\nfinetune:\n  learning_rate: 3e-5  # Slightly higher for multi-class\n  num_train_epochs: 8\n  per_device_train_batch_size: 16\n  metric_for_best_model: \"eval_accuracy\"\n  greater_is_better: true\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#data-format_1","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,0\nGCTAGCTAGCTA,1\nTATATATATATA,2\nCGCGCGCGCGCG,3\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#example-implementation_1","title":"Example Implementation","text":"<pre><code># Load multi-class model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load multi-class dataset\ndataset = DNADataset.load_local_data(\n    \"functional_annotation.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Train and evaluate\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\ntrainer.train()\n\n# Multi-class evaluation\ntest_results = trainer.evaluate(dataset.test_data)\nprint(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")\nprint(f\"Test Macro F1: {test_results['eval_f1_macro']:.4f}\")\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#best-practices_1","title":"Best Practices","text":"<ul> <li>Label Encoding: Use integer labels (0, 1, 2, 3) instead of strings</li> <li>Class Balance: Monitor class distribution and use weighted loss if needed</li> <li>Evaluation: Focus on macro-averaged metrics for imbalanced classes</li> <li>Data Augmentation: Apply class-specific augmentation strategies</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#multi-label-classification","title":"Multi-label Classification","text":""},{"location":"tutorials/fine_tuning/task_guides/#use-cases_2","title":"Use Cases","text":"<ul> <li>Multiple functional annotations per sequence</li> <li>Multiple binding site predictions</li> <li>Multiple regulatory element types</li> <li>Multiple disease associations</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#configuration_2","title":"Configuration","text":"<pre><code>task:\n  task_type: \"multilabel\"\n  num_labels: 5\n  label_names: [\"promoter\", \"enhancer\", \"silencer\", \"insulator\", \"locus_control\"]\n  threshold: 0.5  # Per-label threshold\n\nfinetune:\n  learning_rate: 2e-5\n  num_train_epochs: 6\n  per_device_train_batch_size: 16\n  metric_for_best_model: \"eval_f1_micro\"\n  greater_is_better: true\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#data-format_2","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,\"1,0,1,0,0\"\nGCTAGCTAGCTA,\"0,1,0,1,0\"\nTATATATATATA,\"1,1,0,0,1\"\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#example-implementation_2","title":"Example Implementation","text":"<pre><code># Load multi-label model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load multi-label dataset\ndataset = DNADataset.load_local_data(\n    \"multi_label_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"label\",\n    tokenizer=tokenizer,\n    max_length=512,\n    label_separator=\",\"  # Specify label separator\n)\n\n# Train\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\ntrainer.train()\n\n# Multi-label evaluation\ntest_results = trainer.evaluate(dataset.test_data)\nprint(f\"Test Micro F1: {test_results['eval_f1_micro']:.4f}\")\nprint(f\"Test Macro F1: {test_results['eval_f1_macro']:.4f}\")\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#best-practices_2","title":"Best Practices","text":"<ul> <li>Label Separator: Specify the separator used in your label column</li> <li>Threshold Tuning: Optimize per-label thresholds for your use case</li> <li>Loss Function: Use binary cross-entropy with sigmoid activation</li> <li>Evaluation: Focus on micro-averaged metrics for overall performance</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#regression-tasks","title":"Regression Tasks","text":""},{"location":"tutorials/fine_tuning/task_guides/#use-cases_3","title":"Use Cases","text":"<ul> <li>Expression level prediction</li> <li>Binding affinity prediction</li> <li>Conservation score prediction</li> <li>Functional activity prediction</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#configuration_3","title":"Configuration","text":"<pre><code>task:\n  task_type: \"regression\"\n  num_labels: 1  # Single continuous output\n  # No label_names or threshold needed\n\nfinetune:\n  learning_rate: 1e-4  # Higher learning rate for regression\n  num_train_epochs: 10\n  per_device_train_batch_size: 16\n  metric_for_best_model: \"eval_rmse\"\n  greater_is_better: false  # Lower is better for RMSE\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#data-format_3","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,0.85\nGCTAGCTAGCTA,0.23\nTATATATATATA,0.67\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#example-implementation_3","title":"Example Implementation","text":"<pre><code># Load regression model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load regression dataset\ndataset = DNADataset.load_local_data(\n    \"expression_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"expression_level\",\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Train\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\ntrainer.train()\n\n# Regression evaluation\ntest_results = trainer.evaluate(dataset.test_data)\nprint(f\"Test RMSE: {test_results['eval_rmse']:.4f}\")\nprint(f\"Test MAE: {test_results['eval_mae']:.4f}\")\nprint(f\"Test R\u00b2: {test_results['eval_r2']:.4f}\")\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#best-practices_3","title":"Best Practices","text":"<ul> <li>Data Normalization: Normalize your target values (0-1 or z-score)</li> <li>Loss Function: Use MSE or MAE depending on your needs</li> <li>Evaluation: Monitor RMSE, MAE, and R\u00b2 metrics</li> <li>Outlier Handling: Consider robust loss functions for noisy data</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#generation-tasks","title":"Generation Tasks","text":""},{"location":"tutorials/fine_tuning/task_guides/#use-cases_4","title":"Use Cases","text":"<ul> <li>DNA sequence generation</li> <li>Sequence completion</li> <li>Mutant sequence generation</li> <li>Synthetic promoter design</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#configuration_4","title":"Configuration","text":"<pre><code>task:\n  task_type: \"generation\"\n  # No num_labels, label_names, or threshold needed\n\nfinetune:\n  learning_rate: 5e-5  # Higher learning rate for generation\n  num_train_epochs: 15\n  per_device_train_batch_size: 8  # Smaller batch size\n  metric_for_best_model: \"eval_loss\"\n  greater_is_better: false\n  generation_max_length: 512\n  generation_num_beams: 4\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#data-format_4","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,ATCGATCGATCG\nGCTAGCTAGCTA,GCTAGCTAGCTA\nTATATATATATA,TATATATATATA\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#example-implementation_4","title":"Example Implementation","text":"<pre><code># Load generation model (GPT-style)\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnagpt-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load generation dataset\ndataset = DNADataset.load_local_data(\n    \"generation_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"target_sequence\",\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Train\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\ntrainer.train()\n\n# Test generation\ntest_sequences = [\"ATCG\", \"GCTA\", \"TATA\"]\nfor seq in test_sequences:\n    inputs = tokenizer(seq, return_tensors=\"pt\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        max_length=512,\n        num_beams=4,\n        early_stopping=True\n    )\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Input: {seq} -&gt; Generated: {generated}\")\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#best-practices_4","title":"Best Practices","text":"<ul> <li>Model Architecture: Use GPT-style models for generation tasks</li> <li>Sequence Length: Ensure consistent input/output lengths</li> <li>Beam Search: Use beam search for better generation quality</li> <li>Evaluation: Monitor perplexity and generation quality metrics</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#masked-language-modeling","title":"Masked Language Modeling","text":""},{"location":"tutorials/fine_tuning/task_guides/#use-cases_5","title":"Use Cases","text":"<ul> <li>Sequence completion</li> <li>Mutation prediction</li> <li>Missing data imputation</li> <li>Sequence analysis</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#configuration_5","title":"Configuration","text":"<pre><code>task:\n  task_type: \"mask\"\n  # No num_labels, label_names, or threshold needed\n\nfinetune:\n  learning_rate: 3e-5\n  num_train_epochs: 8\n  per_device_train_batch_size: 16\n  metric_for_best_model: \"eval_loss\"\n  greater_is_better: false\n  mlm_probability: 0.15  # Probability of masking tokens\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#data-format_5","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,ATCGATCGATCG\nGCTAGCTAGCTA,GCTAGCTAGCTA\nTATATATATATA,TATATATATATA\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#example-implementation_5","title":"Example Implementation","text":"<pre><code># Load MLM model (BERT-style)\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load MLM dataset\ndataset = DNADataset.load_local_data(\n    \"mlm_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"sequence\",  # Same as input for MLM\n    tokenizer=tokenizer,\n    max_length=512\n)\n\n# Train\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\ntrainer.train()\n\n# Test MLM\ntest_sequence = \"ATCG[MASK]ATCG\"\ninputs = tokenizer(test_sequence, return_tensors=\"pt\")\noutputs = model(**inputs)\npredictions = outputs.logits.argmax(-1)\npredicted_token = tokenizer.decode([predictions[0][4]])  # Position of [MASK]\nprint(f\"Input: {test_sequence} -&gt; Predicted: {predicted_token}\")\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#best-practices_5","title":"Best Practices","text":"<ul> <li>Masking Strategy: Use appropriate masking probability (15% is standard)</li> <li>Model Architecture: Use BERT-style models for MLM tasks</li> <li>Evaluation: Monitor perplexity and accuracy on masked tokens</li> <li>Data Preparation: Ensure sequences are properly tokenized</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#token-classification","title":"Token Classification","text":""},{"location":"tutorials/fine_tuning/task_guides/#use-cases_6","title":"Use Cases","text":"<ul> <li>Named entity recognition (gene identification)</li> <li>Regulatory element tagging</li> <li>Motif boundary detection</li> <li>Functional region annotation</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#configuration_6","title":"Configuration","text":"<pre><code>task:\n  task_type: \"token\"\n  num_labels: 4  # Number of entity types + O (outside)\n  label_names: [\"O\", \"GENE\", \"PROMOTER\", \"ENHANCER\"]\n  # No threshold needed\n\nfinetune:\n  learning_rate: 2e-5\n  num_train_epochs: 6\n  per_device_train_batch_size: 16\n  metric_for_best_model: \"eval_f1\"\n  greater_is_better: true\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#data-format_6","title":"Data Format","text":"<pre><code>sequence,label\nATCGATCGATCG,\"O O O O O O O O O O O O\"\nGCTAGCTAGCTA,\"O GENE GENE GENE O O O O O O O O\"\nTATATATATATA,\"O O O O O O O O O O O O\"\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#example-implementation_6","title":"Example Implementation","text":"<pre><code># Load token classification model\nmodel, tokenizer = load_model_and_tokenizer(\n    \"zhangtaolab/plant-dnabert-BPE\",\n    task_config=config['task'],\n    source=\"huggingface\"\n)\n\n# Load token classification dataset\ndataset = DNADataset.load_local_data(\n    \"ner_data.csv\",\n    seq_col=\"sequence\",\n    label_col=\"labels\",\n    tokenizer=tokenizer,\n    max_length=512,\n    label_separator=\" \"  # Space-separated labels\n)\n\n# Train\ntrainer = DNATrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.train_data,\n    eval_dataset=dataset.val_data,\n    config=config\n)\n\ntrainer.train()\n\n# Test token classification\ntest_sequence = \"ATCGATCGATCG\"\ninputs = tokenizer(test_sequence, return_tensors=\"pt\")\noutputs = model(**inputs)\npredictions = outputs.logits.argmax(-1)\nlabels = [config['task']['label_names'][p] for p in predictions[0]]\nprint(f\"Sequence: {test_sequence}\")\nprint(f\"Labels: {labels}\")\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#best-practices_6","title":"Best Practices","text":"<ul> <li>Label Encoding: Use BIO or BIOES tagging schemes for better performance</li> <li>Sequence Length: Keep sequences manageable for token-level annotation</li> <li>Evaluation: Use sequence-level F1 score and entity-level metrics</li> <li>Data Quality: Ensure high-quality annotations for training</li> </ul>"},{"location":"tutorials/fine_tuning/task_guides/#task-specific-data-augmentation","title":"Task-Specific Data Augmentation","text":""},{"location":"tutorials/fine_tuning/task_guides/#classification-tasks","title":"Classification Tasks","text":"<pre><code># Apply reverse complement augmentation\naugmented_data = []\nfor item in dataset.train_data:\n    # Original sequence\n    augmented_data.append(item)\n\n    # Reverse complement\n    rc_sequence = reverse_complement(item['sequence'])\n    augmented_data.append({\n        'sequence': rc_sequence,\n        'label': item['label']\n    })\n\n# Apply random mutations\nfor item in dataset.train_data:\n    if random.random() &lt; 0.1:  # 10% mutation rate\n        mutated_sequence = apply_random_mutations(item['sequence'])\n        augmented_data.append({\n            'sequence': mutated_sequence,\n            'label': item['label']\n        })\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#generation-tasks_1","title":"Generation Tasks","text":"<pre><code># Apply sequence truncation for generation\naugmented_data = []\nfor item in dataset.train_data:\n    # Full sequence\n    augmented_data.append(item)\n\n    # Truncated sequences for training\n    for length in [256, 384]:\n        if len(item['sequence']) &gt; length:\n            truncated = item['sequence'][:length]\n            augmented_data.append({\n                'sequence': truncated,\n                'label': truncated\n            })\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#evaluation-strategies","title":"Evaluation Strategies","text":""},{"location":"tutorials/fine_tuning/task_guides/#classification-metrics","title":"Classification Metrics","text":"<pre><code># Binary classification\nfrom sklearn.metrics import classification_report, roc_auc_score\n\npredictions = trainer.predict(dataset.test_data)\ny_true = [item['label'] for item in dataset.test_data]\ny_pred = predictions.predictions.argmax(-1)\n\nprint(classification_report(y_true, y_pred))\nprint(f\"ROC AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#generation-metrics","title":"Generation Metrics","text":"<pre><code># Generation quality metrics\nfrom nltk.translate.bleu_score import sentence_bleu\n\ngenerated_sequences = []\nfor item in dataset.test_data:\n    inputs = tokenizer(item['sequence'], return_tensors=\"pt\")\n    outputs = model.generate(inputs[\"input_ids\"], max_length=512)\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    generated_sequences.append(generated)\n\n# Calculate BLEU score\nbleu_scores = []\nfor pred, ref in zip(generated_sequences, [item['label'] for item in dataset.test_data]):\n    score = sentence_bleu([ref.split()], pred.split())\n    bleu_scores.append(score)\n\nprint(f\"Average BLEU: {np.mean(bleu_scores):.4f}\")\n</code></pre>"},{"location":"tutorials/fine_tuning/task_guides/#next-steps","title":"Next Steps","text":"<p>After mastering task-specific fine-tuning:</p> <ol> <li>Explore Advanced Techniques: Learn about custom training strategies</li> <li>Configuration Options: Check detailed configuration options</li> <li>Real-world Examples: See practical use cases</li> <li>Troubleshooting: Visit common issues and solutions</li> </ol> <p>Ready for advanced techniques? Continue to Advanced Techniques to learn about custom training strategies, optimization, and monitoring.</p>"}]}