#!/usr/bin/env python3
"""
Test script for Task Router
"""

import asyncio
import sys
import os
from pathlib import Path

# Add the project root to the Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dnallm.mcp.task_router import TaskRouter, TaskRouterManager, TaskType, TaskConfig


async def test_task_router():
    """æµ‹è¯•ä»»åŠ¡è·¯ç”±å™¨"""
    print("ğŸ§ª Testing Task Router...")
    
    try:
        # åˆ›å»ºä»»åŠ¡è·¯ç”±å™¨
        router = TaskRouter()
        print("âœ“ Task Router created successfully")
        
        # æµ‹è¯•ä»»åŠ¡ç±»å‹æšä¸¾
        task_type = router.get_task_type("binary")
        assert task_type == TaskType.BINARY
        print("âœ“ Task type enum working correctly")
        
        # æµ‹è¯•ä»»åŠ¡é…ç½®åˆ›å»º
        config_dict = {
            "task_type": "binary",
            "num_labels": 2,
            "label_names": ["Negative", "Positive"],
            "threshold": 0.5,
            "describe": "Binary classification task"
        }
        task_config = router.create_task_config(config_dict)
        assert task_config.task_type == TaskType.BINARY
        assert task_config.num_labels == 2
        print("âœ“ Task config creation working correctly")
        
        # æµ‹è¯•äºŒåˆ†ç±»ç»“æœå¤„ç†
        raw_result = {
            "prediction": 1,
            "probabilities": {"Negative": 0.2, "Positive": 0.8}
        }
        sequence = "ATCGATCGATCGATCG"
        model_name = "test_model"
        
        result = await router.route_prediction(raw_result, sequence, model_name, task_config)
        assert result.task_type == TaskType.BINARY
        assert result.prediction == 1
        assert result.confidence == 0.8
        print("âœ“ Binary task routing working correctly")
        
        # æµ‹è¯•ç»“æœæ ¼å¼åŒ–
        formatted = router.format_prediction_result(result)
        assert "prediction" in formatted
        assert "confidence" in formatted
        assert "probabilities" in formatted
        print("âœ“ Result formatting working correctly")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error testing Task Router: {e}")
        return False


async def test_task_router_manager():
    """æµ‹è¯•ä»»åŠ¡è·¯ç”±å™¨ç®¡ç†å™¨"""
    print("\nğŸ§ª Testing Task Router Manager...")
    
    try:
        # åˆ›å»ºç®¡ç†å™¨
        manager = TaskRouterManager()
        print("âœ“ Task Router Manager created successfully")
        
        # æ³¨å†Œä»»åŠ¡é…ç½®
        config_dict = {
            "task_type": "multiclass",
            "num_labels": 3,
            "label_names": ["Class A", "Class B", "Class C"],
            "describe": "Multiclass classification task"
        }
        manager.register_task_config("test_model", config_dict)
        print("âœ“ Task config registration working correctly")
        
        # è·å–ä»»åŠ¡é…ç½®
        task_config = manager.get_task_config("test_model")
        assert task_config is not None
        assert task_config.task_type == TaskType.MULTICLASS
        print("âœ“ Task config retrieval working correctly")
        
        # æµ‹è¯•é¢„æµ‹å¤„ç†
        raw_result = {
            "prediction": 1,
            "probabilities": {"Class A": 0.1, "Class B": 0.7, "Class C": 0.2}
        }
        sequence = "ATCGATCGATCGATCG"
        
        result = await manager.process_prediction(raw_result, sequence, "test_model")
        assert result.task_type == TaskType.MULTICLASS
        assert result.prediction == 1
        assert result.confidence == 0.7
        print("âœ“ Prediction processing working correctly")
        
        # æµ‹è¯•æ ¼å¼åŒ–
        formatted = manager.format_prediction_result(result)
        assert "task_type" in formatted
        assert formatted["task_type"] == "multiclass"
        print("âœ“ Result formatting working correctly")
        
        # æµ‹è¯•å·²æ³¨å†Œæ¨¡å‹åˆ—è¡¨
        models = manager.get_registered_models()
        assert "test_model" in models
        print("âœ“ Registered models list working correctly")
        
        # æµ‹è¯•æŒ‰ä»»åŠ¡ç±»å‹è·å–æ¨¡å‹
        multiclass_models = manager.get_models_by_task_type(TaskType.MULTICLASS)
        assert "test_model" in multiclass_models
        print("âœ“ Models by task type working correctly")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error testing Task Router Manager: {e}")
        return False


async def test_different_task_types():
    """æµ‹è¯•ä¸åŒä»»åŠ¡ç±»å‹"""
    print("\nğŸ§ª Testing Different Task Types...")
    
    try:
        manager = TaskRouterManager()
        
        # æµ‹è¯•äºŒåˆ†ç±»
        binary_config = {
            "task_type": "binary",
            "num_labels": 2,
            "label_names": ["No", "Yes"],
            "threshold": 0.5
        }
        manager.register_task_config("binary_model", binary_config)
        
        binary_result = await manager.process_prediction(
            {"prediction": 1, "probabilities": {"No": 0.3, "Yes": 0.7}},
            "ATCG", "binary_model"
        )
        assert binary_result.task_type == TaskType.BINARY
        print("âœ“ Binary classification working correctly")
        
        # æµ‹è¯•å¤šåˆ†ç±»
        multiclass_config = {
            "task_type": "multiclass",
            "num_labels": 3,
            "label_names": ["A", "B", "C"]
        }
        manager.register_task_config("multiclass_model", multiclass_config)
        
        multiclass_result = await manager.process_prediction(
            {"prediction": 2, "probabilities": {"A": 0.1, "B": 0.2, "C": 0.7}},
            "ATCG", "multiclass_model"
        )
        assert multiclass_result.task_type == TaskType.MULTICLASS
        print("âœ“ Multiclass classification working correctly")
        
        # æµ‹è¯•å¤šæ ‡ç­¾
        multilabel_config = {
            "task_type": "multilabel",
            "num_labels": 2,
            "label_names": ["Label1", "Label2"],
            "threshold": 0.5
        }
        manager.register_task_config("multilabel_model", multilabel_config)
        
        multilabel_result = await manager.process_prediction(
            {"predictions": [1, 0], "probabilities": {"Label1": 0.8, "Label2": 0.3}},
            "ATCG", "multilabel_model"
        )
        assert multilabel_result.task_type == TaskType.MULTILABEL
        print("âœ“ Multilabel classification working correctly")
        
        # æµ‹è¯•å›å½’
        regression_config = {
            "task_type": "regression",
            "num_labels": 1,
            "label_names": ["value"]
        }
        manager.register_task_config("regression_model", regression_config)
        
        regression_result = await manager.process_prediction(
            {"prediction": 0.75, "confidence": 0.9},
            "ATCG", "regression_model"
        )
        assert regression_result.task_type == TaskType.REGRESSION
        print("âœ“ Regression working correctly")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error testing different task types: {e}")
        return False


async def test_task_summary():
    """æµ‹è¯•ä»»åŠ¡æ‘˜è¦åŠŸèƒ½"""
    print("\nğŸ§ª Testing Task Summary...")
    
    try:
        manager = TaskRouterManager()
        
        # æ³¨å†ŒäºŒåˆ†ç±»æ¨¡å‹
        binary_config = {
            "task_type": "binary",
            "num_labels": 2,
            "label_names": ["No", "Yes"],
            "threshold": 0.5
        }
        manager.register_task_config("binary_model", binary_config)
        
        # åˆ›å»ºå¤šä¸ªé¢„æµ‹ç»“æœ
        results = []
        for i in range(5):
            result = await manager.process_prediction(
                {"prediction": i % 2, "probabilities": {"No": 0.3, "Yes": 0.7}},
                f"ATCG{i}", "binary_model"
            )
            results.append(result)
        
        # è·å–ä»»åŠ¡æ‘˜è¦
        summary = manager.get_task_summary(results)
        assert summary["total_predictions"] == 5
        assert summary["task_type"] == "binary"
        assert "positive_predictions" in summary
        assert "negative_predictions" in summary
        print("âœ“ Task summary working correctly")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error testing task summary: {e}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Starting Task Router Tests\n")
    
    tests = [
        test_task_router,
        test_task_router_manager,
        test_different_task_types,
        test_task_summary
    ]
    
    results = []
    for test in tests:
        try:
            result = await test()
            results.append(result)
        except Exception as e:
            print(f"âŒ Test failed with exception: {e}")
            results.append(False)
    
    # æ€»ç»“ç»“æœ
    passed = sum(results)
    total = len(results)
    
    print(f"\nğŸ“Š Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All tests passed!")
        return True
    else:
        print("âŒ Some tests failed")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
