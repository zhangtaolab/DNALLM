{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DNALLM - DNA Large Language Model Toolkit","text":"<p>DNALLM is an open-source toolkit designed for large language model (LLM) applications in DNA sequence analysis and bioinformatics. It provides a comprehensive suite for model training, fine-tuning, inference, benchmarking, and evaluation, specifically tailored for DNA and genomics tasks.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Model Training &amp; Fine-tuning: Supports a variety of DNA-related tasks, including classification, regression, named entity recognition (NER), masked language modeling (MLM), and more.</li> <li>Inference &amp; Benchmarking: Enables efficient model inference, batch prediction, mutagenesis effect analysis, and multi-model benchmarking with visualization tools.</li> <li>Data Processing: Tools for dataset generation, cleaning, formatting, and adaptation to various DNA sequence formats.</li> <li>Model Management: Flexible loading and switching between different DNA language models, supporting both native mamba and transformer-compatible architectures.</li> <li>Extensibility: Modular design with utility functions and configuration modules for easy integration and secondary development.</li> <li>Protocol Support: Implements Model Context Protocol (MCP) for server/client deployment and integration into larger systems.</li> <li>Rich Examples &amp; Documentation: Includes interactive examples (marimo, notebooks) and detailed documentation to help users get started quickly.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li>Install dependencies (recommended: uv)</li> </ol> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n\ngit clone https://github.com/zhangtaolab/DNALLM.git\n\ncd DNALLM\n\nuv venv\n\nsource .venv/bin/activate\n\nuv pip install -e '.[base]'\n</code></pre> <ol> <li>Launch Jupyter Lab or Marimo for interactive development:</li> </ol> <pre><code>uv run jupyter lab\n   # or\nuv run marimo run xxx.py\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<ul> <li><code>dnallm/</code> : Core library (CLI, configuration, datasets, finetune, inference, models, tasks, utils, MCP)</li> <li><code>example/</code> : Interactive and notebook-based examples</li> <li><code>docs/</code> : Documentation</li> <li><code>scripts/</code> : Utility scripts</li> <li><code>tests/</code> : Test suite</li> </ul> <p>For more details, please refer to the README.md and contribution guidelines.</p>"},{"location":"api/datasets/data/","title":"datasets/data API","text":""},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset","title":"<code>DNADataset</code>","text":"Source code in <code>dnallm/datasets/data.py</code> <pre><code>class DNADataset:\n    def __init__(self, ds: Union[Dataset, DatasetDict], tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512):\n        \"\"\"\n        Args:\n            ds (datasets.Dataset or DatasetDict): A Hugging Face Dataset containing at least 'sequence' and 'label' fields.\n            tokenizer (PreTrainedTokenizerBase, optional): A Hugging Face tokenizer for encoding sequences.\n            max_length (int, optional): Maximum length for tokenization.\n        \"\"\"\n        self.dataset = ds\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.sep = None\n        self.multi_label_sep = None\n        self.stats = None\n\n    @classmethod\n    def load_local_data(cls, file_paths, seq_col: str = \"sequence\", label_col: str = \"labels\",\n                        sep: str = None, fasta_sep: str = \"|\",\n                        multi_label_sep: Union[str, None] = None,\n                        tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512) -&gt; any:\n        \"\"\"\n        Load DNA sequence datasets from one or multiple local files.\n\n        Supports input formats: csv, tsv, json, parquet, arrow, dict, fasta, txt.\n\n        Args:\n            file_paths (str, list, or dict):  \n                - Single dataset: Provide one file path (e.g., \"data.csv\").  \n                - Pre-split datasets: Provide a dict like `{\"train\": \"train.csv\", \"test\": \"test.csv\"}`.\n            seq_col (str): Column name for DNA sequences.\n            label_col (str): Column name for labels.\n            sep (str, optional): Delimiter for CSV, TSV, or TXT.\n            fasta_sep (str, optional): Delimiter for FASTA files.\n            multi_label_sep (str, optional): Delimiter for multi-label sequences.\n            tokenizer (PreTrainedTokenizerBase, optional): A tokenizer.\n            max_length (int, optional): Max token length.\n\n        Returns:\n            DNADataset: An instance wrapping a Dataset or DatasetDict.\n        \"\"\"\n        # Set separators\n        cls.sep = sep\n        cls.multi_label_sep = multi_label_sep\n        # Check if input is a list or dict\n        if isinstance(file_paths, dict):  # Handling multiple files (pre-split datasets)\n            ds_dict = {}\n            for split, path in file_paths.items():\n                ds_dict[split] = cls._load_single_data(path, seq_col, label_col, sep, fasta_sep, multi_label_sep)\n            dataset = DatasetDict(ds_dict)\n        else:  # Handling a single file\n            dataset = cls._load_single_data(file_paths, seq_col, label_col, sep, fasta_sep, multi_label_sep)\n\n        return cls(dataset, tokenizer=tokenizer, max_length=max_length)\n\n    @classmethod\n    def _load_single_data(cls, file_path, seq_col: str = \"sequence\", label_col: str = \"labels\",\n                          sep: str = None, fasta_sep: str = \"|\",\n                          multi_label_sep: Union[str, None] = None) -&gt; Dataset:\n        \"\"\"\n        Load DNA data (sequences and labels) from a local file.\n\n        Supported file types: \n          - For structured formats (CSV, TSV, JSON, Parquet, Arrow, dict), uses load_dataset from datasets.\n          - For FASTA and TXT, uses custom parsing.\n\n        Args:\n            file_path: For most file types, a path (or pattern) to the file(s). For 'dict', a dictionary.\n            seq_col (str): Name of the column containing the DNA sequence.\n            label_col (str): Name of the column containing the label.\n            sep (str, optional): Delimiter for CSV, TSV, or TXT files.\n            fasta_sep (str, optional): Delimiter for FASTA files.\n            multi_label_sep (str, optional): Delimiter for multi-label sequences.\n\n        Returns:\n            DNADataset: An instance wrapping a datasets.Dataset.\n        \"\"\"\n        if isinstance(file_path, list):\n            file_path = [os.path.expanduser(fpath) for fpath in file_path]\n            file_type = os.path.basename(file_path[0]).split(\".\")[-1].lower()\n        else:\n            file_path = os.path.expanduser(file_path)\n            file_type = os.path.basename(file_path).split(\".\")[-1].lower()\n        # Define data type\n        default_types = [\"csv\", \"tsv\", \"json\", \"parquet\", \"arrow\"]\n        dict_types = [\"pkl\", \"pickle\", \"dict\"]\n        fasta_types = [\"fa\", \"fna\", \"fas\", \"fasta\"]\n        # Check if the file contains a header\n        if file_type in [\"csv\", \"tsv\", \"txt\"]:\n            if file_type == \"csv\":\n                sep = sep if sep else \",\"\n            with open(file_path, \"r\") as f:\n                header = f.readline().strip()\n                if not header or (seq_col not in header and label_col not in header):\n                    file_type = \"txt\"  # Treat as TXT if no header found\n        # For structured formats that load via datasets.load_dataset\n        if file_type in default_types:\n            if file_type in [\"csv\", \"tsv\"]:\n                sep = sep or (\",\" if file_type == \"csv\" else \"\\t\")\n                ds = load_dataset(\"csv\", data_files=file_path, split=\"train\", delimiter=sep)\n            elif file_type == \"json\":\n                ds = load_dataset(\"json\", data_files=file_path, split=\"train\")\n            elif file_type in [\"parquet\", \"arrow\"]:\n                ds = load_dataset(file_type, data_files=file_path, split=\"train\")\n            # Rename columns if needed\n            if seq_col != \"sequence\":\n                ds = ds.rename_column(seq_col, \"sequence\")\n            if label_col != \"labels\":\n                ds = ds.rename_column(label_col, \"labels\")\n        elif file_type in dict_types:\n            # Here, file_path is assumed to be a dictionary.\n            import pickle\n            data = pickle.load(open(file_path, 'rb'))\n            ds = Dataset.from_dict(data)\n            if seq_col != \"sequence\" or label_col != \"labels\":\n                if seq_col in ds.column_names:\n                    if \"sequence\" not in ds.features:\n                        ds = ds.rename_column(seq_col, \"sequence\")\n                if label_col in ds.column_names:\n                    if \"labels\" not in ds.features:\n                        ds = ds.rename_column(label_col, \"labels\")\n        elif file_type in fasta_types:\n            sequences, labels = [], []\n            with open(file_path, \"r\") as f:\n                seq = \"\"\n                lab = None\n                for line in f:\n                    line = line.strip()\n                    if line.startswith(\"&gt;\"):\n                        if seq and lab is not None:\n                            sequences.append(seq)\n                            labels.append(lab)\n                        lab = line[1:].strip().split(fasta_sep)[-1]  # Assume label is separated by `fasta_sep` in the header\n                        seq = \"\"\n                    else:\n                        seq += line.strip()\n                if seq and lab is not None:\n                    sequences.append(seq)\n                    labels.append(lab)\n            ds = Dataset.from_dict({\"sequence\": sequences, \"labels\": labels})\n        elif file_type == \"txt\":\n            # Assume each line contains a sequence and a label separated by whitespace or a custom sep.\n            sequences, labels = [], []\n            with open(file_path, \"r\") as f:\n                for i,line in enumerate(f):\n                    if i == 0:\n                        # Contain header, use load_dataset with csv method\n                        if seq_col in line and label_col in line:\n                            ds = load_dataset(\"csv\", data_files=file_path, split=\"train\", delimiter=sep)\n                            break\n                    record = line.strip().split(sep) if sep else line.strip().split()\n                    if len(record) &gt;= 2:\n                        sequences.append(record[0])\n                        labels.append(record[1])\n                    else:\n                        continue\n            ds = Dataset.from_dict({\"sequence\": sequences, \"labels\": labels})\n        else:\n            raise ValueError(f\"Unsupported file type: {file_type}\")\n        # Convert string labels to integer\n        def format_labels(example):\n            labels = example['labels']\n            if isinstance(labels, str):\n                if multi_label_sep:\n                    example['labels'] = [float(x) for x in labels.split(multi_label_sep)]\n                else:\n                    example['labels'] = float(labels) if '.' in labels else int(labels)\n            return example\n        if 'labels' in ds.column_names:\n            ds = ds.map(format_labels, desc=\"Format labels\")\n        # Return processed dataset\n        return ds\n\n    @classmethod\n    def from_huggingface(cls, dataset_name: str,\n                         seq_col: str = \"sequence\", label_col: str = \"labels\",\n                         data_dir: Union[str, None]=None,\n                         tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512) -&gt; any:\n        \"\"\"\n        Load a dataset from the Hugging Face Hub.\n\n        Args:\n            dataset_name (str): Name of the dataset.\n            seq_col (str): Column name for the DNA sequence.\n            label_col (str): Column name for the label.\n            data_dir (str): Data directory in a dataset.\n            tokenizer (PreTrainedTokenizerBase): Tokenizer.\n            max_length (int): Max token length.\n\n        Returns:\n            DNADataset: An instance wrapping a datasets.Dataset.\n        \"\"\"\n        if data_dir:\n            ds = load_dataset(dataset_name, data_dir=data_dir)\n        else:\n            ds = load_dataset(dataset_name)\n        # Rename columns if necessary\n        if seq_col != \"sequence\":\n            ds = ds.rename_column(seq_col, \"sequence\")\n        if label_col != \"labels\":\n            ds = ds.rename_column(label_col, \"labels\")\n        return cls(ds, tokenizer=tokenizer, max_length=max_length)\n\n    @classmethod\n    def from_modelscope(cls, dataset_name: str,\n                        seq_col: str = \"sequence\", label_col: str = \"labels\",\n                        data_dir: Union[str, None]=None,\n                        tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512) -&gt; any:\n        \"\"\"\n        Load a dataset from the ModelScope.\n\n        Args:\n            dataset_name (str): Name of the dataset.\n            seq_col (str): Column name for the DNA sequence.\n            label_col (str): Column name for the label.\n            data_dir (str): Data directory in a dataset.\n            tokenizer: Tokenizer.\n            max_length: Max token length.\n\n        Returns:\n            DNADataset: An instance wrapping a datasets.Dataset.\n        \"\"\"\n        from modelscope import MsDataset\n\n        if data_dir:\n            ds = MsDataset.load(dataset_name, data_dir=data_dir)\n        else:\n            ds = MsDataset.load(dataset_name)\n        # Rename columns if necessary\n        if seq_col != \"sequence\":\n            ds = ds.rename_column(seq_col, \"sequence\")\n        if label_col != \"labels\":\n            ds = ds.rename_column(label_col, \"labels\")\n        return cls(ds, tokenizer=tokenizer, max_length=max_length)\n\n    def encode_sequences(self, padding: str = \"max_length\", return_tensors: str = \"pt\",\n                         remove_unused_columns: bool = False,\n                         uppercase: bool=False, lowercase: bool=False,\n                         task: Optional[str] = 'SequenceClassification'):\n        \"\"\"\n        Encode all sequences using the provided tokenizer.\n        The dataset is mapped to include tokenized fields along with the label,\n        making it directly usable with Hugging Face Trainer.\n\n        Args:\n            padding (str): Padding strategy for sequences. this can be 'max_length' or 'longest'.\n                           Use 'longest' to pad to the length of the longest sequence in case of memory outage.\n            return_tensors (str | TensorType): Returned tensor types, can be 'pt' or 'tf' or 'np'.\n            remove_unused_columns: Whether to remove the original 'sequence' and 'label' columns\n            uppercase (bool): Whether to convert sequences to uppercase.\n            lowercase (bool): Whether to convert sequences to lowercase.\n            task (str, optional): Task type for the tokenizer. If not provided, defaults to 'SequenceClassification'.\n        \"\"\"\n        if self.tokenizer:\n            sp_token_map = self.tokenizer.special_tokens_map\n            pad_token = sp_token_map['pad_token'] if 'pad_token' in sp_token_map else None\n            pad_id = self.tokenizer.encode(pad_token)[-1] if pad_token else None\n            cls_token = sp_token_map['cls_token'] if 'cls_token' in sp_token_map else None\n            sep_token = sp_token_map['sep_token'] if 'sep_token' in sp_token_map else None\n            max_length = self.max_length\n        else:\n            raise ValueError(\"Tokenizer not provided.\")\n        def tokenize_for_sequence_classification(example):\n            sequences = example[\"sequence\"]\n            if uppercase:\n                sequences = [x.upper() for x in sequences]\n            if lowercase:\n                sequences = [x.lower() for x in sequences]\n            tokenized = self.tokenizer(\n                sequences,\n                truncation=True,\n                padding=padding,\n                max_length=max_length\n            )\n            return tokenized\n        def tokenize_for_token_classification(examples):\n\n            tokenized_examples = {'sequence': [],\n                                  'input_ids': [],\n                                  # 'token_type_ids': [],\n                                  'attention_mask': []}\n            if 'labels' in examples:\n                tokenized_examples['labels'] = []\n            input_seqs = examples['sequence']\n            if isinstance(input_seqs, str):\n                input_seqs = input_seqs.split(self.multi_label_sep)\n            for i, example_tokens in enumerate(input_seqs):\n                all_ids = [x for x in self.tokenizer.encode(example_tokens, is_split_into_words=True) if x&gt;=0]\n                if 'labels' in examples:\n                    example_ner_tags = examples['labels'][i]\n                else:\n                    example_ner_tags = [0] * len(example_tokens)\n                pad_len = max_length - len(all_ids)\n                if pad_len &gt;= 0:\n                    all_masks = [1] * len(all_ids) + [0] * pad_len\n                    all_ids = all_ids + [pad_id] * pad_len\n                    if cls_token:\n                        if sep_token:\n                            example_tokens = [cls_token] + example_tokens + [sep_token] + [pad_token] * pad_len\n                            example_ner_tags = [-100] + example_ner_tags + [-100] * (pad_len + 1)\n                        else:\n                            example_tokens = [cls_token] + example_tokens + [pad_token] * pad_len\n                            example_ner_tags = [-100] + example_ner_tags + [-100] * pad_len\n                    else:\n                        example_tokens = example_tokens + [pad_token] * pad_len\n                        example_ner_tags = example_ner_tags + [-100] * pad_len\n                elif pad_len &lt; 0:\n                    all_ids = all_ids[:max_length]\n                    all_masks = [1] * (max_length)\n                    if cls_token:\n                        if sep_token:\n                            example_tokens = [cls_token] + example_tokens[:max_length - 2] + [sep_token]\n                            example_ner_tags = [-100] + example_ner_tags[:max_length - 2] + [-100]\n                        else:\n                            example_tokens = [cls_token] + example_tokens[:max_length - 1]\n                            example_ner_tags = [-100] + example_ner_tags[:max_length - 1]\n                    else:\n                        example_tokens = example_tokens[:max_length]\n                        example_ner_tags = example_ner_tags[:max_length]\n                tokenized_examples['sequence'].append(example_tokens)\n                tokenized_examples['input_ids'].append(all_ids)\n                # tokenized_examples['token_type_ids'].append([0] * max_length)\n                tokenized_examples['attention_mask'].append(all_masks)\n                if 'labels' in examples:\n                    tokenized_examples['labels'].append(example_ner_tags)\n            return BatchEncoding(tokenized_examples)\n        # Judge the task type\n        task = task.lower()\n        if task in ['sequenceclassification', 'binary', 'multiclass', 'multilabel', 'regression']:\n            self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True, desc=\"Encoding inputs\")\n        elif task in ['tokenclassification', 'token', 'ner']:\n            from transformers.tokenization_utils_base import BatchEncoding\n            self.dataset = self.dataset.map(tokenize_for_token_classification, batched=True, desc=\"Encoding inputs\")\n        elif task in ['maskedlm', 'mlm', 'mask', 'embedding']:\n            self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True, desc=\"Encoding inputs\")\n        elif task in ['causallm', 'clm', 'causal', 'generation', 'embedding']:\n            self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True)\n        else:\n            self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True, desc=\"Encoding inputs\")\n        if remove_unused_columns:\n            used_cols = ['labels', 'input_ids', 'attention_mask']\n            if isinstance(self.dataset, DatasetDict):\n                for dt in self.dataset:\n                    unused_cols = [f for f in self.dataset[dt].features if f not in used_cols]\n                    self.dataset[dt] = self.dataset[dt].remove_columns(unused_cols)\n            else:\n                unused_cols = [f for f in self.dataset.features if f not in used_cols]\n                self.dataset = self.dataset.remove_columns(unused_cols)\n        if return_tensors == \"tf\":\n            self.dataset.set_format(type=\"tensorflow\")\n        elif return_tensors == \"jax\":\n            self.dataset.set_format(type=\"jax\")\n        elif return_tensors == \"np\":\n            self.dataset.set_format(type=\"numpy\")\n        else:\n            self.dataset.set_format(type=\"torch\")\n\n    def split_data(self, test_size: float = 0.2, val_size: float = 0.1, seed: int = None):\n        \"\"\"\n        Split the dataset into train, test, and validation sets.\n\n        Args:\n            test_size (float): Proportion of the dataset to include in the test split.\n            val_size (float): Proportion of the dataset to include in the validation split.\n            seed (int): Random seed for reproducibility.\n        \"\"\"\n        # First, split off test+validation from training data\n        split_result = self.dataset.train_test_split(test_size=test_size + val_size, seed=seed)\n        train_ds = split_result['train']\n        temp_ds = split_result['test']\n        # Further split temp_ds into test and validation sets\n        if val_size &gt; 0:\n            rel_val_size = val_size / (test_size + val_size)\n            temp_split = temp_ds.train_test_split(test_size=rel_val_size, seed=seed)\n            test_ds = temp_split['train']\n            val_ds = temp_split['test']\n            self.dataset = DatasetDict({'train': train_ds, 'test': test_ds, 'val': val_ds})\n        else:\n            self.dataset = DatasetDict({'train': train_ds, 'test': test_ds})\n\n    def shuffle(self, seed: int = None):\n        \"\"\"\n        Shuffle the dataset.\n\n        Args:\n            seed (int): Random seed for reproducibility.\n        \"\"\"\n        self.dataset.shuffle(seed=seed)\n\n    def validate_sequences(self, minl: int = 20, maxl: int = 6000, gc: tuple = (0,1), valid_chars: str = \"ACGTN\"):\n        \"\"\"\n        Filter the dataset to keep sequences containing valid DNA bases or allowed length.\n\n        Args:\n            minl (int): Minimum length of the sequences.\n            maxl (int): Maximum length of the sequences.\n            gc (tuple): GC content range between 0 and 1.\n            valid_chars (str): Allowed characters in the sequences.\n        \"\"\"\n        self.dataset = self.dataset.filter(\n            lambda example: check_sequence(example[\"sequence\"], minl, maxl, gc, valid_chars)\n        )\n\n    def random_generate(self, minl: int, maxl: int = 0, samples: int = 1,\n                              gc: tuple = (0,1), N_ratio: float = 0.0,\n                              padding_size: int = 0, seed: int = None,\n                              label_func = None, append: bool = False):\n        \"\"\"\n        Replace the current dataset with randomly generated DNA sequences.\n\n        Args:\n            minl: int, minimum length of the sequences\n            maxl: int, maximum length of the sequences, default is the same as minl\n            samples: int, number of sequences to generate, default 1\n            gc: tuple, GC content range, default (0,1)\n            N_ratio: float, include N base in the generated sequence, default 0.0\n            padding_size: int, padding size for sequence length, default 0\n            seed: int, random seed, default None\n            label_func (callable, optional): A function that generates a label from a sequence.\n            append: bool, append the random generated data to the existed dataset or use the data as a dataset\n        \"\"\"\n        def process(minl, maxl, number, gc, N_ratio, padding_size, seed, label_func):\n            sequences = random_generate_sequences(minl=minl, maxl=maxl, samples=number,\n                                                gc=gc, N_ratio=N_ratio,\n                                                padding_size=padding_size, seed=seed)\n            labels = []\n            for seq in sequences:\n                labels.append(label_func(seq) if label_func else 0)\n            random_ds = Dataset.from_dict({\"sequence\": sequences, \"labels\": labels})\n            return random_ds\n        if append:\n            if isinstance(self.dataset, DatasetDict):\n                for dt in self.dataset:\n                    number = round(samples * len(self.dataset[dt]) / sum(self.__len__().values()))\n                    random_ds = process(minl, maxl, number, gc, N_ratio, padding_size, seed, label_func)\n                    self.dataset[dt] = concatenate_datasets([self.dataset[dt], random_ds])\n            else:\n                random_ds = process(minl, maxl, samples, gc, N_ratio, padding_size, seed, label_func)\n                self.dataset = concatenate_datasets([self.dataset, random_ds])\n        else:\n            self.dataset = process(minl, maxl, samples, gc, N_ratio, padding_size, seed, label_func)\n\n    def process_missing_data(self):\n        \"\"\"\n        Filter out samples with missing or empty sequences or labels.\n        \"\"\"\n        def non_missing(example):\n            return example[\"sequence\"] and example[\"labels\"] is not None and example[\"sequence\"].strip() != \"\"\n        self.dataset = self.dataset.filter(non_missing)\n\n    def raw_reverse_complement(self, ratio: float = 0.5, seed: int = None):\n        \"\"\"\n        Do reverse complement of sequences in the dataset.\n\n        Args:\n            ratio (float): Ratio of sequences to reverse complement.\n            seed (int): Random seed for reproducibility.\n        \"\"\"\n        def process(ds, ratio, seed):\n            random.seed(seed)\n            number = len(ds[\"sequence\"])\n            idxlist = set(random.sample(range(number), int(number * ratio)))\n            def concat_fn(example, idx):\n                rc = reverse_complement(example[\"sequence\"])\n                if idx in idxlist:\n                    example[\"sequence\"] = rc\n                return example\n            # Create a dataset with random reverse complement.\n            ds.map(concat_fn, with_indices=True, desc=\"Reverse complementary\")\n            return ds\n        if isinstance(self.dataset, DatasetDict):\n            for dt in self.dataset:\n                self.dataset[dt] = process(self.dataset[dt], ratio, seed)\n        else:\n            self.dataset = process(self.dataset, ratio, seed)\n\n    def augment_reverse_complement(self, reverse=True, complement=True):\n        \"\"\"\n        Augment the dataset by adding reverse complement sequences.\n        This method doubles the dataset size.\n\n        Args:\n            reverse (bool): Whether to do reverse.\n            complement (bool): Whether to do complement.\n        \"\"\"\n        def process(ds, reverse, complement):\n            # Create a dataset with an extra field for the reverse complement.\n            def add_rc(example):\n                example[\"rc_sequence\"] = reverse_complement(\n                    example[\"sequence\"], reverse=reverse, complement=complement\n                )\n                return example\n            ds_with_rc = ds.map(add_rc, desc=\"Reverse complementary\")\n            # Build a new dataset where the reverse complement becomes the 'sequence'\n            rc_ds = ds_with_rc.map(lambda ex: {\"sequence\": ex[\"rc_sequence\"], \"labels\": ex[\"labels\"]}, desc=\"Data augment\")\n            ds = concatenate_datasets([ds, rc_ds])\n            ds.remove_columns([\"rc_sequence\"])\n            return ds\n        if isinstance(self.dataset, DatasetDict):\n            for dt in self.dataset:\n                self.dataset[dt] = process(self.dataset[dt], reverse, complement)\n        else:\n            self.dataset = process(self.dataset, reverse, complement)\n\n    def concat_reverse_complement(self, reverse=True, complement=True, sep: str = \"\"):\n        \"\"\"\n        Augment each sample by concatenating the sequence with its reverse complement.\n\n        Args:\n            reverse (bool): Whether to do reverse.\n            complement (bool): Whether to do complement.\n            sep (str): Separator between the original and reverse complement sequences.\n        \"\"\"\n        def process(ds, reverse, complement, sep):\n            def concat_fn(example):\n                rc = reverse_complement(example[\"sequence\"], reverse=reverse, complement=complement)\n                example[\"sequence\"] = example[\"sequence\"] + sep + rc\n                return example\n            ds = ds.map(concat_fn, desc=\"Data augment\")\n            return ds\n        if isinstance(self.dataset, DatasetDict):\n            for dt in self.dataset:\n                self.dataset[dt] = process(self.dataset[dt], reverse, complement, sep)\n        else:\n            self.dataset = process(self.dataset, reverse, complement, sep)\n\n    def sampling(self, ratio: float=1.0, seed: int = None, overwrite: bool=False) -&gt; any:\n        \"\"\"\n        Randomly sample a fraction of the dataset.\n\n        Args:\n            ratio (float): Fraction of the dataset to sample. Default is 1.0 (no sampling).\n            seed (int): Random seed for reproducibility.\n            overwrite (bool): Whether to overwrite the original dataset with the sampled one.\n\n        Returns:\n            A sampled dataset.\n        \"\"\"\n        dataset = self.dataset\n        if isinstance(dataset, DatasetDict):\n            for dt in dataset.keys():\n                random.seed(seed)\n                random_idx = random.sample(range(len(dataset[dt])), int(len(dataset[dt]) * ratio))\n                dataset[dt] = dataset[dt].select(random_idx)\n        else:\n            random_idx = random.sample(range(len(dataset)), int(len(dataset) * ratio))\n            dataset = dataset.select(random_idx)\n        if overwrite:\n            self.dataset = dataset\n        else:\n            return dataset\n\n    def head(self, head: int=10, show: bool=False) -&gt; dict:\n        \"\"\"\n        Fetch the head n data from the dataset\n\n        Args:\n            head (int): Number of samples to fetch.\n            show (bool): Whether to print the data or return it.\n\n        Returns:\n            dict: A dictionary containing the first n samples.\n        \"\"\"\n        import pprint\n        def format_convert(data):\n            df = {}\n            length = len(data[\"sequence\"])\n            for i in range(length):\n                df[i] = {}\n                for key in data.keys():\n                    df[i][key] = data[key][i]\n            return df\n        dataset = self.dataset\n        if isinstance(dataset, DatasetDict):\n            df = {}\n            for dt in dataset.keys():\n                data = dataset[dt][:head]\n                if show:\n                    print(f\"Dataset: {dt}\")\n                    pprint.pp(format_convert(data))\n                else:\n                    df[dt] = data\n                    return df\n        else:\n            data = dataset[dt][:head]\n            if show:\n                pprint.pp(format_convert(data))\n            else:\n                return data\n\n    def show(self, head: int=10):\n        \"\"\"\n        Display the dataset\n\n        Args:\n            head (int): Number of samples to display.\n        \"\"\"\n        self.head(head=head, show=True)            \n\n    def iter_batches(self, batch_size: int) -&gt; Dataset:\n        \"\"\"\n        Generator that yields batches of examples from the dataset.\n\n        Args:\n            batch_size (int): Size of each batch.\n\n        Yields:\n            A batch of examples.\n        \"\"\"\n        if isinstance(self.dataset, DatasetDict):\n            raise ValueError(\"Dataset is a DatasetDict Object, please use `DNADataset.dataset[datatype].iter_batches(batch_size)` instead.\")\n        else:\n            for i in range(0, len(self.dataset), batch_size):\n                yield self.dataset[i: i + batch_size]\n\n    def __len__(self):\n        if isinstance(self.dataset, DatasetDict):\n            return {dt: len(self.dataset[dt]) for dt in self.dataset}\n        else:\n            return len(self.dataset)\n\n    def __getitem__(self, idx):\n        if isinstance(self.dataset, DatasetDict):\n            raise ValueError(\"Dataset is a DatasetDict Object, please use `DNADataset.dataset[datatype].__getitem__(idx)` instead.\")\n        else:\n            return self.dataset[idx]\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.__init__","title":"<code>__init__(ds, tokenizer=None, max_length=512)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset or DatasetDict</code> <p>A Hugging Face Dataset containing at least 'sequence' and 'label' fields.</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>A Hugging Face tokenizer for encoding sequences.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Maximum length for tokenization.</p> <code>512</code> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def __init__(self, ds: Union[Dataset, DatasetDict], tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512):\n    \"\"\"\n    Args:\n        ds (datasets.Dataset or DatasetDict): A Hugging Face Dataset containing at least 'sequence' and 'label' fields.\n        tokenizer (PreTrainedTokenizerBase, optional): A Hugging Face tokenizer for encoding sequences.\n        max_length (int, optional): Maximum length for tokenization.\n    \"\"\"\n    self.dataset = ds\n    self.tokenizer = tokenizer\n    self.max_length = max_length\n    self.sep = None\n    self.multi_label_sep = None\n    self.stats = None\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.augment_reverse_complement","title":"<code>augment_reverse_complement(reverse=True, complement=True)</code>","text":"<p>Augment the dataset by adding reverse complement sequences. This method doubles the dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>reverse</code> <code>bool</code> <p>Whether to do reverse.</p> <code>True</code> <code>complement</code> <code>bool</code> <p>Whether to do complement.</p> <code>True</code> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def augment_reverse_complement(self, reverse=True, complement=True):\n    \"\"\"\n    Augment the dataset by adding reverse complement sequences.\n    This method doubles the dataset size.\n\n    Args:\n        reverse (bool): Whether to do reverse.\n        complement (bool): Whether to do complement.\n    \"\"\"\n    def process(ds, reverse, complement):\n        # Create a dataset with an extra field for the reverse complement.\n        def add_rc(example):\n            example[\"rc_sequence\"] = reverse_complement(\n                example[\"sequence\"], reverse=reverse, complement=complement\n            )\n            return example\n        ds_with_rc = ds.map(add_rc, desc=\"Reverse complementary\")\n        # Build a new dataset where the reverse complement becomes the 'sequence'\n        rc_ds = ds_with_rc.map(lambda ex: {\"sequence\": ex[\"rc_sequence\"], \"labels\": ex[\"labels\"]}, desc=\"Data augment\")\n        ds = concatenate_datasets([ds, rc_ds])\n        ds.remove_columns([\"rc_sequence\"])\n        return ds\n    if isinstance(self.dataset, DatasetDict):\n        for dt in self.dataset:\n            self.dataset[dt] = process(self.dataset[dt], reverse, complement)\n    else:\n        self.dataset = process(self.dataset, reverse, complement)\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.concat_reverse_complement","title":"<code>concat_reverse_complement(reverse=True, complement=True, sep='')</code>","text":"<p>Augment each sample by concatenating the sequence with its reverse complement.</p> <p>Parameters:</p> Name Type Description Default <code>reverse</code> <code>bool</code> <p>Whether to do reverse.</p> <code>True</code> <code>complement</code> <code>bool</code> <p>Whether to do complement.</p> <code>True</code> <code>sep</code> <code>str</code> <p>Separator between the original and reverse complement sequences.</p> <code>''</code> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def concat_reverse_complement(self, reverse=True, complement=True, sep: str = \"\"):\n    \"\"\"\n    Augment each sample by concatenating the sequence with its reverse complement.\n\n    Args:\n        reverse (bool): Whether to do reverse.\n        complement (bool): Whether to do complement.\n        sep (str): Separator between the original and reverse complement sequences.\n    \"\"\"\n    def process(ds, reverse, complement, sep):\n        def concat_fn(example):\n            rc = reverse_complement(example[\"sequence\"], reverse=reverse, complement=complement)\n            example[\"sequence\"] = example[\"sequence\"] + sep + rc\n            return example\n        ds = ds.map(concat_fn, desc=\"Data augment\")\n        return ds\n    if isinstance(self.dataset, DatasetDict):\n        for dt in self.dataset:\n            self.dataset[dt] = process(self.dataset[dt], reverse, complement, sep)\n    else:\n        self.dataset = process(self.dataset, reverse, complement, sep)\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.encode_sequences","title":"<code>encode_sequences(padding='max_length', return_tensors='pt', remove_unused_columns=False, uppercase=False, lowercase=False, task='SequenceClassification')</code>","text":"<p>Encode all sequences using the provided tokenizer. The dataset is mapped to include tokenized fields along with the label, making it directly usable with Hugging Face Trainer.</p> <p>Parameters:</p> Name Type Description Default <code>padding</code> <code>str</code> <p>Padding strategy for sequences. this can be 'max_length' or 'longest'.            Use 'longest' to pad to the length of the longest sequence in case of memory outage.</p> <code>'max_length'</code> <code>return_tensors</code> <code>str | TensorType</code> <p>Returned tensor types, can be 'pt' or 'tf' or 'np'.</p> <code>'pt'</code> <code>remove_unused_columns</code> <code>bool</code> <p>Whether to remove the original 'sequence' and 'label' columns</p> <code>False</code> <code>uppercase</code> <code>bool</code> <p>Whether to convert sequences to uppercase.</p> <code>False</code> <code>lowercase</code> <code>bool</code> <p>Whether to convert sequences to lowercase.</p> <code>False</code> <code>task</code> <code>str</code> <p>Task type for the tokenizer. If not provided, defaults to 'SequenceClassification'.</p> <code>'SequenceClassification'</code> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def encode_sequences(self, padding: str = \"max_length\", return_tensors: str = \"pt\",\n                     remove_unused_columns: bool = False,\n                     uppercase: bool=False, lowercase: bool=False,\n                     task: Optional[str] = 'SequenceClassification'):\n    \"\"\"\n    Encode all sequences using the provided tokenizer.\n    The dataset is mapped to include tokenized fields along with the label,\n    making it directly usable with Hugging Face Trainer.\n\n    Args:\n        padding (str): Padding strategy for sequences. this can be 'max_length' or 'longest'.\n                       Use 'longest' to pad to the length of the longest sequence in case of memory outage.\n        return_tensors (str | TensorType): Returned tensor types, can be 'pt' or 'tf' or 'np'.\n        remove_unused_columns: Whether to remove the original 'sequence' and 'label' columns\n        uppercase (bool): Whether to convert sequences to uppercase.\n        lowercase (bool): Whether to convert sequences to lowercase.\n        task (str, optional): Task type for the tokenizer. If not provided, defaults to 'SequenceClassification'.\n    \"\"\"\n    if self.tokenizer:\n        sp_token_map = self.tokenizer.special_tokens_map\n        pad_token = sp_token_map['pad_token'] if 'pad_token' in sp_token_map else None\n        pad_id = self.tokenizer.encode(pad_token)[-1] if pad_token else None\n        cls_token = sp_token_map['cls_token'] if 'cls_token' in sp_token_map else None\n        sep_token = sp_token_map['sep_token'] if 'sep_token' in sp_token_map else None\n        max_length = self.max_length\n    else:\n        raise ValueError(\"Tokenizer not provided.\")\n    def tokenize_for_sequence_classification(example):\n        sequences = example[\"sequence\"]\n        if uppercase:\n            sequences = [x.upper() for x in sequences]\n        if lowercase:\n            sequences = [x.lower() for x in sequences]\n        tokenized = self.tokenizer(\n            sequences,\n            truncation=True,\n            padding=padding,\n            max_length=max_length\n        )\n        return tokenized\n    def tokenize_for_token_classification(examples):\n\n        tokenized_examples = {'sequence': [],\n                              'input_ids': [],\n                              # 'token_type_ids': [],\n                              'attention_mask': []}\n        if 'labels' in examples:\n            tokenized_examples['labels'] = []\n        input_seqs = examples['sequence']\n        if isinstance(input_seqs, str):\n            input_seqs = input_seqs.split(self.multi_label_sep)\n        for i, example_tokens in enumerate(input_seqs):\n            all_ids = [x for x in self.tokenizer.encode(example_tokens, is_split_into_words=True) if x&gt;=0]\n            if 'labels' in examples:\n                example_ner_tags = examples['labels'][i]\n            else:\n                example_ner_tags = [0] * len(example_tokens)\n            pad_len = max_length - len(all_ids)\n            if pad_len &gt;= 0:\n                all_masks = [1] * len(all_ids) + [0] * pad_len\n                all_ids = all_ids + [pad_id] * pad_len\n                if cls_token:\n                    if sep_token:\n                        example_tokens = [cls_token] + example_tokens + [sep_token] + [pad_token] * pad_len\n                        example_ner_tags = [-100] + example_ner_tags + [-100] * (pad_len + 1)\n                    else:\n                        example_tokens = [cls_token] + example_tokens + [pad_token] * pad_len\n                        example_ner_tags = [-100] + example_ner_tags + [-100] * pad_len\n                else:\n                    example_tokens = example_tokens + [pad_token] * pad_len\n                    example_ner_tags = example_ner_tags + [-100] * pad_len\n            elif pad_len &lt; 0:\n                all_ids = all_ids[:max_length]\n                all_masks = [1] * (max_length)\n                if cls_token:\n                    if sep_token:\n                        example_tokens = [cls_token] + example_tokens[:max_length - 2] + [sep_token]\n                        example_ner_tags = [-100] + example_ner_tags[:max_length - 2] + [-100]\n                    else:\n                        example_tokens = [cls_token] + example_tokens[:max_length - 1]\n                        example_ner_tags = [-100] + example_ner_tags[:max_length - 1]\n                else:\n                    example_tokens = example_tokens[:max_length]\n                    example_ner_tags = example_ner_tags[:max_length]\n            tokenized_examples['sequence'].append(example_tokens)\n            tokenized_examples['input_ids'].append(all_ids)\n            # tokenized_examples['token_type_ids'].append([0] * max_length)\n            tokenized_examples['attention_mask'].append(all_masks)\n            if 'labels' in examples:\n                tokenized_examples['labels'].append(example_ner_tags)\n        return BatchEncoding(tokenized_examples)\n    # Judge the task type\n    task = task.lower()\n    if task in ['sequenceclassification', 'binary', 'multiclass', 'multilabel', 'regression']:\n        self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True, desc=\"Encoding inputs\")\n    elif task in ['tokenclassification', 'token', 'ner']:\n        from transformers.tokenization_utils_base import BatchEncoding\n        self.dataset = self.dataset.map(tokenize_for_token_classification, batched=True, desc=\"Encoding inputs\")\n    elif task in ['maskedlm', 'mlm', 'mask', 'embedding']:\n        self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True, desc=\"Encoding inputs\")\n    elif task in ['causallm', 'clm', 'causal', 'generation', 'embedding']:\n        self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True)\n    else:\n        self.dataset = self.dataset.map(tokenize_for_sequence_classification, batched=True, desc=\"Encoding inputs\")\n    if remove_unused_columns:\n        used_cols = ['labels', 'input_ids', 'attention_mask']\n        if isinstance(self.dataset, DatasetDict):\n            for dt in self.dataset:\n                unused_cols = [f for f in self.dataset[dt].features if f not in used_cols]\n                self.dataset[dt] = self.dataset[dt].remove_columns(unused_cols)\n        else:\n            unused_cols = [f for f in self.dataset.features if f not in used_cols]\n            self.dataset = self.dataset.remove_columns(unused_cols)\n    if return_tensors == \"tf\":\n        self.dataset.set_format(type=\"tensorflow\")\n    elif return_tensors == \"jax\":\n        self.dataset.set_format(type=\"jax\")\n    elif return_tensors == \"np\":\n        self.dataset.set_format(type=\"numpy\")\n    else:\n        self.dataset.set_format(type=\"torch\")\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.from_huggingface","title":"<code>from_huggingface(dataset_name, seq_col='sequence', label_col='labels', data_dir=None, tokenizer=None, max_length=512)</code>  <code>classmethod</code>","text":"<p>Load a dataset from the Hugging Face Hub.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset.</p> required <code>seq_col</code> <code>str</code> <p>Column name for the DNA sequence.</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for the label.</p> <code>'labels'</code> <code>data_dir</code> <code>str</code> <p>Data directory in a dataset.</p> <code>None</code> <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>Tokenizer.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length.</p> <code>512</code> <p>Returns:</p> Name Type Description <code>DNADataset</code> <code>any</code> <p>An instance wrapping a datasets.Dataset.</p> Source code in <code>dnallm/datasets/data.py</code> <pre><code>@classmethod\ndef from_huggingface(cls, dataset_name: str,\n                     seq_col: str = \"sequence\", label_col: str = \"labels\",\n                     data_dir: Union[str, None]=None,\n                     tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512) -&gt; any:\n    \"\"\"\n    Load a dataset from the Hugging Face Hub.\n\n    Args:\n        dataset_name (str): Name of the dataset.\n        seq_col (str): Column name for the DNA sequence.\n        label_col (str): Column name for the label.\n        data_dir (str): Data directory in a dataset.\n        tokenizer (PreTrainedTokenizerBase): Tokenizer.\n        max_length (int): Max token length.\n\n    Returns:\n        DNADataset: An instance wrapping a datasets.Dataset.\n    \"\"\"\n    if data_dir:\n        ds = load_dataset(dataset_name, data_dir=data_dir)\n    else:\n        ds = load_dataset(dataset_name)\n    # Rename columns if necessary\n    if seq_col != \"sequence\":\n        ds = ds.rename_column(seq_col, \"sequence\")\n    if label_col != \"labels\":\n        ds = ds.rename_column(label_col, \"labels\")\n    return cls(ds, tokenizer=tokenizer, max_length=max_length)\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.from_modelscope","title":"<code>from_modelscope(dataset_name, seq_col='sequence', label_col='labels', data_dir=None, tokenizer=None, max_length=512)</code>  <code>classmethod</code>","text":"<p>Load a dataset from the ModelScope.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset.</p> required <code>seq_col</code> <code>str</code> <p>Column name for the DNA sequence.</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for the label.</p> <code>'labels'</code> <code>data_dir</code> <code>str</code> <p>Data directory in a dataset.</p> <code>None</code> <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>Tokenizer.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length.</p> <code>512</code> <p>Returns:</p> Name Type Description <code>DNADataset</code> <code>any</code> <p>An instance wrapping a datasets.Dataset.</p> Source code in <code>dnallm/datasets/data.py</code> <pre><code>@classmethod\ndef from_modelscope(cls, dataset_name: str,\n                    seq_col: str = \"sequence\", label_col: str = \"labels\",\n                    data_dir: Union[str, None]=None,\n                    tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512) -&gt; any:\n    \"\"\"\n    Load a dataset from the ModelScope.\n\n    Args:\n        dataset_name (str): Name of the dataset.\n        seq_col (str): Column name for the DNA sequence.\n        label_col (str): Column name for the label.\n        data_dir (str): Data directory in a dataset.\n        tokenizer: Tokenizer.\n        max_length: Max token length.\n\n    Returns:\n        DNADataset: An instance wrapping a datasets.Dataset.\n    \"\"\"\n    from modelscope import MsDataset\n\n    if data_dir:\n        ds = MsDataset.load(dataset_name, data_dir=data_dir)\n    else:\n        ds = MsDataset.load(dataset_name)\n    # Rename columns if necessary\n    if seq_col != \"sequence\":\n        ds = ds.rename_column(seq_col, \"sequence\")\n    if label_col != \"labels\":\n        ds = ds.rename_column(label_col, \"labels\")\n    return cls(ds, tokenizer=tokenizer, max_length=max_length)\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.head","title":"<code>head(head=10, show=False)</code>","text":"<p>Fetch the head n data from the dataset</p> <p>Parameters:</p> Name Type Description Default <code>head</code> <code>int</code> <p>Number of samples to fetch.</p> <code>10</code> <code>show</code> <code>bool</code> <p>Whether to print the data or return it.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the first n samples.</p> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def head(self, head: int=10, show: bool=False) -&gt; dict:\n    \"\"\"\n    Fetch the head n data from the dataset\n\n    Args:\n        head (int): Number of samples to fetch.\n        show (bool): Whether to print the data or return it.\n\n    Returns:\n        dict: A dictionary containing the first n samples.\n    \"\"\"\n    import pprint\n    def format_convert(data):\n        df = {}\n        length = len(data[\"sequence\"])\n        for i in range(length):\n            df[i] = {}\n            for key in data.keys():\n                df[i][key] = data[key][i]\n        return df\n    dataset = self.dataset\n    if isinstance(dataset, DatasetDict):\n        df = {}\n        for dt in dataset.keys():\n            data = dataset[dt][:head]\n            if show:\n                print(f\"Dataset: {dt}\")\n                pprint.pp(format_convert(data))\n            else:\n                df[dt] = data\n                return df\n    else:\n        data = dataset[dt][:head]\n        if show:\n            pprint.pp(format_convert(data))\n        else:\n            return data\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.iter_batches","title":"<code>iter_batches(batch_size)</code>","text":"<p>Generator that yields batches of examples from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Size of each batch.</p> required <p>Yields:</p> Type Description <code>Dataset</code> <p>A batch of examples.</p> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def iter_batches(self, batch_size: int) -&gt; Dataset:\n    \"\"\"\n    Generator that yields batches of examples from the dataset.\n\n    Args:\n        batch_size (int): Size of each batch.\n\n    Yields:\n        A batch of examples.\n    \"\"\"\n    if isinstance(self.dataset, DatasetDict):\n        raise ValueError(\"Dataset is a DatasetDict Object, please use `DNADataset.dataset[datatype].iter_batches(batch_size)` instead.\")\n    else:\n        for i in range(0, len(self.dataset), batch_size):\n            yield self.dataset[i: i + batch_size]\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.load_local_data","title":"<code>load_local_data(file_paths, seq_col='sequence', label_col='labels', sep=None, fasta_sep='|', multi_label_sep=None, tokenizer=None, max_length=512)</code>  <code>classmethod</code>","text":"<p>Load DNA sequence datasets from one or multiple local files.</p> <p>Supports input formats: csv, tsv, json, parquet, arrow, dict, fasta, txt.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>str, list, or dict</code> <ul> <li>Single dataset: Provide one file path (e.g., \"data.csv\").  </li> <li>Pre-split datasets: Provide a dict like <code>{\"train\": \"train.csv\", \"test\": \"test.csv\"}</code>.</li> </ul> required <code>seq_col</code> <code>str</code> <p>Column name for DNA sequences.</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for labels.</p> <code>'labels'</code> <code>sep</code> <code>str</code> <p>Delimiter for CSV, TSV, or TXT.</p> <code>None</code> <code>fasta_sep</code> <code>str</code> <p>Delimiter for FASTA files.</p> <code>'|'</code> <code>multi_label_sep</code> <code>str</code> <p>Delimiter for multi-label sequences.</p> <code>None</code> <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>A tokenizer.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Max token length.</p> <code>512</code> <p>Returns:</p> Name Type Description <code>DNADataset</code> <code>any</code> <p>An instance wrapping a Dataset or DatasetDict.</p> Source code in <code>dnallm/datasets/data.py</code> <pre><code>@classmethod\ndef load_local_data(cls, file_paths, seq_col: str = \"sequence\", label_col: str = \"labels\",\n                    sep: str = None, fasta_sep: str = \"|\",\n                    multi_label_sep: Union[str, None] = None,\n                    tokenizer: PreTrainedTokenizerBase = None, max_length: int = 512) -&gt; any:\n    \"\"\"\n    Load DNA sequence datasets from one or multiple local files.\n\n    Supports input formats: csv, tsv, json, parquet, arrow, dict, fasta, txt.\n\n    Args:\n        file_paths (str, list, or dict):  \n            - Single dataset: Provide one file path (e.g., \"data.csv\").  \n            - Pre-split datasets: Provide a dict like `{\"train\": \"train.csv\", \"test\": \"test.csv\"}`.\n        seq_col (str): Column name for DNA sequences.\n        label_col (str): Column name for labels.\n        sep (str, optional): Delimiter for CSV, TSV, or TXT.\n        fasta_sep (str, optional): Delimiter for FASTA files.\n        multi_label_sep (str, optional): Delimiter for multi-label sequences.\n        tokenizer (PreTrainedTokenizerBase, optional): A tokenizer.\n        max_length (int, optional): Max token length.\n\n    Returns:\n        DNADataset: An instance wrapping a Dataset or DatasetDict.\n    \"\"\"\n    # Set separators\n    cls.sep = sep\n    cls.multi_label_sep = multi_label_sep\n    # Check if input is a list or dict\n    if isinstance(file_paths, dict):  # Handling multiple files (pre-split datasets)\n        ds_dict = {}\n        for split, path in file_paths.items():\n            ds_dict[split] = cls._load_single_data(path, seq_col, label_col, sep, fasta_sep, multi_label_sep)\n        dataset = DatasetDict(ds_dict)\n    else:  # Handling a single file\n        dataset = cls._load_single_data(file_paths, seq_col, label_col, sep, fasta_sep, multi_label_sep)\n\n    return cls(dataset, tokenizer=tokenizer, max_length=max_length)\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.process_missing_data","title":"<code>process_missing_data()</code>","text":"<p>Filter out samples with missing or empty sequences or labels.</p> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def process_missing_data(self):\n    \"\"\"\n    Filter out samples with missing or empty sequences or labels.\n    \"\"\"\n    def non_missing(example):\n        return example[\"sequence\"] and example[\"labels\"] is not None and example[\"sequence\"].strip() != \"\"\n    self.dataset = self.dataset.filter(non_missing)\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.random_generate","title":"<code>random_generate(minl, maxl=0, samples=1, gc=(0, 1), N_ratio=0.0, padding_size=0, seed=None, label_func=None, append=False)</code>","text":"<p>Replace the current dataset with randomly generated DNA sequences.</p> <p>Parameters:</p> Name Type Description Default <code>minl</code> <code>int</code> <p>int, minimum length of the sequences</p> required <code>maxl</code> <code>int</code> <p>int, maximum length of the sequences, default is the same as minl</p> <code>0</code> <code>samples</code> <code>int</code> <p>int, number of sequences to generate, default 1</p> <code>1</code> <code>gc</code> <code>tuple</code> <p>tuple, GC content range, default (0,1)</p> <code>(0, 1)</code> <code>N_ratio</code> <code>float</code> <p>float, include N base in the generated sequence, default 0.0</p> <code>0.0</code> <code>padding_size</code> <code>int</code> <p>int, padding size for sequence length, default 0</p> <code>0</code> <code>seed</code> <code>int</code> <p>int, random seed, default None</p> <code>None</code> <code>label_func</code> <code>callable</code> <p>A function that generates a label from a sequence.</p> <code>None</code> <code>append</code> <code>bool</code> <p>bool, append the random generated data to the existed dataset or use the data as a dataset</p> <code>False</code> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def random_generate(self, minl: int, maxl: int = 0, samples: int = 1,\n                          gc: tuple = (0,1), N_ratio: float = 0.0,\n                          padding_size: int = 0, seed: int = None,\n                          label_func = None, append: bool = False):\n    \"\"\"\n    Replace the current dataset with randomly generated DNA sequences.\n\n    Args:\n        minl: int, minimum length of the sequences\n        maxl: int, maximum length of the sequences, default is the same as minl\n        samples: int, number of sequences to generate, default 1\n        gc: tuple, GC content range, default (0,1)\n        N_ratio: float, include N base in the generated sequence, default 0.0\n        padding_size: int, padding size for sequence length, default 0\n        seed: int, random seed, default None\n        label_func (callable, optional): A function that generates a label from a sequence.\n        append: bool, append the random generated data to the existed dataset or use the data as a dataset\n    \"\"\"\n    def process(minl, maxl, number, gc, N_ratio, padding_size, seed, label_func):\n        sequences = random_generate_sequences(minl=minl, maxl=maxl, samples=number,\n                                            gc=gc, N_ratio=N_ratio,\n                                            padding_size=padding_size, seed=seed)\n        labels = []\n        for seq in sequences:\n            labels.append(label_func(seq) if label_func else 0)\n        random_ds = Dataset.from_dict({\"sequence\": sequences, \"labels\": labels})\n        return random_ds\n    if append:\n        if isinstance(self.dataset, DatasetDict):\n            for dt in self.dataset:\n                number = round(samples * len(self.dataset[dt]) / sum(self.__len__().values()))\n                random_ds = process(minl, maxl, number, gc, N_ratio, padding_size, seed, label_func)\n                self.dataset[dt] = concatenate_datasets([self.dataset[dt], random_ds])\n        else:\n            random_ds = process(minl, maxl, samples, gc, N_ratio, padding_size, seed, label_func)\n            self.dataset = concatenate_datasets([self.dataset, random_ds])\n    else:\n        self.dataset = process(minl, maxl, samples, gc, N_ratio, padding_size, seed, label_func)\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.raw_reverse_complement","title":"<code>raw_reverse_complement(ratio=0.5, seed=None)</code>","text":"<p>Do reverse complement of sequences in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>Ratio of sequences to reverse complement.</p> <code>0.5</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>None</code> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def raw_reverse_complement(self, ratio: float = 0.5, seed: int = None):\n    \"\"\"\n    Do reverse complement of sequences in the dataset.\n\n    Args:\n        ratio (float): Ratio of sequences to reverse complement.\n        seed (int): Random seed for reproducibility.\n    \"\"\"\n    def process(ds, ratio, seed):\n        random.seed(seed)\n        number = len(ds[\"sequence\"])\n        idxlist = set(random.sample(range(number), int(number * ratio)))\n        def concat_fn(example, idx):\n            rc = reverse_complement(example[\"sequence\"])\n            if idx in idxlist:\n                example[\"sequence\"] = rc\n            return example\n        # Create a dataset with random reverse complement.\n        ds.map(concat_fn, with_indices=True, desc=\"Reverse complementary\")\n        return ds\n    if isinstance(self.dataset, DatasetDict):\n        for dt in self.dataset:\n            self.dataset[dt] = process(self.dataset[dt], ratio, seed)\n    else:\n        self.dataset = process(self.dataset, ratio, seed)\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.sampling","title":"<code>sampling(ratio=1.0, seed=None, overwrite=False)</code>","text":"<p>Randomly sample a fraction of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>Fraction of the dataset to sample. Default is 1.0 (no sampling).</p> <code>1.0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the original dataset with the sampled one.</p> <code>False</code> <p>Returns:</p> Type Description <code>any</code> <p>A sampled dataset.</p> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def sampling(self, ratio: float=1.0, seed: int = None, overwrite: bool=False) -&gt; any:\n    \"\"\"\n    Randomly sample a fraction of the dataset.\n\n    Args:\n        ratio (float): Fraction of the dataset to sample. Default is 1.0 (no sampling).\n        seed (int): Random seed for reproducibility.\n        overwrite (bool): Whether to overwrite the original dataset with the sampled one.\n\n    Returns:\n        A sampled dataset.\n    \"\"\"\n    dataset = self.dataset\n    if isinstance(dataset, DatasetDict):\n        for dt in dataset.keys():\n            random.seed(seed)\n            random_idx = random.sample(range(len(dataset[dt])), int(len(dataset[dt]) * ratio))\n            dataset[dt] = dataset[dt].select(random_idx)\n    else:\n        random_idx = random.sample(range(len(dataset)), int(len(dataset) * ratio))\n        dataset = dataset.select(random_idx)\n    if overwrite:\n        self.dataset = dataset\n    else:\n        return dataset\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.show","title":"<code>show(head=10)</code>","text":"<p>Display the dataset</p> <p>Parameters:</p> Name Type Description Default <code>head</code> <code>int</code> <p>Number of samples to display.</p> <code>10</code> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def show(self, head: int=10):\n    \"\"\"\n    Display the dataset\n\n    Args:\n        head (int): Number of samples to display.\n    \"\"\"\n    self.head(head=head, show=True)            \n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.shuffle","title":"<code>shuffle(seed=None)</code>","text":"<p>Shuffle the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>None</code> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def shuffle(self, seed: int = None):\n    \"\"\"\n    Shuffle the dataset.\n\n    Args:\n        seed (int): Random seed for reproducibility.\n    \"\"\"\n    self.dataset.shuffle(seed=seed)\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.split_data","title":"<code>split_data(test_size=0.2, val_size=0.1, seed=None)</code>","text":"<p>Split the dataset into train, test, and validation sets.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>float</code> <p>Proportion of the dataset to include in the test split.</p> <code>0.2</code> <code>val_size</code> <code>float</code> <p>Proportion of the dataset to include in the validation split.</p> <code>0.1</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>None</code> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def split_data(self, test_size: float = 0.2, val_size: float = 0.1, seed: int = None):\n    \"\"\"\n    Split the dataset into train, test, and validation sets.\n\n    Args:\n        test_size (float): Proportion of the dataset to include in the test split.\n        val_size (float): Proportion of the dataset to include in the validation split.\n        seed (int): Random seed for reproducibility.\n    \"\"\"\n    # First, split off test+validation from training data\n    split_result = self.dataset.train_test_split(test_size=test_size + val_size, seed=seed)\n    train_ds = split_result['train']\n    temp_ds = split_result['test']\n    # Further split temp_ds into test and validation sets\n    if val_size &gt; 0:\n        rel_val_size = val_size / (test_size + val_size)\n        temp_split = temp_ds.train_test_split(test_size=rel_val_size, seed=seed)\n        test_ds = temp_split['train']\n        val_ds = temp_split['test']\n        self.dataset = DatasetDict({'train': train_ds, 'test': test_ds, 'val': val_ds})\n    else:\n        self.dataset = DatasetDict({'train': train_ds, 'test': test_ds})\n</code></pre>"},{"location":"api/datasets/data/#dnallm.datasets.data.DNADataset.validate_sequences","title":"<code>validate_sequences(minl=20, maxl=6000, gc=(0, 1), valid_chars='ACGTN')</code>","text":"<p>Filter the dataset to keep sequences containing valid DNA bases or allowed length.</p> <p>Parameters:</p> Name Type Description Default <code>minl</code> <code>int</code> <p>Minimum length of the sequences.</p> <code>20</code> <code>maxl</code> <code>int</code> <p>Maximum length of the sequences.</p> <code>6000</code> <code>gc</code> <code>tuple</code> <p>GC content range between 0 and 1.</p> <code>(0, 1)</code> <code>valid_chars</code> <code>str</code> <p>Allowed characters in the sequences.</p> <code>'ACGTN'</code> Source code in <code>dnallm/datasets/data.py</code> <pre><code>def validate_sequences(self, minl: int = 20, maxl: int = 6000, gc: tuple = (0,1), valid_chars: str = \"ACGTN\"):\n    \"\"\"\n    Filter the dataset to keep sequences containing valid DNA bases or allowed length.\n\n    Args:\n        minl (int): Minimum length of the sequences.\n        maxl (int): Maximum length of the sequences.\n        gc (tuple): GC content range between 0 and 1.\n        valid_chars (str): Allowed characters in the sequences.\n    \"\"\"\n    self.dataset = self.dataset.filter(\n        lambda example: check_sequence(example[\"sequence\"], minl, maxl, gc, valid_chars)\n    )\n</code></pre>"},{"location":"api/finetune/trainer/","title":"finetune/trainer API","text":""},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer","title":"<code>DNATrainer</code>","text":"<p>DNA Language Model Trainer that supports multiple model types.</p> <p>This trainer class provides a unified interface for training, evaluating, and predicting with DNA language models. It supports various task types including classification, regression, and masked language modeling.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The DNA language model to be trained.</p> <code>task_config</code> <code>dict</code> <p>Configuration for the specific task.</p> <code>train_config</code> <code>dict</code> <p>Configuration for training parameters.</p> <code>datasets</code> <code>DNADataset</code> <p>Dataset for training and evaluation.</p> <code>extra_args</code> <code>Dict</code> <p>Additional training arguments.</p> <p>Examples:</p> <pre><code>trainer = DNATrainer(\n    model=model,\n    config=config,\n    datasets=datasets\n)\nmetrics = trainer.train()\n</code></pre> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>class DNATrainer:\n    \"\"\"DNA Language Model Trainer that supports multiple model types.\n\n    This trainer class provides a unified interface for training, evaluating, and predicting\n    with DNA language models. It supports various task types including classification,\n    regression, and masked language modeling.\n\n    Attributes:\n        model: The DNA language model to be trained.\n        task_config (dict): Configuration for the specific task.\n        train_config (dict): Configuration for training parameters.\n        datasets (DNADataset, optional): Dataset for training and evaluation.\n        extra_args (Dict, optional): Additional training arguments.\n\n    Examples:\n        ```python\n        trainer = DNATrainer(\n            model=model,\n            config=config,\n            datasets=datasets\n        )\n        metrics = trainer.train()\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model: any,\n        config: dict,\n        datasets: Optional[DNADataset] = None,\n        extra_args: Optional[Dict] = None,\n    ):\n        \"\"\"Initialize the DNA trainer.\n\n        Args:\n            model: The DNA language model to be trained.\n            config (dict): Configuration dictionary containing task and training settings.\n            datasets (DNADataset, optional): Dataset for training and evaluation.\n            extra_args (Dict, optional): Additional training arguments to override defaults.\n        \"\"\"\n        self.model = model\n        self.task_config = config['task']\n        self.train_config = config['finetune']\n        self.datasets = datasets\n        self.extra_args = extra_args\n\n        self.set_up_trainer()\n\n    def set_up_trainer(self):\n        \"\"\"Set up the HuggingFace Trainer with appropriate configurations.\n\n        This method:\n        1. Configures training arguments\n        2. Sets up dataset splits\n        3. Configures task-specific metrics\n        4. Sets up appropriate data collator\n        5. Initializes the HuggingFace Trainer\n        \"\"\"\n        # Setup training arguments\n        training_args = self.train_config.model_dump()\n        if self.extra_args:\n            training_args.update(self.extra_args)\n        self.training_args = TrainingArguments(\n            **training_args,\n        )\n        # Check if the dataset has been split\n        if isinstance(self.datasets.dataset, DatasetDict):        \n            self.data_split = self.datasets.dataset.keys()\n        else:\n            self.data_split = [None]\n        # Get datasets\n        if \"train\" in self.data_split:\n            train_dataset = self.datasets.dataset[\"train\"]\n        else:\n            if len(self.data_split) == 1:\n                train_dataset = self.datasets.dataset\n            else:\n                raise KeyError(\"Cannot find train data.\")\n        eval_key = [x for x in self.data_split if x not in ['train', 'test']]\n        if eval_key:\n            eval_dataset = self.datasets.dataset[eval_key[0]]\n        elif \"test\" in self.data_split:\n            eval_dataset = self.datasets.dataset['test']\n        else:\n            eval_dataset = None\n\n        # Get compute metrics\n        compute_metrics = self.compute_task_metrics()\n        # Set data collator\n        if self.task_config.task_type == \"mask\":\n            from transformers import DataCollatorForLanguageModeling\n            mlm_probability = self.task_config.mlm_probability\n            mlm_probability = mlm_probability if mlm_probability else 0.15\n            data_collator = DataCollatorForLanguageModeling(\n                tokenizer=self.datasets.tokenizer,\n                mlm=True, mlm_probability=mlm_probability\n            )\n        elif self.task_config.task_type == \"generation\":\n            from transformers import DataCollatorForLanguageModeling\n            data_collator = DataCollatorForLanguageModeling(\n                tokenizer=self.datasets.tokenizer,\n                mlm=False\n            )\n        else:\n            data_collator = None\n        # Initialize trainer\n        self.trainer = Trainer(\n            model=self.model,\n            args=self.training_args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            compute_metrics=compute_metrics,\n            data_collator=data_collator,\n        )\n\n    def compute_task_metrics(self) -&gt; Callable:\n        \"\"\"Compute task-specific evaluation metrics.\n\n        Returns:\n            Callable: A function that computes metrics for the specific task type.\n        \"\"\"\n        return compute_metrics(self.task_config)\n\n    def train(self, save_tokenizer: bool = False) -&gt; Dict[str, float]:\n        \"\"\"Train the model and return training metrics.\n\n        Args:\n            save_tokenizer (bool, optional): Whether to save the tokenizer along with the model.\n                Defaults to False.\n\n        Returns:\n            Dict[str, float]: Dictionary containing training metrics.\n        \"\"\"\n        self.model.train()\n        train_result = self.trainer.train()\n        metrics = train_result.metrics\n        # Save the model\n        self.trainer.save_model()\n        if save_tokenizer:\n            self.datasets.tokenizer.save_pretrained(self.train_config.output_dir)\n        return metrics\n\n    def evaluate(self) -&gt; Dict[str, float]:\n        \"\"\"Evaluate the model on the evaluation dataset.\n\n        Returns:\n            Dict[str, float]: Dictionary containing evaluation metrics.\n        \"\"\"\n        self.model.eval()\n        result = self.trainer.evaluate()\n        return result\n\n    def predict(self) -&gt; Dict[str, float]:\n        \"\"\"Generate predictions on the test dataset.\n\n        Returns:\n            Dict[str, float]: Dictionary containing prediction results and metrics.\n        \"\"\"\n        self.model.eval()\n        result = {}\n        if \"test\" in self.data_split:\n            test_dataset = self.datasets.dataset['test']\n            result = self.trainer.predict(test_dataset)\n        return result\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.__init__","title":"<code>__init__(model, config, datasets=None, extra_args=None)</code>","text":"<p>Initialize the DNA trainer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>any</code> <p>The DNA language model to be trained.</p> required <code>config</code> <code>dict</code> <p>Configuration dictionary containing task and training settings.</p> required <code>datasets</code> <code>DNADataset</code> <p>Dataset for training and evaluation.</p> <code>None</code> <code>extra_args</code> <code>Dict</code> <p>Additional training arguments to override defaults.</p> <code>None</code> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def __init__(\n    self,\n    model: any,\n    config: dict,\n    datasets: Optional[DNADataset] = None,\n    extra_args: Optional[Dict] = None,\n):\n    \"\"\"Initialize the DNA trainer.\n\n    Args:\n        model: The DNA language model to be trained.\n        config (dict): Configuration dictionary containing task and training settings.\n        datasets (DNADataset, optional): Dataset for training and evaluation.\n        extra_args (Dict, optional): Additional training arguments to override defaults.\n    \"\"\"\n    self.model = model\n    self.task_config = config['task']\n    self.train_config = config['finetune']\n    self.datasets = datasets\n    self.extra_args = extra_args\n\n    self.set_up_trainer()\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.compute_task_metrics","title":"<code>compute_task_metrics()</code>","text":"<p>Compute task-specific evaluation metrics.</p> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A function that computes metrics for the specific task type.</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def compute_task_metrics(self) -&gt; Callable:\n    \"\"\"Compute task-specific evaluation metrics.\n\n    Returns:\n        Callable: A function that computes metrics for the specific task type.\n    \"\"\"\n    return compute_metrics(self.task_config)\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.evaluate","title":"<code>evaluate()</code>","text":"<p>Evaluate the model on the evaluation dataset.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary containing evaluation metrics.</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def evaluate(self) -&gt; Dict[str, float]:\n    \"\"\"Evaluate the model on the evaluation dataset.\n\n    Returns:\n        Dict[str, float]: Dictionary containing evaluation metrics.\n    \"\"\"\n    self.model.eval()\n    result = self.trainer.evaluate()\n    return result\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.predict","title":"<code>predict()</code>","text":"<p>Generate predictions on the test dataset.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary containing prediction results and metrics.</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def predict(self) -&gt; Dict[str, float]:\n    \"\"\"Generate predictions on the test dataset.\n\n    Returns:\n        Dict[str, float]: Dictionary containing prediction results and metrics.\n    \"\"\"\n    self.model.eval()\n    result = {}\n    if \"test\" in self.data_split:\n        test_dataset = self.datasets.dataset['test']\n        result = self.trainer.predict(test_dataset)\n    return result\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.set_up_trainer","title":"<code>set_up_trainer()</code>","text":"<p>Set up the HuggingFace Trainer with appropriate configurations.</p> <p>This method: 1. Configures training arguments 2. Sets up dataset splits 3. Configures task-specific metrics 4. Sets up appropriate data collator 5. Initializes the HuggingFace Trainer</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def set_up_trainer(self):\n    \"\"\"Set up the HuggingFace Trainer with appropriate configurations.\n\n    This method:\n    1. Configures training arguments\n    2. Sets up dataset splits\n    3. Configures task-specific metrics\n    4. Sets up appropriate data collator\n    5. Initializes the HuggingFace Trainer\n    \"\"\"\n    # Setup training arguments\n    training_args = self.train_config.model_dump()\n    if self.extra_args:\n        training_args.update(self.extra_args)\n    self.training_args = TrainingArguments(\n        **training_args,\n    )\n    # Check if the dataset has been split\n    if isinstance(self.datasets.dataset, DatasetDict):        \n        self.data_split = self.datasets.dataset.keys()\n    else:\n        self.data_split = [None]\n    # Get datasets\n    if \"train\" in self.data_split:\n        train_dataset = self.datasets.dataset[\"train\"]\n    else:\n        if len(self.data_split) == 1:\n            train_dataset = self.datasets.dataset\n        else:\n            raise KeyError(\"Cannot find train data.\")\n    eval_key = [x for x in self.data_split if x not in ['train', 'test']]\n    if eval_key:\n        eval_dataset = self.datasets.dataset[eval_key[0]]\n    elif \"test\" in self.data_split:\n        eval_dataset = self.datasets.dataset['test']\n    else:\n        eval_dataset = None\n\n    # Get compute metrics\n    compute_metrics = self.compute_task_metrics()\n    # Set data collator\n    if self.task_config.task_type == \"mask\":\n        from transformers import DataCollatorForLanguageModeling\n        mlm_probability = self.task_config.mlm_probability\n        mlm_probability = mlm_probability if mlm_probability else 0.15\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.datasets.tokenizer,\n            mlm=True, mlm_probability=mlm_probability\n        )\n    elif self.task_config.task_type == \"generation\":\n        from transformers import DataCollatorForLanguageModeling\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.datasets.tokenizer,\n            mlm=False\n        )\n    else:\n        data_collator = None\n    # Initialize trainer\n    self.trainer = Trainer(\n        model=self.model,\n        args=self.training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n</code></pre>"},{"location":"api/finetune/trainer/#dnallm.finetune.trainer.DNATrainer.train","title":"<code>train(save_tokenizer=False)</code>","text":"<p>Train the model and return training metrics.</p> <p>Parameters:</p> Name Type Description Default <code>save_tokenizer</code> <code>bool</code> <p>Whether to save the tokenizer along with the model. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary containing training metrics.</p> Source code in <code>dnallm/finetune/trainer.py</code> <pre><code>def train(self, save_tokenizer: bool = False) -&gt; Dict[str, float]:\n    \"\"\"Train the model and return training metrics.\n\n    Args:\n        save_tokenizer (bool, optional): Whether to save the tokenizer along with the model.\n            Defaults to False.\n\n    Returns:\n        Dict[str, float]: Dictionary containing training metrics.\n    \"\"\"\n    self.model.train()\n    train_result = self.trainer.train()\n    metrics = train_result.metrics\n    # Save the model\n    self.trainer.save_model()\n    if save_tokenizer:\n        self.datasets.tokenizer.save_pretrained(self.train_config.output_dir)\n    return metrics\n</code></pre>"},{"location":"api/inference/predictor/","title":"inference/predictor API","text":""},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor","title":"<code>DNAPredictor</code>","text":"<p>DNA sequence predictor using fine-tuned models.</p> <p>This class provides functionality for making predictions using DNA language models. It handles model loading, inference, and result processing.</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>class DNAPredictor:\n    \"\"\"DNA sequence predictor using fine-tuned models.\n\n    This class provides functionality for making predictions using DNA language models.\n    It handles model loading, inference, and result processing.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: any,\n        tokenizer: any,\n        config: dict\n    ):\n        \"\"\"Initialize the predictor.\n\n        Args:\n            model: Fine-tuned model instance.\n            tokenizer: Tokenizer for the model.\n            config: Configuration dictionary containing task settings and inference parameters.\n        \"\"\"\n\n        self.model = model\n        self.tokenizer = tokenizer\n        self.task_config = config['task']\n        self.pred_config = config['inference']\n        self.device = self._get_device()\n        if model:\n            self.model.to(self.device)\n            print(f\"Use device: {self.device}\")\n        self.sequences = []\n        self.labels = []\n\n    def _get_device(self):\n        \"\"\"Get the appropriate device for model inference.\n\n        Returns:\n            torch.device: The device to use for model inference.\n\n        Raises:\n            ValueError: If the specified device type is not supported.\n        \"\"\"\n        # Get the device type\n        device = self.pred_config.device.lower()\n        if device == 'cpu':\n            return torch.device('cpu')\n        elif device in ['cuda', 'nvidia']:\n            if not torch.cuda.is_available():\n                warnings.warn(\"CUDA is not available. Please check your installation. Use CPU instead.\")\n                return torch.device('cpu')\n            else:\n                return torch.device('cuda')\n        elif device in ['mps', 'apple', 'mac']:\n            if not torch.backends.mps.is_available():\n                warnings.warn(\"MPS is not available. Please check your installation. Use CPU instead.\")\n                return torch.device('cpu')\n            else:\n                return torch.device('mps')\n        elif device in ['rocm', 'amd']:\n            if not torch.cuda.is_available():\n                warnings.warn(\"ROCm is not available. Please check your installation. Use CPU instead.\")\n                return torch.device('cpu')\n            else:\n                return torch.device('cuda')\n        elif device == ['tpu', 'xla', 'google']:\n            try:\n                import torch_xla.core.xla_model as xm\n                return torch.device('xla')\n            except:\n                warnings.warn(\"TPU is not available. Please check your installation. Use CPU instead.\")\n                return torch.device('cpu')\n        elif device == ['xpu', 'intel']:\n            if not torch.xpu.is_available():\n                warnings.warn(\"XPU is not available. Please check your installation. Use CPU instead.\")\n                return torch.device('cpu')\n            else:\n                return torch.device('xpu')\n        elif device == 'auto':\n            if torch.cuda.is_available():\n                return torch.device('cuda')\n            elif torch.backends.mps.is_available():\n                return torch.device('mps')\n            elif torch.xpu.is_available():\n                return torch.device('xpu')\n            else:\n                return torch.device('cpu')\n        else:\n            raise ValueError(f\"Unsupported device type: {device}\")\n\n    def generate_dataset(self, seq_or_path: Union[str, List[str]], batch_size: int=1,\n                         seq_col: str=\"sequence\", label_col: str=\"labels\",\n                         sep: str = None, fasta_sep: str = \"|\",\n                         multi_label_sep: Union[str, None] = None,\n                         uppercase: bool=False, lowercase: bool=False,\n                         keep_seqs: bool=True, do_encode: bool=True) -&gt; tuple:\n        \"\"\"Generate dataset from sequences.\n\n        Args:\n            seq_or_path: Single sequence or path to a file containing sequences.\n            batch_size: Batch size for DataLoader.\n            seq_col: Column name for sequences.\n            label_col: Column name for labels.\n            sep (str, optional): Delimiter for CSV, TSV, or TXT files.\n            fasta_sep (str, optional): Delimiter for FASTA files.\n            multi_label_sep (str, optional): Delimiter for multi-label sequences.\n            uppercase (bool): Whether to convert sequences to uppercase.\n            lowercase (bool): Whether to convert sequences to lowercase.\n            keep_seqs: Whether to keep sequences in the dataset.\n            do_encode: Whether to encode sequences.\n\n        Returns:\n            tuple: A tuple containing:\n                - Dataset object\n                - DataLoader object\n\n        Raises:\n            ValueError: If input is neither a file path nor a list of sequences.\n        \"\"\"\n        if isinstance(seq_or_path, str):\n            suffix = seq_or_path.split(\".\")[-1]\n            if suffix and os.path.isfile(seq_or_path):\n                sequences = []\n                dataset = DNADataset.load_local_data(seq_or_path, seq_col=seq_col, label_col=label_col,\n                                                     sep=sep, fasta_sep=fasta_sep, multi_label_sep=multi_label_sep,\n                                                     tokenizer=self.tokenizer, max_length=self.pred_config.max_length)\n            else:\n                sequences = [seq_or_path]\n        elif isinstance(seq_or_path, list):\n            sequences = seq_or_path\n        else:\n            raise ValueError(\"Input should be a file path or a list of sequences.\")\n        if len(sequences) &gt; 0:\n            ds = Dataset.from_dict({\"sequence\": sequences})\n            dataset = DNADataset(ds, self.tokenizer, max_length=self.pred_config.max_length)\n        # If labels are provided, keep labels\n        if keep_seqs:\n            self.sequences = dataset.dataset[\"sequence\"]\n        # Encode sequences\n        if do_encode:\n            task_type = self.task_config.task_type\n            dataset.encode_sequences(remove_unused_columns=True, task=task_type, uppercase=uppercase, lowercase=lowercase)\n        if \"labels\" in dataset.dataset.features:\n            self.labels = dataset.dataset[\"labels\"]\n        # Create DataLoader\n        dataloader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            num_workers=self.pred_config.num_workers\n        )\n\n        return dataset, dataloader\n\n    def logits_to_preds(self, logits: list) -&gt; tuple[torch.Tensor, list]:\n        \"\"\"Convert model logits to predictions.\n\n        Args:\n            logits: Model output logits.\n\n        Returns:\n            tuple: A tuple containing:\n                - torch.Tensor: Model predictions\n                - list: Human-readable labels\n\n        Raises:\n            ValueError: If task type is not supported.\n        \"\"\"\n        # Get task type and threshold from config\n        task_type = self.task_config.task_type\n        threshold = self.task_config.threshold\n        label_names = self.task_config.label_names\n        # Convert logits to predictions based on task type\n        if task_type == \"binary\":\n            probs = torch.softmax(logits, dim=-1)\n            preds = (probs[:, 1] &gt; threshold).long()\n            labels = [label_names[pred] for pred in preds]\n        elif task_type == \"multiclass\":\n            probs = torch.softmax(logits, dim=-1)\n            preds = torch.argmax(probs, dim=-1)\n            labels = [label_names[pred] for pred in preds]\n        elif task_type == \"multilabel\":\n            probs = torch.sigmoid(logits)\n            preds = (probs &gt; threshold).long()\n            labels = []\n            for pred in preds:\n                label = [label_names[i] for i in range(len(pred)) if pred[i] == 1]\n                labels.append(label)\n        elif task_type == \"regression\":\n            preds = logits.squeeze(-1)\n            probs = preds\n            labels = preds.tolist()\n        elif task_type == \"token\":\n            probs = torch.softmax(logits, dim=-1)\n            preds = torch.argmax(logits, dim=-1)\n            labels = []\n            for pred in preds:\n                label = [label_names[pred[i]] for i in range(len(pred))]\n                labels.append(label)\n        else:\n            raise ValueError(f\"Unsupported task type: {task_type}\")\n        return probs, labels\n\n    def format_output(self, predictions: tuple[torch.Tensor, list]) -&gt; dict:\n        \"\"\"Format output predictions.\n\n        Args:\n            predictions: Tuple containing predictions.\n\n        Returns:\n            dict: Dictionary containing formatted predictions.\n        \"\"\"\n        # Get task type from config\n        task_type = self.task_config.task_type\n        formatted_predictions = {}\n        probs, labels = predictions\n        probs = probs.numpy().tolist()\n        keep_seqs = True if len(self.sequences) else False\n        label_names = self.task_config.label_names\n        for i, label in enumerate(labels):\n            prob = probs[i]\n            formatted_predictions[i] = {\n                'sequence': self.sequences[i] if keep_seqs else '',\n                'label': label,\n                'scores': {label_names[j]: p for j, p in enumerate(prob)} if task_type != \"token\"\n                          else [max(x) for x in prob],\n            }\n        return formatted_predictions\n\n    @torch.no_grad()\n    def batch_predict(self, dataloader: DataLoader, do_pred: bool=True,\n                      output_hidden_states: bool=False,\n                      output_attentions: bool=False) -&gt; tuple[torch.Tensor, list]:\n        \"\"\"Predict for a batch of sequences.\n\n        Args:\n            dataloader: DataLoader object containing sequences.\n            do_pred: Whether to do prediction.\n            output_hidden_states: Whether to output hidden states.\n            output_attentions: Whether to output attentions.\n\n        Returns:\n            tuple: A tuple containing:\n                - torch.Tensor: All logits\n                - dict: Predictions dictionary\n                - dict: Embeddings dictionary\n        \"\"\"\n        # Set model to evaluation mode\n        self.model.eval()\n        all_logits = []\n        # Whether or not to output hidden states\n        params = None\n        embeddings = {}\n        if output_hidden_states:\n            import inspect\n            sig = inspect.signature(self.model.forward)\n            params = sig.parameters\n            if 'output_hidden_states' in params:\n                self.model.config.output_hidden_states = True\n            embeddings['hidden_states'] = None\n            embeddings['attention_mask'] = []\n            embeddings['labels'] = []\n        if output_attentions:\n            if not params:\n                import inspect\n                sig = inspect.signature(self.model.forward)\n                params = sig.parameters\n            if 'output_attentions' in params:\n                self.model.config.output_attentions = True\n            embeddings['attentions'] = None\n        # Iterate over batches\n        for batch in tqdm(dataloader, desc=\"Predicting\"):\n            inputs = {k: v.to(self.device) for k, v in batch.items()}\n            if self.pred_config.use_fp16:\n                self.model = self.model.half()\n                with torch.amp.autocast('cuda'):\n                    outputs = self.model(**inputs)\n            else:\n                outputs = self.model(**inputs)\n            # Get logits\n            logits = outputs.logits.cpu().detach()\n            all_logits.append(logits)\n            # Get hidden states\n            if output_hidden_states:\n                hidden_states = [h.cpu().detach() for h in outputs.hidden_states] if hasattr(outputs, 'hidden_states') else None\n                if embeddings['hidden_states'] is None:\n                    embeddings['hidden_states'] = [[h] for h in hidden_states]\n                else:\n                    for i, h in enumerate(hidden_states):\n                        embeddings['hidden_states'][i].append(h)\n                attention_mask = inputs['attention_mask'].cpu().detach() if 'attention_mask' in inputs else None\n                embeddings['attention_mask'].append(attention_mask)\n                labels = inputs['labels'].cpu().detach() if 'labels' in inputs else None\n                embeddings['labels'].append(labels)\n            # Get attentions\n            if output_attentions:\n                attentions = [a.cpu().detach() for a in outputs.attentions] if hasattr(outputs, 'attentions') else None\n                if attentions:\n                    if embeddings['attentions'] is None:\n                        embeddings['attentions'] = [[a] for a in attentions]\n                    else:\n                        for i, a in enumerate(attentions):\n                            embeddings['attentions'][i].append(a)\n        # Concatenate logits\n        all_logits = torch.cat(all_logits, dim=0)\n        if output_hidden_states:\n            if embeddings['hidden_states']:\n                embeddings['hidden_states'] = tuple(torch.cat(lst, dim=0) for lst in embeddings['hidden_states'])\n            if embeddings['attention_mask']:\n                embeddings['attention_mask'] = torch.cat(embeddings['attention_mask'], dim=0)\n            if embeddings['labels']:\n                embeddings['labels'] = torch.cat(embeddings['labels'], dim=0)\n        if output_attentions:\n            if embeddings['attentions']:\n                embeddings['attentions'] = tuple(torch.cat(lst, dim=0) for lst in embeddings['attentions'])\n        # Get predictions\n        predictions = None\n        if do_pred:\n            predictions = self.logits_to_preds(all_logits)\n            predictions = self.format_output(predictions)\n        return all_logits, predictions, embeddings\n\n    def predict_seqs(self, sequences: Union[str, List[str]],\n                     evaluate: bool = False,\n                     output_hidden_states: bool=False,\n                     output_attentions: bool=False,\n                     save_to_file: bool = False) -&gt; Union[tuple, dict]:\n        \"\"\"Predict for sequences.\n\n        Args:\n            sequences: Single sequence or list of sequences.\n            evaluate: Whether to evaluate the predictions.\n            output_hidden_states: Whether to output hidden states and attentions.\n            output_attentions: Whether to output attentions.\n            save_to_file: Whether to save predictions to file.\n\n        Returns:\n            Union[tuple, dict]: Either:\n                - Dictionary containing predictions\n                - Tuple of (predictions, metrics) if evaluate=True\n        \"\"\"\n        # Get dataset and dataloader from sequences\n        _, dataloader = self.generate_dataset(sequences, batch_size=self.pred_config.batch_size)\n        # Do batch prediction\n        logits, predictions, embeddings = self.batch_predict(dataloader,\n                                                             output_hidden_states=output_hidden_states,\n                                                             output_attentions=output_attentions)\n        # Keep hidden states\n        if output_hidden_states or output_attentions:\n            self.embeddings = embeddings\n        # Save predictions\n        if save_to_file and self.pred_config.output_dir:\n            save_predictions(predictions, Path(self.pred_config.output_dir))\n        # Do evaluation\n        if len(self.labels) == len(logits) and evaluate:\n            metrics = self.calculate_metrics(logits, self.labels)\n            metrics_save = dict(metrics)\n            if 'curve' in metrics_save:\n                del metrics_save['curve']\n            if 'scatter' in metrics_save:\n                del metrics_save['scatter']\n            if save_to_file and self.pred_config.output_dir:\n                save_metrics(metrics_save, Path(self.pred_config.output_dir))\n            return predictions, metrics\n\n        return predictions\n\n\n    def predict_file(self, file_path: str, evaluate: bool = False,\n                     output_hidden_states: bool=False,\n                     output_attentions: bool=False,\n                     seq_col: str=\"sequence\", label_col: str=\"labels\",\n                     sep: str = None, fasta_sep: str = \"|\",\n                     multi_label_sep: Union[str, None] = None,\n                     uppercase: bool=False, lowercase: bool=False,\n                     save_to_file: bool=False, plot_metrics: bool=False) -&gt; Union[tuple, dict]:\n        \"\"\"Predict from a file containing sequences.\n\n        Args:\n            file_path: Path to the file containing sequences.\n            evaluate: Whether to evaluate the predictions.\n            output_hidden_states: Whether to output hidden states.\n            output_attentions: Whether to output attentions.\n            seq_col: Column name for sequences.\n            label_col: Column name for labels.\n            sep (str, optional): Delimiter for CSV, TSV, or TXT files.\n            fasta_sep (str, optional): Delimiter for FASTA files.\n            multi_label_sep (str, optional): Delimiter for multi-label sequences.\n            uppercase (bool): Whether to convert sequences to uppercase.\n            lowercase (bool): Whether to convert sequences to lowercase.\n            save_to_file: Whether to save predictions to file.\n            plot_metrics: Whether to plot metrics.\n\n        Returns:\n            Union[tuple, dict]: Either:\n                - List of dictionaries containing predictions\n                - Tuple of (predictions, metrics) if evaluate=True\n        \"\"\"\n        # Get dataset and dataloader from file\n        _, dataloader = self.generate_dataset(file_path, seq_col=seq_col, label_col=label_col,\n                                              sep=sep, fasta_sep=fasta_sep, multi_label_sep=multi_label_sep,\n                                              uppercase=uppercase, lowercase=lowercase,\n                                              batch_size=self.pred_config.batch_size)\n        # Do batch prediction\n        if output_attentions:\n            warnings.warn(\"Cautions: output_attentions may consume a lot of memory.\\n\")\n        logits, predictions, embeddings = self.batch_predict(dataloader,\n                                                             output_hidden_states=output_hidden_states,\n                                                             output_attentions=output_attentions)\n        # Keep hidden states\n        if output_hidden_states or output_attentions:\n            self.embeddings = embeddings\n        # Save predictions\n        if save_to_file and self.pred_config.output_dir:\n            save_predictions(predictions, Path(self.pred_config.output_dir))\n        # Do evaluation\n        if len(self.labels) == len(logits) and evaluate:\n            metrics = self.calculate_metrics(logits, self.labels, plot=plot_metrics)\n            metrics_save = dict(metrics)\n            if 'curve' in metrics_save:\n                del metrics_save['curve']\n            if 'scatter' in metrics_save:\n                del metrics_save['scatter']\n            if save_to_file and self.pred_config.output_dir:\n                save_metrics(metrics, Path(self.pred_config.output_dir))\n            # Whether to plot metrics\n            if plot_metrics:\n                return predictions, metrics\n            else:\n                return predictions, metrics_save\n\n        return predictions\n\n    def calculate_metrics(self, logits: Union[List, torch.Tensor],\n                          labels: Union[List, torch.Tensor], plot: bool=False) -&gt; dict:\n        \"\"\"Calculate evaluation metrics.\n\n        Args:\n            logits: Model predictions.\n            labels: True labels.\n            plot: Whether to plot metrics.\n\n        Returns:\n            dict: Dictionary containing evaluation metrics.\n        \"\"\"\n        # Calculate metrics based on task type\n        compute_metrics = Metrics(self.task_config, plot=plot)\n        metrics = compute_metrics((logits, labels))\n\n        return metrics\n\n    def plot_attentions(self, seq_idx: int = 0, layer: int = -1, head: int = -1,\n                        width: int = 800, height: int = 800,\n                        save_path: Optional[str] = None) -&gt; None:\n        \"\"\"Plot attention map.\n\n        Args:\n            seq_idx: Index of the sequence to plot.\n            layer: Layer index to plot.\n            head: Head index to plot.\n            width: Width of the plot.\n            height: Height of the plot.\n            save_path: Path to save the plot.\n\n        Returns:\n            None: If no attention weights are available.\n            object: Attention map visualization if available.\n        \"\"\"\n        if hasattr(self, 'embeddings'):\n            attentions = self.embeddings['attentions']\n            if save_path:\n                suffix = os.path.splitext(save_path)[-1]\n                if suffix:\n                    heatmap = save_path.replace(suffix, \"_heatmap\" + suffix)\n                else:\n                    heatmap = os.path.join(save_path, \"heatmap.pdf\")\n            else:\n                heatmap = None\n            # Plot attention map\n            attn_map = plot_attention_map(attentions, self.sequences, self.tokenizer,\n                                          seq_idx=seq_idx, layer=layer, head=head,\n                                          width=width, height=height,\n                                          save_path=heatmap)\n            return attn_map\n        else:\n            print(\"No attention weights available to plot.\")\n\n    def plot_hidden_states(self, reducer: str=\"t-SNE\",\n                           ncols: int=4, width: int = 300, height: int = 300,\n                           save_path: Optional[str] = None) -&gt; None:\n        \"\"\"Embedding visualization.\n\n        Args:\n            reducer: Dimensionality reduction method to use.\n            ncols: Number of columns in the plot grid.\n            width: Width of the plot.\n            height: Height of the plot.\n            save_path: Path to save the plot.\n\n        Returns:\n            None: If no hidden states are available.\n            object: Embedding visualization if available.\n        \"\"\"\n        if hasattr(self, 'embeddings'):\n            hidden_states = self.embeddings['hidden_states']\n            attention_mask = torch.unsqueeze(self.embeddings['attention_mask'], dim=-1)\n            labels = self.embeddings['labels']\n            if save_path:\n                suffix = os.path.splitext(save_path)[-1]\n                if suffix:\n                    embedding = save_path.replace(suffix, \"_embedding\" + suffix)\n                else:\n                    embedding = os.path.join(save_path, \"embedding.pdf\")\n            else:\n                embedding = None\n            # Plot hidden states\n            label_names = self.task_config.label_names\n            embeddings_vis = plot_embeddings(hidden_states, attention_mask, reducer=reducer,\n                                             labels=labels, label_names=label_names,\n                                             ncols=ncols, width=width, height=height,\n                                             save_path=embedding)\n            return embeddings_vis\n        else:\n            print(\"No hidden states available to plot.\")\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.__init__","title":"<code>__init__(model, tokenizer, config)</code>","text":"<p>Initialize the predictor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>any</code> <p>Fine-tuned model instance.</p> required <code>tokenizer</code> <code>any</code> <p>Tokenizer for the model.</p> required <code>config</code> <code>dict</code> <p>Configuration dictionary containing task settings and inference parameters.</p> required Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def __init__(\n    self,\n    model: any,\n    tokenizer: any,\n    config: dict\n):\n    \"\"\"Initialize the predictor.\n\n    Args:\n        model: Fine-tuned model instance.\n        tokenizer: Tokenizer for the model.\n        config: Configuration dictionary containing task settings and inference parameters.\n    \"\"\"\n\n    self.model = model\n    self.tokenizer = tokenizer\n    self.task_config = config['task']\n    self.pred_config = config['inference']\n    self.device = self._get_device()\n    if model:\n        self.model.to(self.device)\n        print(f\"Use device: {self.device}\")\n    self.sequences = []\n    self.labels = []\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.batch_predict","title":"<code>batch_predict(dataloader, do_pred=True, output_hidden_states=False, output_attentions=False)</code>","text":"<p>Predict for a batch of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>DataLoader object containing sequences.</p> required <code>do_pred</code> <code>bool</code> <p>Whether to do prediction.</p> <code>True</code> <code>output_hidden_states</code> <code>bool</code> <p>Whether to output hidden states.</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>Whether to output attentions.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Tensor, list]</code> <p>A tuple containing: - torch.Tensor: All logits - dict: Predictions dictionary - dict: Embeddings dictionary</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>@torch.no_grad()\ndef batch_predict(self, dataloader: DataLoader, do_pred: bool=True,\n                  output_hidden_states: bool=False,\n                  output_attentions: bool=False) -&gt; tuple[torch.Tensor, list]:\n    \"\"\"Predict for a batch of sequences.\n\n    Args:\n        dataloader: DataLoader object containing sequences.\n        do_pred: Whether to do prediction.\n        output_hidden_states: Whether to output hidden states.\n        output_attentions: Whether to output attentions.\n\n    Returns:\n        tuple: A tuple containing:\n            - torch.Tensor: All logits\n            - dict: Predictions dictionary\n            - dict: Embeddings dictionary\n    \"\"\"\n    # Set model to evaluation mode\n    self.model.eval()\n    all_logits = []\n    # Whether or not to output hidden states\n    params = None\n    embeddings = {}\n    if output_hidden_states:\n        import inspect\n        sig = inspect.signature(self.model.forward)\n        params = sig.parameters\n        if 'output_hidden_states' in params:\n            self.model.config.output_hidden_states = True\n        embeddings['hidden_states'] = None\n        embeddings['attention_mask'] = []\n        embeddings['labels'] = []\n    if output_attentions:\n        if not params:\n            import inspect\n            sig = inspect.signature(self.model.forward)\n            params = sig.parameters\n        if 'output_attentions' in params:\n            self.model.config.output_attentions = True\n        embeddings['attentions'] = None\n    # Iterate over batches\n    for batch in tqdm(dataloader, desc=\"Predicting\"):\n        inputs = {k: v.to(self.device) for k, v in batch.items()}\n        if self.pred_config.use_fp16:\n            self.model = self.model.half()\n            with torch.amp.autocast('cuda'):\n                outputs = self.model(**inputs)\n        else:\n            outputs = self.model(**inputs)\n        # Get logits\n        logits = outputs.logits.cpu().detach()\n        all_logits.append(logits)\n        # Get hidden states\n        if output_hidden_states:\n            hidden_states = [h.cpu().detach() for h in outputs.hidden_states] if hasattr(outputs, 'hidden_states') else None\n            if embeddings['hidden_states'] is None:\n                embeddings['hidden_states'] = [[h] for h in hidden_states]\n            else:\n                for i, h in enumerate(hidden_states):\n                    embeddings['hidden_states'][i].append(h)\n            attention_mask = inputs['attention_mask'].cpu().detach() if 'attention_mask' in inputs else None\n            embeddings['attention_mask'].append(attention_mask)\n            labels = inputs['labels'].cpu().detach() if 'labels' in inputs else None\n            embeddings['labels'].append(labels)\n        # Get attentions\n        if output_attentions:\n            attentions = [a.cpu().detach() for a in outputs.attentions] if hasattr(outputs, 'attentions') else None\n            if attentions:\n                if embeddings['attentions'] is None:\n                    embeddings['attentions'] = [[a] for a in attentions]\n                else:\n                    for i, a in enumerate(attentions):\n                        embeddings['attentions'][i].append(a)\n    # Concatenate logits\n    all_logits = torch.cat(all_logits, dim=0)\n    if output_hidden_states:\n        if embeddings['hidden_states']:\n            embeddings['hidden_states'] = tuple(torch.cat(lst, dim=0) for lst in embeddings['hidden_states'])\n        if embeddings['attention_mask']:\n            embeddings['attention_mask'] = torch.cat(embeddings['attention_mask'], dim=0)\n        if embeddings['labels']:\n            embeddings['labels'] = torch.cat(embeddings['labels'], dim=0)\n    if output_attentions:\n        if embeddings['attentions']:\n            embeddings['attentions'] = tuple(torch.cat(lst, dim=0) for lst in embeddings['attentions'])\n    # Get predictions\n    predictions = None\n    if do_pred:\n        predictions = self.logits_to_preds(all_logits)\n        predictions = self.format_output(predictions)\n    return all_logits, predictions, embeddings\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.calculate_metrics","title":"<code>calculate_metrics(logits, labels, plot=False)</code>","text":"<p>Calculate evaluation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Union[List, Tensor]</code> <p>Model predictions.</p> required <code>labels</code> <code>Union[List, Tensor]</code> <p>True labels.</p> required <code>plot</code> <code>bool</code> <p>Whether to plot metrics.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing evaluation metrics.</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def calculate_metrics(self, logits: Union[List, torch.Tensor],\n                      labels: Union[List, torch.Tensor], plot: bool=False) -&gt; dict:\n    \"\"\"Calculate evaluation metrics.\n\n    Args:\n        logits: Model predictions.\n        labels: True labels.\n        plot: Whether to plot metrics.\n\n    Returns:\n        dict: Dictionary containing evaluation metrics.\n    \"\"\"\n    # Calculate metrics based on task type\n    compute_metrics = Metrics(self.task_config, plot=plot)\n    metrics = compute_metrics((logits, labels))\n\n    return metrics\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.format_output","title":"<code>format_output(predictions)</code>","text":"<p>Format output predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>tuple[Tensor, list]</code> <p>Tuple containing predictions.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing formatted predictions.</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def format_output(self, predictions: tuple[torch.Tensor, list]) -&gt; dict:\n    \"\"\"Format output predictions.\n\n    Args:\n        predictions: Tuple containing predictions.\n\n    Returns:\n        dict: Dictionary containing formatted predictions.\n    \"\"\"\n    # Get task type from config\n    task_type = self.task_config.task_type\n    formatted_predictions = {}\n    probs, labels = predictions\n    probs = probs.numpy().tolist()\n    keep_seqs = True if len(self.sequences) else False\n    label_names = self.task_config.label_names\n    for i, label in enumerate(labels):\n        prob = probs[i]\n        formatted_predictions[i] = {\n            'sequence': self.sequences[i] if keep_seqs else '',\n            'label': label,\n            'scores': {label_names[j]: p for j, p in enumerate(prob)} if task_type != \"token\"\n                      else [max(x) for x in prob],\n        }\n    return formatted_predictions\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.generate_dataset","title":"<code>generate_dataset(seq_or_path, batch_size=1, seq_col='sequence', label_col='labels', sep=None, fasta_sep='|', multi_label_sep=None, uppercase=False, lowercase=False, keep_seqs=True, do_encode=True)</code>","text":"<p>Generate dataset from sequences.</p> <p>Parameters:</p> Name Type Description Default <code>seq_or_path</code> <code>Union[str, List[str]]</code> <p>Single sequence or path to a file containing sequences.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoader.</p> <code>1</code> <code>seq_col</code> <code>str</code> <p>Column name for sequences.</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for labels.</p> <code>'labels'</code> <code>sep</code> <code>str</code> <p>Delimiter for CSV, TSV, or TXT files.</p> <code>None</code> <code>fasta_sep</code> <code>str</code> <p>Delimiter for FASTA files.</p> <code>'|'</code> <code>multi_label_sep</code> <code>str</code> <p>Delimiter for multi-label sequences.</p> <code>None</code> <code>uppercase</code> <code>bool</code> <p>Whether to convert sequences to uppercase.</p> <code>False</code> <code>lowercase</code> <code>bool</code> <p>Whether to convert sequences to lowercase.</p> <code>False</code> <code>keep_seqs</code> <code>bool</code> <p>Whether to keep sequences in the dataset.</p> <code>True</code> <code>do_encode</code> <code>bool</code> <p>Whether to encode sequences.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - Dataset object - DataLoader object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input is neither a file path nor a list of sequences.</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def generate_dataset(self, seq_or_path: Union[str, List[str]], batch_size: int=1,\n                     seq_col: str=\"sequence\", label_col: str=\"labels\",\n                     sep: str = None, fasta_sep: str = \"|\",\n                     multi_label_sep: Union[str, None] = None,\n                     uppercase: bool=False, lowercase: bool=False,\n                     keep_seqs: bool=True, do_encode: bool=True) -&gt; tuple:\n    \"\"\"Generate dataset from sequences.\n\n    Args:\n        seq_or_path: Single sequence or path to a file containing sequences.\n        batch_size: Batch size for DataLoader.\n        seq_col: Column name for sequences.\n        label_col: Column name for labels.\n        sep (str, optional): Delimiter for CSV, TSV, or TXT files.\n        fasta_sep (str, optional): Delimiter for FASTA files.\n        multi_label_sep (str, optional): Delimiter for multi-label sequences.\n        uppercase (bool): Whether to convert sequences to uppercase.\n        lowercase (bool): Whether to convert sequences to lowercase.\n        keep_seqs: Whether to keep sequences in the dataset.\n        do_encode: Whether to encode sequences.\n\n    Returns:\n        tuple: A tuple containing:\n            - Dataset object\n            - DataLoader object\n\n    Raises:\n        ValueError: If input is neither a file path nor a list of sequences.\n    \"\"\"\n    if isinstance(seq_or_path, str):\n        suffix = seq_or_path.split(\".\")[-1]\n        if suffix and os.path.isfile(seq_or_path):\n            sequences = []\n            dataset = DNADataset.load_local_data(seq_or_path, seq_col=seq_col, label_col=label_col,\n                                                 sep=sep, fasta_sep=fasta_sep, multi_label_sep=multi_label_sep,\n                                                 tokenizer=self.tokenizer, max_length=self.pred_config.max_length)\n        else:\n            sequences = [seq_or_path]\n    elif isinstance(seq_or_path, list):\n        sequences = seq_or_path\n    else:\n        raise ValueError(\"Input should be a file path or a list of sequences.\")\n    if len(sequences) &gt; 0:\n        ds = Dataset.from_dict({\"sequence\": sequences})\n        dataset = DNADataset(ds, self.tokenizer, max_length=self.pred_config.max_length)\n    # If labels are provided, keep labels\n    if keep_seqs:\n        self.sequences = dataset.dataset[\"sequence\"]\n    # Encode sequences\n    if do_encode:\n        task_type = self.task_config.task_type\n        dataset.encode_sequences(remove_unused_columns=True, task=task_type, uppercase=uppercase, lowercase=lowercase)\n    if \"labels\" in dataset.dataset.features:\n        self.labels = dataset.dataset[\"labels\"]\n    # Create DataLoader\n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        num_workers=self.pred_config.num_workers\n    )\n\n    return dataset, dataloader\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.logits_to_preds","title":"<code>logits_to_preds(logits)</code>","text":"<p>Convert model logits to predictions.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>list</code> <p>Model output logits.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Tensor, list]</code> <p>A tuple containing: - torch.Tensor: Model predictions - list: Human-readable labels</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If task type is not supported.</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def logits_to_preds(self, logits: list) -&gt; tuple[torch.Tensor, list]:\n    \"\"\"Convert model logits to predictions.\n\n    Args:\n        logits: Model output logits.\n\n    Returns:\n        tuple: A tuple containing:\n            - torch.Tensor: Model predictions\n            - list: Human-readable labels\n\n    Raises:\n        ValueError: If task type is not supported.\n    \"\"\"\n    # Get task type and threshold from config\n    task_type = self.task_config.task_type\n    threshold = self.task_config.threshold\n    label_names = self.task_config.label_names\n    # Convert logits to predictions based on task type\n    if task_type == \"binary\":\n        probs = torch.softmax(logits, dim=-1)\n        preds = (probs[:, 1] &gt; threshold).long()\n        labels = [label_names[pred] for pred in preds]\n    elif task_type == \"multiclass\":\n        probs = torch.softmax(logits, dim=-1)\n        preds = torch.argmax(probs, dim=-1)\n        labels = [label_names[pred] for pred in preds]\n    elif task_type == \"multilabel\":\n        probs = torch.sigmoid(logits)\n        preds = (probs &gt; threshold).long()\n        labels = []\n        for pred in preds:\n            label = [label_names[i] for i in range(len(pred)) if pred[i] == 1]\n            labels.append(label)\n    elif task_type == \"regression\":\n        preds = logits.squeeze(-1)\n        probs = preds\n        labels = preds.tolist()\n    elif task_type == \"token\":\n        probs = torch.softmax(logits, dim=-1)\n        preds = torch.argmax(logits, dim=-1)\n        labels = []\n        for pred in preds:\n            label = [label_names[pred[i]] for i in range(len(pred))]\n            labels.append(label)\n    else:\n        raise ValueError(f\"Unsupported task type: {task_type}\")\n    return probs, labels\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.plot_attentions","title":"<code>plot_attentions(seq_idx=0, layer=-1, head=-1, width=800, height=800, save_path=None)</code>","text":"<p>Plot attention map.</p> <p>Parameters:</p> Name Type Description Default <code>seq_idx</code> <code>int</code> <p>Index of the sequence to plot.</p> <code>0</code> <code>layer</code> <code>int</code> <p>Layer index to plot.</p> <code>-1</code> <code>head</code> <code>int</code> <p>Head index to plot.</p> <code>-1</code> <code>width</code> <code>int</code> <p>Width of the plot.</p> <code>800</code> <code>height</code> <code>int</code> <p>Height of the plot.</p> <code>800</code> <code>save_path</code> <code>Optional[str]</code> <p>Path to save the plot.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>If no attention weights are available.</p> <code>object</code> <code>None</code> <p>Attention map visualization if available.</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def plot_attentions(self, seq_idx: int = 0, layer: int = -1, head: int = -1,\n                    width: int = 800, height: int = 800,\n                    save_path: Optional[str] = None) -&gt; None:\n    \"\"\"Plot attention map.\n\n    Args:\n        seq_idx: Index of the sequence to plot.\n        layer: Layer index to plot.\n        head: Head index to plot.\n        width: Width of the plot.\n        height: Height of the plot.\n        save_path: Path to save the plot.\n\n    Returns:\n        None: If no attention weights are available.\n        object: Attention map visualization if available.\n    \"\"\"\n    if hasattr(self, 'embeddings'):\n        attentions = self.embeddings['attentions']\n        if save_path:\n            suffix = os.path.splitext(save_path)[-1]\n            if suffix:\n                heatmap = save_path.replace(suffix, \"_heatmap\" + suffix)\n            else:\n                heatmap = os.path.join(save_path, \"heatmap.pdf\")\n        else:\n            heatmap = None\n        # Plot attention map\n        attn_map = plot_attention_map(attentions, self.sequences, self.tokenizer,\n                                      seq_idx=seq_idx, layer=layer, head=head,\n                                      width=width, height=height,\n                                      save_path=heatmap)\n        return attn_map\n    else:\n        print(\"No attention weights available to plot.\")\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.plot_hidden_states","title":"<code>plot_hidden_states(reducer='t-SNE', ncols=4, width=300, height=300, save_path=None)</code>","text":"<p>Embedding visualization.</p> <p>Parameters:</p> Name Type Description Default <code>reducer</code> <code>str</code> <p>Dimensionality reduction method to use.</p> <code>'t-SNE'</code> <code>ncols</code> <code>int</code> <p>Number of columns in the plot grid.</p> <code>4</code> <code>width</code> <code>int</code> <p>Width of the plot.</p> <code>300</code> <code>height</code> <code>int</code> <p>Height of the plot.</p> <code>300</code> <code>save_path</code> <code>Optional[str]</code> <p>Path to save the plot.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>If no hidden states are available.</p> <code>object</code> <code>None</code> <p>Embedding visualization if available.</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def plot_hidden_states(self, reducer: str=\"t-SNE\",\n                       ncols: int=4, width: int = 300, height: int = 300,\n                       save_path: Optional[str] = None) -&gt; None:\n    \"\"\"Embedding visualization.\n\n    Args:\n        reducer: Dimensionality reduction method to use.\n        ncols: Number of columns in the plot grid.\n        width: Width of the plot.\n        height: Height of the plot.\n        save_path: Path to save the plot.\n\n    Returns:\n        None: If no hidden states are available.\n        object: Embedding visualization if available.\n    \"\"\"\n    if hasattr(self, 'embeddings'):\n        hidden_states = self.embeddings['hidden_states']\n        attention_mask = torch.unsqueeze(self.embeddings['attention_mask'], dim=-1)\n        labels = self.embeddings['labels']\n        if save_path:\n            suffix = os.path.splitext(save_path)[-1]\n            if suffix:\n                embedding = save_path.replace(suffix, \"_embedding\" + suffix)\n            else:\n                embedding = os.path.join(save_path, \"embedding.pdf\")\n        else:\n            embedding = None\n        # Plot hidden states\n        label_names = self.task_config.label_names\n        embeddings_vis = plot_embeddings(hidden_states, attention_mask, reducer=reducer,\n                                         labels=labels, label_names=label_names,\n                                         ncols=ncols, width=width, height=height,\n                                         save_path=embedding)\n        return embeddings_vis\n    else:\n        print(\"No hidden states available to plot.\")\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.predict_file","title":"<code>predict_file(file_path, evaluate=False, output_hidden_states=False, output_attentions=False, seq_col='sequence', label_col='labels', sep=None, fasta_sep='|', multi_label_sep=None, uppercase=False, lowercase=False, save_to_file=False, plot_metrics=False)</code>","text":"<p>Predict from a file containing sequences.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file containing sequences.</p> required <code>evaluate</code> <code>bool</code> <p>Whether to evaluate the predictions.</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>Whether to output hidden states.</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>Whether to output attentions.</p> <code>False</code> <code>seq_col</code> <code>str</code> <p>Column name for sequences.</p> <code>'sequence'</code> <code>label_col</code> <code>str</code> <p>Column name for labels.</p> <code>'labels'</code> <code>sep</code> <code>str</code> <p>Delimiter for CSV, TSV, or TXT files.</p> <code>None</code> <code>fasta_sep</code> <code>str</code> <p>Delimiter for FASTA files.</p> <code>'|'</code> <code>multi_label_sep</code> <code>str</code> <p>Delimiter for multi-label sequences.</p> <code>None</code> <code>uppercase</code> <code>bool</code> <p>Whether to convert sequences to uppercase.</p> <code>False</code> <code>lowercase</code> <code>bool</code> <p>Whether to convert sequences to lowercase.</p> <code>False</code> <code>save_to_file</code> <code>bool</code> <p>Whether to save predictions to file.</p> <code>False</code> <code>plot_metrics</code> <code>bool</code> <p>Whether to plot metrics.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[tuple, dict]</code> <p>Union[tuple, dict]: Either: - List of dictionaries containing predictions - Tuple of (predictions, metrics) if evaluate=True</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def predict_file(self, file_path: str, evaluate: bool = False,\n                 output_hidden_states: bool=False,\n                 output_attentions: bool=False,\n                 seq_col: str=\"sequence\", label_col: str=\"labels\",\n                 sep: str = None, fasta_sep: str = \"|\",\n                 multi_label_sep: Union[str, None] = None,\n                 uppercase: bool=False, lowercase: bool=False,\n                 save_to_file: bool=False, plot_metrics: bool=False) -&gt; Union[tuple, dict]:\n    \"\"\"Predict from a file containing sequences.\n\n    Args:\n        file_path: Path to the file containing sequences.\n        evaluate: Whether to evaluate the predictions.\n        output_hidden_states: Whether to output hidden states.\n        output_attentions: Whether to output attentions.\n        seq_col: Column name for sequences.\n        label_col: Column name for labels.\n        sep (str, optional): Delimiter for CSV, TSV, or TXT files.\n        fasta_sep (str, optional): Delimiter for FASTA files.\n        multi_label_sep (str, optional): Delimiter for multi-label sequences.\n        uppercase (bool): Whether to convert sequences to uppercase.\n        lowercase (bool): Whether to convert sequences to lowercase.\n        save_to_file: Whether to save predictions to file.\n        plot_metrics: Whether to plot metrics.\n\n    Returns:\n        Union[tuple, dict]: Either:\n            - List of dictionaries containing predictions\n            - Tuple of (predictions, metrics) if evaluate=True\n    \"\"\"\n    # Get dataset and dataloader from file\n    _, dataloader = self.generate_dataset(file_path, seq_col=seq_col, label_col=label_col,\n                                          sep=sep, fasta_sep=fasta_sep, multi_label_sep=multi_label_sep,\n                                          uppercase=uppercase, lowercase=lowercase,\n                                          batch_size=self.pred_config.batch_size)\n    # Do batch prediction\n    if output_attentions:\n        warnings.warn(\"Cautions: output_attentions may consume a lot of memory.\\n\")\n    logits, predictions, embeddings = self.batch_predict(dataloader,\n                                                         output_hidden_states=output_hidden_states,\n                                                         output_attentions=output_attentions)\n    # Keep hidden states\n    if output_hidden_states or output_attentions:\n        self.embeddings = embeddings\n    # Save predictions\n    if save_to_file and self.pred_config.output_dir:\n        save_predictions(predictions, Path(self.pred_config.output_dir))\n    # Do evaluation\n    if len(self.labels) == len(logits) and evaluate:\n        metrics = self.calculate_metrics(logits, self.labels, plot=plot_metrics)\n        metrics_save = dict(metrics)\n        if 'curve' in metrics_save:\n            del metrics_save['curve']\n        if 'scatter' in metrics_save:\n            del metrics_save['scatter']\n        if save_to_file and self.pred_config.output_dir:\n            save_metrics(metrics, Path(self.pred_config.output_dir))\n        # Whether to plot metrics\n        if plot_metrics:\n            return predictions, metrics\n        else:\n            return predictions, metrics_save\n\n    return predictions\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.DNAPredictor.predict_seqs","title":"<code>predict_seqs(sequences, evaluate=False, output_hidden_states=False, output_attentions=False, save_to_file=False)</code>","text":"<p>Predict for sequences.</p> <p>Parameters:</p> Name Type Description Default <code>sequences</code> <code>Union[str, List[str]]</code> <p>Single sequence or list of sequences.</p> required <code>evaluate</code> <code>bool</code> <p>Whether to evaluate the predictions.</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>Whether to output hidden states and attentions.</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>Whether to output attentions.</p> <code>False</code> <code>save_to_file</code> <code>bool</code> <p>Whether to save predictions to file.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[tuple, dict]</code> <p>Union[tuple, dict]: Either: - Dictionary containing predictions - Tuple of (predictions, metrics) if evaluate=True</p> Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def predict_seqs(self, sequences: Union[str, List[str]],\n                 evaluate: bool = False,\n                 output_hidden_states: bool=False,\n                 output_attentions: bool=False,\n                 save_to_file: bool = False) -&gt; Union[tuple, dict]:\n    \"\"\"Predict for sequences.\n\n    Args:\n        sequences: Single sequence or list of sequences.\n        evaluate: Whether to evaluate the predictions.\n        output_hidden_states: Whether to output hidden states and attentions.\n        output_attentions: Whether to output attentions.\n        save_to_file: Whether to save predictions to file.\n\n    Returns:\n        Union[tuple, dict]: Either:\n            - Dictionary containing predictions\n            - Tuple of (predictions, metrics) if evaluate=True\n    \"\"\"\n    # Get dataset and dataloader from sequences\n    _, dataloader = self.generate_dataset(sequences, batch_size=self.pred_config.batch_size)\n    # Do batch prediction\n    logits, predictions, embeddings = self.batch_predict(dataloader,\n                                                         output_hidden_states=output_hidden_states,\n                                                         output_attentions=output_attentions)\n    # Keep hidden states\n    if output_hidden_states or output_attentions:\n        self.embeddings = embeddings\n    # Save predictions\n    if save_to_file and self.pred_config.output_dir:\n        save_predictions(predictions, Path(self.pred_config.output_dir))\n    # Do evaluation\n    if len(self.labels) == len(logits) and evaluate:\n        metrics = self.calculate_metrics(logits, self.labels)\n        metrics_save = dict(metrics)\n        if 'curve' in metrics_save:\n            del metrics_save['curve']\n        if 'scatter' in metrics_save:\n            del metrics_save['scatter']\n        if save_to_file and self.pred_config.output_dir:\n            save_metrics(metrics_save, Path(self.pred_config.output_dir))\n        return predictions, metrics\n\n    return predictions\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.plot_attention_map","title":"<code>plot_attention_map(attentions, sequences, tokenizer, seq_idx=0, layer=-1, head=-1, width=800, height=800, save_path=None)</code>","text":"<p>Plot attention map. Args:     attentions (tuple): Tuple containing attention weights.     sequences (list): List of sequences.     tokenizer: Tokenizer object.     seq_idx (int): Index of the sequence to plot.     layer (int): Layer index.     head (int): Head index.     width (int): Width of the plot.     height (int): Height of the plot.     save_path (str): Path to save the plot. Returns:     attn_map: Altair chart object.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_attention_map(attentions: Union[tuple, list], sequences: list, tokenizer,\n                       seq_idx: int=0, layer: int=-1, head: int=-1,\n                       width: int = 800, height: int = 800,\n                       save_path: str = None) -&gt; alt.Chart:\n    \"\"\"\n    Plot attention map.\n    Args:\n        attentions (tuple): Tuple containing attention weights.\n        sequences (list): List of sequences.\n        tokenizer: Tokenizer object.\n        seq_idx (int): Index of the sequence to plot.\n        layer (int): Layer index.\n        head (int): Head index.\n        width (int): Width of the plot.\n        height (int): Height of the plot.\n        save_path (str): Path to save the plot.\n    Returns:\n        attn_map: Altair chart object.\n    \"\"\"\n    # Plot attention map\n    attn_layer = attentions[layer].numpy()\n    attn_head = attn_layer[seq_idx][head]\n    # Get the tokens\n    seq = sequences[seq_idx]\n    tokens_id = tokenizer.encode(seq)\n    try:\n        tokens = tokenizer.convert_ids_to_tokens(tokens_id)\n    except:\n        tokens = tokenizer.decode(seq).split()\n    # Create a DataFrame for the attention map\n    num_tokens = len(tokens)\n    flen = len(str(num_tokens))\n    df = {\"token1\": [], 'token2': [], 'attn': []}\n    for i, t1 in enumerate(tokens):\n        for j, t2 in enumerate(tokens):\n            df[\"token1\"].append(str(i).zfill(flen)+t1)\n            df[\"token2\"].append(str(num_tokens-j).zfill(flen)+t2)\n            df[\"attn\"].append(attn_head[i][j])\n    source = pd.DataFrame(df)\n    # Enable VegaFusion for Altair\n    alt.data_transformers.enable(\"vegafusion\")\n    # Plot the attention map\n    attn_map = alt.Chart(source).mark_rect().encode(\n        x=alt.X('token1:O', axis=alt.Axis(\n                    labelExpr = f\"substring(datum.value, {flen}, 100)\",\n                    labelAngle=-45,\n                    )\n                ).title(None),\n        y=alt.Y('token2:O', axis=alt.Axis(\n                    labelExpr = f\"substring(datum.value, {flen}, 100)\",\n                    labelAngle=0,\n                    )\n                ).title(None),\n        color=alt.Color('attn:Q').scale(scheme='viridis'),\n    ).properties(\n        width=width,\n        height=height\n    ).configure_axis(grid=False)\n    # Save the plot\n    if save_path:\n        attn_map.save(save_path)\n        print(f\"Attention map saved to {save_path}\")\n    return attn_map\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.plot_bars","title":"<code>plot_bars(data, show_score=True, ncols=3, width=200, height=50, bar_width=30, domain=(0.0, 1.0), save_path=None, separate=False)</code>","text":"<p>Plot bar chart. Args:     data (dict): Data to be plotted.     show_score (bool): Whether to show the score on the plot.     ncols (int): Number of columns in the plot.     width (int): Width of the plot.     height (int): Height of the plot.     bar_width (int): Width of the bars in the plot.     save_path (str): Path to save the plot. Returns:     pbars: Altair chart object.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_bars(data: dict, show_score: bool = True, ncols: int = 3,\n              width: int = 200, height: int = 50, bar_width: int = 30,\n              domain: Union[tuple, list]= (0.0, 1.0),\n              save_path: str = None, separate: bool=False) -&gt; alt.Chart:\n    \"\"\"\n    Plot bar chart.\n    Args:\n        data (dict): Data to be plotted.\n        show_score (bool): Whether to show the score on the plot.\n        ncols (int): Number of columns in the plot.\n        width (int): Width of the plot.\n        height (int): Height of the plot.\n        bar_width (int): Width of the bars in the plot.\n        save_path (str): Path to save the plot.\n    Returns:\n        pbars: Altair chart object.\n    \"\"\"\n    # Plot bar charts\n    dbar = pd.DataFrame(data)\n    pbar = {}\n    p_separate = {}\n    for n, metric in enumerate([x for x in data if x != 'models']):\n        if metric in ['mae', 'mse']:\n            domain_use = [0, dbar[metric].max()*1.1]\n        else:\n            domain_use = domain\n        bar = alt.Chart(dbar).mark_bar(size=bar_width).encode(\n            x=alt.X(metric + \":Q\").scale(domain=domain_use),\n            y=alt.Y(\"models\").title(None),\n            color=alt.Color('models').legend(None),\n        ).properties(width=width, height=height*len(dbar['models']))\n        if show_score:\n            text = alt.Chart(dbar).mark_text(\n                dx=-10,\n                color='white',\n                baseline='middle',\n                align='right').encode(\n                    x=alt.X(metric + \":Q\"),\n                    y=alt.Y(\"models\").title(None),\n                    text=alt.Text(metric, format='.3f')\n                    )\n            p = bar + text\n        else:\n            p = bar\n        if separate:\n            p_separate[metric] = p.configure_axis(grid=False)\n        idx = n // ncols\n        if n % ncols == 0:\n            pbar[idx] = p\n        else:\n            pbar[idx] |= p\n    # Combine the plots\n    for i, p in enumerate(pbar):\n        if i == 0:\n            pbars = pbar[p]\n        else:\n            pbars &amp;= pbar[p]\n    # Configure the chart\n    pbars = pbars.configure_axis(grid=False)\n    # Save the plot\n    if save_path:\n        pbars.save(save_path)\n        print(f\"Metrics bar charts saved to {save_path}\")\n    if separate:\n        return p_separate\n    else:\n        return pbars\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.plot_curve","title":"<code>plot_curve(data, show_score=True, width=400, height=400, save_path=None, separate=False)</code>","text":"<p>Plot curve chart. Args:     data (dict): Data to be plotted.     show_score (bool): Whether to show the score on the plot.     width (int): Width of the plot.     height (int): Height of the plot.     save_path (str): Path to save the plot. Returns:     plines: Altair chart object.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_curve(data: dict, show_score: bool = True,\n               width: int = 400, height: int = 400,\n               save_path: str = None, separate: bool=False) -&gt; alt.Chart:\n    \"\"\"\n    Plot curve chart.\n    Args:\n        data (dict): Data to be plotted.\n        show_score (bool): Whether to show the score on the plot.\n        width (int): Width of the plot.\n        height (int): Height of the plot.\n        save_path (str): Path to save the plot.\n    Returns:\n        plines: Altair chart object.\n    \"\"\"\n    # Plot curves\n    pline = {}\n    p_separate = {}\n    # Plot ROC curve\n    roc_data = pd.DataFrame(data['ROC'])\n    pline[0] = alt.Chart(roc_data).mark_line().encode(\n        x=alt.X(\"fpr\").scale(domain=(0.0, 1.0)),\n        y=alt.Y(\"tpr\").scale(domain=(0.0, 1.0)),\n        color=\"models\",\n    ).properties(width=width, height=height)\n    if separate:\n        p_separate['ROC'] = pline[0]\n    # Plot PR curve\n    pr_data = pd.DataFrame(data['PR'])\n    pline[1] = alt.Chart(pr_data).mark_line().encode(\n        x=alt.X(\"recall\").scale(domain=(0.0, 1.0)),\n        y=alt.Y(\"precision\").scale(domain=(0.0, 1.0)),\n        color=\"models\",\n    ).properties(width=width, height=height)\n    if separate:\n        p_separate['PR'] = pline[1]\n    # Combine the plots\n    for i, p in enumerate(pline):\n        if i == 0:\n            plines = pline[i]\n        else:\n            plines |= pline[i]\n    # Configure the chart\n    plines = plines.configure_axis(grid=False)\n    # Save the plot\n    if save_path:\n        plines.save(save_path)\n        print(f\"ROC curves saved to {save_path}\")\n    if separate:\n        return p_separate\n    else:\n        return plines\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.plot_embeddings","title":"<code>plot_embeddings(hidden_states, attention_mask, reducer='t-SNE', labels=None, label_names=None, ncols=4, width=300, height=300, save_path=None, separate=False)</code>","text":"<p>Visualize embeddings</p> <p>Parameters:</p> Name Type Description Default <code>hidden_states</code> <code>tuple</code> <p>Tuple containing hidden states.</p> required <code>attention_mask</code> <code>tuple</code> <p>Tuple containing attention mask.</p> required <code>reducer</code> <code>str</code> <p>Dimensionality reduction method. Options: PCA, t-SNE, UMAP.</p> <code>'t-SNE'</code> <code>labels</code> <code>list</code> <p>List of labels for the data points.</p> <code>None</code> <code>label_names</code> <code>list</code> <p>List of label names.</p> <code>None</code> <code>ncols</code> <code>int</code> <p>Number of columns in the plot.</p> <code>4</code> <code>width</code> <code>int</code> <p>Width of the plot.</p> <code>300</code> <code>height</code> <code>int</code> <p>Height of the plot.</p> <code>300</code> <code>save_path</code> <code>str</code> <p>Path to save the plot.</p> <code>None</code> <code>separate</code> <code>bool</code> <p>Whether to return separate plots for each layer.</p> <code>False</code> <p>Returns:     pdots: Altair chart object.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_embeddings(hidden_states: Union[tuple, list], attention_mask: Union[tuple, list], reducer: str=\"t-SNE\",\n                    labels: Union[tuple, list]=None, label_names: Union[str, list]=None,\n                    ncols: int=4, width: int=300, height: int=300,\n                    save_path: str = None, separate: bool=False) -&gt; alt.Chart:\n    '''\n    Visualize embeddings\n\n    Args:\n        hidden_states (tuple): Tuple containing hidden states.\n        attention_mask (tuple): Tuple containing attention mask.\n        reducer (str): Dimensionality reduction method. Options: PCA, t-SNE, UMAP.\n        labels (list): List of labels for the data points.\n        label_names (list): List of label names.\n        ncols (int): Number of columns in the plot.\n        width (int): Width of the plot.\n        height (int): Height of the plot.\n        save_path (str): Path to save the plot.\n        separate (bool): Whether to return separate plots for each layer.\n    Returns:\n        pdots: Altair chart object.\n    '''\n    import torch\n    if reducer.lower() == \"pca\":\n        from sklearn.decomposition import PCA\n        dim_reducer = PCA(n_components=2)\n    elif reducer.lower() == \"t-sne\":\n        from sklearn.manifold import TSNE\n        dim_reducer = TSNE(n_components=2)\n    elif reducer.lower() == \"umap\":\n        from umap import UMAP\n        dim_reducer = UMAP(n_components=2)\n    else:\n        raise(\"Unsupported dim reducer, please try PCA, t-SNE or UMAP.\")\n\n    pdot = {}\n    p_separate = {}\n    for i, hidden in enumerate(hidden_states):\n        embeddings = hidden.numpy()\n        mean_sequence_embeddings = torch.sum(attention_mask*embeddings, axis=-2) / torch.sum(attention_mask, axis=1)\n        layer_dim_reduced_vectors = dim_reducer.fit_transform(mean_sequence_embeddings.numpy())\n        if len(labels) == 0:\n            labels = [\"Uncategorized\"] * layer_dim_reduced_vectors.shape[0]\n        df = {\n            'Dimension 1': layer_dim_reduced_vectors[:,0],\n            'Dimension 2': layer_dim_reduced_vectors[:,1],\n            'labels': [label_names[int(i)] for i in labels]\n        }\n        source = pd.DataFrame(df)\n        dot = alt.Chart(source, title=f\"Layer {i+1}\").mark_point(filled=True).encode(\n            x=alt.X(\"Dimension 1:Q\"),\n            y=alt.Y(\"Dimension 2:Q\"),\n            color=alt.Color(\"labels:N\", legend=alt.Legend(title=\"Labels\")),\n        ).properties(width=width, height=height)\n        if separate:\n            p_separate[f\"Layer{i+1}\"] = dot.configure_axis(grid=False)\n        idx = i // ncols\n        if i % ncols == 0:\n            pdot[idx] = dot\n        else:\n            pdot[idx] |= dot\n    # Combine the plots\n    for i, p in enumerate(pdot):\n        if i == 0:\n            pdots = pdot[p]\n        else:\n            pdots &amp;= pdot[p]\n    # Configure the chart\n    pdots = pdots.configure_axis(grid=False)\n    # Save the plot\n    if save_path:\n        pdots.save(save_path)\n        print(f\"Embeddings visualization saved to {save_path}\")\n    if separate:\n        return p_separate\n    else:\n        return pdots\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.plot_muts","title":"<code>plot_muts(data, show_score=False, width=None, height=100, save_path=None)</code>","text":"<p>Visualize mutation effects</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_muts(data: dict, show_score: bool = False,\n              width: int = None, height: int = 100,\n              save_path: str = None) -&gt; alt.Chart:\n    '''\n    Visualize mutation effects\n    '''\n    # Create dataframe\n    seqlen = len(data['raw']['sequence'])\n    flen = len(str(seqlen))\n    mut_list = [x for x in data.keys() if x != 'raw']\n    raw_bases = [base for base in data['raw']['sequence']]\n    dheat = {\"base\": [], 'mut': [], 'score': []}\n    dline = {\"x\": [str(i).zfill(flen)+x for i,x in enumerate(raw_bases)] * 2,\n             \"score\": [0.0]*seqlen*2,\n             \"type\": [\"gain\"]*seqlen + [\"loss\"]*seqlen}\n    dbar = {\"x\": [], \"score\": [], \"base\": []}\n    # Iterate through mutations\n    for i, base1 in enumerate(raw_bases):\n        ref = \"mut_\" + str(i) + \"_\" + base1 + \"_\" + base1\n        # replacement\n        mut_prefix = \"mut_\" + str(i) + \"_\" + base1 + \"_\"\n        maxabs = 0.0\n        maxscore = 0.0\n        maxabs_index = base1\n        for mut in sorted([x for x in mut_list if x.startswith(mut_prefix)] + [ref]):\n            if mut in data:\n                # for heatmap\n                base2 = mut.split(\"_\")[-1]\n                score = data[mut]['score']\n                dheat[\"base\"].append(str(i).zfill(flen)+base1)\n                dheat[\"mut\"].append(base2)\n                dheat[\"score\"].append(score)\n                # for line\n                if score &gt;= 0:\n                    dline[\"score\"][i] += score\n                elif score &lt; 0:\n                    dline[\"score\"][i+seqlen] -= score\n                # for bar chart\n                if abs(score) &gt; maxabs:\n                    maxabs = abs(score)\n                    maxscore = score\n                    maxabs_index = base2\n            else:\n                dheat[\"base\"].append(str(i).zfill(flen)+base1)\n                dheat[\"mut\"].append(base1)\n                dheat[\"score\"].append(0.0)\n        # for bar chart\n        dbar[\"x\"].append(str(i).zfill(flen)+base1)\n        dbar[\"score\"].append(maxscore)\n        dbar[\"base\"].append(maxabs_index)\n        # deletion\n        del_prefix = \"del_\" + str(i) + \"_\"\n        for mut in [x for x in mut_list if x.startswith(del_prefix)]:\n            base2 = \"del_\" + mut.split(\"_\")[-1]\n            score = data[mut]['score']\n            dheat[\"base\"].append(str(i).zfill(flen)+base1)\n            dheat[\"mut\"].append(base2)\n            dheat[\"score\"].append(score)\n        # insertion\n        ins_prefix = \"ins_\" + str(i) + \"_\"\n        for mut in [x for x in mut_list if x.startswith(ins_prefix)]:\n            base2 = \"ins_\" + mut.split(\"_\")[-1]\n            score = data[mut]['score']\n            dheat[\"base\"].append(str(i).zfill(flen)+base1)\n            dheat[\"mut\"].append(base2)\n            dheat[\"score\"].append(score)\n    # Set color domain and range\n    domain1_min = min([data[mut]['score'] for mut in data])\n    domain1_max = max([data[mut]['score'] for mut in data])\n    domain1 = [-max([abs(domain1_min), abs(domain1_max)]),\n               0.0,\n               max([abs(domain1_min), abs(domain1_max)])]\n    range1_ = ['#2166ac', '#f7f7f7', '#b2182b']\n    domain2 = sorted([x for x in set(dbar['base'])])\n    range2_ = [\"#33a02c\", \"#e31a1c\", \"#1f78b4\", \"#ff7f00\", \"#cab2d6\"][:len(domain2)]\n    # Enable VegaFusion for Altair\n    alt.data_transformers.enable(\"vegafusion\")\n    # Plot the heatmap\n    if width is None:\n        width = int(height * len(raw_bases) / len(set(dheat['mut'])))\n    if dheat['base']:\n        pheat = alt.Chart(pd.DataFrame(dheat)).mark_rect().encode(\n            x=alt.X('base:O', axis=alt.Axis(\n                    labelExpr = f\"substring(datum.value, {flen}, {flen}+1)\",\n                    labelAngle=0,\n                    )\n                ).title(None),\n            y=alt.Y('mut:O').title(\"mutation\"),\n            color=alt.Color('score:Q').scale(domain=domain1, range=range1_),\n        ).properties(\n            width=width, height=height\n        )\n        # Plot gain and loss\n        pline = alt.Chart(pd.DataFrame(dline)).mark_line().encode(\n            x=alt.X('x:O').title(None).axis(labels=False),\n            y=alt.Y('score:Q'),\n            color=alt.Color('type:N').scale(\n                domain=['gain', 'loss'], range=['#b2182b', '#2166ac']\n            ),\n        ).properties(\n            width=width, height=height\n        )\n        pbar = alt.Chart(pd.DataFrame(dbar)).mark_bar().encode(\n            x=alt.X('x:O').title(None).axis(labels=False),\n            y=alt.Y('score:Q'),\n            color=alt.Color('base:N').scale(\n                domain=domain2, range=range2_\n            ),\n        ).properties(\n            width=width, height=height\n        )\n        pmerge = pheat &amp; pbar &amp; pline\n        pmerge = pmerge.configure_axis(grid=False)\n        # Save the plot\n        if save_path:\n            pmerge.save(save_path)\n            print(f\"Mutation effects visualization saved to {save_path}\")\n    return pheat\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.plot_scatter","title":"<code>plot_scatter(data, show_score=True, ncols=3, width=400, height=400, save_path=None, separate=False)</code>","text":"<p>Plot scatter chart. Args:     data (dict): Data to be plotted.     show_score (bool): Whether to show the score on the plot.     ncols (int): Number of columns in the plot.     width (int): Width of the plot.     height (int): Height of the plot.     save_path (str): Path to save the plot. Returns:     pdots: Altair chart object.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def plot_scatter(data: dict, show_score: bool = True, ncols: int = 3,\n                 width: int = 400, height: int = 400,\n                 save_path: str = None, separate: bool=False) -&gt; alt.Chart:\n    \"\"\"\n    Plot scatter chart.\n    Args:\n        data (dict): Data to be plotted.\n        show_score (bool): Whether to show the score on the plot.\n        ncols (int): Number of columns in the plot.\n        width (int): Width of the plot.\n        height (int): Height of the plot.\n        save_path (str): Path to save the plot.\n    Returns:\n        pdots: Altair chart object.\n    \"\"\"\n    # Plot bar charts\n    pdot = {}\n    p_separate = {}\n    for n, model in enumerate(data):\n        scatter_data = dict(data[model])\n        r2 = scatter_data['r2']\n        del scatter_data['r2']\n        ddot = pd.DataFrame(scatter_data)\n        dot = alt.Chart(ddot, title=model).mark_point(filled=True).encode(\n            x=alt.X(\"predicted:Q\"),\n            y=alt.Y(\"experiment:Q\"),\n        ).properties(width=width, height=height)\n        if show_score:\n            min_x = ddot['predicted'].min()\n            max_y = ddot['experiment'].max()\n            text = alt.Chart().mark_text(size=14, align=\"left\", baseline=\"bottom\").encode(\n                x=alt.datum(min_x + 0.5),\n                y=alt.datum(max_y - 0.5),\n                text=alt.datum(\"R\\u00b2=\" + str(r2))\n            )\n            p = dot + text\n        else:\n            p = dot\n        if separate:\n            p_separate[model] = p.configure_axis(grid=False)\n        idx = n // ncols\n        if n % ncols == 0:\n            pdot[idx] = p\n        else:\n            pdot[idx] |= p\n    # Combine the plots\n    for i, p in enumerate(pdot):\n        if i == 0:\n            pdots = pdot[p]\n        else:\n            pdots &amp;= pdot[p]\n    # Configure the chart\n    pdots = pdots.configure_axis(grid=False)\n    # Save the plot\n    if save_path:\n        pdots.save(save_path)\n        print(f\"Metrics scatter plots saved to {save_path}\")\n    if separate:\n        return p_separate\n    else:\n        return pdots\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.prepare_data","title":"<code>prepare_data(metrics, task_type='binary')</code>","text":"<p>Prepare data for plotting. Args:     metrics (dict): Dictionary containing model metrics.     task_type (str): Task type Returns:     tuple: Tuple containing bar data and curve data.</p> Source code in <code>dnallm/inference/plot.py</code> <pre><code>def prepare_data(metrics: dict, task_type: str=\"binary\") -&gt; tuple:\n    \"\"\"\n    Prepare data for plotting.\n    Args:\n        metrics (dict): Dictionary containing model metrics.\n        task_type (str): Task type\n    Returns:\n        tuple: Tuple containing bar data and curve data.\n    \"\"\"\n    # Load the data\n    bars_data = {'models': []}\n    if task_type in ['binary', 'multiclass', 'multilabel', 'token']:\n        curves_data = {'ROC': {'models': [], 'fpr': [], 'tpr': []},\n                    'PR': {'models': [], 'recall': [], 'precision': []}}\n        for model in metrics:\n            if model not in bars_data['models']:\n                bars_data['models'].append(model)\n            for metric in metrics[model]:\n                if metric == 'curve':\n                    for score in metrics[model][metric]:\n                        if score.endswith('pr'):\n                            if score == 'fpr':\n                                curves_data['ROC']['models'].extend([model] * len(metrics[model][metric][score]))\n                            curves_data['ROC'][score].extend(metrics[model][metric][score])\n                        else:\n                            if score == 'precision':\n                                curves_data['PR']['models'].extend([model] * len(metrics[model][metric][score]))\n                            curves_data['PR'][score].extend(metrics[model][metric][score])\n                else:\n                    if metric not in bars_data:\n                        bars_data[metric] = []\n                    bars_data[metric].append(metrics[model][metric])\n        return bars_data, curves_data\n    elif task_type == \"regression\":\n        scatter_data = {}\n        for model in metrics:\n            if model not in bars_data['models']:\n                bars_data['models'].append(model)\n            scatter_data[model] = {'predicted': [], 'experiment': []}\n            for metric in metrics[model]:\n                if metric == 'scatter':\n                    for score in metrics[model][metric]:\n                        scatter_data[model][score].extend(metrics[model][metric][score])\n                else:\n                    if metric not in bars_data:\n                        bars_data[metric] = []\n                    bars_data[metric].append(metrics[model][metric])\n                    if metric == 'r2':\n                        scatter_data[model][metric] = metrics[model][metric]\n        return bars_data, scatter_data\n    else:\n        raise ValueError(f\"Unsupport task type {task_type} for ploting\")\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.save_metrics","title":"<code>save_metrics(metrics, output_dir)</code>","text":"<p>Save metrics to files.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Dict</code> <p>Dictionary containing metrics.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save metrics.</p> required Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def save_metrics(metrics: Dict, output_dir: Path) -&gt; None:\n    \"\"\"Save metrics to files.\n\n    Args:\n        metrics: Dictionary containing metrics.\n        output_dir: Directory to save metrics.\n    \"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save metrics\n    with open(output_dir / \"metrics.json\", \"w\") as f:\n        json.dump(metrics, f, indent=4)\n</code></pre>"},{"location":"api/inference/predictor/#dnallm.inference.predictor.save_predictions","title":"<code>save_predictions(predictions, output_dir)</code>","text":"<p>Save predictions to files.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Dict</code> <p>Dictionary containing predictions.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save predictions.</p> required Source code in <code>dnallm/inference/predictor.py</code> <pre><code>def save_predictions(predictions: Dict, output_dir: Path) -&gt; None:\n    \"\"\"Save predictions to files.\n\n    Args:\n        predictions: Dictionary containing predictions.\n        output_dir: Directory to save predictions.\n    \"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save predictions\n    with open(output_dir / \"predictions.json\", \"w\") as f:\n        json.dump(predictions, f, indent=4)\n</code></pre>"},{"location":"api/utils/sequence/","title":"utils/sequence API","text":"<p>Sequence utility functions for DNA sequence analysis and generation.</p> <p>This module provides functions for: - Calculating GC content - Generating reverse complements - Converting sequences to k-mers - Validating DNA sequences - Randomly generating DNA sequences with constraints</p> <p>All functions are designed for use in DNA language modeling and bioinformatics pipelines.</p>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.calc_gc_content","title":"<code>calc_gc_content(seq)</code>","text":"<p>Calculate the GC content of a DNA sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>str</code> <p>DNA sequence (A/C/G/T/U/N, case-insensitive).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>GC content (0.0 ~ 1.0). Returns 0.0 if sequence is empty.</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def calc_gc_content(seq: str) -&gt; float:\n    \"\"\"\n    Calculate the GC content of a DNA sequence.\n\n    Args:\n        seq (str): DNA sequence (A/C/G/T/U/N, case-insensitive).\n\n    Returns:\n        float: GC content (0.0 ~ 1.0). Returns 0.0 if sequence is empty.\n    \"\"\"\n    seq = seq.upper().replace(\"U\", \"T\").replace(\"N\", \"\")\n    try:\n        gc = (seq.count(\"G\") + seq.count(\"C\")) / len(seq)\n    except ZeroDivisionError as e:\n        print(e)\n        gc = 0.0\n    return gc\n</code></pre>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.check_sequence","title":"<code>check_sequence(seq, minl=1, maxl=500000000, gc=(0, 1), valid_chars='ACGTN')</code>","text":"<p>Check if a DNA sequence is valid based on length, GC content, and allowed characters.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>str</code> <p>DNA sequence.</p> required <code>minl</code> <code>int</code> <p>Minimum length. Defaults to 1.</p> <code>1</code> <code>maxl</code> <code>int</code> <p>Maximum length. Defaults to 500000000.</p> <code>500000000</code> <code>gc</code> <code>tuple</code> <p>GC content range (min, max). Defaults to (0, 1).</p> <code>(0, 1)</code> <code>valid_chars</code> <code>str</code> <p>Allowed characters. Defaults to \"ACGTN\".</p> <code>'ACGTN'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if valid, False otherwise.</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def check_sequence(\n    seq: str,\n    minl: int = 1,\n    maxl: int = 500000000,\n    gc: tuple = (0, 1),\n    valid_chars: str = \"ACGTN\"\n) -&gt; bool:\n    \"\"\"\n    Check if a DNA sequence is valid based on length, GC content, and allowed characters.\n\n    Args:\n        seq (str): DNA sequence.\n        minl (int, optional): Minimum length. Defaults to 1.\n        maxl (int, optional): Maximum length. Defaults to 500000000.\n        gc (tuple, optional): GC content range (min, max). Defaults to (0, 1).\n        valid_chars (str, optional): Allowed characters. Defaults to \"ACGTN\".\n\n    Returns:\n        bool: True if valid, False otherwise.\n    \"\"\"\n    if len(seq) &lt; minl or len(seq) &gt; maxl:\n        return False  # \u5e8f\u5217\u957f\u5ea6\u4e0d\u5728\u6709\u6548\u8303\u56f4\u5185\n    elif gc[0] &gt; calc_gc_content(seq) or gc[1] &lt; calc_gc_content(seq):\n        return False  # GC\u542b\u91cf\u4e0d\u5728\u6709\u6548\u8303\u56f4\u5185\n    elif set(seq.upper()) - set(valid_chars) != set():\n        return False  # \u5e8f\u5217\u5305\u542b\u4e0d\u652f\u6301\u7684\u5b57\u7b26\n    else:\n        return True  # \u5e8f\u5217\u6709\u6548\n</code></pre>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.random_generate_sequences","title":"<code>random_generate_sequences(minl, maxl=0, samples=1, gc=(0, 1), N_ratio=0.0, padding_size=0, seed=None)</code>","text":"<p>Randomly generate DNA sequences with specified length, GC content, and N ratio.</p> <p>Parameters:</p> Name Type Description Default <code>minl</code> <code>int</code> <p>Minimum sequence length.</p> required <code>maxl</code> <code>int</code> <p>Maximum sequence length. If 0, use minl as fixed length. Defaults to 0.</p> <code>0</code> <code>samples</code> <code>int</code> <p>Number of sequences to generate. Defaults to 1.</p> <code>1</code> <code>gc</code> <code>tuple</code> <p>GC content range (min, max). Defaults to (0, 1).</p> <code>(0, 1)</code> <code>N_ratio</code> <code>float</code> <p>Proportion of 'N' bases (0.0 ~ 1.0). Defaults to 0.0.</p> <code>0.0</code> <code>padding_size</code> <code>int</code> <p>Pad length to nearest multiple. Defaults to 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of generated DNA sequences.</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def random_generate_sequences(\n    minl: int,\n    maxl: int = 0,\n    samples: int = 1,\n    gc: tuple = (0, 1),\n    N_ratio: float = 0.0,\n    padding_size: int = 0,\n    seed: int = None\n) -&gt; list[str]:\n    \"\"\"\n    Randomly generate DNA sequences with specified length, GC content, and N ratio.\n\n    Args:\n        minl (int): Minimum sequence length.\n        maxl (int, optional): Maximum sequence length. If 0, use minl as fixed length. Defaults to 0.\n        samples (int, optional): Number of sequences to generate. Defaults to 1.\n        gc (tuple, optional): GC content range (min, max). Defaults to (0, 1).\n        N_ratio (float, optional): Proportion of 'N' bases (0.0 ~ 1.0). Defaults to 0.0.\n        padding_size (int, optional): Pad length to nearest multiple. Defaults to 0.\n        seed (int, optional): Random seed. Defaults to None.\n\n    Returns:\n        list[str]: List of generated DNA sequences.\n    \"\"\"\n    sequences = []\n    basemap = [\"A\", \"C\", \"G\", \"T\"]\n    if 0.0 &lt; N_ratio &lt;= 1.0:\n        basemap.append(\"N\")\n        weights = [(1 - N_ratio) / 4] * 4 + [N_ratio]\n    elif N_ratio &gt; 1.0:\n        basemap.append(\"N\")\n        weights = [(100 - N_ratio) / 4] * 4 + [N_ratio]\n    else:\n        weights = None\n    calc_gc = False if gc == (0, 1) else True # Guanqing Please check this line!\n    if seed:\n        random.seed(seed)\n    # progress bar\n    progress_bar = tqdm(total=samples, desc=\"Generating sequences\")\n    # generate sequences\n    if maxl:\n        # generate sequences with random length\n        while True:\n            if len(sequences) &gt;= samples:\n                break\n            length = random.randint(minl, maxl)\n            if padding_size:\n                length = (length // padding_size + 1) * padding_size if length % padding_size else length\n                if length &gt; maxl:\n                    length -= padding_size\n            seq = \"\".join(random.choices(basemap, weights=weights, k=length))\n            if calc_gc:\n                gc_content = calc_gc_content(seq)\n                if gc[0] &lt;= gc_content &lt;= gc[1]:\n                    sequences.append(seq)\n                    progress_bar.update(1)\n            else:\n                sequences.append(seq)\n                progress_bar.update(1)\n    # generate sequences with fixed length\n    else:\n        maxl = minl\n        length = minl\n        while True:\n            if len(sequences) &gt;= samples:\n                break\n            seq = \"\".join(random.choices(basemap, weights=weights, k=length))\n            # calculate GC content\n            if calc_gc:\n                gc_content = calc_gc_content(seq)\n                if gc[0] &lt;= gc_content &lt;= gc[1]:\n                    sequences.append(seq)\n                    progress_bar.update(1)\n            else:\n                sequences.append(seq)\n                progress_bar.update(1)\n    return sequences\n</code></pre>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.reverse_complement","title":"<code>reverse_complement(seq, reverse=True, complement=True)</code>","text":"<p>Compute the reverse complement of a DNA sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>str</code> <p>DNA sequence.</p> required <code>reverse</code> <code>bool</code> <p>Whether to reverse the sequence. Defaults to True.</p> <code>True</code> <code>complement</code> <code>bool</code> <p>Whether to complement the sequence. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The reverse complement (or as specified) of the input sequence.</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def reverse_complement(seq: str, reverse: bool = True, complement: bool = True) -&gt; str:\n    \"\"\"\n    Compute the reverse complement of a DNA sequence.\n\n    Args:\n        seq (str): DNA sequence.\n        reverse (bool, optional): Whether to reverse the sequence. Defaults to True.\n        complement (bool, optional): Whether to complement the sequence. Defaults to True.\n\n    Returns:\n        str: The reverse complement (or as specified) of the input sequence.\n    \"\"\"\n    mapping = {\n        \"A\": \"T\", \"T\": \"A\", \"C\": \"G\", \"G\": \"C\",\n        \"a\": \"t\", \"t\": \"a\", \"c\": \"g\", \"g\": \"c\",\n        \"N\": \"N\", \"n\": \"n\"\n    }\n    if reverse:\n        seq = seq[::-1]\n    if complement:\n        seq = \"\".join(mapping.get(base, base) for base in seq)\n    return seq\n</code></pre>"},{"location":"api/utils/sequence/#dnallm.utils.sequence.seq2kmer","title":"<code>seq2kmer(seqs, k)</code>","text":"<p>Convert a list of DNA sequences to k-mers (overlapping k-mer tokenization).</p> <p>Parameters:</p> Name Type Description Default <code>seqs</code> <code>list[str]</code> <p>List of DNA sequences.</p> required <code>k</code> <code>int</code> <p>k-mer length.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of k-mer tokenized sequences (space-separated k-mers).</p> Source code in <code>dnallm/utils/sequence.py</code> <pre><code>def seq2kmer(seqs: list[str], k: int) -&gt; list[str]:\n    \"\"\"\n    Convert a list of DNA sequences to k-mers (overlapping k-mer tokenization).\n\n    Args:\n        seqs (list[str]): List of DNA sequences.\n        k (int): k-mer length.\n\n    Returns:\n        list[str]: List of k-mer tokenized sequences (space-separated k-mers).\n    \"\"\"\n    all_kmers = []\n    for seq in seqs:\n        kmer = [seq[x:x+k].upper() for x in range(len(seq)+1-k)]\n        kmers = \" \".join(kmer)\n        all_kmers.append(kmers)\n    return all_kmers\n</code></pre>"},{"location":"getting_started/installation/","title":"\u5b89\u88c5","text":"<p>\u6211\u4eec\u63a8\u8350\u4f7f\u7528 Astral uv \u5305\u7ba1\u7406\u5de5\u5177\u6765\u5b89\u88c5DNALLM\u7684\u8fd0\u884c\u73af\u5883\u548c\u4f9d\u8d56</p> <p>\u3010\u4ec0\u4e48\u662fuv\uff1f\u3011 uv\u662f\u4e00\u4e2a\u57fa\u4e8eRust\u5f00\u53d1\u7684\u5feb\u901f\u7684Python\u5305\u7ba1\u7406\u5de5\u5177\uff0c\u6bd4pip\u7b49\u4f20\u7edf\u5de5\u5177\u5feb10-100\u500d\u3002</p>"},{"location":"getting_started/installation/#1-uv","title":"1. \u5b89\u88c5uv","text":"<pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"getting_started/installation/#2-dnallm","title":"2. \u5b89\u88c5DNALLM","text":"<pre><code># \u514b\u9686\u4ed3\u5e93\ngit clone https://github.com/zhangtaolab/DNALLM.git\ncd DNALLM\n\n# \u521b\u5efa\u865a\u62df\u73af\u5883\u5e76\u5b89\u88c5\u4f9d\u8d56\nuv venv\n# \u6fc0\u6d3b\u865a\u62df\u73af\u5883\n## Linux &amp; MacOS \u7528\u6237\nsource .venv/bin/activate\n## Windows \u7528\u6237\n.venv\\Scripts\\activate\n\n# \u5b89\u88c5\u4f9d\u8d56\nuv pip install -e '.[base]'\n\n# \u5b89\u88c5DNALLM\nuv pip install -e .\n</code></pre> <p>\u5728\u5b89\u88c5\u4f9d\u8d56\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5bf9\u4e8e\u6ca1\u6709GPU\u6216\u8005\u4f7f\u7528\u5e26\u82f9\u679cM\u7cfb\u5217\u82af\u7247\u7684MacOS\u7528\u6237\uff0c\u5b89\u88c5\u4f9d\u8d56\u65f6\u76f4\u63a5\u4f7f\u7528 <code>base</code> \u4f9d\u8d56\u5373\u53ef\uff1b\u5bf9\u4e8e\u4f7f\u7528\u5e26Nvidia\u663e\u5361\u7684Linux\u7528\u6237\uff0c\u901a\u5e38\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>base</code> \u4f9d\u8d56\uff0c\u5305\u7ba1\u7406\u8f6f\u4ef6\u4f1a\u81ea\u52a8\u8bc6\u522b\u5e76\u5b89\u88c5\u5bf9\u5e94cuda\u7248\u672c\u7684torch\u5305\uff1b\u5bf9\u4e8eWindows\u7528\u6237\uff0c\u5efa\u8bae\u5728\u5b89\u88c5\u4f9d\u8d56\u65f6\u6307\u5b9acuda\u7248\u672c\u4ee5\u9632\u6b62\u5305\u7ba1\u7406\u8f6f\u4ef6\u65e0\u6cd5\u6b63\u786e\u8bc6\u522b\u5230\u60a8\u7684\u663e\u5361\u3002</p> <p>\u8003\u8651\u5230\u4e0d\u540c\u7528\u6237\u4f7f\u7528\u7684\u663e\u5361\u652f\u6301\u7684CUDA\u7248\u672c\u4e0d\u540c\uff0c\u5982\u679c\u9700\u8981\u6307\u5b9aCUDA\u7248\u672c\u8fdb\u884c\u5b89\u88c5\uff0c\u53ef\u901a\u8fc7\u4e00\u4e0b\u547d\u4ee4\u8fdb\u884c\u5b89\u88c5 <pre><code># \u9996\u5148\u5b89\u88c5\u6307\u5b9a\u7248\u672c\u7684torch (\u652f\u6301cpu, cuda121, cuda124, cuda126)\nuv pip install -e '.[cuda124]'\n# \u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u5b89\u88c5\u5176\u4ed6\u4f9d\u8d56\nuv pip install -e '.[base]'\n</code></pre></p> <p>\u5bf9\u4e8e\u4f7f\u7528AMD\u663e\u5361\u7684\u7528\u6237\uff0c\u8bf7\u786e\u4fdd\u4f60\u662f\u7528\u7684\u7cfb\u7edf\u65f6Linux\uff0c\u5e76\u624b\u52a8\u5b89\u88c5\u5bf9\u5e94torch\u7248\u672c\u3002 <pre><code>uv pip install 'torch&gt;=2.5' --index-url https://download.pytorch.org/whl/rocm6.2\n</code></pre> \u5bf9\u4e8e\u4f7f\u7528Intel\u663e\u5361\u7684\u7528\u6237\uff0c\u5982\u679c\u60a8\u5e0c\u671b\u4f7f\u7528\u663e\u5361\u52a0\u901f\uff0c\u8bf7\u53c2\u8003 \u5b98\u65b9\u6587\u6863 \u5b89\u88c5\u597d\u5bf9\u5e94\u7684\u9a71\u52a8\u548ctorch\u7248\u672c\u3002</p> <p>\u6709\u4e9b\u6a21\u578b\u9700\u8981\u5176\u4ed6\u4f9d\u8d56\u7684\u652f\u6301\uff0c\u9488\u5bf9\u4e0d\u540c\u6a21\u578b\u9700\u8981\u7684\u4f9d\u8d56\uff0c\u6211\u4eec\u4f1a\u6301\u7eed\u6dfb\u52a0\u3002</p> <ol> <li> <p>\u539f\u751f <code>mamba</code> \u67b6\u6784\u7684\u8fd0\u884c\u901f\u5ea6\u663e\u8457\u4f18\u4e8etransformer\u517c\u5bb9\u7684mamba\u67b6\u6784\uff0c\u4e0d\u8fc7\u539f\u751f mamba \u4f9d\u8d56\u4e8eNvidia\u663e\u5361\uff0c\u5982\u679c\u9700\u8981\u539f\u751f mamba \u67b6\u6784\u652f\u6301\uff0c\u5b89\u88c5\u5b8cDNALLM\u4f9d\u8d56\u540e\uff0c\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a</p> <pre><code>uv pip install -e '.[mamba]' --no-cache-dir --no-build-isolation\n</code></pre> </li> <li> <p><code>EVO2</code> \u6a21\u578b\u7684\u5fae\u8c03\u548c\u63a8\u7406\u4f9d\u8d56\u5176\u81ea\u5df1\u7684\u8f6f\u4ef6\u5305 \u6216 \u7b2c\u4e09\u65b9python\u5e931/\u5e932</p> <pre><code># evo2 repuires python version &gt;=3.11\ngit clone --recurse-submodules git@github.com:ArcInstitute/evo2.git\ncd evo2\n# install required dependency 'vortex'\ncd vortex\nuv pip install .\n# install evo2\ncd ..\nuv pip install .\n\n# add cudnn path to environment\nexport LD_LIBRARY_PATH=[path_to_DNALLM]/.venv/lib64/python3.11/site-packages/nvidia/cudnn/lib:${LD_LIBRARY_PATH}\n</code></pre> </li> <li> <p>\u90e8\u5206\u6a21\u578b\u4f7f\u7528\u4e86\u81ea\u5df1\u5f00\u53d1\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5b83\u4eec\u5c1a\u672a\u88ab\u6574\u5408\u8fdbHuggingFace\u7684transformers\u5e93\u4e2d\uff0c\u56e0\u6b64\u5bf9\u8fd9\u7c7b\u6a21\u578b\u7684\u5fae\u8c03\u548c\u63a8\u7406\u9700\u8981\u63d0\u524d\u5b89\u88c5\u5bf9\u5e94\u7684\u6a21\u578b\u4f9d\u8d56\u5e93\u3002</p> <ul> <li>(1) GPN \u9879\u76ee\u5730\u5740\uff1ahttps://github.com/songlab-cal/gpn <pre><code>git clone https://github.com/songlab-cal/gpn\ncd gpn\nuv pip install .\n</code></pre></li> <li>(2) Omni-DNA \u9879\u76ee\u5730\u5740\uff1ahttps://huggingface.co/zehui127/Omni-DNA-20M <pre><code>uv pip install ai2-olmo\n</code></pre></li> <li>(3) megaDNA \u9879\u76ee\u5730\u5740\uff1ahttps://github.com/lingxusb/megaDNA <pre><code>git clone https://github.com/lingxusb/megaDNA\ncd megaDNA\nuv pip install .\n</code></pre></li> <li>(4) Enformer \u9879\u76ee\u5730\u5740\uff1ahttps://github.com/lucidrains/enformer-pytorch <pre><code>uv pip install enformer-pytorch\n</code></pre></li> <li>(5) Borzoi \u9879\u76ee\u5730\u5740\uff1ahttps://github.com/johahi/borzoi-pytorch <pre><code>uv pip install borzoi-pytorch\n</code></pre></li> </ul> </li> <li> <p>\u6709\u4e9b\u6a21\u578b\u652f\u6301Flash Attention\u52a0\u901f\uff0c\u5982\u679c\u9700\u8981\u5b89\u88c5\u8be5\u4f9d\u8d56\uff0c\u53ef\u4ee5\u53c2\u8003 \u9879\u76eeGitHub \u5b89\u88c5\u3002\u9700\u8981\u6ce8\u610f <code>flash-attn</code> \u7684\u7248\u672c\u548c\u4e0d\u540c\u7684python\u7248\u672c\u3001pytorch\u7248\u672c\u4ee5\u53cacuda\u7248\u672c\u6302\u94a9\uff0c\u8bf7\u5148\u68c0\u67e5 GitHub Releases \u4e2d\u662f\u5426\u6709\u5339\u914d\u7248\u672c\u7684\u5b89\u88c5\u5305\uff0c\u5426\u5219\u53ef\u80fd\u4f1a\u51fa\u73b0 <code>HTTP Error 404: Not Found</code> \u7684\u62a5\u9519\u3002     <pre><code>uv pip install flash-attn --no-build-isolation\n</code></pre></p> </li> <li> <p>\u5982\u679c\u51fa\u73b0\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u9700\u8981\u7f16\u8bd1\uff0c\u800c\u7f16\u8bd1\u8fc7\u7a0b\u62a5\u9519\uff0c\u8bf7\u524d\u5b89\u88c5\u597d\u53ef\u80fd\u9700\u8981\u7684\u4f9d\u8d56\uff0c\u63a8\u8350\u4f7f\u7528 <code>conda</code> \u5b89\u88c5\u4f9d\u8d56\u3002     <pre><code>conda install -c conda-forge gxx clang\n</code></pre></p> </li> </ol>"},{"location":"getting_started/installation/#3","title":"3. \u68c0\u67e5\u662f\u5426\u5b89\u88c5\u6210\u529f","text":"<pre><code>sh tests/test_all.sh\n</code></pre>"},{"location":"getting_started/quick_start/","title":"Quick Start","text":""},{"location":"getting_started/quick_start/#quick-start","title":"Quick Start","text":"<ol> <li>Install dependencies (recommended: uv)</li> </ol> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n\ngit clone https://github.com/zhangtaolab/DNALLM.git\n\ncd DNALLM\n\nuv venv\n\nsource .venv/bin/activate\n\nuv pip install -e '.[base]'\n</code></pre> <ol> <li>Fine-tune DNA LLM</li> </ol> <pre><code>\n</code></pre> <ol> <li>Predict for sequences</li> </ol> <pre><code>\n</code></pre>"},{"location":"resources/datasets/","title":"datasets","text":"<p>\u8be5\u6a21\u5757\u4e3b\u8981\u7528\u4e8e\u8bfb\u53d6\u5728\u7ebf\u6216\u8005\u672c\u5730\u7684\u6570\u636e\u96c6\u3001\u751f\u6210\u6570\u636e\u96c6\u6216\u5904\u7406\u5df2\u8bfb\u53d6\u7684\u6570\u636e\u96c6\u3002\u6a21\u5757\u5b9a\u4e49\u4e86\u4e00\u4e2a <code>DNADataset</code> \u7684\u7c7b\u3002 \u5e38\u7528\u529f\u80fd\u4e3b\u8981\u5305\u542b\u4ee5\u4e0b\u51e0\u79cd\uff1a</p> <ul> <li> <p>\u8bfb\u53d6\u6570\u636e   \u5728\u7ebf\u6570\u636e\u96c6\u53ef\u4ee5\u901a\u8fc7\u4ece <code>HuggingFace</code> \u6216 <code>Modelscope</code> \u4e24\u4e2a\u6570\u636e\u5e93\u8fdb\u884c\u4e0b\u8f7d\u548c\u8bfb\u53d6\u3002\u672c\u5730\u6570\u636e\u96c6\u652f\u6301\u8bfb\u53d6\u591a\u79cd\u7c7b\u578b\u4fdd\u5b58\u7684\u5e8f\u5217\u6570\u636e\uff08.csv, .tsv, .json, .arrow, .parquet, .txt, dict, fasta\u7b49\uff09\u3002  </p> </li> <li> <p>\u6570\u636e\u68c0\u67e5\uff08\u652f\u6301\u5e8f\u5217\u957f\u5ea6\u8fc7\u6ee4\u3001GC\u542b\u91cf\u8fc7\u6ee4\u3001\u5305\u542b\u7684\u78b1\u57fa\u68c0\u67e5\uff0c\u4e0d\u7b26\u5408\u8981\u6c42\u7684\u5e8f\u5217\u5c06\u88ab\u8fc7\u6ee4\uff09</p> </li> <li> <p>\u6570\u636e\u6e05\u6d17\uff08\u8fc7\u6ee4\u6389\u5e8f\u5217\u6216\u6807\u7b7e\u4fe1\u606f\u7f3a\u5931\u7684\u6570\u636e\uff09</p> </li> <li> <p>\u6570\u636e\u589e\u5f3a</p> </li> <li>\u968f\u673a\u53cd\u5411\u4e92\u8865</li> <li>\u6dfb\u52a0\u53cd\u5411\u4e92\u8865\u5e8f\u5217\uff08\u539f\u6709\u6570\u636e\u96c6\u6269\u59271\u500d\uff09</li> <li> <p>\u6dfb\u52a0\u968f\u673a\u6570\u636e\uff08\u53ef\u8c03\u6574\u5e8f\u5217\u957f\u5ea6\u3001\u5e8f\u5217\u6570\u3001GC\u542b\u91cf\u5206\u5e03\u3001\u662f\u5426\u5305\u542b\u78b1\u57faN\uff0c\u5e8f\u5217\u662f\u5426\u9700\u8981\u4e3a\u6307\u5b9a\u500d\u6570\u7684\u957f\u5ea6\uff09</p> </li> <li> <p>\u6570\u636e\u6253\u4e71</p> </li> <li> <p>\u6570\u636e\u62c6\u5206</p> </li> <li> <p>\u5e8f\u5217\u5206\u8bcd\uff08tokenization\uff09</p> </li> </ul>"},{"location":"resources/datasets/#_1","title":"\u793a\u4f8b\uff1a","text":"<pre><code>from dnallm import DNADataset\nfrom transformers import AutoTokenizer\n</code></pre> <pre><code># \u8bfb\u53d6tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"zhangtaolab/plant-dnabert-BPE\")\n\n# \u8bfb\u53d6\u6570\u636e\n# 1. \u8bfb\u53d6\u672c\u5730\u6570\u636e\uff08\u9700\u8981\u5236\u5b9a\u5e8f\u5217\u548c\u6807\u7b7e\u5217\u7684\u5934\u540d\uff09\uff0c\u6307\u5b9atokenizer\n# 1.1 \u5355\u4e2a\u6587\u4ef6\ndna_ds = DNADataset.load_local_data(\n    \"/path_to_your_datasets/data.csv\", seq_col=\"sequence\", label_col=\"labels\",\n    tokenizer=tokenizer, max_length=512\n)\n# 1.2 \u591a\u4e2a\u6587\u4ef6\uff08\u5982\u6570\u636e\u96c6\u5df2\u62c6\u5206\uff09\ndna_ds = DNADataset.load_local_data(\n    {\"train\": \"train.csv\", \"test\": \"test.csv\", \"validation\": \"validation.csv\"},\n    seq_col=\"sequence\", label_col=\"labels\",\n    tokenizer=tokenizer, max_length=512\n)\n\n# 2. \u8bfb\u53d6\u5728\u7ebf\u6570\u636e\n# 2.1 HuggingFace\ndna_ds = DNADataset.from_huggingface(\"zhangtaolab/plant-multi-species-open-chromatin\", seq_field=\"sequence\", label_field=\"label\", tokenizer=tokenizer, max_length=512)\n# 2.2 ModelScope\ndna_ds = DNADataset.from_modelscope(\"zhangtaolab/plant-multi-species-open-chromatin\", seq_field=\"sequence\", label_field=\"label\", tokenizer=tokenizer, max_length=512)\n</code></pre> <pre><code># \u5e38\u7528\u529f\u80fd\u4ecb\u7ecd\n# 1. \u6570\u636e\u68c0\u67e5\uff08\u652f\u6301\u5e8f\u5217\u957f\u5ea6\u8fc7\u6ee4\u3001GC\u542b\u91cf\u8fc7\u6ee4\u3001\u5305\u542b\u7684\u78b1\u57fa\u68c0\u67e5\uff0c\u4e0d\u7b26\u5408\u8981\u6c42\u7684\u5e8f\u5217\u5c06\u88ab\u8fc7\u6ee4\uff09\ndna_ds.validate_sequences(minl=200, maxl=1000, valid_chars=\"ACGT\")\n\n# 2. \u6570\u636e\u6e05\u6d17\uff08\u8fc7\u6ee4\u6389\u7f3a\u4e4f\u5e8f\u5217\u6216\u6807\u7b7e\u4fe1\u606f\u7684\u6570\u636e\uff09\ndna_ds.process_missing_data()\n\n# 3. \u62c6\u5206\u6570\u636e\ndna_ds.split_data(test_size=0.2, val_size=0.1)\n\n# 4. \u6570\u636e\u6253\u4e71\ndna_ds.shuffle(seed=42)\n\n# 5. \u6570\u636e\u589e\u5f3a\n# 5.1 \u968f\u673a\u53cd\u5411\u4e92\u8865\ndna_ds.raw_reverse_complement(ratio=0.5) # 50%\u7684\u5e8f\u5217\u8fdb\u884c\u53cd\u5411\u4e92\u8865\n\n# 5.2 \u6dfb\u52a0\u53cd\u5411\u4e92\u8865\u5e8f\u5217\uff08\u539f\u6709\u6570\u636e\u96c6\u6269\u59271\u500d\uff09\ndna_ds.augment_reverse_complement()\n\n# 5.3 \u52a0\u5165\u968f\u673a\u6570\u636e\uff08\u53ef\u8c03\u6574\u5e8f\u5217\u957f\u5ea6\u3001\u5e8f\u5217\u6570\u3001GC\u542b\u91cf\u5206\u5e03\u3001\u662f\u5426\u5305\u542b\u78b1\u57faN\uff0c\u5e8f\u5217\u662f\u5426\u9700\u8981\u4e3a\u6307\u5b9a\u500d\u6570\u7684\u957f\u5ea6\uff09\ndna_ds.random_generate(minl=200, maxl=2000, samples=3000, gc=(0.1, 0.9), N_ratio=0.0, padding_size=1, append=True, label_func=None)\n## `label_func`\u662f\u7528\u6765\u81ea\u5b9a\u4e49\u6570\u636e\u6807\u7b7e\u7684\u51fd\u6570\n## \u5982\u679c\u4e0d\u52a0`append=True`\u7684\u53c2\u6570\uff0c\u5373\u4f7f\u7528\u751f\u6210\u7684\u6570\u636e\u96c6\u8986\u76d6\u539f\u59cb\u6570\u636e\u96c6\uff08\u53ef\u7528\u4e8e\u968f\u673a\u521d\u59cb\u5316\u6570\u636e\u96c6\uff09\n\n# 6. \u6570\u636e\u964d\u91c7\u6837\nnew_ds = dna_ds.sampling(ratio=0.1)\n\n# 7. \u6570\u636e\u5c55\u793a\ndna_ds.show(head=20)          # \u663e\u793a\u683c\u5f0f\u5316\u540e\u7684\u524dN\u4e2a\u6570\u636e\ntmp_ds = dna_ds.head(head=5)  # \u63d0\u53d6\u524dN\u4e2a\u6570\u636e\n\n# 8. \u5e8f\u5217tokenization\uff08\u8981\u6c42\u63d0\u524d\u5b9a\u4e49\u597dDNADataset.tokenizer\uff09\ndna_ds.encode_sequences()\n</code></pre>"},{"location":"resources/datasets/#_2","title":"\u51fd\u6570\u53ca\u53c2\u6570\u8bf4\u660e","text":"<p>\u8bf7\u53c2\u8003 API \u90e8\u5206\u8bf4\u660e</p>"},{"location":"resources/model_zoo/","title":"\u6a21\u578b\u5e93","text":"<p>DNALLM\u4e2d\u6536\u5f55\u4e86\u51e0\u4e4e\u6240\u6709\u53ef\u4ee5\u5728\u7ebf\u83b7\u53d6\u7684 DNA\u5927\u8bed\u8a00\u6a21\u578b \u548c\u90e8\u5206\u57fa\u4e8eDNA\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u9488\u5bf9\u4e0d\u540c\u7684\u6a21\u578b\uff0c\u6211\u4eec\u5bf9\u5176\u8fdb\u884c\u4e86\u9002\u914d\uff0c\u4f7f\u5176\u53ef\u4ee5\u901a\u8fc7 DNALLM\u5305 \u8fdb\u884c\u6a21\u578b\u7684\u5fae\u8c03\u548c\u63a8\u7406\u3002</p>"},{"location":"resources/model_zoo/#_2","title":"\u6a21\u578b\u5217\u8868","text":"<p>\u76ee\u524d\u5df2\u6536\u5f55\u7684\u6a21\u578b\u548c\u6a21\u578b\u7684\u5fae\u8c03/\u63a8\u7406\u652f\u6301\u60c5\u51b5\u5982\u4e0b\uff1a</p> \u6a21\u578b\u540d\u79f0 \u6a21\u578b\u4f5c\u8005 \u6a21\u578b\u7c7b\u578b \u6a21\u578b\u67b6\u6784 \u6a21\u578b\u5927\u5c0f \u6a21\u578b\u6570\u91cf \u6a21\u578b\u6765\u6e90 \u662f\u5426\u652f\u6301\u5fae\u8c03 Nucleotide Transformer InstaDeepAI MaskedLM ESM 50M / 100M / 250M / 500M / 2.5B 8 Nature Methods \u662f AgroNT InstaDeepAI MaskedLM ESM 1B 1 Current Biology \u662f Caduceus-Ph Kuleshov-Group MaskedLM Caduceus 0.5M / 2M / 8M 3 arXiv \u662f Caduceus-Ps Kuleshov-Group MaskedLM Caduceus 0.5M / 2M / 8M 3 arXiv \u662f PlantCaduceus Kuleshov-Group MaskedLM Caduceus 20M / 40M / 112M / 225M 4 bioRxiv \u662f DNABERT Zhihan1996 MaskedLM BERT 100M 4 Bioinformatics \u662f DNABERT-2 Zhihan1996 MaskedLM BERT 117M 1 arXiv \u662f DNABERT-S Zhihan1996 MaskedLM BERT 117M 1 arXiv \u662f GENA-LM AIRI-Institute MaskedLM BERT 150M / 500M 7 Nucleic Acids Research \u662f GENA-LM-BigBird AIRI-Institute MaskedLM BigBird 150M 3 Nucleic Acids Research \u662f GENERator GenerTeam CausalLM Llama 0.5B / 1.2B / 3B 4 arXiv \u662f GenomeOcean pGenomeOcean CausalLM Mistral 100M / 500M / 4B 3 bioRxiv \u662f GPN songlab MaskedLM ConvNet 60M 1 PNAS \u5426 GROVER PoetschLab MaskedLM BERT 100M 1 Nature Machine Intelligence \u662f HyenaDNA LongSafari CausalLM HyenaDNA 0.5M / 0.7M / 2M / 4M / 15M / 30M / 55M 7 arXiv \u662f Jamba-DNA RaphaelMourad CausalLM Jamba 114M 1 GitHub \u662f Mistral-DNA RaphaelMourad CausalLM Mistral 1M / 17M / 138M / 417M / 422M 10 GitHub \u662f ModernBert-DNA RaphaelMourad MaskedLM ModernBert 37M 3 GitHub \u662f MutBERT JadenLong MaskedLM RoPEBert 86M 3 bioRxiv \u662f OmniNA XLS CausalLM Llama 66M / 220M 2 bioRxiv \u662f Omni-DNA zehui127 CausalLM OLMoModel 20M / 60M / 116M / 300M / 700M / 1B 6 arXiv \u5426 EVO-1 togethercomputer CausalLM StripedHyena 6.5B 2 GitHub \u662f EVO-2 arcinstitute CausalLM StripedHyena2 1B / 7B / 40B 3 GitHub \u5426 ProkBERT neuralbioinfo MaskedLM MegatronBert 21M / 25M / 27M 3 Frontiers in Microbiology \u662f Plant DNABERT zhangtaolab MaskedLM BERT 100M 1 Molecular Plant \u662f Plant DNAGPT zhangtaolab CausalLM GPT2 100M 1 Molecular Plant \u662f Plant Nucleotide Transformer zhangtaolab MaskedLM ESM 100M 1 Molecular Plant \u662f Plant DNAGemma zhangtaolab CausalLM Gemma 150M 1 Molecular Plant \u662f Plant DNAMamba zhangtaolab CausalLM Mamba 100M 1 Molecular Plant \u662f Plant DNAModernBert zhangtaolab MaskedLM ModernBert 100M 1 Molecular Plant \u662f"}]}