{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "max_length = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_generate_sequences(minl, maxl=0, samples=1, with_N=False, padding_size=0, gc=0, seed=None):\n",
    "    sequences = []\n",
    "    basemap = [\"A\", \"C\", \"G\", \"T\"]\n",
    "    if with_N:\n",
    "        basemap.append(\"N\")\n",
    "    baseidx = len(basemap) - 1\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "    if maxl:\n",
    "        for i in range(samples):\n",
    "            length = random.randint(minl, maxl)\n",
    "            if padding_size:\n",
    "                length = (length // padding_size + 1) * padding_size if length % padding_size else length\n",
    "                if length > maxl:\n",
    "                    length -= padding_size\n",
    "            seq = \"\".join([basemap[random.randint(0,baseidx)] for _ in range(length)])\n",
    "            sequences.append(seq)\n",
    "    else:\n",
    "        for i in range(samples):\n",
    "            seq = \"\".join([basemap[random.randint(0,baseidx)] for _ in range(minl)])\n",
    "            sequences.append(seq)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCGACAGCAGGATAGATCGGCGGTGGTCGCCATTGTATGCTGCACCGGGGGCGGTTATGAGTACAGTTGGTCTATCAAGTACACAGTCTTTTATCCCACCTCTTCCGAGCTCACTGGTATGAAGGGCCGAGACGGAGTGGCCTAAGTCGTCGTTCGAGGGAATGGCAAACAATTCTAGTTGAAAGCGCCACAACCCTTCGACAGACTATTGTTAGTGAGCGCACCGCTGTGAGGGGGAACACACTGCACGGATTCGCACCCCATACCGTTCTTTCCATAGTAGAATTGATTGGTATATCCCTAGTATTTCAAGCATACTCTGTTTCATTATGTCTGTCCACGCCGCAACACCCAGAACTAGGCAGGCGCCCATGGGTCGTGTCCACGCTCTCATGAAGGGACGCTCAGGGGCGTTTACTAGAAGCA',\n",
       " 'AGGAGTCTGACTCAGATCGAAAAGCTGCAGAACTAGTTAAGGTTCTTACTACTGGCTAAATCAACAGCGAGA',\n",
       " 'AGAAACTAAGTGGACCATGCAATCACAAAGAGGCGTATATCTCCCAGCACTACGCAGGAGTACGCAGAAGGGGGCTCATATTGACCATTTTTCCCGCCCGTGTCTCATCCATGATATTCATACTTCCGTGTACCGATTGGCCTTTCCTTTAGTGACATACTAACAACCATTGCAGAGTCACGGAATGCGGGGTAGACTACAAAGTCTTTAAAGTTCATACCACAGAATTCACTAGTGAACAAGTTGATGGCCGTTATGATGTGATTGACAGGAACTCGCTCAAGCTGGATCTCTTCTTAACCTACAGTCAATCGCATTGGGGCAAGGGCTCAGCGAAAGCGTAAATATATCGGAAATAAATAGCAGCGATAGAGTGCGGCACTAAGTGGGTGTTTGTGTGTATTCCTATCTGGCCAGGGGTCTATGCGATTCCGCGGC',\n",
       " 'TTACTGTATTGTTATATCCAGTTTAACATCAATTCCGTTCCGCCACACGGCATCCAAATGGAGGCCAAAAATATCCTGCCTTGGAGCGCGAAAATACTCGGATTTTTAAGCTTTGTAAGCGGTCCATCAAGGCTGCTGTATATTCAAGCCTTTAGTGAAGGACCCGTCCAATTCCGGTGAGGAGGCCCGTTTGATCGGCCACATCCTAGAATCGCATCCGTA',\n",
       " 'TGGAAGTACGAGCTCGCTTCCACCGAGGATAGTGAGAAACCAACCGGCTGAGGCAGCCGGGCGGCACGGGAACCTGTTCTCGTAATCCACGGGATCCTTAATTTAGGACCCGGCTGCCATGTTTCACGGATTGCCTTGTGAAGCTGCGAATCCTCGCATCTTGCGACGCGAGCTGAGCTACGATCGCGTTAATGCCGCCTAGGTGGAGGTAATCGATGTAGCGTTTCTTGCGAATCGCATCATGCACTTAAAATCTCTCTAAACGATCTCCCGAGGCTATTCACCGTAGGAGGAGACCGGTAGCGCTGAACGTCGCATTCATGACAGTGACTGTTACATTAAAACGCGGTTAGACGTGTA',\n",
       " 'GGACACGCCTAACCCTTAAAAATCCCTACACTGTGAGCGTAAACCTTGTACGCATGGGAC',\n",
       " 'TTTCATTATAAGATGCTCGTATGAGACTTTACAGACCTGATAACTCACAAGCCGGCCAGTAATTTTCACTCACAACCACATGAATGCCCACTGGTGAACTATTGGTTTATCCATAGCACCGCGGCCTCTTGGCTCGACACTGGTTAAGTCCCCTGTAATACCGCCTATCCATTGGCGTAAAGTTCTAAGCAGATAGACTAGAGCACGTCACTCGCGAGATCTTAAGATGTCACTCGGTCGCCGTGACCCATTCCAGCGGGTCGCGCAACAGACAGCAATCGGCCGTAACTGGCTTCTGTGGAAAGTCGTACATGTTCTAGTCAGGGTTACGCGGTCCTGGCGGTGAGACAAAGCTATTGAAGAGTTATTATACAGTGGGTCCCCCCACTCTCGTCCGGGCAAGGCTGTAACCAGATACTTCTGACATGTACCAGCGACCCGGGCAGAACATAGGGATGTGAGGTACCCAGATGAGGTTATGGCCTA',\n",
       " 'CGCTTAACGGGGAATTTCGCAGTGGTCCACGATGGAAATGAGCGCACACGCCCTACTCTCCACATGTAGCGGTGTGAGCTTCTAGGGAGCGAATGGACAAATTGCTACGCAGCCATGTTCCGACATCTGGCCACTTACGAATTGCTTAATAGACTAGAAACTTCGATGCGCGAGAGATACGACGAAGAGATCTCTAAGATCTAGGAGCAAGGCGTCCAATATCTAAACTCTTGCGACTAAACCCCTGCTCGTATCGCTCGAGATGGAGCG',\n",
       " 'CCCGTACGATCCACAATTTAGTTTTTCCAGACTCGTGATCGCAGCCGTATACTCGTTAAAAAGGACCAAGCGTGAAGTCGTGAATAAGCAGCTCATCACAATTCGACCCTTATTTTCAGCCCACGTTGACATCGGGCTTGACTTCGGCGTTTCTTGATCTACCTCCACGATCTTTAGGCCAGCCTAGATTGGATAGTT',\n",
       " 'AGCAACTATGAGGCTATTAGCACGCCGGCGGTAGGCTACCATCAGTCGGAGCAGCACGGCACTACATAACACCCCTGCGAAAAATTCCAAAAGTTAAATGCTTCATGTAGTCCCGCTCTCGGCAGCATAAAGGAAAATAAGGTGCGAACTCGTATTTAGCATGGACTCAATCGCGCAATTAGAGCTTCCCACAGAAGATGAGAAATTAAAACTAGAACTTCTCGACGGCTAGTTATGGGA']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequences = [\"ATTCCGATTCCGATTCCG\", \"ATTTCTCTCTCTCTCTGAGATCGATCGATCGAT\"]\n",
    "sequences = random_generate_sequences(30, 500, 100, padding_size=6)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    sequences,\n",
    "    truncation=True, padding='longest',\n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "tokens_ids = inputs['input_ids'].detach()\n",
    "tokens_str = [b.split() for b in tokenizer.batch_decode(tokens_ids)]\n",
    "tokens_idx = [[False if s in tokenizer.all_special_tokens else True for i, s in enumerate(tokens)] for tokens in tokens_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "sig = inspect.signature(model.forward)\n",
    "params = sig.parameters\n",
    "if \"output_attentions\" in params:\n",
    "    outputs = model(\n",
    "        **inputs,\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "else:\n",
    "    outputs = model(\n",
    "        **inputs,\n",
    "        output_hidden_states=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_map(attentions, tokens_str, tokens_idx, layer=-1, idx=0, ncols=3, scale_width=5, scale_height=4):\n",
    "    tokens = [tokens_str[idx][i] for i,b in enumerate(tokens_idx[idx]) if b]\n",
    "    n_heads = len(attentions)\n",
    "    nrows = (n_heads + ncols - 1) // ncols\n",
    "    figsize = (ncols * scale_width, nrows * scale_height)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    if n_heads == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    for i, data in enumerate(attentions):\n",
    "        data = data[layer][idx].detach().numpy()\n",
    "        data = [[data[j][jj] for jj,bb in enumerate(tokens_idx[idx]) if bb]\n",
    "                             for j,b in enumerate(tokens_idx[idx]) if b]\n",
    "        sns.heatmap(\n",
    "            data,\n",
    "            ax=axes[i], cmap=\"viridis\",\n",
    "            xticklabels=tokens,\n",
    "            yticklabels=tokens\n",
    "        )\n",
    "        axes[i].set_title(f\"Head {i+1}\")\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention map\n",
    "if hasattr(outputs, 'attentions'):\n",
    "    attentions = outputs.attentions  # ((seq_num, heads, max_token_len, max_token_len) x layers)\n",
    "    plot_attention_map(attentions, tokens_str, tokens_idx, layer=-1, idx=1, ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_embeddings(hidden_states, attention_mask, layers=[0,1], labels=None, reducer=\"t-SNE\", ncols=4, scale_width=5, scale_height=4):\n",
    "    if reducer.lower() == \"pca\":\n",
    "        dim_reducer = PCA(n_components=2)\n",
    "    elif reducer.lower() == \"t-sne\":\n",
    "        dim_reducer = TSNE(n_components=2)\n",
    "    elif reducer.lower() == \"umap\":\n",
    "        dim_reducer = UMAP(n_components=2)\n",
    "    else:\n",
    "        raise(\"Unsupported dim reducer, please try PCA, t-SNE or UMAP.\")\n",
    "    n_layers = len(layers)\n",
    "    nrows = (n_layers + ncols - 1) // ncols\n",
    "    figsize = (ncols * scale_width, nrows * scale_height)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    if n_layers == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    for i,layer_i in enumerate(layers):\n",
    "        embeddings = hidden_states[layer_i].detach().numpy()\n",
    "        mean_sequence_embeddings = torch.sum(attention_mask*embeddings, axis=-2) / torch.sum(attention_mask, axis=1)\n",
    "        layer_dim_reduced_vectors = dim_reducer.fit_transform(mean_sequence_embeddings.detach().numpy())\n",
    "        if not labels:\n",
    "            labels = [\"Uncategorized\"] * layer_dim_reduced_vectors.shape[0]\n",
    "        dataframe = {\n",
    "            'Dimension 1': layer_dim_reduced_vectors[:,0],\n",
    "            'Dimension 2': layer_dim_reduced_vectors[:,1],\n",
    "            'labels': labels\n",
    "            }\n",
    "        df = pd.DataFrame.from_dict(dataframe)\n",
    "        sns.scatterplot(\n",
    "            data=df,\n",
    "            x='Dimension 1',\n",
    "            y='Dimension 2',\n",
    "            hue='labels', ax=axes[i]\n",
    "        )\n",
    "        axes[i].set_title(f\"Layer {layer_i+1}\")\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the layer embeddings\n",
    "hidden_states = outputs['hidden_states']\n",
    "attention_mask = torch.unsqueeze(torch.tensor(tokens_idx), dim=-1)\n",
    "\n",
    "plot_layer_embeddings(hidden_states, attention_mask, layers=range(6), labels=None, reducer=\"t-SNE\", ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_probability(probabilities, idx=0, top_k=5):\n",
    "    tokens_probs = []\n",
    "    probas = probabilities[idx]\n",
    "    for pos, probs in enumerate(probas):\n",
    "        sorted_positions = np.argsort(-probs)\n",
    "        sorted_probs = probs[sorted_positions]\n",
    "        token_probs = {}\n",
    "        for k in range(top_k):\n",
    "            predicted_token = tokenizer.id_to_token(int(sorted_positions[k]))\n",
    "            prob = sorted_probs[k]\n",
    "            # print(f\"seq_id: {idx}, token_position: {pos}, k: {k}, token: {predicted_token}, probability: {prob * 100:.2f}%\")\n",
    "            token_probs[predicted_token] = prob\n",
    "        tokens_probs.append(token_probs)\n",
    "    return tokens_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs['logits'].detach().numpy()\n",
    "probabilities = []\n",
    "# get probabilities separately for each seq as they have different lengths\n",
    "for idx in range(logits.shape[0]):\n",
    "    logits_seq = logits[idx]\n",
    "    logits_seq = [logits_seq[i] for i,b in enumerate(tokens_idx[idx]) if b]\n",
    "    probs = softmax(logits_seq, axis=-1)  # use softmax to transform logits into probabilities\n",
    "    probabilities.append(probs)\n",
    "\n",
    "tokens_probs = get_token_probability(probabilities, idx=1, top_k=5)\n",
    "tokens_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
