{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57934fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
      "Adjust how often this is run with `$HOMEBREW_AUTO_UPDATE_SECS` or disable with\n",
      "`$HOMEBREW_NO_AUTO_UPDATE=1`. Hide these hints with `$HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/portable-ruby/blobs/sha256:1c98fa49eacc935640a6f8e10a2bf33f14cfc276804b71ddb658ea45ba99d167\u001b[0m\n",
      "######################################################################### 100.0%41.5%            50.3%                        57.0%              58.8%                           59.0%   59.8%          62.0%              63.1%             65.3%         66.3%  67.8%8.5%             71.2%1.9%              72.6%######                      72.8%.0% 75.1%           75.6%   76.3%                   76.6%##                   76.9%###################################                   77.1%               78.2%             78.4%###########                  78.9%%80.1% 80.1% 80.8%#######              84.5% 85.3%            85.6%###             85.7%##            86.6%   87.3%#####           89.0%       91.9% 93.0%     93.1%###############       94.4%###############    97.5%#####################    98.0%   98.7%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring portable-ruby-3.4.8.arm64_big_sur.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mUpdated Homebrew from 5.0.3 (c3790fc6e4) to 5.0.7 (d61f229fd2).\u001b[0m\n",
      "Updated 2 taps (homebrew/core and homebrew/cask).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Formulae\u001b[0m\n",
      "astra: Command-Line Interface for DataStax Astra\n",
      "bookokrat: Terminal EPUB Book Reader\n",
      "calm-cli: CLI allows you to interact with the Common Architecture Language Model (CALM)\n",
      "carl: Calendar for the command-line\n",
      "cinecli: Browse, inspect, and launch movie torrents directly from your terminal\n",
      "ctre: Compile-time PCRE-compatible regular expression matcher for C++\n",
      "depot: Build your Docker images in the cloud\n",
      "dnspyre: CLI tool for a high QPS DNS benchmark\n",
      "docker-language-server: Language server for Dockerfiles, Compose files, and Bake files\n",
      "flecs: Fast entity component system for C & C++\n",
      "garage: S3 object store so reliable you can run it outside datacenters\n",
      "git-get: Better way to clone, organize and manage multiple git repositories\n",
      "git-pages: Scalable static site server for Git forges\n",
      "git-pages-cli: Tool for publishing a site to a git-pages server\n",
      "goat: General purpose AT Protocol CLI in Go\n",
      "gup: Update binaries installed by go install\n",
      "hayagriva: Bibliography management tool\n",
      "jsonfmt: Like gofmt, but for JSON files\n",
      "khaos: Kafka traffic simulator for observability and chaos engineering\n",
      "klog: Command-line tool for time tracking in a human-readable, plain-text file format\n",
      "kubernetes-cli@1.34: Kubernetes command-line interface\n",
      "kyua: Testing framework for infrastructure software\n",
      "libevdev: Wrapper library for evdev devices\n",
      "lispkit: Scheme framework for extension and scripting languages on macOS and iOS\n",
      "macchanger: Change your mac address, for macOS\n",
      "mapscii: Whole World In Your Console\n",
      "mcp-scan: Constrain, log and scan your MCP connections for security vulnerabilities\n",
      "mistral-vibe: Minimal CLI coding agent\n",
      "mole: Deep clean and optimize your Mac\n",
      "mq: Jq-like command-line tool for markdown processing\n",
      "mufetch: Neofetch-style music cli\n",
      "neo4j-mcp: Neo4j official Model Context Protocol server for AI tools\n",
      "netshow: Interactive network connection monitor with friendly service names\n",
      "nkt: TUI for fast and simple interacting with your BibLaTeX database\n",
      "octodns: Tools for managing DNS across multiple providers\n",
      "papis: Powerful command-line document and bibliography manager\n",
      "phantom: CLI tool for seamless parallel development with Git worktrees\n",
      "pixlet: App runtime and UX toolkit for pixel-based apps\n",
      "pony-language-server: Language server for Pony\n",
      "rad: Modern CLI scripts made easy\n",
      "redu: Ncdu for your restic repository\n",
      "rmrfrs: Filesystem cleaning tool\n",
      "rockcraft: Tool to create OCI images using the language from Snapcraft and Charmcraft\n",
      "ruby@3.4: Powerful, clean, object-oriented scripting language\n",
      "slicot: Fortran subroutines library for systems and control\n",
      "snitch: Prettier way to inspect network connections\n",
      "superseedr: BitTorrent Client in your Terminal\n",
      "svu: Semantic version utility\n",
      "talm: Manage Talos Linux configurations the GitOps way\n",
      "tfclean: Remove applied moved block, import block, etc\n",
      "tree-sitter@0.25: Incremental parsing library\n",
      "tronbyt-server: Manage your apps on your Tronbyt (flashed Tidbyt) completely locally\n",
      "ty: Extremely fast Python type checker, written in Rust\n",
      "vacuum: World's fastest OpenAPI & Swagger linter\n",
      "wasm-bindgen: Facilitating high-level interactions between Wasm modules and JavaScript\n",
      "wifitui: Fast featureful friendly wifi terminal UI\n",
      "witr: Why is this running?\n",
      "wuchale: Protobuf-like i18n from plain code\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Casks\u001b[0m\n",
      "8bitdo-ultimate-software-v2: Control every piece of your controller\n",
      "airscroll: Smooth mouse scrolling utility\n",
      "aks-desktop: Azure Kubernetes Service desktop application\n",
      "alma: AI chat application\n",
      "appbox: iOS app distribution tool\n",
      "athas: Lightweight code editor\n",
      "cardinal-search: Fastest file searching tool\n",
      "comet: Web browser with integrated AI assistant\n",
      "conar: AI-powered database and data management tool\n",
      "copilot-cli: Brings the power of Copilot coding agent directly to your terminal\n",
      "copilot-cli@prerelease: Brings the power of Copilot coding agent directly to your terminal\n",
      "datadog-security-cli: Datadog Security Product CLI\n",
      "digiexam: Academic testing platform with device lockdown\n",
      "dnclient: Peer-to-peer VPN client for managed nebula networks\n",
      "elgato-studio: Capture and manage Elgato devices for content creation\n",
      "font-amarna\n",
      "font-bbh-bartle\n",
      "font-bbh-bogle\n",
      "font-bbh-hegarty\n",
      "font-cause\n",
      "font-geom\n",
      "font-guguru-sans-code\n",
      "font-guguru-sans-code-nf\n",
      "font-gveret-levin\n",
      "font-line-seed-jp\n",
      "font-sekuya\n",
      "fontra-pak: Browser-based font editor\n",
      "glkvm: App for controlling GL.iNet KVM devices\n",
      "hyperwhisper: AI-powered speech-to-text transcription\n",
      "locu: Daily planner and focus timer\n",
      "m32-edit: Remote control for Midas M32 audio consoles\n",
      "macdown-3000: Markdown editor with live preview and syntax highlighting\n",
      "mace: Simplify compliance baseline creation, auditing, and management\n",
      "maestro: AI agent command center\n",
      "maru-jan: Play japanese mahjong online\n",
      "monocle-app: Window dimming utility\n",
      "motionik: Screen recording software\n",
      "mountmate: Menubar app to easily manage external drives\n",
      "mozregression-gui: Interactive regression range finder for Firefox and other Mozilla products\n",
      "mpluginmanager: Installer for MeldaProduction audio plugins\n",
      "nanoleaf: Control your Nanoleaf lights\n",
      "nessie-app: Knowledge base from AI chats\n",
      "opencode-desktop: AI coding agent desktop client\n",
      "oracle-data-modeler: Graphical tool for data modeling tasks\n",
      "papercut-mobility-print-client: Client for printing to PaperCut Mobility Print queues\n",
      "portalbox: Share a region of your screen in video calls\n",
      "rocketman-choices-packager: Utility for customising installer package choices\n",
      "smartsheet: Spreadsheet-style project management solution\n",
      "snapmaker-orca: Slicing software for Snapmaker 3D printers, a fork of OrcaSlicer\n",
      "sourcegit: Git GUI client\n",
      "stirling-pdf: PDF utility\n",
      "stremio@beta: Open-source media center\n",
      "support: Menu bar app for user and help desk support\n",
      "swiftdialog: Admin utility that presents custom dialogs or messages from shell scripts\n",
      "typeless: AI voice dictation that turns speech into polished text\n",
      "uuremote: NetEase UU remote desktop access and control tool\n",
      "vcamapp: Face-tracking virtual avatar app\n",
      "visualdiffer: Visually compare folders and files\n",
      "vocaster-hub: Interface controller for Focusrite Vocaster One and Two\n",
      "wailbrew: Manage Homebrew packages with a UI\n",
      "wireless-workbench: Desktop app for RF coordination and wireless system management\n",
      "xmlmind-editor: Strictly validating near WYSIWYG XML editor\n",
      "yingfu-online: Education app for teens\n",
      "yoink: Drag and drop utility\n",
      "zo: Friendly personal server\n",
      "\n",
      "You have \u001b[1m46\u001b[0m outdated formulae installed.\n",
      "\n",
      "\n",
      "The 5.0.7 changelog can be found at:\n",
      "  \u001b[4mhttps://github.com/Homebrew/brew/releases/tag/5.0.7\u001b[24m\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching downloads for: \u001b[32mbedtools\u001b[39m\u001b[0m\n",
      "\u001b[?25l\u001b[K\u001b[34m‚†ã\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ã\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ô\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ô\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ö\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ö\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†û\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†û\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ñ\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ñ\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†¶\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†¶\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†¥\u001b[0m Bottle Manifest bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†¥\u001b[0m Bottle Manifest bedtools (2.31.1) ---------------- Downloading   4.1KB/-------\u001b[0G\u001b[K\u001b[34m‚†≤\u001b[0m Bottle Manifest bedtools (2.31.1) ---------------- Downloading   4.1KB/-------\u001b[0G\u001b[K\u001b[32m‚úîÔ∏é\u001b[0m Bottle Manifest bedtools (2.31.1)                  Downloaded   13.6KB/ 13.6KB\n",
      "\u001b[?25h\u001b[?25l\u001b[K\u001b[34m‚†≥\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†≥\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ì\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ì\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ã\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ã\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ô\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ô\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ö\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ö\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†û\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†û\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ñ\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ñ\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†¶\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†¶\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†¥\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†¥\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†≤\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†≤\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†≥\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†≥\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ì\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ì\u001b[0m Bottle bedtools (2.31.1)\u001b[0G\u001b[K\u001b[34m‚†ã\u001b[0m Bottle bedtools (2.31.1) ------------------------- Downloading  12.3KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ã\u001b[0m Bottle bedtools (2.31.1) #------------------------ Downloading  24.6KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ô\u001b[0m Bottle bedtools (2.31.1) #------------------------ Downloading  24.6KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ô\u001b[0m Bottle bedtools (2.31.1) #------------------------ Downloading  24.6KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ö\u001b[0m Bottle bedtools (2.31.1) #------------------------ Downloading  24.6KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ö\u001b[0m Bottle bedtools (2.31.1) ###---------------------- Downloading  77.8KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†û\u001b[0m Bottle bedtools (2.31.1) ####--------------------- Downloading  98.3KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†û\u001b[0m Bottle bedtools (2.31.1) #####-------------------- Downloading 131.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ñ\u001b[0m Bottle bedtools (2.31.1) #####-------------------- Downloading 131.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ñ\u001b[0m Bottle bedtools (2.31.1) #####-------------------- Downloading 131.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¶\u001b[0m Bottle bedtools (2.31.1) #####-------------------- Downloading 139.3KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¶\u001b[0m Bottle bedtools (2.31.1) #####-------------------- Downloading 139.3KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¥\u001b[0m Bottle bedtools (2.31.1) #####-------------------- Downloading 139.3KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¥\u001b[0m Bottle bedtools (2.31.1) ########----------------- Downloading 196.6KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≤\u001b[0m Bottle bedtools (2.31.1) ########----------------- Downloading 208.9KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≤\u001b[0m Bottle bedtools (2.31.1) #########---------------- Downloading 217.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≥\u001b[0m Bottle bedtools (2.31.1) #########---------------- Downloading 217.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≥\u001b[0m Bottle bedtools (2.31.1) #########---------------- Downloading 217.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ì\u001b[0m Bottle bedtools (2.31.1) #########---------------- Downloading 237.6KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ì\u001b[0m Bottle bedtools (2.31.1) #########---------------- Downloading 237.6KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ã\u001b[0m Bottle bedtools (2.31.1) ##########--------------- Downloading 258.0KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ã\u001b[0m Bottle bedtools (2.31.1) ##########--------------- Downloading 258.0KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ô\u001b[0m Bottle bedtools (2.31.1) ##########--------------- Downloading 262.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ô\u001b[0m Bottle bedtools (2.31.1) ###########-------------- Downloading 270.3KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ö\u001b[0m Bottle bedtools (2.31.1) ###########-------------- Downloading 278.5KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ö\u001b[0m Bottle bedtools (2.31.1) ###########-------------- Downloading 278.5KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†û\u001b[0m Bottle bedtools (2.31.1) ###########-------------- Downloading 278.5KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†û\u001b[0m Bottle bedtools (2.31.1) ############------------- Downloading 303.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ñ\u001b[0m Bottle bedtools (2.31.1) ############------------- Downloading 307.2KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ñ\u001b[0m Bottle bedtools (2.31.1) ############------------- Downloading 307.2KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¶\u001b[0m Bottle bedtools (2.31.1) #############------------ Downloading 319.5KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¶\u001b[0m Bottle bedtools (2.31.1) #############------------ Downloading 319.5KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¥\u001b[0m Bottle bedtools (2.31.1) #############------------ Downloading 331.8KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¥\u001b[0m Bottle bedtools (2.31.1) #############------------ Downloading 335.9KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≤\u001b[0m Bottle bedtools (2.31.1) ##############----------- Downloading 344.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≤\u001b[0m Bottle bedtools (2.31.1) ##############----------- Downloading 352.3KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≥\u001b[0m Bottle bedtools (2.31.1) ##############----------- Downloading 360.4KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≥\u001b[0m Bottle bedtools (2.31.1) ###############---------- Downloading 368.6KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ì\u001b[0m Bottle bedtools (2.31.1) ###############---------- Downloading 372.7KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ì\u001b[0m Bottle bedtools (2.31.1) ###############---------- Downloading 372.7KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ã\u001b[0m Bottle bedtools (2.31.1) ###############---------- Downloading 385.0KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ã\u001b[0m Bottle bedtools (2.31.1) ################--------- Downloading 393.2KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ô\u001b[0m Bottle bedtools (2.31.1) ################--------- Downloading 401.4KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ô\u001b[0m Bottle bedtools (2.31.1) ################--------- Downloading 405.5KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ö\u001b[0m Bottle bedtools (2.31.1) ################--------- Downloading 417.8KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ö\u001b[0m Bottle bedtools (2.31.1) #################-------- Downloading 426.0KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†û\u001b[0m Bottle bedtools (2.31.1) #################-------- Downloading 434.2KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†û\u001b[0m Bottle bedtools (2.31.1) ##################------- Downloading 446.5KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ñ\u001b[0m Bottle bedtools (2.31.1) ##################------- Downloading 446.5KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ñ\u001b[0m Bottle bedtools (2.31.1) ##################------- Downloading 462.8KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¶\u001b[0m Bottle bedtools (2.31.1) ##################------- Downloading 462.8KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¶\u001b[0m Bottle bedtools (2.31.1) ###################------ Downloading 471.0KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¥\u001b[0m Bottle bedtools (2.31.1) ###################------ Downloading 483.3KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¥\u001b[0m Bottle bedtools (2.31.1) ###################------ Downloading 491.5KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≤\u001b[0m Bottle bedtools (2.31.1) ####################----- Downloading 499.7KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≤\u001b[0m Bottle bedtools (2.31.1) ####################----- Downloading 503.8KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≥\u001b[0m Bottle bedtools (2.31.1) ####################----- Downloading 503.8KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≥\u001b[0m Bottle bedtools (2.31.1) #####################---- Downloading 528.4KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ì\u001b[0m Bottle bedtools (2.31.1) #####################---- Downloading 528.4KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ì\u001b[0m Bottle bedtools (2.31.1) #####################---- Downloading 540.7KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ã\u001b[0m Bottle bedtools (2.31.1) #####################---- Downloading 540.7KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ã\u001b[0m Bottle bedtools (2.31.1) ######################--- Downloading 548.9KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ô\u001b[0m Bottle bedtools (2.31.1) ######################--- Downloading 561.2KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ô\u001b[0m Bottle bedtools (2.31.1) ######################--- Downloading 569.3KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ö\u001b[0m Bottle bedtools (2.31.1) ######################--- Downloading 569.3KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ö\u001b[0m Bottle bedtools (2.31.1) ######################--- Downloading 569.3KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†û\u001b[0m Bottle bedtools (2.31.1) #######################-- Downloading 585.7KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†û\u001b[0m Bottle bedtools (2.31.1) #######################-- Downloading 585.7KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ñ\u001b[0m Bottle bedtools (2.31.1) ########################- Downloading 602.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†ñ\u001b[0m Bottle bedtools (2.31.1) ########################- Downloading 602.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¶\u001b[0m Bottle bedtools (2.31.1) ########################- Downloading 602.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¶\u001b[0m Bottle bedtools (2.31.1) ########################- Downloading 602.1KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¥\u001b[0m Bottle bedtools (2.31.1) ########################- Downloading 610.3KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†¥\u001b[0m Bottle bedtools (2.31.1) ######################### Downloading 622.6KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≤\u001b[0m Bottle bedtools (2.31.1) ######################### Downloading 622.6KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≤\u001b[0m Bottle bedtools (2.31.1) ######################### Downloading 626.7KB/633.1KB\u001b[0G\u001b[K\u001b[34m‚†≥\u001b[0m Bottle bedtools (2.31.1)                           Extracting  633.1KB/633.1KB\u001b[0G\u001b[K\u001b[32m‚úîÔ∏é\u001b[0m Bottle bedtools (2.31.1)                           Downloaded  633.1KB/633.1KB\n",
      "\u001b[?25h\u001b[34m==>\u001b[0m \u001b[1mPouring bedtools--2.31.1.arm64_tahoe.bottle.tar.gz\u001b[0m\n",
      "üç∫  /opt/homebrew/Cellar/bedtools/2.31.1: 43 files, 1.7MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup bedtools`...\u001b[0m\n",
      "Disable this behaviour by setting `HOMEBREW_NO_INSTALL_CLEANUP=1`.\n",
      "Hide these hints with `HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n",
      "\u001b[32m==>\u001b[0m \u001b[1m`brew cleanup` has not been run in the last 30 days, running now...\u001b[0m\n",
      "Disable this behaviour by setting `HOMEBREW_NO_INSTALL_CLEANUP=1`.\n",
      "Hide these hints with `HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n",
      "Removing: /Users/forrest/Library/Caches/Homebrew/portable-ruby-3.4.7.arm64_big_sur.bottle.tar.gz... (12.2MB)\n",
      "Removing: /Users/forrest/Library/Caches/Homebrew/bootsnap/e480d26faefcc0649aee4a1b4bb5df1bf6ea887486b0f1485789bf3d26f5e27a... (650 files, 5.6MB)\n",
      "Removing: /Users/forrest/Library/Caches/Homebrew/bootsnap/42e939983ed75547f42207cad9f1e0fde134291f63f94bcb8df8abbd25416d42... (635 files, 5.5MB)\n",
      "Removing: /Users/forrest/Library/Logs/Homebrew/ca-certificates... (64B)\n",
      "\u001b[2mUsing Python 3.12.8 environment at: /Users/forrest/GitHub/DNALLM/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 984ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpybedtools\u001b[0m\u001b[2m==0.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyfastx\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpysam\u001b[0m\u001b[2m==0.23.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# First install two dependencies for generating NER datasets, and make sure you have installed bedtools in your system\n",
    "# please make sure you have installed bedtools in your system\n",
    "# !conda install -c bioconda bedtools\n",
    "# or in macos, you can use brew install bedtools\n",
    "!brew install bedtools\n",
    "\n",
    "# Omit it if you have already installed these two packages\n",
    "!uv pip install pyfastx pybedtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de084b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/forrest/miniconda3/bin/intersectBed\n"
     ]
    }
   ],
   "source": [
    "# verify  intersectBed\n",
    "!which intersectBed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b77ac004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import random\n",
    "import numpy as np\n",
    "from pyfastx import Fasta\n",
    "from pybedtools import BedTool\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from dnallm import load_config, load_model_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d435cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "847621be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set minimum and maximum extend length around a gene\n",
    "min_ext = 50\n",
    "max_ext = 100\n",
    "ext_list = [[random.randint(min_ext, max_ext), random.randint(min_ext, max_ext)] for x in range(60000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b5d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Named Entity Recognition (NER) tags and corresponding id\n",
    "# NER includes IO, IOB, IOE, IOBES, BI, IE and BIES schemes, here we use IOB scheme\n",
    "# Example:\n",
    "# ..........[ exon1 ]-----[ exon2 ]-------[ exon3 ]........\n",
    "# 000000000012222222234444122222222344444412222222200000000\n",
    "named_entities = {\n",
    "    'intergenic': 'O',\n",
    "    'exon0': 'B-EXON',\n",
    "    'exon1': 'I-EXON',\n",
    "    'intron0': 'B-INTRON',\n",
    "    'intron1': 'I-INTRON',\n",
    "}\n",
    "tags_id = {\n",
    "    'O': 0,\n",
    "    'B-EXON': 1,\n",
    "    'I-EXON': 2,\n",
    "    'B-INTRON': 3,\n",
    "    'I-INTRON': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "409379cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_annotation(gene_anno):\n",
    "    cnt = 0\n",
    "    gene_info = {}\n",
    "    for gene in gene_anno:\n",
    "        gene_info[gene] = []\n",
    "        chrom = gene_anno[gene][\"chrom\"]\n",
    "        start = gene_anno[gene][\"start\"]\n",
    "        end = gene_anno[gene][\"end\"]\n",
    "        strand = gene_anno[gene][\"strand\"]\n",
    "        isoforms = gene_anno[gene][\"isoform\"]\n",
    "        # Get representative isoformÔºàlongestÔºâ\n",
    "        if not isoforms:\n",
    "            continue\n",
    "        lso_lens = [(iso, sum([(x[2]-x[1]) for x in isoforms[iso]])) for iso in isoforms]\n",
    "        representative = sorted(lso_lens, key=lambda x:x[1])[-1][0]\n",
    "        isoform_info = isoforms[representative]\n",
    "        iso_start = min([x[1] for x in isoform_info])\n",
    "        iso_end = max([x[2] for x in isoform_info])\n",
    "\n",
    "        if iso_start == start and iso_end == end:\n",
    "            is_reverse = False if strand == \"+\" else True\n",
    "            # Get intron annotation\n",
    "            last = 0\n",
    "            for region in sorted(isoform_info, key=lambda x:x[1], reverse=is_reverse):\n",
    "                if strand == \"+\":\n",
    "                    if last:\n",
    "                        intron = [chrom, last, region[1], \"intron\", strand]\n",
    "                        if intron[1] < intron[2]:\n",
    "                            gene_info[gene].append(intron)\n",
    "                    last = region[2]\n",
    "                else:\n",
    "                    if last:\n",
    "                        intron = [chrom, region[2], last, \"intron\", strand]\n",
    "                        if intron[1] < intron[2]:\n",
    "                            gene_info[gene].append(intron)\n",
    "                    last = region[1]\n",
    "                gene_info[gene].append([chrom, region[1], region[2], region[0], strand])\n",
    "        cnt += 1\n",
    "\n",
    "    return gene_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddaa7d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(genome, gene_anno, gene_info, tokenizer, outfile, ext_list, sampling=1e7):\n",
    "    \"\"\"\n",
    "    For each gene in `gene_anno`, extract the annotated exonic (and flanking) DNA subsequences,\n",
    "    run the tokenizer once per subsequence with offset_mapping enabled, \n",
    "    and record the (genomic_start, genomic_end, token) tuples for all non-special tokens.\n",
    "\n",
    "    - genome: dict mapping chromosome ‚Üí SeqRecord (so that genome[chrom][start:end].seq gives a Seq)\n",
    "    - gene_anno: dict mapping gene_name ‚Üí { \"chrom\": str, \"strand\": \"+\" or \"-\", ... }\n",
    "    - gene_info: dict mapping gene_name ‚Üí list of (feature_id, exon_start, exon_end) or similar\n",
    "    - tokenizer: a HuggingFace‚Äêstyle tokenizer that supports return_offsets_mapping\n",
    "    - outfile: (unused here, but you can write token_pos to it later)\n",
    "    - ext_list: list of (left_extension, right_extension) tuples parallel to gene_anno order\n",
    "    - sampling: random sampling the given number of genes for tokenization\n",
    "    \"\"\"\n",
    "    # 1) Precompute special-tokens set for O(1) membership checks\n",
    "    sp_tokens = set(tokenizer.special_tokens_map.values())\n",
    "\n",
    "    token_pos = {}\n",
    "    # Since gene_anno is likely a dict, we need a stable way of iterating + indexing ext_list.\n",
    "    # We'll assume ext_list[i] corresponds to the i-th gene in `list(gene_anno.keys())`.\n",
    "    gene_list = list(gene_anno.keys())\n",
    "    if len(gene_list) > sampling:\n",
    "        gene_list = random.sample(gene_list, int(sampling))\n",
    "\n",
    "    for num, gene in enumerate(tqdm(gene_list, desc=\"Genes\")):\n",
    "        chrom = gene_anno[gene][\"chrom\"]\n",
    "        strand = gene_anno[gene][\"strand\"]\n",
    "\n",
    "        # Skip genes not in gene_info or with empty annotation\n",
    "        if gene not in gene_info or not gene_info[gene]:\n",
    "            continue\n",
    "\n",
    "        # Determine exon‚Äêrange and extended boundaries\n",
    "        exon_coords = gene_info[gene]\n",
    "        # start = minimum exon_start; end = maximum exon_end\n",
    "        start = min(exon[1] for exon in exon_coords)\n",
    "        end   = max(exon[2] for exon in exon_coords)\n",
    "\n",
    "        left_ext, right_ext = ext_list[num]\n",
    "        ext_start = max(0, start - left_ext)\n",
    "        ext_end   = end + right_ext\n",
    "\n",
    "        # Shortcut: grab the full chromosome record once\n",
    "        chrom_record = genome[chrom]\n",
    "\n",
    "        # Build a list of (genomic_anchor, seq_string) for \"+\" or \"-\" strand\n",
    "        seqinfo = []\n",
    "        if strand == \"+\":\n",
    "            #  1) upstream flank\n",
    "            try:\n",
    "                upstream_seq = chrom_record[ext_start:start].seq\n",
    "            except Exception:\n",
    "                # If slicing fails, log and skip\n",
    "                print(f\"ERROR: {chrom}\\t{ext_start}\\t{start}\")\n",
    "                upstream_seq = \"\"\n",
    "            seqinfo.append((ext_start, str(upstream_seq)))\n",
    "\n",
    "            #  2) each exon\n",
    "            for feature in exon_coords:\n",
    "                exon_start = feature[1]\n",
    "                exon_end   = feature[2]\n",
    "                if exon_start >= exon_end:\n",
    "                    continue\n",
    "                seq = chrom_record[exon_start:exon_end].seq\n",
    "                seqinfo.append((exon_start, str(seq)))\n",
    "\n",
    "            #  3) downstream flank\n",
    "            downstream_seq = chrom_record[end:ext_end].seq\n",
    "            seqinfo.append((end, str(downstream_seq)))\n",
    "\n",
    "        else:  # strand == \"-\"\n",
    "            # On the reverse‚Äêstrand, we want the reverse complement (\"antisense\").\n",
    "            # Note: .antisense == .reverse_complement() for most SeqRecord slicing.\n",
    "            # We still record the genomic anchor as if it were the left index on the + strand.\n",
    "            # But because the sequence is reversed, offset_mapping will need to be mapped differently.\n",
    "\n",
    "            #  1) ‚Äúupstream‚Äù on reverse strand = (end ‚Üí ext_end) in forward coords, but take antisense\n",
    "            try:\n",
    "                flank_seq = chrom_record[end:ext_end].antisense\n",
    "            except Exception:\n",
    "                print(f\"ERROR (rev): {chrom}\\t{end}\\t{ext_end}\")\n",
    "                flank_seq = \"\"\n",
    "            seqinfo.append((ext_end, str(flank_seq)))\n",
    "\n",
    "            #  2) each exon (reverse‚Äêcomplement)\n",
    "            for feature in exon_coords:\n",
    "                exon_start = feature[1]\n",
    "                exon_end   = feature[2]\n",
    "                if exon_start >= exon_end:\n",
    "                    continue\n",
    "                seq = chrom_record[exon_start:exon_end].antisense\n",
    "                # For mapping, we‚Äôll anchor each token by the 5‚Ä≤-most position on the minus strand,\n",
    "                # but because the sequence is reversed, the ‚Äúfirst character‚Äù of seq actually corresponds\n",
    "                # to genomic position = exon_end - 1 in forward coordinates, and the ‚Äúlast character‚Äù ‚Ü¶ exon_start.\n",
    "                seqinfo.append((exon_end, str(seq)))\n",
    "\n",
    "            #  3) downstream on reverse strand = (ext_start ‚Üí start) in forward coords, but antisense\n",
    "            flank_seq = chrom_record[ext_start:start].antisense\n",
    "            seqinfo.append((start, str(flank_seq)))\n",
    "\n",
    "        # Initialize the list for this gene\n",
    "        token_pos[gene] = []\n",
    "\n",
    "        # For each (anchor, raw_seq), run a single tokenizer(...) call\n",
    "        for anchor, raw_seq in seqinfo:\n",
    "            if not raw_seq:\n",
    "                continue\n",
    "\n",
    "            # 1) Tokenize with offsets (add_special_tokens=False so we skip [CLS], [SEP], etc.)\n",
    "            #    ‚Äúoffset_mapping‚Äù is a list of (char_start, char_end) for each token in raw_seq.\n",
    "            # encoding = tokenizer(\n",
    "            #     raw_seq,\n",
    "            #     return_offsets_mapping=True,\n",
    "            #     add_special_tokens=False\n",
    "            # )\n",
    "            # offsets = encoding[\"offset_mapping\"]\n",
    "            # token_ids = encoding[\"input_ids\"]\n",
    "            token_ids = tokenizer.encode(raw_seq, add_special_tokens=False)\n",
    "            tok_strs = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "            offsets = []\n",
    "            cursor  = 0\n",
    "            for tok in tok_strs:\n",
    "                char_start = cursor\n",
    "                char_end   = cursor + len(tok)\n",
    "                offsets.append((char_start, char_end))\n",
    "                cursor = char_end\n",
    "            if len(offsets) != len(token_ids):\n",
    "                # This should never happen in a well‚Äêformed tokenizer, but just in case:\n",
    "                raise RuntimeError(\"Offset mapping length ‚â† token_ids length\")\n",
    "\n",
    "            # 2) Iterate through each token + offset, skip special tokens, then map back to genome coords\n",
    "            for idx, (token_id, (char_start, char_end)) in enumerate(zip(token_ids, offsets)):\n",
    "                token_str = tokenizer.convert_ids_to_tokens(token_id)\n",
    "\n",
    "                # Skip if it‚Äôs one of the special tokens (‚Äú[PAD]‚Äù, ‚Äú[CLS]‚Äù, etc.)\n",
    "                if token_str in sp_tokens:\n",
    "                    continue\n",
    "\n",
    "                if strand == \"+\":\n",
    "                    # On the forward strand, raw_seq[0] ‚Ü¶ genomic position ‚Äúanchor‚Äù.\n",
    "                    # So any token covering raw_seq[char_start:char_end] ‚Ü¶ genome positions [anchor+char_start : anchor+char_end]\n",
    "                    g_start = anchor + char_start\n",
    "                    g_end   = anchor + char_end\n",
    "\n",
    "                else:\n",
    "                    # On the reverse strand, raw_seq was already antisense (reverse), and ‚Äúanchor‚Äù is the forward‚Äêstrand coordinate\n",
    "                    # of the first character in raw_seq.  That first character of raw_seq is actually genome position (anchor-1),\n",
    "                    # and the last character of raw_seq is genome position (anchor - len(raw_seq)).\n",
    "                    # More generally, for raw_seq index i, the corresponding forward‚Äêstrand position is:\n",
    "                    #     g_pos = anchor - 1 - i\n",
    "                    #\n",
    "                    # Thus, if the token covers raw_seq[char_start:char_end] (i.e. from i = char_start to i = char_end-1),\n",
    "                    # its genomic coordinates (inclusive‚Äêexclusive) on the forward strand are:\n",
    "                    #   g_end = (anchor - 1 - char_start) + 1  = anchor - char_start\n",
    "                    #   g_start = (anchor - 1 - (char_end - 1))  = anchor - char_end\n",
    "                    #\n",
    "                    # We want to store them as [g_start, g_end] with g_start < g_end.  So:\n",
    "                    g_start = anchor - char_end\n",
    "                    g_end   = anchor - char_start\n",
    "\n",
    "                token_pos[gene].append([g_start, g_end, token_str])\n",
    "\n",
    "    # save sequences and tokens\n",
    "    with open(outfile, \"w\") as outf:\n",
    "        for gene in tqdm(token_pos, desc=\"Save token positions\"):\n",
    "            chrom = gene_anno[gene][\"chrom\"]\n",
    "            strand = gene_anno[gene][\"strand\"]\n",
    "            for token in token_pos[gene]:\n",
    "                print(chrom, token[0], token[1], token[2], gene, strand, sep=\"\\t\", file=outf)\n",
    "\n",
    "    return token_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01235176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_nerdata(tokens_bed, annotation_bed, outfile, named_entities, tags_id):\n",
    "    \"\"\"\n",
    "    Build a token‚Äêlevel NER dataset by intersecting `tokens_bed` with `annotation_bed`.\n",
    "    Returns a dict: { 'id': [...geneIDs...], 'sequence': [[token1, token2, ‚Ä¶], ‚Ä¶],\n",
    "                     'labels': [[label1, label2, ‚Ä¶], ‚Ä¶] } \n",
    "    and also writes two files:\n",
    "      1) ‚Äúoutfile‚Äù as a pickle of ner_info,\n",
    "      2) ‚Äú<outfile>.token_sizes‚Äù containing ‚Äúgene<TAB>token_count‚Äù for each gene.\n",
    "    \"\"\"\n",
    "\n",
    "    ne = named_entities\n",
    "    # Build a map from ‚ÄúbaseName + '0' ‚Üí named_entities[...] ‚Üí tags_id[...]‚Äù\n",
    "    zero_map = {}\n",
    "    one_map  = {}\n",
    "    for base_name, ner_label in ne.items():\n",
    "        # ‚Äúintergenic‚Äù maps to 'O' no matter whether we‚Äôre at a ‚Äústart‚Äù or ‚Äúinside‚Äù ‚Äî\n",
    "        # so we do it for both 'intergenic0' and 'intergenic1'.\n",
    "        if base_name == \"intergenic\":\n",
    "            zero_map[\"intergenic0\"] = ner_label\n",
    "            one_map[\"intergenic1\"] = ner_label\n",
    "            continue\n",
    "\n",
    "        # base_name will be something like ‚Äúexon0‚Äù or ‚Äúexon1‚Äù, ‚Äúintron0‚Äù, ‚Äúintron1‚Äù\n",
    "        # We want to know, whenever the token‚Äôs name is exactly ‚Äúexon‚Äù and we‚Äôre at a ‚Äústart‚Äù boundary,\n",
    "        # pick the B-EXON label.  If the name is ‚Äúexon‚Äù but it matched the previous gene-level ‚Äúname‚Äù,\n",
    "        # then we call named_entities[\"exon1\"] to get ‚ÄúI-EXON‚Äù.\n",
    "        if base_name.endswith(\"0\"):\n",
    "            zero_map[base_name] = ner_label\n",
    "        else:\n",
    "            one_map[base_name]  = ner_label\n",
    "\n",
    "    # 2) Perform the intersection once (Loj = ‚Äúleft outer join‚Äù) so we keep every token\n",
    "    intersection = BedTool(tokens_bed).intersect(annotation_bed, loj=True)\n",
    "\n",
    "    # 3) Prepare our output containers\n",
    "    ner_info = {\n",
    "        \"id\":       [],  # list of gene IDs (in the same order as we append)\n",
    "        \"sequence\": [],  # each element is a list-of-strings (tokens)\n",
    "        \"labels\":   []   # each element is a list-of-ints (NER tags)\n",
    "    }\n",
    "\n",
    "    # We'll accumulate (gene, token_count) pairs in-memory, then write them in bulk\n",
    "    sizes_buffer = []\n",
    "\n",
    "    # 4) Use defaultdict(set) to track ‚Äúwhich token‚ÄêIDs we‚Äôve already seen for each gene‚Äù\n",
    "    token_seen = defaultdict(set)\n",
    "\n",
    "    current_gene = None\n",
    "    tokens_list  = []\n",
    "    labels_list  = []\n",
    "    last_name    = None  # to know if ‚Äúname == last_name‚Äù (inside vs start)\n",
    "\n",
    "    # 5) Iterate through every interval from the intersection\n",
    "    #    We rely on the fact that BedTool.intersect(...) returns results in ascending\n",
    "    #    genomic order, and within each gene, that will appear ‚Äúin order of token positions.‚Äù\n",
    "    for iv in intersection:\n",
    "        # Instead of ‚Äústr(iv).split('\\t')‚Äù, do:\n",
    "        chrom   = iv.chrom\n",
    "        start   = iv.start   # integer\n",
    "        end     = iv.end     # integer\n",
    "        token   = iv.name    # 4th column of tokens_bed\n",
    "        gene    = iv.fields[4]   # 5th column of tokens_bed (original gene ID)\n",
    "        gene2   = iv.fields[9]   # 10th field (unused here, but was in your code)\n",
    "        name    = iv.fields[10]  # 11th field = the annotation ‚Äúname‚Äù\n",
    "        # Build a unique‚ÄêID for this token instance\n",
    "        token_id = (start, end)\n",
    "\n",
    "        # 6) When we see a new gene (i.e. ‚Äúgene != current_gene‚Äù), we flush the previous gene‚Äôs data\n",
    "        if gene != current_gene:\n",
    "            # flush old gene if it exists\n",
    "            if current_gene is not None:\n",
    "                # Only append if we actually collected ‚â•1 token for current_gene\n",
    "                if tokens_list:\n",
    "                    sizes_buffer.append((current_gene, len(tokens_list)))\n",
    "                    ner_info[\"id\"].append(current_gene)\n",
    "                    ner_info[\"sequence\"].append(tokens_list)\n",
    "                    ner_info[\"labels\"].append(labels_list)\n",
    "                    count = len(ner_info[\"id\"])\n",
    "                    if count % 100 == 0:\n",
    "                        print(count)\n",
    "            # Reset for the new gene\n",
    "            current_gene = gene\n",
    "            tokens_list  = []\n",
    "            labels_list  = []\n",
    "            last_name    = None\n",
    "\n",
    "        # 7) If we‚Äôve already seen this exact (start,end) ‚Äútoken_id‚Äù for this gene, skip\n",
    "        if token_id in token_seen[gene]:\n",
    "            continue\n",
    "        token_seen[gene].add(token_id)\n",
    "\n",
    "        # 8) Determine the correct NER‚Äêtag (integer) for this token\n",
    "        #    - If name == \"-1\" ‚Üí treat as ‚Äúintergenic‚Äù\n",
    "        #    - If name == last_name ‚Üí we pick ‚Äúinside‚Äù (use one_map[name + \"1\"])\n",
    "        #    - else ‚Üí we pick ‚Äústart‚Äù   (use zero_map[name + \"0\"])\n",
    "        if name == \"-1\":\n",
    "            base_name = \"intergenic\"\n",
    "            ner_label = ne[base_name]          # always ‚ÄúO‚Äù\n",
    "        else:\n",
    "            # If it matched the previous token‚Äôs annotation name, choose inside\n",
    "            if name == last_name:\n",
    "                lookup_key = name + \"1\"       # e.g. ‚Äúexon1‚Äù ‚Üí I-EXON\n",
    "                ner_label  = one_map.get(lookup_key)\n",
    "                # If somehow it‚Äôs missing, fall back to ‚Äústart‚Äù logic\n",
    "                if ner_label is None:\n",
    "                    ner_label = zero_map[name + \"0\"]\n",
    "            else:\n",
    "                # new annotation segment ‚Üí start\n",
    "                lookup_key = name + \"0\"       # e.g. ‚Äúexon0‚Äù ‚Üí B-EXON\n",
    "                ner_label  = zero_map.get(lookup_key)\n",
    "                # If it‚Äôs missing, fall back to ‚Äúintergenic‚Äù\n",
    "                if ner_label is None:\n",
    "                    ner_label = ne[\"intergenic\"]\n",
    "\n",
    "        ner_tag = tags_id[ner_label]\n",
    "        last_name = name\n",
    "\n",
    "        # 9) Append the token string + numeric label\n",
    "        tokens_list.append(token)\n",
    "        labels_list.append(ner_tag)\n",
    "\n",
    "    # 10) Don‚Äôt forget to flush the final gene once the loop ends\n",
    "    if current_gene is not None and tokens_list:\n",
    "        sizes_buffer.append((current_gene, len(tokens_list)))\n",
    "        ner_info[\"id\"].append(current_gene)\n",
    "        ner_info[\"sequence\"].append(tokens_list)\n",
    "        ner_info[\"labels\"].append(labels_list)\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "    # 11) Write out the token_sizes file in one go\n",
    "    sizes_file = outfile.rsplit(\".\", 1)[0] + \".token_sizes\"\n",
    "    with open(sizes_file, \"w\") as tsf:\n",
    "        for gene_name, count in sizes_buffer:\n",
    "            tsf.write(f\"{gene_name}\\t{count}\\n\")\n",
    "\n",
    "    # 12) Finally, pickle‚Äêdump ner_info\n",
    "    with open(outfile, \"wb\") as handle:\n",
    "        pickle.dump(ner_info, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return ner_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72a7ba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-28 14:42:11--  https://rice.uga.edu/osa1r7_download/osa1_r7.asm.fa.gz\n",
      "Resolving rice.uga.edu (rice.uga.edu)... 128.192.162.131\n",
      "Connecting to rice.uga.edu (rice.uga.edu)|128.192.162.131|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 115858754 (110M) [application/x-gzip]\n",
      "Saving to: ‚Äòosa1_r7.asm.fa.gz‚Äô\n",
      "\n",
      "osa1_r7.asm.fa.gz   100%[===================>] 110.49M   290KB/s    in 3m 0s   \n",
      "\n",
      "2025-12-28 14:45:13 (629 KB/s) - ‚Äòosa1_r7.asm.fa.gz‚Äô saved [115858754/115858754]\n",
      "\n",
      "--2025-12-28 14:45:13--  https://rice.uga.edu/osa1r7_download/osa1_r7.all_models.gff3.gz\n",
      "Resolving rice.uga.edu (rice.uga.edu)... 128.192.162.131\n",
      "Connecting to rice.uga.edu (rice.uga.edu)|128.192.162.131|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8901387 (8.5M) [application/x-gzip]\n",
      "Saving to: ‚Äòosa1_r7.all_models.gff3.gz‚Äô\n",
      "\n",
      "osa1_r7.all_models. 100%[===================>]   8.49M  1.49MB/s    in 10s     \n",
      "\n",
      "2025-12-28 14:45:25 (850 KB/s) - ‚Äòosa1_r7.all_models.gff3.gz‚Äô saved [8901387/8901387]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download genome and gene annotation (make sure you have wget command in your path)\n",
    "!wget -c https://rice.uga.edu/osa1r7_download/osa1_r7.asm.fa.gz\n",
    "!wget -c https://rice.uga.edu/osa1r7_download/osa1_r7.all_models.gff3.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53f6bceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "813791it [00:01, 467221.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load genome sequence\n",
    "genome_file = \"osa1_r7.asm.fa.gz\"\n",
    "genome = Fasta(genome_file)\n",
    "# Load annotation\n",
    "gene_anno = {}\n",
    "with gzip.open(\"osa1_r7.all_models.gff3.gz\", \"rt\") as infile:\n",
    "    for line in tqdm(infile):\n",
    "        if line.startswith(\"#\") or line.startswith(\"\\n\"):\n",
    "            continue\n",
    "        info = line.strip().split(\"\\t\")\n",
    "        chrom = info[0]\n",
    "        datatype = info[2]\n",
    "        start = int(info[3]) - 1\n",
    "        end = int(info[4])\n",
    "        strand = info[6]\n",
    "        description = info[8].split(\";\")\n",
    "        if datatype == \"gene\":\n",
    "            for item in description:\n",
    "                if item.startswith(\"Name=\"):\n",
    "                    gene = item[5:]\n",
    "            if gene not in gene_anno:\n",
    "                gene_anno[gene] = {}\n",
    "                gene_anno[gene][\"chrom\"] = chrom\n",
    "                gene_anno[gene][\"start\"] = start\n",
    "                gene_anno[gene][\"end\"] = end\n",
    "                gene_anno[gene][\"strand\"] = strand\n",
    "                gene_anno[gene][\"isoform\"] = {}\n",
    "        elif datatype in [\"exon\"]:\n",
    "            for item in description:\n",
    "                if item.startswith(\"Parent=\"):\n",
    "                    isoform = item[7:].split(',')[0]\n",
    "            if isoform not in gene_anno[gene][\"isoform\"]:\n",
    "                gene_anno[gene][\"isoform\"][isoform] = []\n",
    "            gene_anno[gene][\"isoform\"][isoform].append([datatype, start, end])\n",
    "\n",
    "# Get full gene annotation information and save\n",
    "gene_info = get_gene_annotation(gene_anno)\n",
    "annotation_bed = \"rice_annotation.bed\"\n",
    "with open(annotation_bed, \"w\") as outf:\n",
    "    for gene in sorted(gene_anno, key=lambda x: (gene_anno[x][\"chrom\"], gene_anno[x][\"start\"])):\n",
    "        chrom = gene_anno[gene][\"chrom\"]\n",
    "        strand = gene_anno[gene][\"strand\"]\n",
    "        if strand == \"+\":\n",
    "            for item in gene_info[gene]:\n",
    "                print(item[0], item[1], item[2], gene, item[3], item[4], sep=\"\\t\", file=outf)\n",
    "        else:\n",
    "            for item in gene_info[gene][::-1]:\n",
    "                print(item[0], item[1], item[2], gene, item[3], item[4], sep=\"\\t\", file=outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99c6686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-6mer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-28 14:45:31,636 - modelscope - INFO - Got 8 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c990911acde2400c864350162ad1dee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 8 items:   0%|          | 0.00/8.00 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2f373293924ef980d49faac105ee39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7e82dba7774d2da91e2ef3b15beb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/910 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04b03540c2440948900cc4dc90b90f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/1.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16ee731bd7f44e4b2a722b065d907e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/2.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dced468f2d1d45d28de3f251efa00ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [generation_config.json]:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c25fa3ca0f4befa0dc49ffc47398a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [special_tokens_map.json]:   0%|          | 0.00/581 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8e74274f0c422398427a845d45a1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.txt]:   0%|          | 0.00/28.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809135f5d9c64a0ab7d702741543ac89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors]:   0%|          | 0.00/340M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-28 14:46:04,983 - modelscope - INFO - Download model 'zhangtaolab/plant-dnagpt-6mer' successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:46:04 - dnallm.utils.support - INFO - Model files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-6mer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-6mer and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load configs, model and tokenizer\n",
    "configs = load_config(\"./ner_task_config.yaml\")\n",
    "model_name = \"zhangtaolab/plant-dnagpt-6mer\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ca8ab95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Performing sequence tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Genes:   0%|          | 5/2000 [00:00<00:44, 45.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Genes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:30<00:00, 64.84it/s] \n",
      "Save token positions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1926/1926 [00:01<00:00, 1243.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize gene sequences and generate NER dataset format\n",
    "print(\"# Performing sequence tokenization...\")\n",
    "tokens_bed = \"rice_genes_tokens.bed\"\n",
    "\n",
    "token_pos = tokenization(genome, gene_anno, gene_info, tokenizer, tokens_bed, ext_list, sampling=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b150244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Generate NER dataset...\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "."
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"# Generate NER dataset...\")\n",
    "\n",
    "dataset = 'rice_gene_ner.pkl'\n",
    "ner_info = tokens_to_nerdata(tokens_bed, annotation_bed, dataset, named_entities, tags_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f1ffe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnallm import DNADataset, DNATrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bdcb245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3513d5b1910447e185367794528067db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Format labels:   0%|          | 0/1926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8239f5d92d4823a27e804d76f8fa25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding inputs:   0%|          | 0/1926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "datasets = DNADataset.load_local_data(\"./rice_gene_ner.pkl\", seq_col=\"sequence\", label_col=\"labels\", tokenizer=tokenizer, max_length=1024)\n",
    "\n",
    "# Encode the sequences with given task's data collator\n",
    "datasets.encode_sequences(task=configs['task'].task_type, remove_unused_columns=True)\n",
    "\n",
    "# Split the dataset into train, test, and validation sets\n",
    "datasets.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61fbc92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1348 samples\n",
      "test: 385 samples\n",
      "val: 193 samples\n"
     ]
    }
   ],
   "source": [
    "# check the dataset\n",
    "if hasattr(datasets.dataset, 'keys'):\n",
    "    for split_name in datasets.dataset.keys():\n",
    "        print(f\"{split_name}: {len(datasets.dataset[split_name])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "074a4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = DNATrainer(\n",
    "    model=model,\n",
    "    config=configs,\n",
    "    datasets=datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fabda915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1011' max='1011' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1011/1011 47:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.494600</td>\n",
       "      <td>0.288843</td>\n",
       "      <td>0.905063</td>\n",
       "      <td>0.585748</td>\n",
       "      <td>0.515507</td>\n",
       "      <td>0.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0.193303</td>\n",
       "      <td>0.931075</td>\n",
       "      <td>0.598708</td>\n",
       "      <td>0.574776</td>\n",
       "      <td>0.586498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2882.0198, 'train_samples_per_second': 1.403, 'train_steps_per_second': 0.351, 'total_flos': 2113463702642688.0, 'train_loss': 0.33529989231234136, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "metrics = trainer.train()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a30aec62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.21177801489830017, 'test_accuracy': 0.9327967958079203, 'test_precision': 0.6445709638064254, 'test_recall': 0.6024325351577346, 'test_f1': 0.6227897838899803, 'test_runtime': 75.823, 'test_samples_per_second': 5.078, 'test_steps_per_second': 0.33}\n"
     ]
    }
   ],
   "source": [
    "# Do prediction on the test set\n",
    "predictions = trainer.infer()\n",
    "print(predictions.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd37555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede3559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
