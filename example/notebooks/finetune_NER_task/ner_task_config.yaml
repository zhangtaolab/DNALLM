# Config example
# Note:
# task configurations are mandatory for both finetune and inference.
# when performing finetune or inference, leave the other one as it is.
task:
    # 'mask' - mask language model; 'generation' - casual language model;
    # 'binary' - binary classification; 'multiclass' - multi-class classification; 'multilabel' - multi-label classification;
    # 'regression' - regression; 'token' - token classification, such as NER;
    # 'embedding' - get embedding, attention map and token probability
    task_type: "token"
    # Labels information
    num_labels: 7
    label_names: ['O', 'B-UTR', 'I-UTR', 'B-CDS', 'I-CDS', 'B-INT', 'I-INT']   # IOB2 Scheme
    # Threshold to distinguish between true and false sample in binary/multi-class classification tasks
    threshold: 0.5
finetune:
    # For detailed arguments, please refer to https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments
    # Unlisted arguments can be added in the `extra_args` of DNATrainer
    output_dir: "./outputs_ner"
    num_train_epochs: 3
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 16
    gradient_accumulation_steps: 1
    max_steps: -1
    logging_strategy: "steps"
    logging_steps: 500
    eval_strategy: "steps"
    eval_steps: 500
    save_strategy: "steps"
    save_steps: 1000
    save_total_limit: 20
    save_safetensors: True
    learning_rate: 2e-5
    weight_decay: 0.01
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_epsilon: 1e-8
    max_grad_norm: 1.0
    warmup_ratio: 0.1
    lr_scheduler_type: "linear"
    lr_scheduler_kwargs: 
    seed: 42
    bf16: False
    fp16: False
    load_best_model_at_end: True
    metric_for_best_model: "eval_loss"
    report_to: "tensorboard"
    resume_from_checkpoint: 
inference:
    batch_size: 16
    max_length: 512
    device: "auto"
    num_workers: 4
    use_fp16: False
    output_dir: "."