{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57934fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install two dependencies for generating NER datasets, and make sure you have installed bedtools in your system\n",
    "# please make sure you have installed bedtools in your system\n",
    "# conda install -c bioconda bedtools\n",
    "# or in macos, you can use brew install bedtools\n",
    "# brew install bedtools\n",
    "\n",
    "# Omit it if you have already installed these two packages\n",
    "# !uv pip install pyfastx pybedtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77ac004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import random\n",
    "import numpy as np\n",
    "from pyfastx import Fasta\n",
    "from pybedtools import BedTool\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from dnallm import load_config, load_model_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d435cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847621be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set minimum and maximum extend length around a gene\n",
    "min_ext = 50\n",
    "max_ext = 100\n",
    "ext_list = [[random.randint(min_ext, max_ext), random.randint(min_ext, max_ext)] for x in range(60000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42b5d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Named Entity Recognition (NER) tags and corresponding id\n",
    "# NER includes IO, IOB, IOE, IOBES, BI, IE and BIES schemes, here we use IOB scheme\n",
    "# Example:\n",
    "# ..........[ exon1 ]-----[ exon2 ]-------[ exon3 ]........\n",
    "# 000000000012222222234444122222222344444412222222200000000\n",
    "named_entities = {\n",
    "    'intergenic': 'O',\n",
    "    'exon0': 'B-EXON',\n",
    "    'exon1': 'I-EXON',\n",
    "    'intron0': 'B-INTRON',\n",
    "    'intron1': 'I-INTRON',\n",
    "}\n",
    "tags_id = {\n",
    "    'O': 0,\n",
    "    'B-EXON': 1,\n",
    "    'I-EXON': 2,\n",
    "    'B-INTRON': 3,\n",
    "    'I-INTRON': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "409379cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_annotation(gene_anno):\n",
    "    cnt = 0\n",
    "    gene_info = {}\n",
    "    for gene in gene_anno:\n",
    "        gene_info[gene] = []\n",
    "        chrom = gene_anno[gene][\"chrom\"]\n",
    "        start = gene_anno[gene][\"start\"]\n",
    "        end = gene_anno[gene][\"end\"]\n",
    "        strand = gene_anno[gene][\"strand\"]\n",
    "        isoforms = gene_anno[gene][\"isoform\"]\n",
    "        # Get representative isoform（longest）\n",
    "        if not isoforms:\n",
    "            continue\n",
    "        lso_lens = [(iso, sum([(x[2]-x[1]) for x in isoforms[iso]])) for iso in isoforms]\n",
    "        representative = sorted(lso_lens, key=lambda x:x[1])[-1][0]\n",
    "        isoform_info = isoforms[representative]\n",
    "        iso_start = min([x[1] for x in isoform_info])\n",
    "        iso_end = max([x[2] for x in isoform_info])\n",
    "\n",
    "        if iso_start == start and iso_end == end:\n",
    "            is_reverse = False if strand == \"+\" else True\n",
    "            # Get intron annotation\n",
    "            last = 0\n",
    "            for region in sorted(isoform_info, key=lambda x:x[1], reverse=is_reverse):\n",
    "                if strand == \"+\":\n",
    "                    if last:\n",
    "                        intron = [chrom, last, region[1], \"intron\", strand]\n",
    "                        if intron[1] < intron[2]:\n",
    "                            gene_info[gene].append(intron)\n",
    "                    last = region[2]\n",
    "                else:\n",
    "                    if last:\n",
    "                        intron = [chrom, region[2], last, \"intron\", strand]\n",
    "                        if intron[1] < intron[2]:\n",
    "                            gene_info[gene].append(intron)\n",
    "                    last = region[1]\n",
    "                gene_info[gene].append([chrom, region[1], region[2], region[0], strand])\n",
    "        cnt += 1\n",
    "\n",
    "    return gene_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddaa7d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(genome, gene_anno, gene_info, tokenizer, outfile, ext_list, sampling=1e7):\n",
    "    \"\"\"\n",
    "    For each gene in `gene_anno`, extract the annotated exonic (and flanking) DNA subsequences,\n",
    "    run the tokenizer once per subsequence with offset_mapping enabled, \n",
    "    and record the (genomic_start, genomic_end, token) tuples for all non-special tokens.\n",
    "\n",
    "    - genome: dict mapping chromosome → SeqRecord (so that genome[chrom][start:end].seq gives a Seq)\n",
    "    - gene_anno: dict mapping gene_name → { \"chrom\": str, \"strand\": \"+\" or \"-\", ... }\n",
    "    - gene_info: dict mapping gene_name → list of (feature_id, exon_start, exon_end) or similar\n",
    "    - tokenizer: a HuggingFace‐style tokenizer that supports return_offsets_mapping\n",
    "    - outfile: (unused here, but you can write token_pos to it later)\n",
    "    - ext_list: list of (left_extension, right_extension) tuples parallel to gene_anno order\n",
    "    - sampling: random sampling the given number of genes for tokenization\n",
    "    \"\"\"\n",
    "    # 1) Precompute special-tokens set for O(1) membership checks\n",
    "    sp_tokens = set(tokenizer.special_tokens_map.values())\n",
    "\n",
    "    token_pos = {}\n",
    "    # Since gene_anno is likely a dict, we need a stable way of iterating + indexing ext_list.\n",
    "    # We'll assume ext_list[i] corresponds to the i-th gene in `list(gene_anno.keys())`.\n",
    "    gene_list = list(gene_anno.keys())\n",
    "    if len(gene_list) > sampling:\n",
    "        gene_list = random.sample(gene_list, int(sampling))\n",
    "\n",
    "    for num, gene in enumerate(tqdm(gene_list, desc=\"Genes\")):\n",
    "        chrom = gene_anno[gene][\"chrom\"]\n",
    "        strand = gene_anno[gene][\"strand\"]\n",
    "\n",
    "        # Skip genes not in gene_info or with empty annotation\n",
    "        if gene not in gene_info or not gene_info[gene]:\n",
    "            continue\n",
    "\n",
    "        # Determine exon‐range and extended boundaries\n",
    "        exon_coords = gene_info[gene]\n",
    "        # start = minimum exon_start; end = maximum exon_end\n",
    "        start = min(exon[1] for exon in exon_coords)\n",
    "        end   = max(exon[2] for exon in exon_coords)\n",
    "\n",
    "        left_ext, right_ext = ext_list[num]\n",
    "        ext_start = max(0, start - left_ext)\n",
    "        ext_end   = end + right_ext\n",
    "\n",
    "        # Shortcut: grab the full chromosome record once\n",
    "        chrom_record = genome[chrom]\n",
    "\n",
    "        # Build a list of (genomic_anchor, seq_string) for \"+\" or \"-\" strand\n",
    "        seqinfo = []\n",
    "        if strand == \"+\":\n",
    "            #  1) upstream flank\n",
    "            try:\n",
    "                upstream_seq = chrom_record[ext_start:start].seq\n",
    "            except Exception:\n",
    "                # If slicing fails, log and skip\n",
    "                print(f\"ERROR: {chrom}\\t{ext_start}\\t{start}\")\n",
    "                upstream_seq = \"\"\n",
    "            seqinfo.append((ext_start, str(upstream_seq)))\n",
    "\n",
    "            #  2) each exon\n",
    "            for feature in exon_coords:\n",
    "                exon_start = feature[1]\n",
    "                exon_end   = feature[2]\n",
    "                if exon_start >= exon_end:\n",
    "                    continue\n",
    "                seq = chrom_record[exon_start:exon_end].seq\n",
    "                seqinfo.append((exon_start, str(seq)))\n",
    "\n",
    "            #  3) downstream flank\n",
    "            downstream_seq = chrom_record[end:ext_end].seq\n",
    "            seqinfo.append((end, str(downstream_seq)))\n",
    "\n",
    "        else:  # strand == \"-\"\n",
    "            # On the reverse‐strand, we want the reverse complement (\"antisense\").\n",
    "            # Note: .antisense == .reverse_complement() for most SeqRecord slicing.\n",
    "            # We still record the genomic anchor as if it were the left index on the + strand.\n",
    "            # But because the sequence is reversed, offset_mapping will need to be mapped differently.\n",
    "\n",
    "            #  1) “upstream” on reverse strand = (end → ext_end) in forward coords, but take antisense\n",
    "            try:\n",
    "                flank_seq = chrom_record[end:ext_end].antisense\n",
    "            except Exception:\n",
    "                print(f\"ERROR (rev): {chrom}\\t{end}\\t{ext_end}\")\n",
    "                flank_seq = \"\"\n",
    "            seqinfo.append((ext_end, str(flank_seq)))\n",
    "\n",
    "            #  2) each exon (reverse‐complement)\n",
    "            for feature in exon_coords:\n",
    "                exon_start = feature[1]\n",
    "                exon_end   = feature[2]\n",
    "                if exon_start >= exon_end:\n",
    "                    continue\n",
    "                seq = chrom_record[exon_start:exon_end].antisense\n",
    "                # For mapping, we’ll anchor each token by the 5′-most position on the minus strand,\n",
    "                # but because the sequence is reversed, the “first character” of seq actually corresponds\n",
    "                # to genomic position = exon_end - 1 in forward coordinates, and the “last character” ↦ exon_start.\n",
    "                seqinfo.append((exon_end, str(seq)))\n",
    "\n",
    "            #  3) downstream on reverse strand = (ext_start → start) in forward coords, but antisense\n",
    "            flank_seq = chrom_record[ext_start:start].antisense\n",
    "            seqinfo.append((start, str(flank_seq)))\n",
    "\n",
    "        # Initialize the list for this gene\n",
    "        token_pos[gene] = []\n",
    "\n",
    "        # For each (anchor, raw_seq), run a single tokenizer(...) call\n",
    "        for anchor, raw_seq in seqinfo:\n",
    "            if not raw_seq:\n",
    "                continue\n",
    "\n",
    "            # 1) Tokenize with offsets (add_special_tokens=False so we skip [CLS], [SEP], etc.)\n",
    "            #    “offset_mapping” is a list of (char_start, char_end) for each token in raw_seq.\n",
    "            # encoding = tokenizer(\n",
    "            #     raw_seq,\n",
    "            #     return_offsets_mapping=True,\n",
    "            #     add_special_tokens=False\n",
    "            # )\n",
    "            # offsets = encoding[\"offset_mapping\"]\n",
    "            # token_ids = encoding[\"input_ids\"]\n",
    "            token_ids = tokenizer.encode(raw_seq, add_special_tokens=False)\n",
    "            tok_strs = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "            offsets = []\n",
    "            cursor  = 0\n",
    "            for tok in tok_strs:\n",
    "                char_start = cursor\n",
    "                char_end   = cursor + len(tok)\n",
    "                offsets.append((char_start, char_end))\n",
    "                cursor = char_end\n",
    "            if len(offsets) != len(token_ids):\n",
    "                # This should never happen in a well‐formed tokenizer, but just in case:\n",
    "                raise RuntimeError(\"Offset mapping length ≠ token_ids length\")\n",
    "\n",
    "            # 2) Iterate through each token + offset, skip special tokens, then map back to genome coords\n",
    "            for idx, (token_id, (char_start, char_end)) in enumerate(zip(token_ids, offsets)):\n",
    "                token_str = tokenizer.convert_ids_to_tokens(token_id)\n",
    "\n",
    "                # Skip if it’s one of the special tokens (“[PAD]”, “[CLS]”, etc.)\n",
    "                if token_str in sp_tokens:\n",
    "                    continue\n",
    "\n",
    "                if strand == \"+\":\n",
    "                    # On the forward strand, raw_seq[0] ↦ genomic position “anchor”.\n",
    "                    # So any token covering raw_seq[char_start:char_end] ↦ genome positions [anchor+char_start : anchor+char_end]\n",
    "                    g_start = anchor + char_start\n",
    "                    g_end   = anchor + char_end\n",
    "\n",
    "                else:\n",
    "                    # On the reverse strand, raw_seq was already antisense (reverse), and “anchor” is the forward‐strand coordinate\n",
    "                    # of the first character in raw_seq.  That first character of raw_seq is actually genome position (anchor-1),\n",
    "                    # and the last character of raw_seq is genome position (anchor - len(raw_seq)).\n",
    "                    # More generally, for raw_seq index i, the corresponding forward‐strand position is:\n",
    "                    #     g_pos = anchor - 1 - i\n",
    "                    #\n",
    "                    # Thus, if the token covers raw_seq[char_start:char_end] (i.e. from i = char_start to i = char_end-1),\n",
    "                    # its genomic coordinates (inclusive‐exclusive) on the forward strand are:\n",
    "                    #   g_end = (anchor - 1 - char_start) + 1  = anchor - char_start\n",
    "                    #   g_start = (anchor - 1 - (char_end - 1))  = anchor - char_end\n",
    "                    #\n",
    "                    # We want to store them as [g_start, g_end] with g_start < g_end.  So:\n",
    "                    g_start = anchor - char_end\n",
    "                    g_end   = anchor - char_start\n",
    "\n",
    "                token_pos[gene].append([g_start, g_end, token_str])\n",
    "\n",
    "    # save sequences and tokens\n",
    "    with open(outfile, \"w\") as outf:\n",
    "        for gene in tqdm(token_pos, desc=\"Save token positions\"):\n",
    "            chrom = gene_anno[gene][\"chrom\"]\n",
    "            strand = gene_anno[gene][\"strand\"]\n",
    "            for token in token_pos[gene]:\n",
    "                print(chrom, token[0], token[1], token[2], gene, strand, sep=\"\\t\", file=outf)\n",
    "\n",
    "    return token_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01235176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_nerdata(tokens_bed, annotation_bed, outfile, named_entities, tags_id):\n",
    "    \"\"\"\n",
    "    Build a token‐level NER dataset by intersecting `tokens_bed` with `annotation_bed`.\n",
    "    Returns a dict: { 'id': [...geneIDs...], 'sequence': [[token1, token2, …], …],\n",
    "                     'labels': [[label1, label2, …], …] } \n",
    "    and also writes two files:\n",
    "      1) “outfile” as a pickle of ner_info,\n",
    "      2) “<outfile>.token_sizes” containing “gene<TAB>token_count” for each gene.\n",
    "    \"\"\"\n",
    "\n",
    "    ne = named_entities\n",
    "    # Build a map from “baseName + '0' → named_entities[...] → tags_id[...]”\n",
    "    zero_map = {}\n",
    "    one_map  = {}\n",
    "    for base_name, ner_label in ne.items():\n",
    "        # “intergenic” maps to 'O' no matter whether we’re at a “start” or “inside” —\n",
    "        # so we do it for both 'intergenic0' and 'intergenic1'.\n",
    "        if base_name == \"intergenic\":\n",
    "            zero_map[\"intergenic0\"] = ner_label\n",
    "            one_map[\"intergenic1\"] = ner_label\n",
    "            continue\n",
    "\n",
    "        # base_name will be something like “exon0” or “exon1”, “intron0”, “intron1”\n",
    "        # We want to know, whenever the token’s name is exactly “exon” and we’re at a “start” boundary,\n",
    "        # pick the B-EXON label.  If the name is “exon” but it matched the previous gene-level “name”,\n",
    "        # then we call named_entities[\"exon1\"] to get “I-EXON”.\n",
    "        if base_name.endswith(\"0\"):\n",
    "            zero_map[base_name] = ner_label\n",
    "        else:\n",
    "            one_map[base_name]  = ner_label\n",
    "\n",
    "    # 2) Perform the intersection once (Loj = “left outer join”) so we keep every token\n",
    "    intersection = BedTool(tokens_bed).intersect(annotation_bed, loj=True)\n",
    "\n",
    "    # 3) Prepare our output containers\n",
    "    ner_info = {\n",
    "        \"id\":       [],  # list of gene IDs (in the same order as we append)\n",
    "        \"sequence\": [],  # each element is a list-of-strings (tokens)\n",
    "        \"labels\":   []   # each element is a list-of-ints (NER tags)\n",
    "    }\n",
    "\n",
    "    # We'll accumulate (gene, token_count) pairs in-memory, then write them in bulk\n",
    "    sizes_buffer = []\n",
    "\n",
    "    # 4) Use defaultdict(set) to track “which token‐IDs we’ve already seen for each gene”\n",
    "    token_seen = defaultdict(set)\n",
    "\n",
    "    current_gene = None\n",
    "    tokens_list  = []\n",
    "    labels_list  = []\n",
    "    last_name    = None  # to know if “name == last_name” (inside vs start)\n",
    "\n",
    "    # 5) Iterate through every interval from the intersection\n",
    "    #    We rely on the fact that BedTool.intersect(...) returns results in ascending\n",
    "    #    genomic order, and within each gene, that will appear “in order of token positions.”\n",
    "    for iv in intersection:\n",
    "        # Instead of “str(iv).split('\\t')”, do:\n",
    "        chrom   = iv.chrom\n",
    "        start   = iv.start   # integer\n",
    "        end     = iv.end     # integer\n",
    "        token   = iv.name    # 4th column of tokens_bed\n",
    "        gene    = iv.fields[4]   # 5th column of tokens_bed (original gene ID)\n",
    "        gene2   = iv.fields[9]   # 10th field (unused here, but was in your code)\n",
    "        name    = iv.fields[10]  # 11th field = the annotation “name”\n",
    "        # Build a unique‐ID for this token instance\n",
    "        token_id = (start, end)\n",
    "\n",
    "        # 6) When we see a new gene (i.e. “gene != current_gene”), we flush the previous gene’s data\n",
    "        if gene != current_gene:\n",
    "            # flush old gene if it exists\n",
    "            if current_gene is not None:\n",
    "                # Only append if we actually collected ≥1 token for current_gene\n",
    "                if tokens_list:\n",
    "                    sizes_buffer.append((current_gene, len(tokens_list)))\n",
    "                    ner_info[\"id\"].append(current_gene)\n",
    "                    ner_info[\"sequence\"].append(tokens_list)\n",
    "                    ner_info[\"labels\"].append(labels_list)\n",
    "                    count = len(ner_info[\"id\"])\n",
    "                    if count % 100 == 0:\n",
    "                        print(count)\n",
    "            # Reset for the new gene\n",
    "            current_gene = gene\n",
    "            tokens_list  = []\n",
    "            labels_list  = []\n",
    "            last_name    = None\n",
    "\n",
    "        # 7) If we’ve already seen this exact (start,end) “token_id” for this gene, skip\n",
    "        if token_id in token_seen[gene]:\n",
    "            continue\n",
    "        token_seen[gene].add(token_id)\n",
    "\n",
    "        # 8) Determine the correct NER‐tag (integer) for this token\n",
    "        #    - If name == \"-1\" → treat as “intergenic”\n",
    "        #    - If name == last_name → we pick “inside” (use one_map[name + \"1\"])\n",
    "        #    - else → we pick “start”   (use zero_map[name + \"0\"])\n",
    "        if name == \"-1\":\n",
    "            base_name = \"intergenic\"\n",
    "            ner_label = ne[base_name]          # always “O”\n",
    "        else:\n",
    "            # If it matched the previous token’s annotation name, choose inside\n",
    "            if name == last_name:\n",
    "                lookup_key = name + \"1\"       # e.g. “exon1” → I-EXON\n",
    "                ner_label  = one_map.get(lookup_key)\n",
    "                # If somehow it’s missing, fall back to “start” logic\n",
    "                if ner_label is None:\n",
    "                    ner_label = zero_map[name + \"0\"]\n",
    "            else:\n",
    "                # new annotation segment → start\n",
    "                lookup_key = name + \"0\"       # e.g. “exon0” → B-EXON\n",
    "                ner_label  = zero_map.get(lookup_key)\n",
    "                # If it’s missing, fall back to “intergenic”\n",
    "                if ner_label is None:\n",
    "                    ner_label = ne[\"intergenic\"]\n",
    "\n",
    "        ner_tag = tags_id[ner_label]\n",
    "        last_name = name\n",
    "\n",
    "        # 9) Append the token string + numeric label\n",
    "        tokens_list.append(token)\n",
    "        labels_list.append(ner_tag)\n",
    "\n",
    "    # 10) Don’t forget to flush the final gene once the loop ends\n",
    "    if current_gene is not None and tokens_list:\n",
    "        sizes_buffer.append((current_gene, len(tokens_list)))\n",
    "        ner_info[\"id\"].append(current_gene)\n",
    "        ner_info[\"sequence\"].append(tokens_list)\n",
    "        ner_info[\"labels\"].append(labels_list)\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "    # 11) Write out the token_sizes file in one go\n",
    "    sizes_file = outfile.rsplit(\".\", 1)[0] + \".token_sizes\"\n",
    "    with open(sizes_file, \"w\") as tsf:\n",
    "        for gene_name, count in sizes_buffer:\n",
    "            tsf.write(f\"{gene_name}\\t{count}\\n\")\n",
    "\n",
    "    # 12) Finally, pickle‐dump ner_info\n",
    "    with open(outfile, \"wb\") as handle:\n",
    "        pickle.dump(ner_info, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return ner_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a7ba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-23 09:40:33--  https://rice.uga.edu/osa1r7_download/osa1_r7.asm.fa.gz\n",
      "正在解析主机 rice.uga.edu (rice.uga.edu)... 128.192.162.131\n",
      "正在连接 rice.uga.edu (rice.uga.edu)|128.192.162.131|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    文件已下载完成；不会进行任何操作。\n",
      "\n",
      "--2025-08-23 09:40:35--  https://rice.uga.edu/osa1r7_download/osa1_r7.all_models.gff3.gz\n",
      "正在解析主机 rice.uga.edu (rice.uga.edu)... 128.192.162.131\n",
      "正在连接 rice.uga.edu (rice.uga.edu)|128.192.162.131|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    文件已下载完成；不会进行任何操作。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download genome and gene annotation (make sure you have wget command in your path)\n",
    "!wget -c https://rice.uga.edu/osa1r7_download/osa1_r7.asm.fa.gz\n",
    "!wget -c https://rice.uga.edu/osa1r7_download/osa1_r7.all_models.gff3.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f6bceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "813791it [00:01, 651827.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load genome sequence\n",
    "genome_file = \"osa1_r7.asm.fa.gz\"\n",
    "genome = Fasta(genome_file)\n",
    "# Load annotation\n",
    "gene_anno = {}\n",
    "with gzip.open(\"osa1_r7.all_models.gff3.gz\", \"rt\") as infile:\n",
    "    for line in tqdm(infile):\n",
    "        if line.startswith(\"#\") or line.startswith(\"\\n\"):\n",
    "            continue\n",
    "        info = line.strip().split(\"\\t\")\n",
    "        chrom = info[0]\n",
    "        datatype = info[2]\n",
    "        start = int(info[3]) - 1\n",
    "        end = int(info[4])\n",
    "        strand = info[6]\n",
    "        description = info[8].split(\";\")\n",
    "        if datatype == \"gene\":\n",
    "            for item in description:\n",
    "                if item.startswith(\"Name=\"):\n",
    "                    gene = item[5:]\n",
    "            if gene not in gene_anno:\n",
    "                gene_anno[gene] = {}\n",
    "                gene_anno[gene][\"chrom\"] = chrom\n",
    "                gene_anno[gene][\"start\"] = start\n",
    "                gene_anno[gene][\"end\"] = end\n",
    "                gene_anno[gene][\"strand\"] = strand\n",
    "                gene_anno[gene][\"isoform\"] = {}\n",
    "        elif datatype in [\"exon\"]:\n",
    "            for item in description:\n",
    "                if item.startswith(\"Parent=\"):\n",
    "                    isoform = item[7:].split(',')[0]\n",
    "            if isoform not in gene_anno[gene][\"isoform\"]:\n",
    "                gene_anno[gene][\"isoform\"][isoform] = []\n",
    "            gene_anno[gene][\"isoform\"][isoform].append([datatype, start, end])\n",
    "\n",
    "# Get full gene annotation information and save\n",
    "gene_info = get_gene_annotation(gene_anno)\n",
    "annotation_bed = \"rice_annotation.bed\"\n",
    "with open(annotation_bed, \"w\") as outf:\n",
    "    for gene in sorted(gene_anno, key=lambda x: (gene_anno[x][\"chrom\"], gene_anno[x][\"start\"])):\n",
    "        chrom = gene_anno[gene][\"chrom\"]\n",
    "        strand = gene_anno[gene][\"strand\"]\n",
    "        if strand == \"+\":\n",
    "            for item in gene_info[gene]:\n",
    "                print(item[0], item[1], item[2], gene, item[3], item[4], sep=\"\\t\", file=outf)\n",
    "        else:\n",
    "            for item in gene_info[gene][::-1]:\n",
    "                print(item[0], item[1], item[2], gene, item[3], item[4], sep=\"\\t\", file=outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99c6686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-6mer\n",
      "Model files are stored in /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-6mer\n",
      "Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-6mer\n",
      "Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-6mer\n",
      "Downloading Model from https://www.modelscope.cn to directory: /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-6mer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at /Users/forrest/.cache/modelscope/hub/models/zhangtaolab/plant-dnagpt-6mer and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load configs, model and tokenizer\n",
    "configs = load_config(\"./ner_task_config.yaml\")\n",
    "model_name = \"zhangtaolab/plant-dnagpt-6mer\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, task_config=configs['task'], source=\"modelscope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ca8ab95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Performing sequence tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Genes:   0%|          | 8/2000 [00:00<00:26, 74.41it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Genes: 100%|██████████| 2000/2000 [00:20<00:00, 98.11it/s] \n",
      "Save token positions: 100%|██████████| 1926/1926 [00:00<00:00, 2858.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# 将基因序列tokenization并生成命名体识别所需格式数据集\n",
    "print(\"# Performing sequence tokenization...\")\n",
    "tokens_bed = \"rice_genes_tokens.bed\"\n",
    "\n",
    "token_pos = tokenization(genome, gene_anno, gene_info, tokenizer, tokens_bed, ext_list, sampling=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b150244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Generate NER dataset...\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"# Generate NER dataset...\")\n",
    "\n",
    "dataset = 'rice_gene_ner.pkl'\n",
    "ner_info = tokens_to_nerdata(tokens_bed, annotation_bed, dataset, named_entities, tags_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f1ffe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnallm import DNADataset, DNATrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bdcb245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568a97b9d9d1495183aa2a5a7c8b8cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Format labels:   0%|          | 0/1926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2f0887434244e781145f832be8d3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding inputs:   0%|          | 0/1926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "datasets = DNADataset.load_local_data(\"./rice_gene_ner.pkl\", seq_col=\"sequence\", label_col=\"labels\", tokenizer=tokenizer, max_length=1024)\n",
    "\n",
    "# Encode the sequences with given task's data collator\n",
    "datasets.encode_sequences(task=configs['task'].task_type, remove_unused_columns=True)\n",
    "\n",
    "# Split the dataset into train, test, and validation sets\n",
    "datasets.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbc92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1348 samples\n",
      "test: 385 samples\n",
      "val: 193 samples\n"
     ]
    }
   ],
   "source": [
    "# check the dataset\n",
    "if hasattr(datasets.dataset, 'keys'):\n",
    "    for split_name in datasets.dataset.keys():\n",
    "        print(f\"{split_name}: {len(datasets.dataset[split_name])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "074a4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = DNATrainer(\n",
    "    model=model,\n",
    "    config=configs,\n",
    "    datasets=datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fabda915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1011' max='1011' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1011/1011 12:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.512200</td>\n",
       "      <td>0.237552</td>\n",
       "      <td>0.918376</td>\n",
       "      <td>0.572056</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.542952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.195971</td>\n",
       "      <td>0.937131</td>\n",
       "      <td>0.639966</td>\n",
       "      <td>0.602381</td>\n",
       "      <td>0.620605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 761.5615, 'train_samples_per_second': 5.31, 'train_steps_per_second': 1.328, 'total_flos': 2113463702642688.0, 'train_loss': 0.3411682097777415, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "metrics = trainer.train()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a30aec62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.21138842403888702, 'test_accuracy': 0.9275411185150473, 'test_precision': 0.6377063423110338, 'test_recall': 0.5921742638160549, 'test_f1': 0.6140974691487138, 'test_runtime': 23.6895, 'test_samples_per_second': 16.252, 'test_steps_per_second': 1.055}\n"
     ]
    }
   ],
   "source": [
    "# Do prediction on the test set\n",
    "predictions = trainer.predict()\n",
    "print(predictions.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd37555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
