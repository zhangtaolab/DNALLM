# Config example
# Note:
# task configurations are mandatory for both finetune and inference.
# when performing finetune or inference, leave the other one as it is.
task:
    # 'mask' - mask language model; 'generation' - casual language model;
    # 'binary' - binary classification; 'multiclass' - multi-class classification; 'multilabel' - multi-label classification;
    # 'regression' - regression; 'token' - token classification, such as NER;
    # 'embedding' - get embedding, attention map and token probability
    task_type: "multiclass"
    # Labels information
    num_labels: 3
    label_names: ["Not open chromatin", "Partial open chromatin", "Full open chromatin"]
    # Threshold to distinguish between true and false sample in binary/multi-class classification tasks
    threshold: 0.5
finetune:
    # For detailed arguments, please refer to https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments
    # Unlisted arguments can be added in the `extra_args` of DNATrainer
    output_dir: "./outputs"
    num_train_epochs: 3
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 16
    gradient_accumulation_steps: 1
    max_steps: -1
    logging_strategy: "steps"
    logging_steps: 100
    eval_strategy: "steps"
    eval_steps: 100
    save_strategy: "steps"
    save_steps: 500
    save_total_limit: 20
    save_safetensors: True
    learning_rate: 2e-5
    weight_decay: 0.01
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_epsilon: 1e-8
    max_grad_norm: 1.0
    warmup_ratio: 0.1
    lr_scheduler_type: "linear"
    lr_scheduler_kwargs: 
    seed: 42
    bf16: False
    fp16: False
    load_best_model_at_end: True
    metric_for_best_model: "eval_loss"
    report_to: "tensorboard"
    resume_from_checkpoint: 
inference:
    batch_size: 16
    max_length: 512
    device: "auto"
    num_workers: 4
    use_fp16: False
    output_dir: "./results"
